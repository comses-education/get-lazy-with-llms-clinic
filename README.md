# get-lazy-with-llms-clinic (by humans for humans) ü§ù
A toolkit and guide for using LLMs effectively on moderately complex tasks through better prompting.

Throughout this repository we will use emojis in document titles: 
- ü§ù written by humans and intended for humans to read 
- ü¶æüëå generated by LLMs and checked or edited by humans
- ü¶æ generated by LLMs and **NOT** checked by humans

## What this repository includes
1. `/theory`:
   - [*"I can't rely on an LLM to do this..."* - Try better prompts!](/theory/Examples%20of%20Better%20Prompts.md)
   - [Use LLMs to Help You Write Better Prompts](./theory/Use%20LLMs%20to%20Help%20You%20Write%20a%20Better%20Prompt.md)
   - [Best Practices for Solving Tasks with LLMs](./theory/Best%20Practices%20for%20Solving%20Tasks%20with%20LLMs.md)
   - [A list of recommended tools](./theory/Recommended%20Tools.md)
2. `/practice`: Walkthroughs for solving tasks with LLMs:
    - [example 1: explain a concept from Earth Surface Modelling](./practice/01_explain_concept/README.md)
    - example 2: generate a model skeleton that uses the [landlab framework](https://landlab.csdms.io/)
    - example 3: containerize an existing Earth Surface Modeling Application (from example 2)
    - example 4: create a python CLI tool to convert contents of a directory into a single text file for LLMs
    - example 5: create a python CLI tool to help you build better prompts: promptbuilder tool

## Recommendations on how to use this repository
### 30 seconds: read
Dissatisfied with LLM results? Cannot rely on LLM information? - **use better prompts**:

**For any task:**

Never write the prompt yourself. The LLM has seen a HUGE amount of questions and answers from the internet. It knows how to formulate prompts. Give the LLM your TASK and ask to write a better prompt for you. Supply additional instructions or context if needed.

**For coding tasks:**
1. Ask the LLM to come up with an IMPLEMENTATION PLAN first.
2. Ask LLM to PLAN a TESTING FRAMEWORK.
3. Generate TESTS and check them MANUALLY. Ask to test the FUNCTIONALITY, NOT IMPLEMENTATION. THIS IS YOUR SOURCE OF TRUTH.
4. Ask to split the PLAN into modular TODOs.
5. Use TEST DRIVEN DEVELOPMENT when iterating on TODOs
6. For DEBUGGING ask LLM: *"Add debug logs, so that I can give you the output for bug analysis. YOU ARE ALLOWED TO ADD DEBUG LOGS ONLY. DO NOT CHANGE ANY OTHER CODE."*
7. Once a TODO is implemented and tests pass, commit.

**For tasks with "unknown unknowns":** ask LLM to give you best practices on how to accomplish the task.
Ask to provide trustworthy sources and order them by importance.

---

### 5 minutes: read
1. use https://gitingest.com/ to convert this whole repo into a single text file (`REPO_CONTENT`). Ask an LLM (for example Gemini 2.5 Pro using [https://aistudio.google.com/](https://aistudio.google.com/)) to summarize it for you:
    
    ```yml
        "I am a <ROLE> (e.g., product manager, software architect, researcher).         
        Given the following repository content, create a clear, concise briefing document I can read in under 5 minutes. Focus on summarizing the purpose, key components, examples, theoretical resoning and any critical considerations. The tone should be informative and executive-friendly. Here's the content:        
        <REPO_CONTENT>"
    ```
---

### 15 minutes: read
1. Use the prompt above and read the LLM summary **(5 min)**. 
2. Choose an example that is relevant to your work and follow it's *Task Solving Process* **(10 min)**

---

### 30 minutes: read + practice
1. Clone this repo to your computer
2. Continue with this README.md
3. Choose an example that is relevant to you and follow the walkthrough
4. Ask the LLM to make some small changes in the existing code. Copy/paste the existing code manually or using the [`dirdigest`](./tools/dirdigest/README.md) inside the tool's directory to combine all the code into a single text file.
5. Regenerate tool code by modifying existing requirement documents and generating code changes
6. Generate new tools

---

# Practical resources for getting the most use out of LLMs

## LLM Terms

| Concept | tl;dr |
|---------|-------|
| **Pre-training** | Scrape the internet and compress it into a lossy neural network |
| **Fine-tuning** | Continue training on curated, labelled data so the model follows instructions |
| **Tokens** | numeric encoding of text that the LLM uses for I/O, 1 token ~= 0.7 words |
| **Context window** | How many tokens fit in one request (history *included*) |
| **Few-shot prompting** | providing specific input/output examples to clarify your task |
| **Chain of thought prompting** | asking the model to "show your work" in an effort to boost problem solving ability |
| **Retrieval-Augmented Generation (RAG)** | a technique to automically fetch relevant external data to provide as context |
| **Tools** | External APIs/functions the model can call (i.e. write to a file, search the web) |
| **Agents** | A loop where the model plans, uses tools, observes results, and iterates toward a goal |
| **Multimodal** | Refers to models that can work with data types beyond text, such as audio, images, and video |

---

## Chatbot services

| Tool | What it‚Äôs good for | Pricing notes |
|------|--------------------|---------------|
| [ChatGPT (GPT-3.5)](https://chat.openai.com/) | Several models for quick questions or more advanced reasoning | Limited free tier |
| [Google Gemini](https://aistudio.google.com) | Gemini 2.5 is the top dog in code-generation and other complex tasks at the time of writing | Generous free tier |
| [Claude](https://claude.ai/) | Another option similarly advanced to ChatGPT or Gemini | $20‚ÄØ/‚ÄØmo (Pro) |
| [Perplexity](https://www.perplexity.ai/) | Web search and research with fairly reliable citations | Limited free tier |
| [Ollama](https://ollama.com/) | Run open‚Äësource LLMs locally (Llama, DeepSeek, etc.) | Completely free! |
---

## IDE integrations / code assistants

| Tool | What it does | Pricing notes |
|------|--------------|---------------|
| [GitHub Copilot](https://github.com/features/copilot) | Autocompletion, chat, and agent-mode in VScode | Limited free tier, **[Pro is free for students/teachers/OSS](https://docs.github.com/en/copilot/managing-copilot/managing-copilot-as-an-individual-subscriber/getting-started-with-copilot-on-your-personal-account/getting-free-access-to-copilot-pro-as-a-student-teacher-or-maintainer)** |
| [Cursor](https://www.cursor.com/)* | VScode fork with autocomplete, chat, and agent mode | Free trial only, **[Free year of Pro for students](https://www.cursor.com/students)** |
| [Open Hands](https://www.cursor.com/)** | A FOSS alternative "coding agent" | Free but requires a local LLM or API access |

*There are many similar paid "AI editors" such as [Windsurf](https://windsurf.com/editor), [Trae](https://www.trae.ai/), [Devin](https://devin.ai/), etc..

**[Roo Code](https://github.com/RooVetGit/Roo-Code) is another popular open source agent

---

## Tips for good results üöß


|                                                                                                  |
|--------------------------------------------------------------------------------------------------|
| Remain the 'expert', think of the LLM/chatbot as an amazingly fast student intern |
| The more detailed the prompt the better |
| Include examples of how the model should respond (‚Äúfew-shot prompting‚Äù) |
| Include relevant context (like documentation) in a readable format within your prompts |
| Break up jobs into smaller pieces just like you would for yourself or a team, models (especially reasoning models) can often help out with this step |
| Keep sessions short and try to focus on particular tasks to avoid 'overwhelming' the model |
| Take advantage of memory management tools like ChatGPT projects or even files in your workspace |
| Break up jobs into smaller pieces just like you would for yourself or a team |
| Define the scope for specific changes to avoid re-writes, potentially breaking unrelated things |
| LLMs can natively handle markdown, use this for formatting, delimiters like horizontal rules (`---`) are particularly useful to break up prompts |
| Don't shy away from mathematical expressions, TeX notation is usually able to be understood
| Gravitate towards languages/tools that both you and the LLM know well (popular) |
| Ask for explanations or documentation to make it easier to review outputs |
| Validate everything, ideally as you receive it ("clean as you go"), automated tests are very helpful here |
| Use git and commit regularly to save "stable" states if you are using AI-generated code heavily |

## Responsible use üöß

|                                                                                                  |
|--------------------------------------------------------------------------------------------------|
|  |

---

## Further Learning

[Curated educational content on LLMs](./theory/Curated%20LLM%20Resources.md)