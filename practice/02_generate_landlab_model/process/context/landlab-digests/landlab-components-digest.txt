Directory structure:
└── components/
    ├── __init__.py
    ├── advection/
    │   ├── __init__.py
    │   ├── advection_solver_tvd.py
    │   └── flux_limiters.py
    ├── area_slope_transporter/
    │   ├── __init__.py
    │   └── area_slope_transporter.py
    ├── bedrock_landslider/
    │   ├── __init__.py
    │   ├── bedrock_landslider.py
    │   └── cfuncs.pyx
    ├── carbonate/
    │   ├── __init__.py
    │   └── carbonate_producer.py
    ├── chi_index/
    │   ├── __init__.py
    │   └── channel_chi.py
    ├── concentration_tracker/
    │   ├── __init__.py
    │   ├── concentration_tracker_for_diffusion.py
    │   └── concentration_tracker_for_space.py
    ├── depression_finder/
    │   ├── __init__.py
    │   ├── cfuncs.pyx
    │   ├── floodstatus.py
    │   └── lake_mapper.py
    ├── depth_dependent_diffusion/
    │   ├── __init__.py
    │   └── hillslope_depth_dependent_linear_flux.py
    ├── depth_dependent_taylor_soil_creep/
    │   ├── __init__.py
    │   └── hillslope_depth_dependent_taylor_flux.py
    ├── detachment_ltd_erosion/
    │   ├── __init__.py
    │   ├── generate_detachment_ltd_erosion.py
    │   └── generate_erosion_by_depth_slope.py
    ├── diffusion/
    │   ├── __init__.py
    │   └── diffusion.py
    ├── dimensionless_discharge/
    │   ├── __init__.py
    │   └── dimensionless_discharge.py
    ├── discharge_diffuser/
    │   ├── __init__.py
    │   └── diffuse_by_discharge.py
    ├── drainage_density/
    │   ├── __init__.py
    │   ├── cfuncs.pyx
    │   └── drainage_density.py
    ├── erosion_deposition/
    │   ├── __init__.py
    │   ├── cfuncs.pyx
    │   ├── erosion_deposition.py
    │   ├── generalized_erosion_deposition.py
    │   └── shared_stream_power.py
    ├── fire_generator/
    │   ├── __init__.py
    │   └── generate_fire.py
    ├── flexure/
    │   ├── __init__.py
    │   ├── flexure.py
    │   ├── flexure_1d.py
    │   ├── funcs.py
    │   └── _ext/
    │       ├── __init__.py
    │       ├── flexure1d.pyx
    │       ├── flexure2d.pyx
    │       └── flexure2d_slow.pyx
    ├── flow_accum/
    │   ├── __init__.py
    │   ├── cfuncs.pyx
    │   ├── flow_accum_bw.py
    │   ├── flow_accum_to_n.py
    │   ├── flow_accumulator.py
    │   └── lossy_flow_accumulator.py
    ├── flow_director/
    │   ├── __init__.py
    │   ├── cfuncs.pyx
    │   ├── flow_direction_DN.py
    │   ├── flow_direction_dinf.py
    │   ├── flow_direction_mfd.py
    │   ├── flow_director.py
    │   ├── flow_director_d8.py
    │   ├── flow_director_dinf.py
    │   ├── flow_director_mfd.py
    │   ├── flow_director_steepest.py
    │   ├── flow_director_to_many.py
    │   └── flow_director_to_one.py
    ├── flow_router/
    │   ├── __init__.py
    │   └── ext/
    │       ├── __init__.py
    │       └── single_flow/
    │           ├── __init__.py
    │           └── priority_routing/
    │               ├── __init__.py
    │               ├── _priority_queue.hpp
    │               ├── _test_breach_c.pxd
    │               ├── _test_breach_c.pyx
    │               ├── breach.pxd
    │               └── breach.pyx
    ├── fracture_grid/
    │   ├── __init__.py
    │   └── fracture_grid.py
    ├── gflex/
    │   ├── __init__.py
    │   └── flexure.py
    ├── gravel_bedrock_eroder/
    │   ├── __init__.py
    │   └── gravel_bedrock_eroder.py
    ├── gravel_river_transporter/
    │   ├── __init__.py
    │   └── gravel_river_transporter.py
    ├── groundwater/
    │   ├── README.md
    │   ├── __init__.py
    │   └── dupuit_percolator.py
    ├── hack_calculator/
    │   ├── __init__.py
    │   └── hack_calculator.py
    ├── hand_calculator/
    │   ├── __init__.py
    │   └── hand_calculator.py
    ├── lake_fill/
    │   ├── __init__.py
    │   └── lake_fill_barnes.py
    ├── landslides/
    │   ├── __init__.py
    │   └── landslide_probability.py
    ├── lateral_erosion/
    │   ├── __init__.py
    │   ├── lateral_erosion.py
    │   └── node_finder.py
    ├── lithology/
    │   ├── README.md
    │   ├── __init__.py
    │   ├── litholayers.py
    │   └── lithology.py
    ├── marine_sediment_transport/
    │   ├── __init__.py
    │   └── simple_submarine_diffuser.py
    ├── mass_wasting_runout/
    │   ├── __init__.py
    │   ├── mass_wasting_runout.py
    │   └── mass_wasting_saver.py
    ├── network_sediment_transporter/
    │   ├── README.md
    │   ├── __init__.py
    │   ├── bed_parcel_initializers.py
    │   ├── network_sediment_transporter.py
    │   ├── sediment_pulser_at_links.py
    │   ├── sediment_pulser_base.py
    │   └── sediment_pulser_each_parcel.py
    ├── nonlinear_diffusion/
    │   ├── Perron_nl_diffuse.py
    │   └── __init__.py
    ├── normal_fault/
    │   ├── __init__.py
    │   └── normal_fault.py
    ├── overland_flow/
    │   ├── __init__.py
    │   ├── _links.py
    │   ├── _neighbors_at_link.pyx
    │   ├── generate_overland_flow_Bates.py
    │   ├── generate_overland_flow_deAlmeida.py
    │   ├── generate_overland_flow_implicit_kinwave.py
    │   ├── generate_overland_flow_kinwave.py
    │   ├── kinematic_wave_rengers.py
    │   └── linear_diffusion_overland_flow_router.py
    ├── pet/
    │   ├── __init__.py
    │   └── potential_evapotranspiration_field.py
    ├── plant_competition_ca/
    │   ├── __init__.py
    │   └── plant_competition_ca.py
    ├── potentiality_flowrouting/
    │   ├── __init__.py
    │   └── route_flow_by_boundary.py
    ├── priority_flood_flow_router/
    │   ├── README.md
    │   ├── __init__.py
    │   ├── cfuncs.pyx
    │   └── priority_flood_flow_router.py
    ├── profiler/
    │   ├── __init__.py
    │   ├── base_profiler.py
    │   ├── channel_profiler.py
    │   ├── profiler.py
    │   └── trickle_down_profiler.py
    ├── radiation/
    │   ├── __init__.py
    │   └── radiation.py
    ├── river_flow_dynamics/
    │   ├── __init__.py
    │   └── river_flow_dynamics.py
    ├── sink_fill/
    │   ├── __init__.py
    │   ├── fill_sinks.py
    │   └── sink_fill_barnes.py
    ├── soil_moisture/
    │   ├── __init__.py
    │   ├── infiltrate_soil_green_ampt.py
    │   └── soil_moisture_dynamics.py
    ├── space/
    │   ├── __init__.py
    │   ├── space.py
    │   ├── space_large_scale_eroder.py
    │   └── ext/
    │       ├── calc_qs.pyx
    │       └── calc_sequential_ero_depo.pyx
    ├── spatial_precip/
    │   ├── __init__.py
    │   └── generate_spatial_precip.py
    ├── species_evolution/
    │   ├── README.md
    │   ├── __init__.py
    │   ├── base_taxon.py
    │   ├── record.py
    │   ├── species_evolver.py
    │   ├── zone.py
    │   ├── zone_controller.py
    │   └── zone_taxon.py
    ├── steepness_index/
    │   ├── __init__.py
    │   └── channel_steepness.py
    ├── stream_power/
    │   ├── __init__.py
    │   ├── cfuncs.pyx
    │   ├── fastscape_stream_power.py
    │   ├── sed_flux_dep_incision.py
    │   ├── stream_power.py
    │   └── stream_power_smooth_threshold.py
    ├── taylor_nonlinear_hillslope_flux/
    │   ├── __init__.py
    │   └── taylor_nonlinear_hillslope_flux.py
    ├── tectonics/
    │   ├── __init__.py
    │   └── listric_kinematic_extender.py
    ├── threshold_eroder/
    │   ├── __init__.py
    │   ├── cfuncs.pyx
    │   └── threshold_eroder.py
    ├── tidal_flow/
    │   ├── __init__.py
    │   └── tidal_flow_calculator.py
    ├── transport_length_diffusion/
    │   ├── __init__.py
    │   └── transport_length_hillslope_diffusion.py
    ├── uniform_precip/
    │   ├── __init__.py
    │   └── generate_uniform_precip.py
    ├── vegetation_dynamics/
    │   ├── __init__.py
    │   └── vegetation_dynamics.py
    └── weathering/
        ├── __init__.py
        ├── exponential_weathering.py
        └── exponential_weathering_integrated.py

================================================
File: __init__.py
================================================
from .advection import AdvectionSolverTVD
from .area_slope_transporter import AreaSlopeTransporter
from .bedrock_landslider import BedrockLandslider
from .carbonate import CarbonateProducer
from .chi_index import ChiFinder
from .concentration_tracker import ConcentrationTrackerForDiffusion
from .concentration_tracker import ConcentrationTrackerForSpace
from .depression_finder import DepressionFinderAndRouter
from .depth_dependent_diffusion import DepthDependentDiffuser
from .depth_dependent_taylor_soil_creep import DepthDependentTaylorDiffuser
from .detachment_ltd_erosion import DepthSlopeProductErosion
from .detachment_ltd_erosion import DetachmentLtdErosion
from .diffusion import LinearDiffuser
from .dimensionless_discharge import DimensionlessDischarge
from .discharge_diffuser import DischargeDiffuser
from .drainage_density import DrainageDensity
from .erosion_deposition import ErosionDeposition
from .erosion_deposition import SharedStreamPower
from .fire_generator import FireGenerator
from .flexure import Flexure
from .flexure import Flexure1D
from .flow_accum import FlowAccumulator
from .flow_accum import LossyFlowAccumulator
from .flow_director import FlowDirectorD8
from .flow_director import FlowDirectorDINF
from .flow_director import FlowDirectorMFD
from .flow_director import FlowDirectorSteepest
from .fracture_grid import FractureGridGenerator
from .gflex import gFlex
from .gravel_bedrock_eroder import GravelBedrockEroder
from .gravel_river_transporter import GravelRiverTransporter
from .groundwater import GroundwaterDupuitPercolator
from .hack_calculator import HackCalculator
from .hand_calculator import HeightAboveDrainageCalculator
from .lake_fill import LakeMapperBarnes
from .landslides import LandslideProbability
from .lateral_erosion import LateralEroder
from .lithology import LithoLayers
from .lithology import Lithology
from .marine_sediment_transport import SimpleSubmarineDiffuser
from .mass_wasting_runout import MassWastingRunout
from .network_sediment_transporter import NetworkSedimentTransporter
from .network_sediment_transporter.bed_parcel_initializers import (
    BedParcelInitializerArea,
)
from .network_sediment_transporter.bed_parcel_initializers import (
    BedParcelInitializerDepth,
)
from .network_sediment_transporter.bed_parcel_initializers import (
    BedParcelInitializerDischarge,
)
from .network_sediment_transporter.bed_parcel_initializers import (
    BedParcelInitializerUserD50,
)
from .network_sediment_transporter.sediment_pulser_at_links import SedimentPulserAtLinks
from .network_sediment_transporter.sediment_pulser_each_parcel import (
    SedimentPulserEachParcel,
)
from .nonlinear_diffusion import PerronNLDiffuse
from .normal_fault import NormalFault
from .overland_flow import KinematicWaveRengers
from .overland_flow import KinwaveImplicitOverlandFlow
from .overland_flow import KinwaveOverlandFlowModel
from .overland_flow import LinearDiffusionOverlandFlowRouter
from .overland_flow import OverlandFlow
from .overland_flow import OverlandFlowBates
from .pet import PotentialEvapotranspiration
from .plant_competition_ca import VegCA
from .potentiality_flowrouting import PotentialityFlowRouter
from .priority_flood_flow_router import PriorityFloodFlowRouter
from .profiler import ChannelProfiler
from .profiler import Profiler
from .profiler import TrickleDownProfiler
from .radiation import Radiation
from .river_flow_dynamics import RiverFlowDynamics
from .sink_fill import SinkFiller
from .sink_fill import SinkFillerBarnes
from .soil_moisture import SoilInfiltrationGreenAmpt
from .soil_moisture import SoilMoisture
from .space import Space
from .space import SpaceLargeScaleEroder
from .spatial_precip import SpatialPrecipitationDistribution
from .species_evolution import SpeciesEvolver
from .steepness_index import SteepnessFinder
from .stream_power import FastscapeEroder
from .stream_power import SedDepEroder
from .stream_power import StreamPowerEroder
from .stream_power import StreamPowerSmoothThresholdEroder
from .taylor_nonlinear_hillslope_flux import TaylorNonLinearDiffuser
from .tectonics import ListricKinematicExtender
from .threshold_eroder import ThresholdEroder
from .tidal_flow import TidalFlowCalculator
from .transport_length_diffusion import TransportLengthHillslopeDiffuser
from .uniform_precip import PrecipitationDistribution
from .vegetation_dynamics import Vegetation
from .weathering import ExponentialWeatherer
from .weathering import ExponentialWeathererIntegrated

COMPONENTS = [
    AdvectionSolverTVD,
    AreaSlopeTransporter,
    BedrockLandslider,
    CarbonateProducer,
    ChannelProfiler,
    ChiFinder,
    ConcentrationTrackerForDiffusion,
    ConcentrationTrackerForSpace,
    DepressionFinderAndRouter,
    DepthDependentDiffuser,
    DepthDependentTaylorDiffuser,
    DepthSlopeProductErosion,
    DetachmentLtdErosion,
    DischargeDiffuser,
    DimensionlessDischarge,
    DrainageDensity,
    ErosionDeposition,
    ExponentialWeatherer,
    ExponentialWeathererIntegrated,
    FastscapeEroder,
    FireGenerator,
    Flexure,
    Flexure1D,
    FlowAccumulator,
    PriorityFloodFlowRouter,
    FlowDirectorD8,
    FlowDirectorDINF,
    FlowDirectorMFD,
    FlowDirectorSteepest,
    FractureGridGenerator,
    gFlex,
    GravelBedrockEroder,
    GravelRiverTransporter,
    GroundwaterDupuitPercolator,
    HackCalculator,
    HeightAboveDrainageCalculator,
    KinematicWaveRengers,
    KinwaveImplicitOverlandFlow,
    KinwaveOverlandFlowModel,
    LakeMapperBarnes,
    LandslideProbability,
    LateralEroder,
    LinearDiffuser,
    LinearDiffusionOverlandFlowRouter,
    ListricKinematicExtender,
    LithoLayers,
    Lithology,
    LossyFlowAccumulator,
    MassWastingRunout,
    NetworkSedimentTransporter,
    NormalFault,
    OverlandFlow,
    OverlandFlowBates,
    PerronNLDiffuse,
    PotentialEvapotranspiration,
    PotentialityFlowRouter,
    PrecipitationDistribution,
    Profiler,
    Radiation,
    RiverFlowDynamics,
    SedDepEroder,
    SedimentPulserAtLinks,
    SedimentPulserEachParcel,
    SharedStreamPower,
    SimpleSubmarineDiffuser,
    SinkFiller,
    SinkFillerBarnes,
    SoilMoisture,
    SoilInfiltrationGreenAmpt,
    Space,
    SpaceLargeScaleEroder,
    SpatialPrecipitationDistribution,
    SpeciesEvolver,
    SteepnessFinder,
    StreamPowerEroder,
    StreamPowerSmoothThresholdEroder,
    BedParcelInitializerDischarge,
    BedParcelInitializerDepth,
    BedParcelInitializerArea,
    BedParcelInitializerUserD50,
    TaylorNonLinearDiffuser,
    TidalFlowCalculator,
    TransportLengthHillslopeDiffuser,
    TrickleDownProfiler,
    ThresholdEroder,
    VegCA,
    Vegetation,
]

__all__ = [cls.__name__ for cls in COMPONENTS]



================================================
File: advection/__init__.py
================================================
#!/usr/bin/env python
"""
.. codeauthor:: G Tucker

.. sectionauthor:: G Tucker
"""

from .advection_solver_tvd import AdvectionSolverTVD
from .advection_solver_tvd import find_upwind_link_at_link
from .advection_solver_tvd import upwind_to_local_grad_ratio

__all__ = [
    "AdvectionSolverTVD",
    "find_upwind_link_at_link",
    "upwind_to_local_grad_ratio",
]



================================================
File: advection/advection_solver_tvd.py
================================================
#!/usr/bin/env python3
"""Solve advection numerically using Total Variation Diminishing method."""

import numpy as np

from landlab import Component
from landlab import LinkStatus
from landlab.components.advection.flux_limiters import flux_lim_vanleer
from landlab.field.errors import FieldError
from landlab.utils.return_array import return_array_at_link
from landlab.utils.return_array import return_array_at_node


def find_upwind_link_at_link(grid, u):
    """Return the upwind link at every link.

    For all links, return ID of upwind link, defined based on the sign of `u`.
    If `u` is zero, the upwind link is found as though `u` were positive.

    For instance (see examples below), consider a 3x4 raster grid with link
    numbering::

        .-14-.-15-.-16-.
        |    |    |    |
        10  11   12   13
        |    |    |    |
        .--7-.--8-.--9-.
        |    |    |    |
        3    4    5    6
        |    |    |    |
        .--0-.--1-.--2-.

    There are at most 7 active links (4, 5, 7, 8, 9, 11, 12).
    If `u` is positive everywhere, then the upwind links are::

        .----.-14-.-15-.
        |    |    |    |
        3    4    5    6
        |    |    |    |
        .----.--7-.--8-.
        |    |    |    |
        |    |    |    |
        |    |    |    |
        .----.--0-.--1-.

    If `u` is negative everywhere, then the upwind links are::

        .-15-.-16-.----.
        |    |    |    |
        |    |    |    |
        |    |    |    |
        .--8-.--9-.----.
        |    |    |    |
        10  11   12   13
        |    |    |    |
        .--1-.--2-.----.

    Parameters
    ----------
    grid : RasterModelGrid or HexModelGrid
        A landlab grid.
    u : float or (n_links,) ndarray
        Array of *at-link* values used to determine which node is
        upwind.

    Returns
    -------
    (n_links,) ndarray of int
        The upwind links.

    Examples
    --------
    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> grid = RasterModelGrid((3, 4))

    >>> uwl = find_upwind_link_at_link(grid, 1.0)
    >>> uwl[grid.vertical_links].reshape((2, 4))
    array([[-1, -1, -1, -1],
           [ 3,  4,  5,  6]])
    >>> uwl[grid.horizontal_links].reshape((3, 3))
    array([[-1,  0,  1],
           [-1,  7,  8],
           [-1, 14, 15]])

    >>> uwl = find_upwind_link_at_link(grid, -1.0)
    >>> uwl[grid.vertical_links].reshape((2, 4))
    array([[10, 11, 12, 13],
           [-1, -1, -1, -1]])
    >>> uwl[grid.horizontal_links].reshape((3, 3))
    array([[ 1,  2, -1],
           [ 8,  9, -1],
           [15, 16, -1]])

    >>> u = np.zeros(grid.number_of_links)
    >>> u[4:6] = -1
    >>> u[7] = -1
    >>> u[8:10] = 1
    >>> u[11:13] = 1
    >>> u[grid.vertical_links].reshape((2, 4))
    array([[ 0., -1., -1.,  0.],
           [ 0.,  1.,  1.,  0.]])
    >>> u[grid.horizontal_links].reshape((3, 3))
    array([[ 0.,  0.,  0.],
           [-1.,  1.,  1.],
           [ 0.,  0.,  0.]])
    >>> uwl = find_upwind_link_at_link(grid, u)
    >>> uwl[grid.vertical_links].reshape((2, 4))
    array([[-1, 11, 12, -1],
           [ 3,  4, 5,   6]])
    >>> uwl[grid.horizontal_links].reshape((3, 3))
    array([[-1,  0,  1],
           [ 8,  7,  8],
           [-1, 14, 15]])
    """
    pll = grid.parallel_links_at_link

    cols = np.choose(np.broadcast_to(u, len(pll)) >= 0, [1, 0])
    uwl = pll[np.arange(len(pll)), cols]

    return uwl


def upwind_to_local_grad_ratio(grid, v, uwll, out=None):
    """Calculate and return ratio of upwind to local gradient in v.

    Gradients are defined on links. Upwind is pre-determined via
    parameter uwll (upwind link at link), which can be obtained
    using the find_upwind_link_at_link function.

    In Total Variation Diminishing (TVD) numerical schemes, this
    ratio is input to a flux limiter to calculate the weighting factor
    for higher-order vs. lower-order terms.

    Parameters
    ----------
    grid : RasterModelGrid or HexModelGrid
        A landlab grid.
    v : (n_links,) ndarray
        Array of *at-link* values of which to calculate the gradient.
    uwll : (n_links,) ndarray
        Array of upwind links for every link (as returned, for example, by
        :func:`~.find_upwind_link_at_link`).
    out : (n_links,) ndarray, optional
        If provided, place output into this array. Otherwise, create a new array.

    Returns
    -------
    (n_links,) ndarray of int
        The ratio of the gradients. For links that have a gradient of zero, the ratio
        is set to one. For links that do not have an upwind link, the ratio is also
        set to one.
    """
    if out is None:
        out = np.ones(grid.number_of_links)
    else:
        out[:] = 1.0

    local_diff = v[grid.node_at_link_head] - v[grid.node_at_link_tail]

    np.divide(
        local_diff[uwll], local_diff, where=(uwll != -1) & (local_diff != 0.0), out=out
    )

    return out


class AdvectionSolverTVD(Component):
    """Numerical solution for advection using a Total Variation Diminishing method.

    The component is restricted to regular grids (e.g., Raster or Hex).
    If multiple fields are advected, the advection__flux field will apply to
    the last one listed.

    Parameters
    ----------
    grid : RasterModelGrid or HexModelGrid
        A Landlab grid object.
    fields_to_advect : field name or list or (n_nodes,) array (default None)
        A node field of scalar values that will be advected, or list of fields.
        If not given, the component creates a generic field, initialized to zeros,
        called advected__quantity. If list >1 element given, advection will be
        applied to each field in it.
    advection_direction_is_steady : bool (default False)
        Indicates whether the directions of advection are expected to remain
        steady throughout a run. If True, some computation time is saved
        by calculating upwind links only once.

    Examples
    --------
    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import AdvectionSolverTVD
    >>> grid = RasterModelGrid((3, 7))
    >>> s = grid.add_zeros("advected__quantity", at="node")
    >>> s[9:12] = np.array([1.0, 2.0, 1.0])
    >>> u = grid.add_zeros("advection__velocity", at="link")
    >>> u[grid.horizontal_links] = 1.0
    >>> advec = AdvectionSolverTVD(grid, fields_to_advect="advected__quantity")
    >>> for _ in range(5):
    ...     advec.update(0.2)
    ...
    >>> np.argmax(s[7:14])
    4
    """

    _name = "AdvectionSolverTVD"

    _unit_agnostic = True

    _info = {
        "advected__quantity": {
            "dtype": float,
            "intent": "out",
            "optional": True,
            "units": "-",
            "mapping": "node",
            "doc": "Scalar quantity advected",
        },
        "advection__flux": {
            "dtype": float,
            "intent": "out",
            "optional": True,
            "units": "m2/y",
            "mapping": "link",
            "doc": "Link-parallel advection flux",
        },
        "advection__velocity": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m/y",
            "mapping": "link",
            "doc": "Link-parallel advection velocity magnitude",
        },
    }

    def __init__(
        self,
        grid,
        fields_to_advect=None,
        advection_direction_is_steady=False,
    ):
        """Initialize AdvectionSolverTVD."""

        # Call base class methods to check existence of input fields,
        # create output fields, etc.
        super().__init__(grid)
        self.initialize_output_fields()

        self._scalars = []  # list of fields to advect
        self._fluxes = []  # list of flux fields
        if fields_to_advect is None:
            try:
                self._scalars.append(self.grid.at_node["advected__quantity"])
            except KeyError:
                self._scalars.append(
                    self.grid.add_zeros("advected__quantity", at="node")
                )
            try:
                self._fluxes.append(self.grid.at_link["advection__flux"])
            except KeyError:
                self._fluxes.append(self.grid.add_zeros("advection__flux", at="link"))
        elif isinstance(fields_to_advect, list):
            flux_counter = 0
            for field in fields_to_advect:
                self._scalars.append(return_array_at_node(self.grid, field))
                if isinstance(field, str):
                    flux_name = "flux_of_" + field
                else:
                    flux_name = "advection__flux_" + str(flux_counter)
                    flux_counter += 1
                try:
                    flux = return_array_at_link(self.grid, flux_name)
                except FieldError:
                    flux = grid.add_zeros(flux_name, at="link")
                self._fluxes.append(flux)
        else:
            self._scalars.append(return_array_at_node(self.grid, fields_to_advect))
            if isinstance(fields_to_advect, str):
                flux_name = "flux_of_" + fields_to_advect
            else:
                flux_name = "advection__flux"
            try:
                flux = return_array_at_link(self.grid, flux_name)
            except FieldError:
                flux = grid.add_zeros(flux_name, at="link")
            self._fluxes.append(flux)

        self._vel = self.grid.at_link["advection__velocity"]

        self._advection_direction_is_steady = advection_direction_is_steady
        if advection_direction_is_steady:  # if so, only need to do this once
            self._upwind_link_at_link = find_upwind_link_at_link(self.grid, self._vel)
            self._upwind_link_at_link[
                self.grid.status_at_link == LinkStatus.INACTIVE
            ] = -1

    def calc_rate_of_change_at_nodes(self, scalar, flux, dt, update_upwind_links=False):
        """Calculate time rate of change in the advected quantity at nodes.

        Parameters
        ----------
        scalar : (n_nodes, ) array
            Scalar at-node field of values to be advected.
        dt : float
            Time-step duration. Needed to calculate the Courant number.
        update_upwind_links : bool (optional; default False)
            If True, upwind links will be updated (set to True if the
            direction of advection is changing through time; if there are
            multiple advected quantities, it only needs to be True for the
            first one updated, and the update will be used for the others)
        """
        if update_upwind_links:
            self._upwind_link_at_link = find_upwind_link_at_link(self.grid, self._vel)
            self._upwind_link_at_link[
                self.grid.status_at_link == LinkStatus.INACTIVE
            ] = -1
        s_link_low = self.grid.map_node_to_link_linear_upwind(scalar, self._vel)
        s_link_high = self.grid.map_node_to_link_lax_wendroff(
            scalar, dt * self._vel / self.grid.length_of_link
        )
        r = upwind_to_local_grad_ratio(self.grid, scalar, self._upwind_link_at_link)
        psi = flux_lim_vanleer(r)
        s_at_link = psi * s_link_high + (1.0 - psi) * s_link_low
        flux[self.grid.active_links] = (
            self._vel[self.grid.active_links] * s_at_link[self.grid.active_links]
        )
        return -self.grid.calc_flux_div_at_node(flux)

    def update(self, dt):
        """Update the solution by one time step dt.

        Same as :meth:`~.run_one_step`.

        Parameters
        ----------
        dt : float
            Time-step duration. Needed to calculate the Courant number.
        """
        update_upwinds = not self._advection_direction_is_steady
        for i in range(len(self._scalars)):  # update each of the advected scalars
            scalar = self._scalars[i]
            flux = self._fluxes[i]
            roc = self.calc_rate_of_change_at_nodes(
                scalar, flux, dt, update_upwind_links=update_upwinds
            )
            scalar[self.grid.core_nodes] += roc[self.grid.core_nodes] * dt
            update_upwinds = False  # should always be False after 1st scalar done

    def run_one_step(self, dt):
        """Update the solution by one time step dt.

        Same as :meth:`~.update`.

        Parameters
        ----------
        dt : float
            Time-step duration. Needed to calculate the Courant number.
        """
        self.update(dt)



================================================
File: advection/flux_limiters.py
================================================
#!/usr/bin/env python3
"""Flux limiter functions for advection solver.

There are many flux-limiter functions. For second-order TVD
schemes, there is an envelope of acceptable values of ``phi(r)``::

    r <= phi(r) <= 2r, (0 <= r <= 1)
    phi(1) = 1
    1 <= phi(r) <= r, (1 <= r <= 2)
    1 <= phi(r) <= 2, (r > 2)
"""

import numpy as np


def flux_lim_vanleer(r):
    """Apply Van Leer flux-limiter function."""
    return (r + np.abs(r)) / (1.0 + np.abs(r))



================================================
File: area_slope_transporter/__init__.py
================================================
from .area_slope_transporter import AreaSlopeTransporter

__all__ = ["AreaSlopeTransporter"]



================================================
File: area_slope_transporter/area_slope_transporter.py
================================================
from landlab import Component


class AreaSlopeTransporter(Component):
    """Model drainage network evolution for a network of transport-limited
    rivers in which sediment transport rate is calculated as a power-law
    function of drainage area and local streamwise slope gradient.

    AreaSlopeTransporter is designed to operate together with a flow-routing
    component such as PriorityFloodFlowRouter, so that each grid node has
    a defined flow direction toward one of its neighbor nodes. Each core node
    is assumed to contain one outgoing fluvial channel, and (depending on
    the drainage structure) zero, one, or more incoming channels. These channels are
    treated as effectively sub-grid-scale features that are embedded in valleys
    that have a width of one grid cell. The rate of sediment transport out of
    a given node is calculated as a generic power function of drainage area,
    local slope, and a user-specified transport coefficient.
    Similar power-law formulations have been used, for example, by
    Willgoose et al. (1991a,b,c, and many papers following that use the
    SIBERIA model) and Howard (1994, in Water Resources Research).

    Parameters
    ----------
    grid : ModelGrid
        A Landlab model grid object
    transport_coefficient : float (default 0.0055)
        Dimensional transport efficiency factor
    area_exponent : float (default 1.4)
        Exponent on effective total discharge
    slope_exponent : float (default 2.1)
        Exponent on local streamwise slope gradient

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import FlowAccumulator
    >>> grid = RasterModelGrid((3, 3), xy_spacing=1000.0)
    >>> elev = grid.add_zeros("topographic__elevation", at="node")
    >>> grid.status_at_node[grid.perimeter_nodes] = grid.BC_NODE_IS_CLOSED
    >>> grid.status_at_node[5] = grid.BC_NODE_IS_FIXED_VALUE
    >>> fa = FlowAccumulator(grid)
    >>> fa.run_one_step()
    >>> transporter = AreaSlopeTransporter(grid)
    >>> for _ in range(200):
    ...     fa.run_one_step()
    ...     elev[grid.core_nodes] += 1.0
    ...     transporter.run_one_step(10000.0)
    ...
    >>> int(round(elev[4] * 100))
    1068
    """

    _name = "AreaSlopeTransporter"

    _unit_agnostic = True

    _info = {
        "sediment__volume_influx": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m**3/y",
            "mapping": "node",
            "doc": "Volumetric incoming streamwise sediment transport rate",
        },
        "sediment__volume_outflux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m**3/y",
            "mapping": "node",
            "doc": "Volumetric outgoing streamwise sediment transport rate",
        },
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__upstream_node_order": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array containing downstream-to-upstream ordered list of node IDs",
        },
        "sediment__rate_of_change": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/y",
            "mapping": "node",
            "doc": "Time rate of change of sediment thickness",
        },
        "drainage_area": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m**2",
            "mapping": "node",
            "doc": "Upstream accumulated surface area contributing to the node's discharge",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__steepest_slope": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "The steepest *downhill* slope",
        },
    }

    def __init__(
        self,
        grid,
        transport_coefficient=0.0055,
        area_exponent=1.4,
        slope_exponent=2.1,
    ):
        """Initialize AreaSlopeTransporter."""

        super().__init__(grid)

        # Parameters
        self._trans_coef = transport_coefficient
        self._area_exponent = area_exponent
        self._slope_exponent = slope_exponent

        # Fields and arrays
        self._elev = grid.at_node["topographic__elevation"]
        self._area = grid.at_node["drainage_area"]
        self._slope = grid.at_node["topographic__steepest_slope"]
        self._receiver_node = grid.at_node["flow__receiver_node"]
        self._receiver_link = grid.at_node["flow__link_to_receiver_node"]
        super().initialize_output_fields()
        self._sediment_influx = grid.at_node["sediment__volume_influx"]
        self._sediment_outflux = grid.at_node["sediment__volume_outflux"]
        self._dzdt = grid.at_node["sediment__rate_of_change"]

    def calc_transport_capacity(self):
        """Calculate and return bed-load transport capacity.

        Calculation uses power-law approach, and provides
        volume per time rate.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> grid = RasterModelGrid((3, 3), xy_spacing=100.0)
        >>> elev = grid.add_zeros("topographic__elevation", at="node")
        >>> elev[3:] = 1.0
        >>> fa = FlowAccumulator(grid)
        >>> fa.run_one_step()
        >>> transporter = AreaSlopeTransporter(grid)
        >>> transporter.calc_transport_capacity()
        >>> int(transporter._sediment_outflux[4] * 1000)
        138
        """
        self._sediment_outflux[:] = (
            self._trans_coef
            * self._area**self._area_exponent
            * self._slope**self._slope_exponent
        )

    def calc_sediment_rate_of_change(self):
        """Update the rate of thickness change of sediment at each core node.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> grid = RasterModelGrid((3, 4), xy_spacing=100.0)
        >>> elev = grid.add_zeros("topographic__elevation", at="node")
        >>> elev[:] = 0.01 * grid.x_of_node
        >>> grid.status_at_node[grid.perimeter_nodes] = grid.BC_NODE_IS_CLOSED
        >>> grid.status_at_node[4] = grid.BC_NODE_IS_FIXED_VALUE
        >>> fa = FlowAccumulator(grid)
        >>> fa.run_one_step()
        >>> transporter = AreaSlopeTransporter(grid)
        >>> transporter.calc_sediment_rate_of_change()
        >>> np.round(transporter._sediment_outflux[4:7], 3)
        array([0.   , 0.365, 0.138])
        >>> np.round(transporter._sediment_influx[4:7], 3)
        array([0.365, 0.138, 0.   ])
        >>> np.round(transporter._dzdt[5:7], 8)
        array([-2.264e-05, -1.382e-05])
        """
        self.calc_transport_capacity()
        cores = self.grid.core_nodes
        self._sediment_influx[:] = 0.0
        for c in cores:  # send sediment downstream
            r = self._receiver_node[c]
            self._sediment_influx[r] += self._sediment_outflux[c]
        self._dzdt[cores] = (
            self._sediment_influx[cores] - self._sediment_outflux[cores]
        ) / self.grid.area_of_cell[self.grid.cell_at_node[cores]]

    def run_one_step(self, dt):
        """Advance solution by time interval dt.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> grid = RasterModelGrid((3, 4), xy_spacing=100.0)
        >>> elev = grid.add_zeros("topographic__elevation", at="node")
        >>> elev[:] = 0.01 * grid.x_of_node
        >>> grid.status_at_node[grid.perimeter_nodes] = grid.BC_NODE_IS_CLOSED
        >>> grid.status_at_node[4] = grid.BC_NODE_IS_FIXED_VALUE
        >>> fa = FlowAccumulator(grid)
        >>> fa.run_one_step()
        >>> transporter = AreaSlopeTransporter(grid)
        >>> transporter.run_one_step(10000.0)
        >>> np.round(elev[4:7], 4)
        array([0.    , 0.7736, 1.8618])
        """
        self.calc_sediment_rate_of_change()
        self._elev += self._dzdt * dt



================================================
File: bedrock_landslider/__init__.py
================================================
from .bedrock_landslider import BedrockLandslider

__all__ = ["BedrockLandslider"]



================================================
File: bedrock_landslider/bedrock_landslider.py
================================================
"""Grid-based simulation of bedrock landslides.

Benjamin Campforts
"""

import numpy as np

from landlab import Component
from landlab.grid.nodestatus import NodeStatus

from ..depression_finder.lake_mapper import _FLOODED
from .cfuncs import _landslide_runout

MAX_HEIGHT_SLOPE = 100  # in m


class BedrockLandslider(Component):
    """Calculate the location and magnitude of episodic bedrock landsliding.

    Landlab component that calculates the location and magnitude of episodic
    bedrock landsliding following the Cullman criterion.
    See the publication:

    Campforts B., Shobe C.M., Steer P., Vanmaercke M., Lague D., Braun J.
    (2020) HyLands 1.0: a hybrid landscape evolution model to simulate the
    impact of landslides and landslide-derived sediment on landscape evolution.
    Geosci Model Dev: 13(9):3863–86.
    `https://dx.doi.org/10.5194/esurf-6-1-2018 <https://dx.doi.org/10.5194/esurf-6-1-2018>`_

    Campforts, B., Shobe, C. M., Overeem, I., & Tucker, G. E. (2022).
    The Art of Landslides: How Stochastic Mass Wasting Shapes Topography and
    Influences Landscape Dynamics.
    Journal of Geophysical Research: Earth Surface,
    127(8), 1–16. https://doi.org/10.1029/2022JF006745


    Examples
    --------

    >>> import numpy as np
    >>> from numpy import testing
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import PriorityFloodFlowRouter, BedrockLandslider

    Make a ``RasterModelGrid`` and create a plateau.

    * 5x5 grid
    * Initial topography is set to plateau value of 10

    >>> mg = RasterModelGrid((5, 5), xy_spacing=1.0)
    >>> z = mg.add_zeros("topographic__elevation", at="node")
    >>> s = mg.add_zeros("soil__depth", at="node")
    >>> b = mg.add_zeros("bedrock__elevation", at="node")

    Make plateau at 10 m

    >>> b += 10

    Lower boundary cell to 0

    >>> b[2] = 0
    >>> z[:] = b + s

    Instantiate the :class:`~.priority_flood_flow_router.PriorityFloodFlowRouter`
    for flow accumulation and the ``BedrockLandslider``

    >>> fd = PriorityFloodFlowRouter(
    ...     mg,
    ...     separate_hill_flow=True,
    ...     suppress_out=True,
    ... )
    >>> hy = BedrockLandslider(mg, landslides_return_time=1)

    Run the flow director and ``BedrockLandslider`` for one timestep

    >>> fd.run_one_step()
    >>> vol_suspended_sediment_yield, volume_leaving = hy.run_one_step(dt=1)

    After one timestep, we can predict exactly where the landslide will occur.
    The return time is set to 1 year so that probability for sliding is 100%.
    The angle of internal friction is 1 m/m, the topographical gradient is 10 m/m.
    At cardinal cells, the sliding plane will be at *(1 + 10) / 2 = 5.5* m/m.
    With a *dx* of 1, the cardinal cell next to the critical sliding node must
    be 5.5 m and the diagonal one at *5.5 * sqrt(2) = 7.8* m

    >>> testing.assert_almost_equal(
    ...     [5.5 * np.sqrt(2), 5.5, 5.5 * np.sqrt(2)], z[6:9], decimal=5
    ... )

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Campforts B., Shobe C.M., Steer P., Vanmaercke M., Lague D., Braun J.
    (2020) BedrockLandslider 1.0: a hybrid landscape evolution model to simulate the
    impact of landslides and landslide-derived sediment on landscape evolution.
    Geosci Model Dev: 13(9):3863–86.
    `https://dx.doi.org/10.5194/esurf-6-1-2018 <https://dx.doi.org/10.5194/esurf-6-1-2018>`_

    **Additional References**

    None Listed

    """

    _name = "BedrockLandslider"

    _unit_agnostic = True

    _info = {
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__steepest_slope": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "The steepest *downhill* slope",
        },
        "soil__depth": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of soil or weathered bedrock",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__upstream_node_order": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array containing downstream-to-upstream ordered list of node IDs",
        },
        # Note that this field has to be provided in addition to the \
        # flow__receiver_node and will be used to route sediments over the hillslope
        "hill_flow__receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        # Note that this field has to be provided in addition to the \
        # flow__receiver_proportions and will be used to route sediments
        # over the hillslope
        "hill_flow__receiver_proportions": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of proportion of flow sent to each receiver.",
        },
        "hill_topographic__steepest_slope": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "The steepest *downhill* slope",
        },
        "LS_sediment__flux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m3/s",
            "mapping": "node",
            "doc": "Sediment flux originating from landslides \
                (volume per unit time of sediment entering each node)",
        },
        "landslide__erosion": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Total erosion caused by landsliding ",
        },
        "landslide__deposition": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Total deposition of derived sediment",
        },
        "landslide_sediment_point_source": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m3",
            "mapping": "node",
            "doc": "Landslide derived sediment, as point sources on all the \
                critical nodes where landslides initiate, \
                before landslide runout is calculated ",
        },
    }

    _cite_as = """
    @Article{gmd-13-3863-2020,
        AUTHOR = {Campforts B., Shobe C.M., Steer P., Vanmaercke M., Lague D., Braun J.},
        TITLE = {BedrockLandslider 1.0: a hybrid landscape evolution model to
                 simulate the impact of landslides and landslide-derived sediment
                 on landscape evolution.},
        JOURNAL = {Geoscientific Model Development},
        VOLUME = {13},
        YEAR = {2020},
        NUMBER = {9},
        PAGES = {3863--3886},
        URL = {https://doi.org/10.5194/gmd-13-3863-2020},
        DOI = {10.5194/gmd-13-3863-2020}
    }"""

    def __init__(
        self,
        grid,
        angle_int_frict=1.0,
        threshold_slope=None,
        cohesion_eff=1e4,
        landslides_return_time=1e5,
        rho_r=2700,
        grav=9.81,
        fraction_fines_LS=0,
        phi=0,
        max_pixelsize_landslide=1e9,
        seed=2021,
        verbose_landslides=False,
        landslides_on_boundary_nodes=True,
        critical_sliding_nodes=None,
        min_deposition_slope=0,
    ):
        """Initialize the BedrockLandslider model.

        Parameters
        ----------
        grid : ModelGrid
            Landlab ModelGrid object
        angle_int_frict: float, optional
            Materials angle of internal friction in [m/m]
        threshold_slope: float, optional
            Threshold slope used in non-linear deposition scheme [m/m]
            Default value is set to angle_int_frict if not specified
        cohesion_eff : float, optional
            Effective cohesion of material [m L^-1 T^-2].
        landslides_return_time : float, optional
            Return time for stochastic landslide events to occur
        rho_r : float, optional
            Bulk density rock [m L^-3].
        fraction_fines_LS : float
            Fraction of permanently suspendable fines in bedrock
            Value must be between 0 and 1 [-].
        phi : float, optional
            Sediment porosity, value must be between 0 and 1 [-].
        max_pixelsize_landslide : int , optional
            Maximum size for landslides in number of pixels
        verbose_landslides : bool , optional
            Print output as number of simulated landslides per timestep
        seed : float , optional
            Provide seed to set stochastic model.
            If not provided, seed is set to 2021.
            Provide None to keep current seed.
        landslides_on_boundary_nodes : bool, optional
            Allow landslides to initiate (critical node) and extend over
            boundary nodes.
        critical_sliding_nodes : list, optional
            Provide list with critical nodes where landslides have to initiate
            This cancels the stochastic part of the algorithm and allows the
            user to form landslides at the provided critical nodes.
        """
        super().__init__(grid)

        topo = self.grid.at_node["topographic__elevation"]
        soil = self.grid.at_node["soil__depth"]

        if "bedrock__elevation" not in grid.at_node:
            grid.add_field("bedrock__elevation", topo - soil, at="node", dtype=float)

        # Check consistency of bedrock, soil and topographic elevation fields
        if not np.allclose(
            grid.at_node["bedrock__elevation"] + grid.at_node["soil__depth"],
            grid.at_node["topographic__elevation"],
        ):
            raise RuntimeError(
                "The sum of bedrock elevation and topographic elevation should be equal"
            )

        self.initialize_output_fields()

        # Store grid and parameters
        self._angle_int_frict = angle_int_frict
        if threshold_slope is None:
            self._threshold_slope = angle_int_frict
        else:
            self._threshold_slope = threshold_slope
        self._cohesion_eff = cohesion_eff
        self._rho_r = rho_r
        self._grav = grav
        self._fraction_fines_LS = fraction_fines_LS
        self._phi = phi
        self._landslides_return_time = landslides_return_time
        self._max_pixelsize_landslide = max_pixelsize_landslide
        self._verbose_landslides = verbose_landslides
        self._landslides_on_boundary_nodes = landslides_on_boundary_nodes
        self._critical_sliding_nodes = critical_sliding_nodes
        self._min_deposition_slope = min_deposition_slope

        # Data structures to store properties of simulated landslides.
        self._landslides_size = []
        self._landslides_volume = []
        self._landslides_volume_sed = []
        self._landslides_volume_bed = []

        # Check input values
        if phi >= 1.0 or phi < 0.0:
            raise ValueError(f"Porosity must be between 0 and 1 ({phi})")

        if fraction_fines_LS > 1.0 or fraction_fines_LS < 0.0:
            raise ValueError(
                f"Fraction of fines must be between 0 and 1 ({fraction_fines_LS})"
            )

        # Set seed
        if seed is not None:
            np.random.seed(seed)

    # Getters for properties
    @property
    def fraction_fines(self):
        """
        Fraction of permanently suspendable fines in bedrock.
        Value must be between 0 and 1 [-].
        """
        return self._fraction_fines_LS

    @property
    def phi(self):
        """
        Sediment porosity, value must be between 0 and 1 [-].
        """
        return self._phi

    @property
    def landslides_size(self):
        """
        List with the size of simulated landslides.
        The list is reset every time the _landslide_erosion function is called
        """
        return self._landslides_size

    @property
    def landslides_volume(self):
        """
        List with the volume of simulated landslides.
        The list is reset every time the _landslide_erosion function is called
        """
        return self._landslides_volume

    @property
    def landslides_volume_sed(self):
        """
        List with the volume of sediment eroded by landslides.
        The list is reset every time the _landslide_erosion function is called
        """
        return self._landslides_volume_sed

    @property
    def landslides_volume_bed(self):
        """
        List with the volume of bedrock eroded by landslides.
        The list is reset every time the _landslide_erosion function is called
        """
        return self._landslides_volume_bed

    def _landslide_erosion(self, dt):
        """
        Calculate bedrock landsliding for a time period 'dt'.

        Parameters
        ----------
        dt: float
            The imposed timestep.

        Returns
        -------
        suspended_sed : float
            Volume of suspended sediment.

        """
        # Pointers
        topo = self.grid.at_node["topographic__elevation"]
        bed = self.grid.at_node["bedrock__elevation"]
        steepest_slope = self.grid.at_node["topographic__steepest_slope"]
        soil_d = self.grid.at_node["soil__depth"]
        landslide_sed_in = self.grid.at_node["landslide_sediment_point_source"]
        landslide__ero = self.grid.at_node["landslide__erosion"]

        # Reset LS Plains
        landslide__ero.fill(0.0)
        # Reset landslide sediment point source field
        landslide_sed_in.fill(0.0)

        # Reset data structures to store properties of simulated landslides.
        self._landslides_size = []
        self._landslides_volume = []
        self._landslides_volume_sed = []
        self._landslides_volume_bed = []

        # Identify flooded nodes
        flood_status = self.grid.at_node["flood_status_code"]
        flooded_nodes = np.nonzero(flood_status == _FLOODED)[0]

        # In the following section the location of critical nodes where
        # landsldies are initatated is calcualted, unless these critical nodes
        # are provided as critical_sliding_nodes
        if self._critical_sliding_nodes is None:
            # Calculate gradients
            height_cell = topo - topo[self.grid.at_node["flow__receiver_node"]]

            height_cell[flooded_nodes] = 0
            height_cell[height_cell > MAX_HEIGHT_SLOPE] = MAX_HEIGHT_SLOPE

            angle_int_frict_radians = np.arctan(self._angle_int_frict)
            height_critical = np.divide(
                (4 * self._cohesion_eff / (self._grav * self._rho_r))
                * (np.sin(np.arctan(steepest_slope)) * np.cos(angle_int_frict_radians)),
                1 - np.cos(np.arctan(steepest_slope) - angle_int_frict_radians),
                where=(1 - np.cos(np.arctan(steepest_slope) - angle_int_frict_radians))
                > 0,
                out=np.zeros_like(steepest_slope),
            )
            spatial_prob = np.divide(
                height_cell,
                height_critical,
                where=height_critical > 0,
                out=np.zeros_like(height_critical),
            )
            spatial_prob[np.arctan(steepest_slope) <= angle_int_frict_radians] = 0
            spatial_prob[spatial_prob > 1] = 1

            # Temporal probability
            temporal_prob = 1 - np.exp(-dt / self._landslides_return_time)

            # Combined probability
            combined_prob = temporal_prob * spatial_prob
            sliding = np.random.rand(combined_prob.size) < combined_prob

            # Now, find the critical node, which is the receiver of critical_landslide_nodes
            # Critical nodes must be unique (a given node can have more receivers...)
            critical_landslide_nodes = np.unique(
                self.grid.at_node["flow__receiver_node"][np.where(sliding)]
            )
            # Remove boundary nodes
            if not self._landslides_on_boundary_nodes:
                critical_landslide_nodes = critical_landslide_nodes[
                    ~self.grid.node_is_boundary(critical_landslide_nodes)
                ]
        else:
            critical_landslide_nodes = np.array(self._critical_sliding_nodes)

        # output variables
        suspended_sed = 0.0
        if self._verbose_landslides:
            print(f"nbSlides = {len(critical_landslide_nodes)}")

        store_cumul_volume = 0.0
        while critical_landslide_nodes.size > 0:
            crit_node = critical_landslide_nodes[0]  # start at first critical node
            crit_node_el = topo[crit_node]

            # get 8 neighbors and only keep those to active nodes which are upstream
            neighbors = np.concatenate(
                (
                    self.grid.active_adjacent_nodes_at_node[crit_node],
                    self.grid.diagonal_adjacent_nodes_at_node[crit_node],
                )
            )
            neighbors = neighbors[neighbors != -1]
            neighbors_up = neighbors[topo[neighbors] > crit_node_el]

            x_crit_node = self.grid.node_x[crit_node]
            y_crit_node = self.grid.node_y[crit_node]

            dist_to_initial_node = np.sqrt(
                np.add(
                    np.square(x_crit_node - self.grid.node_x[neighbors_up]),
                    np.square(y_crit_node - self.grid.node_y[neighbors_up]),
                )
            )
            slope_neighbors_to_crit_node = (
                topo[neighbors_up] - crit_node_el
            ) / dist_to_initial_node

            neighbors_up = neighbors_up[
                slope_neighbors_to_crit_node > self._angle_int_frict
            ]
            slope_neighbors_to_crit_node = slope_neighbors_to_crit_node[
                slope_neighbors_to_crit_node > self._angle_int_frict
            ]

            if slope_neighbors_to_crit_node.size > 0:
                slope_slide = max(slope_neighbors_to_crit_node)
                store_volume_bed = 0.0
                store_volume_sed = 0.0
                upstream_count = 0
                upstream_neighbors = neighbors_up
                if not self._landslides_on_boundary_nodes:
                    upstream_neighbors = upstream_neighbors[
                        ~self.grid.node_is_boundary(upstream_neighbors)
                    ]
                # Fix sliding angle of particular LS
                sliding_angle = (self._angle_int_frict + slope_slide) / 2.0
                nb_landslide_cells = 0

                # If landslides become unrealistically big, exit algorithm
                while upstream_neighbors.size > 0 and (
                    upstream_count <= self._max_pixelsize_landslide
                    and nb_landslide_cells < 1e5
                ):
                    distance_to_crit_node = np.sqrt(
                        np.add(
                            np.square(
                                x_crit_node - self.grid.node_x[upstream_neighbors[0]]
                            ),
                            np.square(
                                y_crit_node - self.grid.node_y[upstream_neighbors[0]]
                            ),
                        )
                    )
                    new_el = crit_node_el + distance_to_crit_node * sliding_angle
                    nb_landslide_cells += 1
                    if new_el < topo[upstream_neighbors[0]]:
                        # Do actual slide
                        upstream_count += 1
                        sed_landslide_ero = np.clip(
                            min(
                                soil_d[upstream_neighbors[0]],
                                topo[upstream_neighbors[0]] - new_el,
                            ),
                            a_min=0.0,
                            a_max=None,
                        )
                        soil_d[upstream_neighbors[0]] -= sed_landslide_ero
                        topo[upstream_neighbors[0]] = new_el
                        bed_landslide_ero = np.clip(
                            bed[upstream_neighbors[0]]
                            - (new_el - soil_d[upstream_neighbors[0]]),
                            a_min=0.0,
                            a_max=None,
                        )
                        bed[upstream_neighbors[0]] -= bed_landslide_ero
                        topo[upstream_neighbors[0]] = new_el

                        vol_sed = (
                            sed_landslide_ero * (1 - self._phi) * (self.grid.dx**2)
                        )
                        vol_bed = bed_landslide_ero * (self.grid.dx**2)
                        store_volume_sed = store_volume_sed + vol_sed
                        store_volume_bed = store_volume_bed + vol_bed

                        neighbors = np.concatenate(
                            (
                                self.grid.active_adjacent_nodes_at_node[
                                    upstream_neighbors[0]
                                ],
                                self.grid.diagonal_adjacent_nodes_at_node[
                                    upstream_neighbors[0]
                                ],
                            )
                        )
                        neighbors = neighbors[neighbors != -1]
                        neighbors_up = neighbors[topo[neighbors] > crit_node_el]
                        upstream_neighbors = [*upstream_neighbors, *neighbors_up]

                        temp, idx = np.unique(upstream_neighbors, return_index=True)
                        upstream_neighbors = np.array(upstream_neighbors)
                        upstream_neighbors = upstream_neighbors[np.sort(idx)]
                        if not self._landslides_on_boundary_nodes:
                            upstream_neighbors = upstream_neighbors[
                                ~self.grid.node_is_boundary(upstream_neighbors)
                            ]
                        # if one of the LS pixels also appears in critical_landslide_nodes list,
                        # remove it there so that no new landslide is initialized
                        critical_landslide_nodes = critical_landslide_nodes[
                            np.where(critical_landslide_nodes != upstream_neighbors[0])
                        ]

                        landslide__ero[upstream_neighbors[0]] = (
                            sed_landslide_ero + bed_landslide_ero
                        )

                    upstream_neighbors = np.delete(upstream_neighbors, 0, 0)

                store_volume = store_volume_sed + store_volume_bed
                store_cumul_volume += store_volume
                if upstream_count > 0:
                    landslide_sed_in[crit_node] += (store_volume / dt) * (
                        1.0 - self._fraction_fines_LS
                    )
                    suspended_sed += (store_volume / dt) * self._fraction_fines_LS

                    self._landslides_size.append(upstream_count)
                    self._landslides_volume.append(store_volume)
                    self._landslides_volume_sed.append(store_volume_sed)
                    self._landslides_volume_bed.append(store_volume_bed)

            if critical_landslide_nodes.size > 0:
                critical_landslide_nodes = np.delete(critical_landslide_nodes, 0, 0)

        return suspended_sed

    def _landslide_runout(self, dt):
        """
        Calculate landslide runout using a non-local deposition algorithm based on:
        * Carretier S., Martinod P., Reich M., Godderis Y. (2016) Modelling
          sediment clasts transport during landscape evolution.
          Earth Surf Dyn: 4(1):237–51.
        * Campforts B., Shobe C.M., Steer P., Vanmaercke M., Lague D., Braun J.
          (2020) HyLands 1.0: a hybrid landscape evolution model to simulate
          the impact of landslides and landslide-derived sediment on landscape
          evolution. Geosci Model Dev: 13(9):3863–86.

        Parameters
        ----------
        dt : float
            Timestep.

        Returns
        -------
        dh_hill : float
            Hillslope erosion over the simulated domain.
        volume_leaving : float
            Total volume of sediment leaving the simulated domain.
        flux_core_nodes : float
            Sediment flux over the simulated domain.

        """
        topo = self.grid.at_node["topographic__elevation"]
        bed = self.grid.at_node["bedrock__elevation"]
        soil_d = self.grid.at_node["soil__depth"]
        sed_flux = self.grid.at_node["LS_sediment__flux"]
        stack_rev = np.flip(self.grid.at_node["flow__upstream_node_order"])
        landslide_depo = self.grid.at_node["landslide__deposition"]
        landslide_sed_in = self.grid.at_node["landslide_sediment_point_source"]
        node_status = self.grid.status_at_node

        # Only process core nodes
        stack_rev_sel = stack_rev[node_status[stack_rev] == NodeStatus.CORE]
        receivers = self.grid.at_node["hill_flow__receiver_node"]
        fract_receivers = self.grid.at_node["hill_flow__receiver_proportions"]

        # keep only steepest slope
        slope = np.max(self.grid.at_node["hill_topographic__steepest_slope"], axis=1)
        slope[slope < 0] = 0.0

        flux_in = landslide_sed_in * dt  # flux_in, in m3 per timestep

        # L following carretier 2016
        transport_length_hill = np.where(
            slope < self._threshold_slope,
            self.grid.dx / (1 - (slope / self._threshold_slope) ** 2),
            1e6,
        )

        flux_out = np.zeros(topo.shape)
        dh_hill = np.zeros(topo.shape)
        topo_copy = np.array(topo)
        max_depo = np.zeros(topo.shape)
        length_adjacent_cells = np.array(
            [
                self.grid.dx,
                self.grid.dx,
                self.grid.dx,
                self.grid.dx,
                self.grid.dx * np.sqrt(2),
                self.grid.dx * np.sqrt(2),
                self.grid.dx * np.sqrt(2),
                self.grid.dx * np.sqrt(2),
            ]
        )

        _landslide_runout(
            self.grid.dx,
            self._phi,
            self._min_deposition_slope,
            stack_rev_sel,
            receivers,
            fract_receivers,
            flux_in,
            transport_length_hill,
            flux_out,
            dh_hill,
            topo_copy,
            max_depo,
            length_adjacent_cells,
        )
        sed_flux[:] = flux_out

        flux_core_nodes = np.sum(flux_in[self.grid.status_at_node == 0])
        volume_leaving = np.sum(flux_in)  # Qs_leaving # in m3 per timestep

        # Change sediment layer
        soil_d[:] += dh_hill
        topo[:] = bed + soil_d

        # Reset Qs
        landslide_sed_in.fill(0.0)
        # Update deposition field
        landslide_depo[:] = dh_hill

        return dh_hill, volume_leaving, flux_core_nodes

    def run_one_step(self, dt):
        """Advance BedrockLandslider component by one time step of size dt.

        Parameters
        ----------
        dt: float
            The imposed timestep.

        Returns
        -------
        vol_suspended_sediment_yield : float
            volume of sediment evacuated as syspended sediment.
        volume_leaving : float
            Volume of sediment leaving the domain.
        """
        dt = float(dt)

        if self.current_time is None:
            self.current_time = dt
        else:
            self.current_time += dt

        # Landslides
        vol_suspended_sediment_yield = self._landslide_erosion(dt)
        dh_hill, volume_leaving, flux_core_nodes = self._landslide_runout(dt)

        return vol_suspended_sediment_yield, volume_leaving



================================================
File: bedrock_landslider/cfuncs.pyx
================================================
cimport cython

# https://cython.readthedocs.io/en/stable/src/userguide/fusedtypes.html
ctypedef fused id_t:
    cython.integral
    long long


@cython.boundscheck(False)
cpdef _landslide_runout(
    double dx,
    double phi,
    double min_deposition_slope,
    id_t [:] stack_rev_sel,
    id_t [:, :] receivers,
    cython.floating [:, :] fract,
    cython.floating [:] Qs_in,
    cython.floating [:] L_Hill,
    cython.floating [:] Qs_out,
    cython.floating [:] dH_Hill,
    cython.floating [:] H_i_temp,
    cython.floating [:] max_D,
    cython.floating [:] length_adjacent_cells,
):
    """
    Calculate landslide runout using a non-local deposition algorithm, see:

    * Campforts B., Shobe C.M., Steer P., Vanmaercke M., Lague D., Braun J.
      (2020) HyLands 1.0: a hybrid landscape evolution model to simulate
      the impact of landslides and landslide-derived sediment on landscape
      evolution. Geosci Model Dev: 13(9):3863–86.
    """
    # define internal variables
    cdef int donor, rcvr, r
    cdef double proportion, dH

    # Iterate backward through the stack, which means we work from upstream to
    # downstream.
    for donor in stack_rev_sel:
        dH = max(
                0,
                min(((Qs_in[donor] / dx) / L_Hill[donor]) / (1 - phi), max_D[donor])
            )
        dH_Hill[donor] += dH
        H_i_temp[donor] += dH

        Qs_in[donor] -= dH * dx * dx * (1 - phi)
        Qs_out[donor] += Qs_in[donor]

        for r in range(receivers.shape[1]):
            rcvr = receivers[donor, r]

            max_D_angle = (
                H_i_temp[donor]
                - min_deposition_slope * length_adjacent_cells[r]
                - H_i_temp[rcvr]
            )
            max_D[rcvr] = min(
                max(max_D[rcvr], H_i_temp[donor] - H_i_temp[rcvr]), max_D_angle
            )

            proportion = fract[donor, r]
            if proportion > 0. and donor != rcvr:
                Qs_in[rcvr] += Qs_out[donor] * proportion
                Qs_in[donor] -= Qs_out[donor] * proportion



================================================
File: carbonate/__init__.py
================================================
from .carbonate_producer import CarbonateProducer

__all__ = ["CarbonateProducer"]



================================================
File: carbonate/carbonate_producer.py
================================================
#! /usr/bin/env python
from contextlib import contextmanager

import numpy as np

from landlab import Component

_EPSILON = 1.0e-6  # tiny negative depth avoid fast carb growth at zero depth


@contextmanager
def set_numpy_err(*args, **kwds):
    settings = np.seterr(*args, **kwds)
    try:
        yield settings
    finally:
        np.seterr(**settings)


def smooth_heaviside(x, width=0.5, out=None):
    """Return a smoothed heaviside function (step function).

    Parameters
    ----------
    x : array or float
        Dependent variable
    width : float (default 0.5)
        Width parameter for smoothing (same units as x)
    out : array (default None)
        Optional array in which to store result; must have len(x)

    Examples
    --------
    >>> import numpy as np
    >>> np.round(smooth_heaviside(np.array([-1, 0, 1])), 3)
    array([0.018, 0.5  , 0.982])
    >>> smooth_heaviside(np.array([-1, 0, 1]), width=0.0)
    array([0. , 0.5, 1. ])
    """
    if width > 0.0:
        with set_numpy_err(over="ignore"):
            return np.divide(1.0, (1.0 + np.exp(-2.0 / width * np.asarray(x))), out=out)
    else:
        return np.heaviside(x, 0.5)


class CarbonateProducer(Component):
    """Calculate marine carbonate production and deposition.

    Uses the growth-rate of Bosscher and Schlager (1992), which represents the
    vertical growth rate :math:`G` as:

    ..math::

        G = G_m \tanh (I_0 e^{-kz} / I_k)

    where :math:`G_m` is the maximum growth rate, :math:`I_0` is the surface
    light intensity, :math:`I_k` is the saturating light intensity, :math:`z` is
    water depth, and :math:`k` is the light extinction coefficient.

    Bosscher and Schlager (1992) suggest plausible values or ranges for these
    parameters as follows: :math:`G_m` = 10 to 15 mm/y, :math:`I_0` = 2,000 to
    2,250 micro Einsteins per square meter per second in the tropics, :math:`I_k`
    = 50 to 450 micro Einsteins per square meter per second, and :math:`k` =
    0.04 to 0.16 m:math:`^{-1}` (corresponding to a decay depth of 6.25 to 16 m).
    The default values used in this component are based on these estimates.

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import CarbonateProducer
    >>> grid = RasterModelGrid((3, 3))
    >>> elev = grid.add_zeros("topographic__elevation", at="node")
    >>> sealevel = grid.add_field("sea_level__elevation", 0.0, at="grid")
    >>> elev[:] = -1.0
    >>> cp = CarbonateProducer(grid)
    >>> np.round(cp.calc_carbonate_production_rate()[4], 2)
    0.01
    >>> elev[:] = -40.0
    >>> np.round(cp.calc_carbonate_production_rate()[4], 5)
    0.00091
    >>> cp.sea_level = -39.0
    >>> np.round(cp.calc_carbonate_production_rate()[4], 2)
    0.01
    >>> thickness = cp.produce_carbonate(10.0)
    >>> np.round(10 * grid.at_node["carbonate_thickness"][4])
    1.0
    >>> thickness is grid.at_node["carbonate_thickness"]
    True
    >>> cp.run_one_step(10.0)
    >>> np.round(10 * thickness[4])
    2.0

    References
    ----------
    Bosscher, H., & Schlager, W. (1992). Computer simulation of reef growth.
    Sedimentology, 39(3), 503-512.
    """

    _name = "CarbonateProducer"

    _unit_agnostic = True

    _info = {
        "sea_level__elevation": {
            "dtype": "float",
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "grid",
            "doc": "Sea level elevation",
        },
        "topographic__elevation": {
            "dtype": "float",
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",  # and seafloor
        },
        "carbonate_production_rate": {
            "dtype": "float",
            "intent": "out",
            "optional": False,
            "units": "m / y",
            "mapping": "node",
            "doc": "Carbonate production rate",
        },
        "carbonate_thickness": {
            "dtype": "float",
            "intent": "out",
            "optional": True,
            "units": "m",
            "mapping": "node",
            "doc": "Carbonate thickness",
        },
        "water_depth": {
            "dtype": "float",
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Water depth",
        },
    }

    def __init__(
        self,
        grid,
        max_carbonate_production_rate=0.01,
        extinction_coefficient=0.1,
        surface_light=2000.0,
        saturating_light=400.0,
        tidal_range=0.0,
    ):
        """
        Parameters
        ----------
        grid: ModelGrid (RasterModelGrid, HexModelGrid, etc.)
            A landlab grid object.
        max_carbonate_production_rate: float (default 0.01 m/y)
            Maximum rate of carbonate production, in m thickness per year
        extinction_coefficient: float (default 0.1 m^-1)
            Coefficient of light extinction in water column
        surface_light: float (default 2000.0 micro Einstein per m2 per s)
            Light intensity (photosynthetic photon flux density) at sea surface.
        saturating_light: float (default 400.0 micro Einstein per m2 per s)
            Saturating light intensity (photosynthetic photon flux density)
        tidal_range: float (default zero)
            Tidal range used to smooth growth rate when surface is near mean
            sea level.
        """
        super().__init__(grid)

        self.extinction_coefficient = extinction_coefficient
        self.max_carbonate_production_rate = max_carbonate_production_rate
        self.surface_light = surface_light
        self.tidal_range = tidal_range
        self.saturating_light = saturating_light

        super().initialize_output_fields()
        self._carb_prod_rate = grid.at_node["carbonate_production_rate"]
        self._depth = grid.at_node["water_depth"]

        if "carbonate_thickness" not in grid.at_node:
            grid.add_zeros("carbonate_thickness", at="node")
        self._carbonate_thickness = grid.at_node["carbonate_thickness"]

    @property
    def extinction_coefficient(self):
        return self._extinction_coefficient

    @extinction_coefficient.setter
    def extinction_coefficient(self, value):
        if value > 0.0:
            self._extinction_coefficient = float(value)
        else:
            raise ValueError("must be non-negative")

    @property
    def max_carbonate_production_rate(self):
        return self._max_carbonate_production_rate

    @max_carbonate_production_rate.setter
    def max_carbonate_production_rate(self, value):
        if value >= 0.0:
            self._max_carbonate_production_rate = float(value)
        else:
            raise ValueError("must be non-negative")

    @property
    def surface_light(self):
        return self._surface_light

    @surface_light.setter
    def surface_light(self, value):
        if value >= 0.0:
            self._surface_light = float(value)
        else:
            raise ValueError("must be non-negative")

    @property
    def tidal_range(self):
        return self._tidal_range

    @tidal_range.setter
    def tidal_range(self, value):
        if value >= 0.0:
            self._tidal_range = float(value)
        else:
            raise ValueError("tidal range must be non-negative")

    @property
    def saturating_light(self):
        return self._saturating_light

    @saturating_light.setter
    def saturating_light(self, value):
        if value >= 0.0:
            self._saturating_light = float(value)
        else:
            raise ValueError("surface light must be non-negative")

    @property
    def sea_level(self):
        return self.grid.at_grid["sea_level__elevation"]

    @sea_level.setter
    def sea_level(self, sea_level):
        self.grid.at_grid["sea_level__elevation"] = float(sea_level)

    def _create_carbonate_thickness_field(self):
        """Create and return optional carbonate_thickness field."""
        return self.grid.add_zeros("carbonate_thickness", at="node")

    def calc_carbonate_production_rate(self):
        """Update carbonate production rate and store in field.

        Returns
        -------
        float array x number of grid nodes
            Reference to updated carbonate_production_rate field
        """
        self._depth[:] = self.sea_level - self._grid.at_node["topographic__elevation"]
        self._depth.clip(min=-2.0 * self._tidal_range - _EPSILON, out=self._depth)
        self._carb_prod_rate[self.grid.core_nodes] = (
            self._max_carbonate_production_rate
            * np.tanh(
                self.surface_light
                * np.exp(
                    -self.extinction_coefficient * self._depth[self.grid.core_nodes]
                )
                / self.saturating_light
            )
        )
        self._carb_prod_rate *= smooth_heaviside(self._depth, width=self.tidal_range)
        return self._carb_prod_rate

    def produce_carbonate(self, dt):
        """Grow carbonate for one time step & add to carbonate thickness field.

        If optional carbonate_thickness field does not already exist, this
        function creates it and initializes all values to zero.

        Returns
        -------
        float array x number of grid nodes
            Reference to updated carbonate_thickness field
        """
        # if self._carbonate_thickness is None:
        #     self._carbonate_thickness = self._create_carbonate_thickness_field()

        self.calc_carbonate_production_rate()
        # print(self._depth)
        # print(self._carb_prod_rate)
        added_thickness = self._carb_prod_rate * dt
        # print(added_thickness)
        self._carbonate_thickness += added_thickness
        # print(self._carbonate_thickness)
        self.grid.at_node["topographic__elevation"] += added_thickness
        # print(self.grid.at_node["topographic__elevation"])

        return self._carbonate_thickness

    def run_one_step(self, dt):
        """Advance by one time step.

        Simply calls the produce_carbonate() function.

        Parameters
        ----------
        dt : float
            Time step duration (normally in years)
        """
        self.produce_carbonate(dt)



================================================
File: chi_index/__init__.py
================================================
from .channel_chi import ChiFinder

__all__ = ["ChiFinder"]



================================================
File: chi_index/channel_chi.py
================================================
"""Created March 2016.

@author: dejh
"""

import numpy as np

from landlab import Component
from landlab import RasterModelGrid

try:
    from itertools import izip
except ImportError:
    izip = zip


class ChiFinder(Component):
    """Calculate Chi Indices.

    This component calculates chi indices, sensu Perron & Royden, 2013,
    for a Landlab landscape.

    Examples
    --------
    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import FlowAccumulator, FastscapeEroder
    >>> from landlab.components import ChiFinder

    >>> mg = RasterModelGrid((3, 4))
    >>> for nodes in (
    ...     mg.nodes_at_right_edge,
    ...     mg.nodes_at_bottom_edge,
    ...     mg.nodes_at_top_edge,
    ... ):
    ...     mg.status_at_node[nodes] = mg.BC_NODE_IS_CLOSED
    >>> _ = mg.add_field("topographic__elevation", mg.node_x, at="node")
    >>> fr = FlowAccumulator(mg, flow_director="D8")
    >>> cf = ChiFinder(mg, min_drainage_area=1.0, reference_concavity=1.0)
    >>> fr.run_one_step()
    >>> cf.calculate_chi()
    >>> mg.at_node["channel__chi_index"].reshape(mg.shape)[1, :]
    array([0.5, 1. , 2. , 0. ])

    >>> mg2 = RasterModelGrid((5, 5), xy_spacing=100.0)
    >>> for nodes in (
    ...     mg2.nodes_at_right_edge,
    ...     mg2.nodes_at_bottom_edge,
    ...     mg2.nodes_at_top_edge,
    ... ):
    ...     mg2.status_at_node[nodes] = mg2.BC_NODE_IS_CLOSED
    >>> _ = mg2.add_zeros("topographic__elevation", at="node")
    >>> mg2.at_node["topographic__elevation"][mg2.core_nodes] = (
    ...     mg2.node_x[mg2.core_nodes] / 1000.0
    ... )
    >>> np.random.seed(0)
    >>> mg2.at_node["topographic__elevation"][mg2.core_nodes] += np.random.rand(
    ...     mg2.number_of_core_nodes
    ... )
    >>> fr2 = FlowAccumulator(mg2, flow_director="D8")
    >>> sp2 = FastscapeEroder(mg2, K_sp=0.01)
    >>> cf2 = ChiFinder(mg2, min_drainage_area=0.0, reference_concavity=0.5)
    >>> for i in range(10):
    ...     mg2.at_node["topographic__elevation"][mg2.core_nodes] += 10.0
    ...     fr2.run_one_step()
    ...     sp2.run_one_step(1000.0)
    ...
    >>> fr2.run_one_step()
    >>> cf2.calculate_chi()
    >>> mg2.at_node["channel__chi_index"].reshape(mg2.shape)
    array([[0.        , 0.        , 0.        , 0.        , 0.        ],
           [0.77219416, 1.54438833, 2.63643578, 2.61419437, 0.        ],
           [1.09204746, 2.18409492, 1.52214691, 2.61419437, 0.        ],
           [0.44582651, 0.89165302, 1.66384718, 2.75589464, 0.        ],
           [0.        , 0.        , 0.        , 0.        , 0.        ]])

    >>> cf3 = ChiFinder(
    ...     mg2,
    ...     min_drainage_area=20000.0,
    ...     use_true_dx=True,
    ...     reference_concavity=0.5,
    ...     reference_area=mg2.at_node["drainage_area"].max(),
    ...     clobber=True,
    ... )
    >>> cf3.calculate_chi()
    >>> cf3.chi_indices.reshape(mg2.shape)
    array([[  0. ,   0.        ,   0.        ,   0. ,  0. ],
           [  0. , 173.20508076,   0.        ,   0. ,  0. ],
           [  0. ,   0.        , 270.71067812,   0. ,  0. ],
           [  0. , 100.        , 236.60254038,   0. ,  0. ],
           [  0. ,   0.        ,   0.        ,   0. ,  0. ]])
    >>> cf3.hillslope_mask.reshape(mg2.shape)
    array([[ True,  True,  True,  True,  True],
           [False, False,  True,  True,  True],
           [ True,  True, False,  True,  True],
           [False, False, False,  True,  True],
           [ True,  True,  True,  True,  True]])

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    Perron, J., Royden, L. (2012). An integral approach to bedrock river
    profile analysis Earth Surface Processes and Landforms  38(6), 570-576.
    https://dx.doi.org/10.1002/esp.3302

    """

    _name = "ChiFinder"

    _unit_agnostic = True

    _info = {
        "channel__chi_index": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "variable",
            "mapping": "node",
            "doc": "the local steepness index",
        },
        "drainage_area": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m**2",
            "mapping": "node",
            "doc": "Upstream accumulated surface area contributing to the node's discharge",
        },
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__upstream_node_order": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array containing downstream-to-upstream ordered list of node IDs",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__steepest_slope": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "The steepest *downhill* slope",
        },
    }

    def __init__(
        self,
        grid,
        reference_concavity=0.5,
        min_drainage_area=1.0e6,
        reference_area=1.0,
        use_true_dx=False,
        clobber=False,
    ):
        """
        Parameters
        ----------
        grid : RasterModelGrid
            A landlab RasterModelGrid.
        reference_concavity : float
            The reference concavity to use in the calculation.
        min_drainage_area : float (m**2)
            The drainage area down to which to calculate chi.
        reference_area : float or None (m**2)
            If None, will default to the mean core cell area on the grid.
            Else, provide a value to use. Essentially becomes a prefactor on the
            value of chi.
        use_true_dx : bool (default False)
            If True, integration to give chi is performed using each value of node
            spacing along the channel (which can lead to a quantization effect,
            and is not preferred by Taylor & Royden). If False, the mean value of
            node spacing along the all channels is assumed everywhere.
        clobber : bool (default False)
            Raise an exception if adding an already existing field.

        """
        super().__init__(grid)

        if grid.at_node["flow__receiver_node"].size != grid.size("node"):
            raise NotImplementedError(
                "A route-to-multiple flow director has been "
                "run on this grid. The landlab development team has not "
                "verified that ChiFinder is compatible with "
                "route-to-multiple methods. Please open a GitHub Issue "
                "to start this process."
            )

        if isinstance(self._grid, RasterModelGrid):
            self._link_lengths = self._grid.length_of_d8
        else:
            self._link_lengths = self._grid.length_of_link  # not tested

        self._reftheta = reference_concavity
        self._min_drainage = min_drainage_area

        self._set_up_reference_area(reference_area)

        self._use_true_dx = use_true_dx
        self._chi = self._grid.add_zeros(
            "channel__chi_index", at="node", clobber=clobber
        )
        self._mask = self._grid.ones(at="node", dtype=bool)
        self._elev = self._grid.at_node["topographic__elevation"]

    def _set_up_reference_area(self, reference_area):
        """Set up and validate reference_area."""
        if reference_area <= 0.0:
            raise ValueError(
                "ChiFinder: reference_area must be positive."
            )  # not tested
        self._A0 = reference_area

    def calculate_chi(self):
        """Calculate local chi indices.

        This is the main method. Call it to calculate local chi indices
        at all points with drainage areas greater than `min_drainage_area`.

        Chi of any node without a defined value is reported as 0. These nodes
        are also identified in the mask retrieved with :func:`hillslope_mask`.
        """
        self._mask.fill(True)
        self._chi.fill(0.0)

        reftheta = self._reftheta
        min_drainage = self._min_drainage
        reference_area = self._A0
        self._set_up_reference_area(reference_area)

        use_true_dx = self._use_true_dx

        upstr_order = self._grid.at_node["flow__upstream_node_order"]
        # get an array of only nodes with A above threshold:
        valid_upstr_order = upstr_order[
            self._grid.at_node["drainage_area"][upstr_order] >= min_drainage
        ]
        valid_upstr_areas = self._grid.at_node["drainage_area"][valid_upstr_order]
        if not use_true_dx:
            chi_integrand = (self._A0 / valid_upstr_areas) ** reftheta
            mean_dx = self.mean_channel_node_spacing(valid_upstr_order)
            self.integrate_chi_avg_dx(
                valid_upstr_order, chi_integrand, self._chi, mean_dx
            )
        else:
            chi_integrand = self._grid.zeros(at="node")
            chi_integrand[valid_upstr_order] = (
                self._A0 / valid_upstr_areas
            ) ** reftheta
            self.integrate_chi_each_dx(valid_upstr_order, chi_integrand, self._chi)
        # stamp over the closed nodes, as it's possible they can receive infs
        # if min_drainage_area < grid.cell_area_at_node
        self._chi[self._grid.status_at_node == self._grid.BC_NODE_IS_CLOSED] = 0.0
        self._mask[valid_upstr_order] = False

    def integrate_chi_avg_dx(
        self, valid_upstr_order, chi_integrand, chi_array, mean_dx
    ):
        """Calculates chi at each channel node by summing chi_integrand.

        This method assumes a uniform, mean spacing between nodes. Method is
        deliberately split out for potential cythonization at a later stage.

        Parameters
        ----------
        valid_upstr_order : array of ints
            nodes in the channel network in upstream order.
        chi_integrand : array of floats
            The value (A0/A)**concavity, in upstream order.
        chi_array : array of floats
            Array in which to store chi.
        mean_dx : float
            The mean node spacing in the network.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> from landlab.components import ChiFinder
        >>> mg = RasterModelGrid((5, 4))
        >>> for nodes in (
        ...     mg.nodes_at_right_edge,
        ...     mg.nodes_at_bottom_edge,
        ...     mg.nodes_at_top_edge,
        ... ):
        ...     mg.status_at_node[nodes] = mg.BC_NODE_IS_CLOSED
        >>> z = mg.node_x.copy()
        >>> z[[5, 13]] = z[6]  # guard nodes
        >>> _ = mg.add_field("topographic__elevation", z, at="node")
        >>> fr = FlowAccumulator(mg, flow_director="D8")
        >>> cf = ChiFinder(mg)
        >>> fr.run_one_step()
        >>> ch_nodes = np.array([4, 8, 12, 5, 9, 13, 6, 10, 14])
        >>> ch_integrand = 3.0 * np.ones(9, dtype=float)  # to make calc clearer
        >>> chi_array = np.zeros(mg.number_of_nodes, dtype=float)
        >>> cf.integrate_chi_avg_dx(ch_nodes, ch_integrand, chi_array, 0.5)
        >>> chi_array.reshape(mg.shape)
        array([[0. , 0. , 0. , 0. ],
               [1.5, 3. , 4.5, 0. ],
               [1.5, 3. , 4.5, 0. ],
               [1.5, 3. , 4.5, 0. ],
               [0. , 0. , 0. , 0. ]])
        """
        receivers = self._grid.at_node["flow__receiver_node"]
        # because chi_array is all zeros, BC cases where node is receiver
        # resolve themselves
        for node, integrand in izip(valid_upstr_order, chi_integrand):
            dstr_node = receivers[node]
            chi_array[node] = chi_array[dstr_node] + integrand
        chi_array *= mean_dx

    def integrate_chi_each_dx(
        self, valid_upstr_order, chi_integrand_at_nodes, chi_array
    ):
        """Calculates chi at each channel node by summing chi_integrand*dx.

        This method accounts explicitly for spacing between each node. Method
        is deliberately split out for potential cythonization at a later
        stage. Uses a trapezium integration method.

        Parameters
        ----------
        valid_upstr_order : array of ints
            nodes in the channel network in upstream order.
        chi_integrand_at_nodes : array of floats
            The value (A0/A)**concavity, in *node* order.
        chi_array : array of floats
            Array in which to store chi.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> from landlab.components import ChiFinder
        >>> mg = RasterModelGrid((5, 4), xy_spacing=3.0)
        >>> for nodes in (
        ...     mg.nodes_at_right_edge,
        ...     mg.nodes_at_bottom_edge,
        ...     mg.nodes_at_top_edge,
        ... ):
        ...     mg.status_at_node[nodes] = mg.BC_NODE_IS_CLOSED
        >>> z = mg.node_x.copy()
        >>> z[[5, 13]] = z[6]  # guard nodes
        >>> _ = mg.add_field("topographic__elevation", z, at="node")
        >>> fr = FlowAccumulator(mg, flow_director="D8")
        >>> cf = ChiFinder(mg)
        >>> fr.run_one_step()
        >>> ch_nodes = np.array([4, 8, 12, 5, 9, 13, 6, 10, 14])
        >>> ch_integrand = 2.0 * np.ones(
        ...     mg.number_of_nodes, dtype=float
        ... )  # to make calc clearer
        >>> chi_array = np.zeros(mg.number_of_nodes, dtype=float)
        >>> cf.integrate_chi_each_dx(ch_nodes, ch_integrand, chi_array)
        >>> chi_array.reshape(mg.shape)
        array([[ 0.        ,  0.        ,  0.        ,  0.        ],
               [ 0.        ,  6.        , 14.48528137,  0.        ],
               [ 0.        ,  6.        , 12.        ,  0.        ],
               [ 0.        ,  6.        , 14.48528137,  0.        ],
               [ 0.        ,  0.        ,  0.        ,  0.        ]])
        >>> from landlab.components import FastscapeEroder
        >>> mg2 = RasterModelGrid((5, 5), xy_spacing=100.0)
        >>> for nodes in (
        ...     mg2.nodes_at_right_edge,
        ...     mg2.nodes_at_bottom_edge,
        ...     mg2.nodes_at_top_edge,
        ... ):
        ...     mg2.status_at_node[nodes] = mg2.BC_NODE_IS_CLOSED
        >>> _ = mg2.add_zeros("topographic__elevation", at="node")
        >>> mg2.at_node["topographic__elevation"][mg2.core_nodes] = (
        ...     mg2.node_x[mg2.core_nodes] / 1000.0
        ... )
        >>> np.random.seed(0)
        >>> mg2.at_node["topographic__elevation"][mg2.core_nodes] += np.random.rand(
        ...     mg2.number_of_core_nodes
        ... )
        >>> fr2 = FlowAccumulator(mg2, flow_director="D8")
        >>> sp2 = FastscapeEroder(mg2, K_sp=0.01)
        >>> cf2 = ChiFinder(
        ...     mg2, min_drainage_area=1.0, reference_concavity=0.5, use_true_dx=True
        ... )
        >>> for i in range(10):
        ...     mg2.at_node["topographic__elevation"][mg2.core_nodes] += 10.0
        ...     fr2.run_one_step()
        ...     sp2.run_one_step(1000.0)
        ...
        >>> fr2.run_one_step()
        >>> output_array = np.zeros(25, dtype=float)
        >>> cf2.integrate_chi_each_dx(
        ...     mg2.at_node["flow__upstream_node_order"],
        ...     np.ones(25, dtype=float),
        ...     output_array,
        ... )
        >>> output_array.reshape(mg2.shape)
        array([[  0. ,   0. ,   0.        ,   0.        ,   0. ],
               [  0. , 100. , 200.        , 382.84271247,   0. ],
               [  0. , 100. , 241.42135624, 341.42135624,   0. ],
               [  0. , 100. , 200.        , 300.        ,   0. ],
               [  0. ,   0. ,   0.        ,   0.        ,   0. ]])
        """
        receivers = self._grid.at_node["flow__receiver_node"]
        links = self._grid.at_node["flow__link_to_receiver_node"]

        # because chi_array is all zeros, BC cases where node is receiver
        # resolve themselves
        half_integrand = 0.5 * chi_integrand_at_nodes
        for node in valid_upstr_order:
            dstr_node = receivers[node]
            dstr_link = links[node]
            if dstr_link != self._grid.BAD_INDEX:
                dstr_length = self._link_lengths[dstr_link]
                half_head_val = half_integrand[node]
                half_tail_val = half_integrand[dstr_node]
                mean_val = half_head_val + half_tail_val
                chi_to_add = mean_val * dstr_length
                chi_array[node] = chi_array[dstr_node] + chi_to_add

    def mean_channel_node_spacing(self, ch_nodes):
        """Calculates the mean spacing between all adjacent channel nodes.

        Parameters
        ----------
        ch_nodes : array of ints
            The nodes within the defined channel network.

        Returns
        -------
        mean_spacing : float (m)
            The mean spacing between all nodes in the network.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> from landlab.components import ChiFinder
        >>> mg = RasterModelGrid((5, 4), xy_spacing=2.0)
        >>> for nodes in (
        ...     mg.nodes_at_right_edge,
        ...     mg.nodes_at_bottom_edge,
        ...     mg.nodes_at_top_edge,
        ... ):
        ...     mg.status_at_node[nodes] = mg.BC_NODE_IS_CLOSED
        >>> z = mg.node_x.copy()
        >>> z[[5, 13]] = z[6]  # guard nodes
        >>> _ = mg.add_field("topographic__elevation", z, at="node")
        >>> fr = FlowAccumulator(mg, flow_director="D8")
        >>> cf = ChiFinder(mg)
        >>> fr.run_one_step()
        >>> ch_nodes = np.array([4, 8, 12, 5, 9, 13, 6, 10, 14])
        >>> cf.mean_channel_node_spacing(ch_nodes)
        2.2761423749153966
        """
        ch_links = self._grid.at_node["flow__link_to_receiver_node"][ch_nodes]
        ch_links_valid = ch_links[ch_links != self._grid.BAD_INDEX]

        valid_link_lengths = self._link_lengths[ch_links_valid]
        return valid_link_lengths.mean()

    @property
    def chi_indices(self):
        """Return the array of channel steepness indices.

        Nodes not in the channel receive zeros.
        """
        return self._chi

    @property
    def hillslope_mask(self):
        """Return a boolean array, False where steepness indices exist."""
        return self._mask

    def best_fit_chi_elevation_gradient_and_intercept(self, ch_nodes=None):
        """Returns least squares best fit for a straight line through a chi
        plot.

        Parameters
        ----------
        ch_nodes : array of ints or None
            Nodes at which to consider chi and elevation values. If None,
            will use all nodes in grid with area greater than the component
            min_drainage_area.

        Returns
        -------
        coeffs : array(gradient, intercept)
            A len-2 array containing the m then z0, where z = z0 + m * chi.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> from landlab.components import ChiFinder
        >>> mg = RasterModelGrid((3, 4))
        >>> for nodes in (
        ...     mg.nodes_at_right_edge,
        ...     mg.nodes_at_bottom_edge,
        ...     mg.nodes_at_top_edge,
        ... ):
        ...     mg.status_at_node[nodes] = mg.BC_NODE_IS_CLOSED
        >>> z = mg.add_field("topographic__elevation", mg.node_x.copy(), at="node")
        >>> z[4:8] = np.array([0.5, 1.0, 2.0, 0.0])
        >>> fr = FlowAccumulator(mg, flow_director="D8")
        >>> cf = ChiFinder(mg, min_drainage_area=1.0, reference_concavity=1.0)
        >>> fr.run_one_step()
        >>> cf.calculate_chi()
        >>> mg.at_node["channel__chi_index"].reshape(mg.shape)[1, :]
        array([0.5, 1. , 2. , 0. ])
        >>> coeffs = cf.best_fit_chi_elevation_gradient_and_intercept()
        >>> np.allclose(np.array([1.0, 0.0]), coeffs)
        True
        """
        if ch_nodes is None:
            good_vals = np.logical_not(self._mask)
        else:
            good_vals = np.array(ch_nodes)  # not tested
        chi_vals = self._chi[good_vals]
        elev_vals = self._grid.at_node["topographic__elevation"][good_vals]
        coeffs = np.polyfit(chi_vals, elev_vals, 1)
        return coeffs

    def nodes_downstream_of_channel_head(self, channel_head):
        """Find and return an array with nodes downstream of channel_head.

        Parameters
        ----------
        channel_head : int
            Node ID of channel head from which to get downstream nodes.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator, ChiFinder
        >>> mg = RasterModelGrid((3, 4))
        >>> for nodes in (
        ...     mg.nodes_at_right_edge,
        ...     mg.nodes_at_bottom_edge,
        ...     mg.nodes_at_top_edge,
        ... ):
        ...     mg.status_at_node[nodes] = mg.BC_NODE_IS_CLOSED
        >>> z = mg.add_field("topographic__elevation", mg.node_x.copy(), at="node")
        >>> z[4:8] = np.array([0.5, 1.0, 2.0, 0.0])
        >>> fr = FlowAccumulator(mg, flow_director="D8")
        >>> fr.run_one_step()
        >>> mg.at_node["flow__receiver_node"]
        array([ 0,  1,  2,  3,  4,  4,  5,  7,  8,  9, 10, 11])
        >>> cf = ChiFinder(mg, min_drainage_area=0.0, reference_concavity=1.0)
        >>> cf.calculate_chi()
        >>> cf.nodes_downstream_of_channel_head(6)
        [6, 5, 4]
        """
        ch_nodes = []
        current_node = channel_head
        while True:
            ch_A = self._grid.at_node["drainage_area"][current_node]
            if ch_A > self._min_drainage:
                ch_nodes.append(current_node)
            next_node = self._grid.at_node["flow__receiver_node"][current_node]
            if next_node == current_node:
                break
            else:
                current_node = next_node
        return ch_nodes

    def create_chi_plot(
        self,
        channel_heads=None,
        label_axes=True,
        symbol="kx",
        plot_line=False,
        line_symbol="r-",
    ):
        """Plots a "chi plot" (chi vs elevation for points in channel network).

        If channel_heads is provided, only the channel nodes downstream of
        the provided points (and with area > min_drainage_area) will be
        plotted.

        Parameters
        ----------

        channel_heads : int, list or array of ints, or None
            Node IDs of channel heads to from which plot downstream.
        label_axes : bool
            If True, labels the axes as "Chi" and "Elevation (m)".
        symbol : str
            A matplotlib-style string for the style to use for the points.
        plot_line : bool
            If True, will plot a linear best fit line through the data cloud.
        line_symbol : str
            A matplotlib-style string for the style to use for the line, if
            plot_line.
        """
        from matplotlib.pyplot import clf
        from matplotlib.pyplot import figure
        from matplotlib.pyplot import plot
        from matplotlib.pyplot import xlabel
        from matplotlib.pyplot import ylabel

        figure("Chi plot")
        clf()
        if channel_heads is not None:
            if plot_line:
                good_nodes = set()
            if isinstance(channel_heads, int):
                channel_heads = [channel_heads]
            for head in channel_heads:
                ch_nodes = self.nodes_downstream_of_channel_head(head)
                plot(
                    self._chi[ch_nodes],
                    self._grid.at_node["topographic__elevation"][ch_nodes],
                    symbol,
                )
                if plot_line:
                    good_nodes.update(ch_nodes)
        else:
            ch_nodes = np.logical_not(self._mask)
            plot(
                self._chi[ch_nodes],
                self._grid.at_node["topographic__elevation"][ch_nodes],
                symbol,
            )
            good_nodes = ch_nodes
        if plot_line:
            coeffs = self.best_fit_chi_elevation_gradient_and_intercept(good_nodes)
            p = np.poly1d(coeffs)
            chirange = np.linspace(
                self._chi[good_nodes].min(), self._chi[good_nodes].max(), 100
            )
            plot(chirange, p(chirange), line_symbol)
        if label_axes:
            ylabel("Elevation (m)")
            xlabel("Chi")

    @property
    def masked_chi_indices(self):
        """Returns a masked array version of the 'channel__chi_index' field.
        This enables easier plotting of the values with.

        :func:`landlab.imshow_grid_at_node` or similar.

        Examples
        --------
        Make a topographic map with an overlay of chi values:

        >>> from landlab import imshow_grid_at_node
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator, FastscapeEroder
        >>> from landlab.components import ChiFinder
        >>> mg = RasterModelGrid((5, 5), xy_spacing=100.0)
        >>> for nodes in (
        ...     mg.nodes_at_right_edge,
        ...     mg.nodes_at_bottom_edge,
        ...     mg.nodes_at_top_edge,
        ... ):
        ...     mg.status_at_node[nodes] = mg.BC_NODE_IS_CLOSED
        >>> _ = mg.add_zeros("topographic__elevation", at="node")
        >>> mg.at_node["topographic__elevation"][mg.core_nodes] = (
        ...     mg.node_x[mg.core_nodes] / 1000.0
        ... )
        >>> np.random.seed(0)
        >>> mg.at_node["topographic__elevation"][mg.core_nodes] += np.random.rand(
        ...     mg.number_of_core_nodes
        ... )
        >>> fr = FlowAccumulator(mg, flow_director="D8")
        >>> sp = FastscapeEroder(mg, K_sp=0.01)
        >>> cf = ChiFinder(mg, min_drainage_area=20000.0)
        >>> for i in range(10):
        ...     mg.at_node["topographic__elevation"][mg.core_nodes] += 10.0
        ...     fr.run_one_step()
        ...     sp.run_one_step(1000.0)
        ...
        >>> fr.run_one_step()
        >>> cf.calculate_chi()

        >>> imshow_grid_at_node(mg, "topographic__elevation", allow_colorbar=False)
        >>> imshow_grid_at_node(
        ...     mg, cf.masked_chi_indices, color_for_closed=None, cmap="winter"
        ... )
        """
        return np.ma.array(self._chi, mask=self._mask)



================================================
File: concentration_tracker/__init__.py
================================================
from .concentration_tracker_for_diffusion import ConcentrationTrackerForDiffusion
from .concentration_tracker_for_space import ConcentrationTrackerForSpace

__all__ = [
    "ConcentrationTrackerForDiffusion",
    "ConcentrationTrackerForSpace",
]



================================================
File: concentration_tracker/concentration_tracker_for_diffusion.py
================================================
"""
Created on Wed May 31 11:41:20 2023

@author: LaurentRoberge
"""

import numpy as np

from landlab import Component
from landlab import LinkStatus
from landlab.grid.mappers import map_value_at_max_node_to_link
from landlab.utils.return_array import return_array_at_node


class ConcentrationTrackerForDiffusion(Component):
    """Track the concentration of any user-defined property.

    This component tracks the concentration of any user-defined property of
    sediment using a mass balance approach in which the concentration :math:`C`
    is calculated as:

    .. math::

        ∂CH / ∂t = [-(∂q_x C_x) / ∂x - (∂q_y C_y) / ∂y] + C_br * H_brw

    where :math:`H` is sediment depth, :math:`q_x` and :math:`q_y` are sediment
    fluxed in the x and y directions, :math:`C_br` is concentration in parent
    bedrock, and :math:`H_brw` is the height of bedrock weathered into soil.

    .. note::

        This component requires a soil flux field calculated by a hillslope diffusion
        component. This component is implemented by applying a :meth:`start_tracking`
        method immediately before every diffusion step and a :meth:`stop_tracking`
        method immediately after every diffusion step. These methods are applied instead
        of the typical :meth:`run_one_step` method. See the docstring examples below.

        Currently, this component will only work if coupled with the
        :class:`~.DepthDependentDiffuser` or the :class:`~.DepthDependentTaylorDiffuser`
        (without the dynamic timestep option).

    Examples
    --------

    A 1-D hillslope:

    >>> import numpy as np
    >>> from landlab import NodeStatus, RasterModelGrid
    >>> from landlab.components import DepthDependentDiffuser
    >>> from landlab.components import ConcentrationTrackerForDiffusion

    >>> mg = RasterModelGrid((3, 5), xy_spacing=2.0)

    >>> mg.set_status_at_node_on_edges(
    ...     right=NodeStatus.CLOSED,
    ...     top=NodeStatus.CLOSED,
    ...     left=NodeStatus.CLOSED,
    ...     bottom=NodeStatus.CLOSED,
    ... )
    >>> mg.status_at_node[5] = NodeStatus.FIXED_VALUE
    >>> mg.status_at_node.reshape(mg.shape)
    array([[4, 4, 4, 4, 4],
           [1, 0, 0, 0, 4],
           [4, 4, 4, 4, 4]], dtype=uint8)

    >>> mg.at_node["sediment_property__concentration"] = [
    ...     [0.0, 0.0, 0.0, 0.0, 0.0],
    ...     [0.0, 0.0, 0.0, 1.0, 0.0],
    ...     [0.0, 0.0, 0.0, 0.0, 0.0],
    ... ]
    >>> mg.at_node["soil__depth"] = mg.node_x.copy()
    >>> mg.at_node["bedrock__elevation"] = mg.node_x.copy()
    >>> mg.at_node["topographic__elevation"] = (
    ...     mg.at_node["soil__depth"] + mg.at_node["bedrock__elevation"]
    ... )
    >>> _ = mg.add_zeros("soil_production__rate", at="node")

    >>> ddd = DepthDependentDiffuser(mg)
    >>> ct = ConcentrationTrackerForDiffusion(mg)
    >>> ct.start_tracking()
    >>> ddd.run_one_step(1.0)
    >>> ct.stop_tracking(1.0)

    >>> mg.at_node["topographic__elevation"].reshape(mg.shape)
    array([[ 0. ,  4.        ,  8.        , 12.        , 16. ],
           [ 0. ,  4.11701964,  8.01583689, 11.00247875, 16. ],
           [ 0. ,  4.        ,  8.        , 12.        , 16. ]])
    >>> mg.at_node["sediment_property__concentration"].reshape(mg.shape)
    array([[0. , 0. , 0.        , 0. , 0. ],
           [0. , 0. , 0.24839685, 1. , 0. ],
           [0. , 0. , 0.        , 0. , 0. ]])

    Now, a 2-D pyramid-shaped hillslope.

    >>> mg = RasterModelGrid((5, 5), xy_spacing=2.0)

    >>> mg.at_node["sediment_property__concentration"] = [
    ...     [0.0, 0.0, 0.0, 0.0, 0.0],
    ...     [0.0, 0.0, 0.0, 0.0, 0.0],
    ...     [0.0, 0.0, 1.0, 0.0, 0.0],
    ... ]
    >>> h = mg.add_full("soil__depth", 2.0, at="node")
    >>> z_br = mg.add_field(
    ...     "bedrock__elevation",
    ...     8.0 - abs(4.0 - mg.node_x) - abs(4.0 - mg.node_y),
    ...     at="node",
    ... )
    >>> z = mg.add_field("topographic__elevation", z_br + h, at="node")
    >>> _ = mg.add_zeros("soil_production__rate", at="node")

    >>> ddd = DepthDependentDiffuser(mg)
    >>> ct = ConcentrationTrackerForDiffusion(mg)
    >>> ct.start_tracking()
    >>> ddd.run_one_step(1.0)
    >>> ct.stop_tracking(1.0)

    >>> mg.at_node["topographic__elevation"][mg.core_nodes].reshape((3, 3))
    array([[6. ,        7.13533528, 6. ],
           [7.13533528, 8.27067057, 7.13533528],
           [6. ,        7.13533528, 6. ]])
    >>> mg.at_node["sediment_property__concentration"][mg.core_nodes].reshape((3, 3))
    array([[0.        , 0.38079708, 0. ],
           [0.38079708, 1.        , 0.38079708],
           [0.        , 0.38079708, 0. ]])

    And running one more step.

    >>> ct.start_tracking()
    >>> ddd.run_one_step(1.0)
    >>> ct.stop_tracking(1.0)

    >>> mg.at_node["topographic__elevation"][mg.core_nodes].reshape((3, 3))
    array([[5.52060315, 6.62473963, 5.52060315],
           [6.62473963, 8.00144598, 6.62473963],
           [5.52060315, 6.62473963, 5.52060315]])
    >>> mg.at_node["sediment_property__concentration"][mg.core_nodes].reshape((3, 3))
    array([[0.09648071, 0.44750673, 0.09648071],
           [0.44750673, 1.        , 0.44750673],
           [0.09648071, 0.44750673, 0.09648071]])

    Finally, the same 2D hillslope now using the
    :class:`~.DepthDependentTaylorDiffuser`. Note that the timestep must be smaller
    than 1 to maintain stability in the diffusion calculation. Typically, one could
    use the dynamic timestepping option. However, here it will provide incorrect
    soil flux values to the :class:`~.ConcentrationTrackerForDiffusion`, which
    cannot do sub-timestep calculations. Use the ``if_unstable="warn"`` flag when
    instantiating the Taylor diffuser and pick a timestep that is stable.

    >>> from landlab.components import DepthDependentTaylorDiffuser
    >>> mg = RasterModelGrid((5, 5), xy_spacing=2.0)

    >>> mg.at_node["sediment_property__concentration"] = [
    ...     [0.0, 0.0, 0.0, 0.0, 0.0],
    ...     [0.0, 0.0, 0.0, 0.0, 0.0],
    ...     [0.0, 0.0, 1.0, 0.0, 0.0],
    ... ]
    >>> h = mg.add_full("soil__depth", 2.0, at="node")
    >>> z_br = mg.add_field(
    ...     "bedrock__elevation",
    ...     8.0 - abs(4.0 - mg.node_x) - abs(4.0 - mg.node_y),
    ...     at="node",
    ... )
    >>> z = mg.add_field("topographic__elevation", z_br + h, at="node")
    >>> _ = mg.add_zeros("soil_production__rate", at="node")

    >>> ddtd = DepthDependentTaylorDiffuser(mg, if_unstable="warn")
    >>> ct = ConcentrationTrackerForDiffusion(mg)
    >>> ct.start_tracking()
    >>> ddtd.run_one_step(0.4)
    >>> ct.stop_tracking(0.4)

    >>> mg.at_node["topographic__elevation"][mg.core_nodes].reshape((3, 3))
    array([[6.        , 7.30826823, 6.        ],
           [7.30826823, 8.61653645, 7.30826823],
           [6.        , 7.30826823, 6.        ]])
    >>> mg.at_node["sediment_property__concentration"][mg.core_nodes].reshape((3, 3))
    array([[0.        , 0.26436925, 0.        ],
           [0.26436925, 1.        , 0.26436925],
           [0.        , 0.26436925, 0.        ]])
    """

    _name = "ConcentrationTrackerForDiffusion"

    _unit_agnostic = True

    _cite_as = ""

    _info = {
        "soil__depth": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of soil or weathered bedrock",
        },
        "soil__flux": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m^2/yr",
            "mapping": "link",
            "doc": "flux of soil in direction of link",
        },
        "soil_production__rate": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m/yr",
            "mapping": "node",
            "doc": "rate of soil production at nodes",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "sediment_property__concentration": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "-/m^3",
            "mapping": "node",
            "doc": "Mass concentration of property per unit volume of sediment",
        },
        "bedrock_property__concentration": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "-/m^3",
            "mapping": "node",
            "doc": "Mass concentration of property per unit volume of bedrock",
        },
    }

    def __init__(
        self,
        grid,
        concentration_initial=0,
        concentration_in_bedrock=0,
        concentration_from_weathering=None,
    ):
        """
        Parameters
        ----------
        grid: ModelGrid
            Landlab ModelGrid object
        concentration_initial: float, array, or str, optional
            Initial concentration in soil/sediment as either a scalar, array,
            or the name of an existing node field, -/m^3.
        concentration_in_bedrock: float, array, or str, optional
            Concentration in bedrock as either a scalar, array, or the name
            of an existing node field, -/m^3.
        concentration_from_weathering: float or array, optional
            Concentration generated during the weathering process, -/m^3.
            Defaults to ``None``, which causes all weathered bedrock to retain its
            original parent material concentration (`concentration_in_bedrock`)
            as it weathers to soil. Use this parameter to differentiate between
            the concentration in weathered material compared to its parent bedrock.
        """

        super().__init__(grid)
        # Store grid and parameters

        # use setters for conc_init, conc_br, and conc_w defined below
        self.conc_init = concentration_initial
        self.conc_br = concentration_in_bedrock

        # get reference to inputs
        self._soil__depth = self._grid.at_node["soil__depth"]
        self._soil__depth_old = self._soil__depth.copy()
        self._soil_prod_rate = self._grid.at_node["soil_production__rate"]
        self._flux = self._grid.at_link["soil__flux"]

        # create outputs if necessary and get reference.
        self.initialize_output_fields()

        # Define concentration field (if all zeros, then add conc_init)
        if np.allclose(self._grid.at_node["sediment_property__concentration"], 0.0):
            self._grid.at_node["sediment_property__concentration"] += self.conc_init
        self._concentration = self._grid.at_node["sediment_property__concentration"]

        if np.allclose(self._grid.at_node["bedrock_property__concentration"], 0.0):
            self._grid.at_node["bedrock_property__concentration"] += self.conc_br
        self.conc_br = self._grid.at_node["bedrock_property__concentration"]

        # use setter for conc_w defined below
        self.conc_w = concentration_from_weathering

        # Sediment property concentration field (at links, to calculate dqconc_dx)
        self._conc_at_links = np.zeros(self._grid.number_of_links)

        # Sediment property mass field (at links, to calculate dqconc_dx)
        self._qconc_at_links = np.zeros(self._grid.number_of_links)

    @property
    def conc_init(self):
        """Initial concentration in soil/sediment (kg/m^3)."""
        return self._conc_init

    @property
    def conc_br(self):
        """Concentration in bedrock (kg/m^3)."""
        return self._conc_br

    @property
    def conc_w(self):
        """Concentration from the weathering process (kg/m^3)."""
        return self._conc_w

    @conc_init.setter
    def conc_init(self, new_val):
        if np.any(new_val < 0.0):
            raise ValueError("Concentration cannot be negative")
        self._conc_init = return_array_at_node(self._grid, new_val)

    @conc_br.setter
    def conc_br(self, new_val):
        if np.any(new_val < 0.0):
            raise ValueError("Concentration in bedrock cannot be negative")
        self._conc_br = return_array_at_node(self._grid, new_val)

    @conc_w.setter
    def conc_w(self, new_val):
        if new_val is None:
            new_val = self._conc_br
        if np.any(new_val < 0.0):
            raise ValueError("Concentration cannot be negative")
        self._conc_w = new_val

    def _copy_old_soil_depth(self):
        """Store a copy of soil depth. This is used as the old soil depth when
        calculating changes in concentration.
        """

        self._soil__depth_old = self._soil__depth.copy()

    def _calc_concentration(self, dt):
        """Calculate change in concentration for a time period 'dt'.

        Parameters
        ----------
        dt: float (time)
            The imposed timestep.
        """

        # Define concentration at previous timestep
        conc_old = self._concentration.copy()

        # Map concentration from nodes to links (following soil flux direction)
        self._conc_at_links = map_value_at_max_node_to_link(
            self._grid, "topographic__elevation", "sediment_property__concentration"
        )
        # Replace values with zero for all INACTIVE links
        self._conc_at_links[self._grid.status_at_link == LinkStatus.INACTIVE] = 0.0

        # Calculate qconc at links (sediment flux times concentration)
        self._qconc_at_links = self._grid.at_link["soil__flux"] * self._conc_at_links

        # Calculate flux concentration divergence
        dqconc_dx = self._grid.calc_flux_div_at_node(self._qconc_at_links)

        # Calculate other components of mass balance equation
        is_soil = self._soil__depth > 0.0

        old_depth_over_new = np.divide(
            self._soil__depth_old, self._soil__depth, where=is_soil
        )
        old_depth_over_new[~is_soil] = 0.0

        dt_over_depth = np.divide(dt, self._soil__depth, where=is_soil)
        dt_over_depth[~is_soil] = 0.0

        conc_local = conc_old * old_depth_over_new
        conc_from_weathering = np.divide(
            self._conc_w * self._soil_prod_rate * dt, self._soil__depth, where=is_soil
        )

        # Calculate concentration
        self._concentration[:] = (
            conc_local + conc_from_weathering + dt_over_depth * (-dqconc_dx)
        )

        self._concentration[~is_soil] = 0.0

    def start_tracking(self):
        """Stores values necessary for calculating changes in concentration.
        This method must be called prior to running the sediment flux component
        that changes physical properties of bedrock, soil, and/or topography.
        """

        self._copy_old_soil_depth()

    def stop_tracking(self, dt):
        """Calculate changes in concentration that have occurred in the timestep
        since tracking was started. This method must be called after running the
        sediment flux component that changes physical properties of bedrock,
        soil, and/or topography.

        Parameters
        ----------
        dt: float (time)
            The imposed timestep.
        """

        self._calc_concentration(dt)

    def run_one_step(self):
        """run_one_step is not implemented for this component."""
        raise NotImplementedError("run_one_step")



================================================
File: concentration_tracker/concentration_tracker_for_space.py
================================================
"""
Created on Fri Jun 16 15:06:50 2023

@author: LaurentRoberge
"""

import numpy as np

from landlab import Component
from landlab import NodeStatus
from landlab.utils.return_array import return_array_at_node


class ConcentrationTrackerForSpace(Component):
    """This component tracks the concentration of any user-defined property of
    sediment using a mass balance approach in which concentration :math:`C_s`
    is calculated as:

    .. math::

                        ∂C_sH/∂t = C_s_w*D_s_w + C_s*E_s

    where :math:`H` is sediment depth, :math:`C_s_w` is concentration in
    sediment suspended in the water column, :math:`D_s_w` is volumetric
    depositional flux of sediment from the water column per unit bed area, and
    :math:`E_s` is volumetric erosional flux of sediment from the bed per unit
    bed area.

    .. note::

        This component requires the "sediment__outflux", "bedrock__erosion_flux"
        "sediment__erosion_flux", and "sediment__deposition_flux" fields
        calculated by the :class:`~.SpaceLargeScaleEroder` component. This
        component does not use the typical run_one_step(dt) method. Instead,
        a start_tracking() method is implemented immediately before every
        :class:`~.SpaceLargeScaleEroder` step and a stop_tracking(dt) method
        immediately after every :class:`~.SpaceLargeScaleEroder` step.
        See the docstring examples below.

        The required inputs "phi", "fraction_fines", and "settling_velocity"
        must have the same value as those used for the instance of
        :class:`~.SpaceLargeScaleEroder`.

    Examples
    --------

    A 1-D stream channel:

    >>> import numpy as np
    >>> from landlab import NodeStatus, RasterModelGrid
    >>> from landlab.components import PriorityFloodFlowRouter
    >>> from landlab.components import SpaceLargeScaleEroder
    >>> from landlab.components import ConcentrationTrackerForSpace

    >>> mg = RasterModelGrid((3, 5), xy_spacing=10.0)

    >>> mg.set_status_at_node_on_edges(
    ...     right=NodeStatus.CLOSED,
    ...     top=NodeStatus.CLOSED,
    ...     left=NodeStatus.CLOSED,
    ...     bottom=NodeStatus.CLOSED,
    ... )
    >>> mg.status_at_node[5] = mg.BC_NODE_IS_FIXED_VALUE
    >>> mg.status_at_node.reshape(mg.shape)
    array([[4, 4, 4, 4, 4],
           [1, 0, 0, 0, 4],
           [4, 4, 4, 4, 4]], dtype=uint8)

    >>> mg.at_node["sediment_property__concentration"] = [
    ...     [0.0, 0.0, 0.0, 0.0, 0.0],
    ...     [0.0, 0.0, 0.0, 1.0, 0.0],
    ...     [0.0, 0.0, 0.0, 0.0, 0.0],
    ... ]
    >>> mg.at_node["soil__depth"] = [
    ...     [1.0, 1.0, 1.0, 1.0, 1.0],
    ...     [1.0, 1.0, 1.0, 1.0, 1.0],
    ...     [1.0, 1.0, 1.0, 1.0, 1.0],
    ... ]
    >>> mg.at_node["bedrock__elevation"] = mg.node_x / 100
    >>> mg.at_node["topographic__elevation"] = (
    ...     mg.at_node["soil__depth"] + mg.at_node["bedrock__elevation"]
    ... )

    >>> fr = PriorityFloodFlowRouter(mg)
    >>> fr.run_one_step()
    >>> sp = SpaceLargeScaleEroder(mg, phi=0, F_f=0, v_s=1)
    >>> ct = ConcentrationTrackerForSpace(
    ...     mg,
    ...     phi=0,
    ...     fraction_fines=0,
    ...     settling_velocity=1,
    ... )

    >>> for i in range(40):
    ...     fr.run_one_step()
    ...     ct.start_tracking()
    ...     sp.run_one_step(10.0)
    ...     ct.stop_tracking(10.0)
    ...

    Erosion has lowered the topography and reduced channel bed sediment depth.
    >>> np.allclose(
    ...     mg.at_node["topographic__elevation"][mg.core_nodes],
    ...     np.array([1.00292211, 1.00902572, 1.0258774]),
    ... )
    True
    >>> np.allclose(
    ...     mg.at_node["soil__depth"][mg.core_nodes],
    ...     np.array([0.90294696, 0.80909071, 0.72601329]),
    ... )
    True

    Some high-concentration sediment has been transported from upstream to be
    deposited on the channel bed further downstream.
    >>> np.allclose(
    ...     mg.at_node["sediment_property__concentration"][mg.core_nodes],
    ...     np.array([0.0496547, 0.0997232, 0.9999151]),
    ... )
    True


    Now, a 2-D landscape with stream channels. All boundaries are closed except
    for Node 0, which is the outlet of the catchment.

    >>> mg = RasterModelGrid((6, 6), xy_spacing=10.0)

    >>> mg.set_status_at_node_on_edges(
    ...     right=NodeStatus.CLOSED,
    ...     top=NodeStatus.CLOSED,
    ...     left=NodeStatus.CLOSED,
    ...     bottom=NodeStatus.CLOSED,
    ... )
    >>> mg.status_at_node[0] = mg.BC_NODE_IS_FIXED_VALUE
    >>> mg.status_at_node.reshape(mg.shape)
    array([[4, 4, 4, 4, 4, 4],
           [4, 0, 0, 0, 0, 4],
           [4, 0, 0, 0, 0, 4],
           [4, 0, 0, 0, 0, 4],
           [4, 0, 0, 0, 0, 4],
           [1, 4, 4, 4, 4, 4]], dtype=uint8)


    >>> mg.at_node["sediment_property__concentration"] = [
    ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    ... ]
    >>> mg.at_node["soil__depth"] = [
    ...     [1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
    ...     [1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
    ...     [1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
    ...     [1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
    ...     [1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
    ...     [0.0, 1.0, 1.0, 1.0, 1.0, 1.0],
    ... ]

    # Add noise to the bedrock to create some topographic structure.
    >>> np.random.seed(5)
    >>> mg.add_zeros("bedrock__elevation", at="node")
    >>> mg.at_node["bedrock__elevation"] += np.random.rand(mg.number_of_nodes) / 100
    >>> mg.at_node["bedrock__elevation"][0] = 0

    >>> mg.at_node["topographic__elevation"] = (
    ...     mg.at_node["soil__depth"] + mg.at_node["bedrock__elevation"]
    ... )

    # Instantiate components.
    >>> fr = PriorityFloodFlowRouter(mg)
    >>> fr.run_one_step()
    >>> sp = SpaceLargeScaleEroder(mg, phi=0, F_f=0, v_s=1)
    >>> ct = ConcentrationTrackerForSpace(
    ...     mg,
    ...     phi=0,
    ...     fraction_fines=0,
    ...     settling_velocity=1,
    ... )

    # Run SPACE for 1,000 years to generate a fluvial network.
    >>> for i in range(1000):
    ...     mg.at_node["bedrock__elevation"][mg.core_nodes] += 0.001
    ...     mg.at_node["topographic__elevation"][:] = (
    ...         mg.at_node["soil__depth"] + mg.at_node["bedrock__elevation"]
    ...     )
    ...     fr.run_one_step()
    ...     sp.run_one_step(1.0)
    ...

    # Set high concentration at a headwater node to trace sediment downstream.
    >>> mg.at_node["sediment_property__concentration"][22] += 1

    >>> for i in range(100):
    ...     mg.at_node["bedrock__elevation"][mg.core_nodes] += 0.001
    ...     mg.at_node["topographic__elevation"][:] = (
    ...         mg.at_node["soil__depth"] + mg.at_node["bedrock__elevation"]
    ...     )
    ...     fr.run_one_step()
    ...     ct.start_tracking()
    ...     sp.run_one_step(1.0)
    ...     ct.stop_tracking(1.0)
    ...

    Some high-concentration sediment has been transported from the headwaters
    to be deposited on the channel bed further downstream. We can trace this
    sediment and see where the channel lies within the landscape.
    >>> np.allclose(
    ...     mg.at_node["sediment_property__concentration"][mg.core_nodes],
    ...     np.array(
    ...         [
    ...             [0.0288311, 0.0447778, 0.0, 0.0],
    ...             [0.0, 0.0, 0.0598574, 0.0],
    ...             [0.0, 0.0, 0.0, 0.9548471],
    ...             [0.0, 0.0, 0.0, 0.0],
    ...         ]
    ...     ).flatten(),
    ... )
    True

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    CITATION

    """

    _name = "ConcentrationTrackerForSpace"

    _unit_agnostic = True

    _cite_as = """
    CITATION
    """

    _info = {
        "soil__depth": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of soil or weathered bedrock",
        },
        "sediment__outflux": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m^3/yr",
            "mapping": "node",
            "doc": "Sediment flux (volume per unit time of sediment leaving each node)",
        },
        "bedrock__erosion_flux": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m/yr",
            "mapping": "node",
            "doc": (
                "Bedrock erosion flux from bedrock to water column (depth eroded per"
                " unit time)"
            ),
        },
        "sediment__erosion_flux": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m/yr",
            "mapping": "node",
            "doc": (
                "Sediment erosion flux from bed to water column (depth eroded per"
                " unit time)"
            ),
        },
        "sediment__deposition_flux": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m/yr",
            "mapping": "node",
            "doc": (
                "Sediment deposition flux from water column to bed (depth deposited"
                " per unit time)"
            ),
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "sediment_property__concentration": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "-/m^3",
            "mapping": "node",
            "doc": "Mass concentration of property per unit volume of sediment",
        },
        "bedrock_property__concentration": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "-/m^3",
            "mapping": "node",
            "doc": "Mass concentration of property per unit volume of bedrock",
        },
    }

    def __init__(
        self,
        grid,
        phi: float | None = None,
        fraction_fines: float | None = None,
        settling_velocity: float | None = None,
        concentration_initial=0,
        concentration_in_bedrock=0,
    ):
        """
        Parameters
        ----------
        grid: ModelGrid
            Landlab ModelGrid object
        phi: float
            Sediment porosity, [-]
        fraction_fines: float
            Fraction of permanently suspendable fines in bedrock, [-].
        settling_velocity: float
            Net effective settling velocity for chosen grain size metric, [L/T]
        concentration_initial: positive float, array, or field name (optional)
            Initial concentration in soil/sediment, [-/L^3]
        concentration_in_bedrock: positive float, array, or field name (optional)
            Concentration in bedrock, [-/L^3]
        """

        if phi is None:
            raise ValueError(
                "`phi` is a required input parameter. "
                "It must have the same value as the `phi` input "
                "parameter used for the `SpaceLargeScaleEroder` "
                "instance to which this component is coupled. "
                "See the docstring in each component for details."
            )
        if fraction_fines is None:
            raise ValueError(
                "`fraction_fines` is a required input parameter. "
                "It must have the same value as the same input "
                "parameter used for the `SpaceLargeScaleEroder` "
                "instance to which this component is coupled. "
                "The parameter is named `F_f` in SpaceLargeScaleEroder. "
                "See the docstring in each component for details."
            )
        if settling_velocity is None:
            raise ValueError(
                "`settling_velocity` is a required input parameter. "
                "It must have the same value as the same input "
                "parameter used for the `SpaceLargeScaleEroder` "
                "instance to which this component is coupled. "
                "It is named `v_s` in SpaceLargeScaleEroder. "
                "See the docstring in each component for details."
            )

        super().__init__(grid)
        # Store grid and parameters

        # use setters for C_init, C_br defined below
        self.C_init = concentration_initial
        self.C_br = concentration_in_bedrock

        # get reference to inputs
        self._phi = phi
        self._fraction_fines = fraction_fines
        self._settling_velocity = settling_velocity
        self._soil__depth = self._grid.at_node["soil__depth"]
        self._soil__depth_old = self._soil__depth.copy()
        self._Qs_out = self._grid.at_node["sediment__outflux"]

        # Define variables used for internal calculations
        self._cell_area = self._grid.cell_area_at_node
        self._C_sw = np.zeros(self._grid.number_of_nodes)
        self._QsCsw_in = np.zeros(self._grid.number_of_nodes)
        self._QsCsw_out = np.zeros(self._grid.number_of_nodes)
        self._BED_ero_depo_term = np.zeros(self._grid.number_of_nodes)

        # create outputs if necessary and get reference.
        self.initialize_output_fields()

        # Define concentration field (if all zeros, then add C_init)
        if np.allclose(self._grid.at_node["sediment_property__concentration"], 0.0):
            self._grid.at_node["sediment_property__concentration"] += self.C_init
        self._concentration = self._grid.at_node["sediment_property__concentration"]

        if np.allclose(self._grid.at_node["bedrock_property__concentration"], 0.0):
            self._grid.at_node["bedrock_property__concentration"] += self.C_br
        self.C_br = self._grid.at_node["bedrock_property__concentration"]

        if phi >= 1.0:
            raise ValueError("Porosity must be < 1.0")

        if fraction_fines > 1.0:
            raise ValueError("Fraction of fines must be <= 1.0")

        if phi < 0.0:
            raise ValueError("Porosity must be > 0.0")

        if fraction_fines < 0.0:
            raise ValueError("Fraction of fines must be > 0.0")

    @property
    def C_init(self):
        """Initial concentration in soil/sediment (kg/m^3)."""
        return self._C_init

    @property
    def C_br(self):
        """Concentration in bedrock (kg/m^3)."""
        return self._C_br

    @C_init.setter
    def C_init(self, new_val):
        if np.any(new_val < 0.0):
            raise ValueError("Concentration in sediment cannot be negative")
        self._C_init = return_array_at_node(self._grid, new_val)

    @C_br.setter
    def C_br(self, new_val):
        if np.any(new_val < 0.0):
            raise ValueError("Concentration in bedrock cannot be negative")
        self._C_br = return_array_at_node(self._grid, new_val)

    def _copy_old_soil_depth(self):
        """Store a copy of soil depth. This is used as the old soil depth when
        calculating changes in concentration.
        """

        self._soil__depth_old = self._soil__depth.copy()

    def _calc_concentration_watercolumn_and_bed(self, dt):
        """Calculate change in concentration within sediment transported in
        the water column and within sediment on the bed for a time period 'dt'.

        Parameters
        ----------

        dt: float (time)
            The imposed timestep.
        """
        # Define values generated by SPACE/SpaceLargeScaleEroder
        flow_receivers = self._grid.at_node["flow__receiver_node"]
        q = self._grid.at_node["surface_water__discharge"]
        Er = self._grid.at_node["bedrock__erosion_flux"]
        Es = self._grid.at_node["sediment__erosion_flux"]
        D_sw = self._grid.at_node["sediment__deposition_flux"]

        # Calculate portions of equation that have soil depth as denominator
        is_soil = self._soil__depth > 0.0

        old_depth_over_new = np.divide(
            self._soil__depth_old, self._soil__depth, where=is_soil
        )
        old_depth_over_new[~is_soil] = 0.0

        dt_over_depth = np.divide(dt, self._soil__depth, where=is_soil)
        dt_over_depth[~is_soil] = 0.0

        # Calculate mass balance terms that don't need downstream iteration
        WC_Es_term = Es * self._cell_area
        WC_Er_term = (1 - self._fraction_fines) * Er * self._cell_area
        WC_denominator_term = np.ones(np.shape(q))
        WC_denominator_term[q != 0] = (
            1 + self._settling_velocity * self._cell_area[q != 0] / q[q != 0]
        )
        BED_C_local_term = self._concentration * old_depth_over_new

        # Get stack of node ids from top to bottom of channel network
        node_status = self._grid.status_at_node
        stack_flip_ud = np.flipud(self._grid.at_node["flow__upstream_node_order"])
        # Select core nodes where qs >0
        stack_flip_ud_sel = stack_flip_ud[
            (node_status[stack_flip_ud] == NodeStatus.CORE) & (q[stack_flip_ud] > 0.0)
        ]

        # zero out array values that were updated in the old stack
        self._C_sw[:] = 0
        self._QsCsw_in[:] = 0
        self._BED_ero_depo_term[:] = 0

        # Iterate concentration calc (first BED, then WC) at each node
        for node_id in stack_flip_ud_sel:
            # Calculate QsCsw_out (i.e., QsCs in the water column)
            self._QsCsw_out[node_id] = (
                self._QsCsw_in[node_id]
                + self._concentration[node_id] * WC_Es_term[node_id]
                + self.C_br[node_id] * WC_Er_term[node_id]
            ) / WC_denominator_term[node_id]

            # Send QsCsw_out values to flow receiver nodes
            self._QsCsw_in[flow_receivers[node_id]] += self._QsCsw_out[node_id]

            # Divide QsCsw_out (from above) by Qs_out to get C_sw
            if self._Qs_out[node_id] > 0:
                self._C_sw[node_id] = self._QsCsw_out[node_id] / self._Qs_out[node_id]
            else:
                self._C_sw[node_id] = 0.0

            # Calculate BED erosion/deposition term (requires C_sw from above)
            self._BED_ero_depo_term[node_id] = (
                self._C_sw[node_id] * D_sw[node_id]
                - self._concentration[node_id] * Es[node_id]
            ) / (1 - self._phi)

            # Calculate BED concentration
            self._concentration[node_id] = (
                BED_C_local_term[node_id]
                + dt_over_depth[node_id] * self._BED_ero_depo_term[node_id]
            )

            self._concentration[~is_soil] = 0.0

    def start_tracking(self):
        """Stores values necessary for calculating changes in concentration.
        This method must be called prior to running the sediment flux component
        that changes physical properties of bedrock, soil, and/or topography.
        """

        self._copy_old_soil_depth()

    def stop_tracking(self, dt):
        """Calculate changes in concentration that have occurred in the timestep
        since tracking was started. This method must be called after running the
        sediment flux component that changes physical properties of bedrock,
        soil, and/or topography.

        Parameters
        ----------
        dt: float (time)
            The imposed timestep.
        """

        self._calc_concentration_watercolumn_and_bed(dt)

    def run_one_step(self):
        """run_one_step is not implemented for this component."""
        raise NotImplementedError("run_one_step()")



================================================
File: depression_finder/__init__.py
================================================
from .lake_mapper import DepressionFinderAndRouter

__all__ = ["DepressionFinderAndRouter"]



================================================
File: depression_finder/cfuncs.pyx
================================================
cimport cython

from landlab.core.messages import warning_message

# https://cython.readthedocs.io/en/stable/src/userguide/fusedtypes.html
ctypedef fused id_t:
    cython.integral
    long long


@cython.boundscheck(False)
cpdef find_lowest_node_on_lake_perimeter_c(
    const id_t [:, :] node_nbrs,
    id_t [:] flood_status,
    cython.floating [:] elev,
    id_t [:] nodes_this_depression,
    long pit_count,
    double BIG_ELEV,
):
    """Locate the lowest node on the margin of the "lake".

    Parameters
    ----------
    node_nbrs : (nnodes, 4) or (nnodes, 8) array of int
        The node neighbors, as stored by a DepressionFinderAndRouter
        component
    flood_status : nnodes array of int
        The node flooded status at the point of the function call, as stored
        by a DepressionFinderAndRouter component
    elev : nnodes array of float
        The elevations of each node in the grid
    nodes_this_depression : nnodes array of int
        Nodes that form a pit, followed by padding values to make up a nnodes-
        long array. This should be passed in with the first value as the
        pit node, then padding values, but it will be updated in place to
        reflect the nodes in the lake.
    pit_count : int
        The number of nodes currently in the lake.

    Returns
    -------
    (int, int)
        (Lowest node on the perimeter of a depression, updated pit_count)

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import FlowAccumulator, DepressionFinderAndRouter
    >>> from landlab.components.flow_routing.cfuncs import (
    ...     find_lowest_node_on_lake_perimeter_c
    ... )
    >>> mg = RasterModelGrid((7, 7), xy_spacing=0.5)
    >>> z = mg.add_field('node', 'topographic__elevation', mg.node_x.copy())
    >>> z += 0.01 * mg.node_y
    >>> mg.at_node['topographic__elevation'].reshape(mg.shape)[2:5, 2:5] *= 0.1
    >>> fr = FlowAccumulator(mg, flow_director='D8')
    >>> fr.run_one_step()  # the flow "gets stuck" in the hole
    >>> df = DepressionFinderAndRouter(mg)

    >>> node_nbrs = df._node_nbrs
    >>> flood_status = df.flood_status
    >>> elev = df._elev
    >>> BIG_ELEV = df._BIG_ELEV
    >>> nodes_this_depression = mg.zeros('node', dtype=int)
    >>> nodes_this_depression[0] = 16
    >>> pit_count = 1

    >>> find_lowest_node_on_lake_perimeter_c(
    ...     node_nbrs, flood_status, elev, nodes_this_depression, pit_count,
    ...     BIG_ELEV
    ... )
    (23, 1)
    >>> nodes_this_depression[1] = 8
    >>> pit_count = 2
    >>> find_lowest_node_on_lake_perimeter_c(
    ...     node_nbrs, flood_status, elev, nodes_this_depression, pit_count,
    ...     BIG_ELEV
    ... )
    (0, 2)
    """
    # Start with the first node on the list, and an arbitrarily large elev
    cdef int lowest_node = nodes_this_depression[0]
    cdef double lowest_elev = BIG_ELEV

    # set up a worst-case scanario array for the pits, and a counter to pull
    # the good entries later:
    cdef int current_iter = 0

    # Codes for depression status
    cdef int _UNFLOODED = 0
    cdef int _PIT = 1
    cdef int _CURRENT_LAKE = 2
    cdef int _FLOODED = 3

    cdef int n
    cdef int nbr
    cdef int i

    while current_iter < pit_count:
        n = nodes_this_depression[current_iter]
        for nbr in node_nbrs[n]:
            if nbr != -1:
                if flood_status[nbr] == _UNFLOODED:
                    if elev[nbr] < lowest_elev:
                        lowest_node = nbr
                        lowest_elev = elev[nbr]
                elif (
                    flood_status[nbr] == _PIT
                    or flood_status[nbr] == _FLOODED
                ):
                    nodes_this_depression[pit_count] = nbr
                    pit_count += 1
                    flood_status[nbr] = _CURRENT_LAKE
        current_iter += 1
    if lowest_elev == BIG_ELEV:
        print("Unable to find drainage outlet for a lake.")
        print("In lake with " + str(len(nodes_this_depression)), "nodes:")
        print(str(nodes_this_depression))

        for i in nodes_this_depression:
            print("Node ID: ", i)
            print("Node Elevation: ", elev[i])
            print("Node Flood Status: ", flood_status[i])
            # print("Node Neigbors: ", node_nbrs[i])
            # print("Neighbor Elevations: ", elev[node_nbrs[i]])
            # print("Neigbor Flood Status: ", flood_status[node_nbrs[i]])
        warning_message(
            """If you see no data values in any of the elevation terms
            this may because you have disconnected open nodes (which
            sometimes occurs during raster clipping.

            Consider running
            set_open_nodes_disconnected_from_watershed_to_closed
            which will remove isolated open nodes."""
        )
    return lowest_node, pit_count



================================================
File: depression_finder/floodstatus.py
================================================
from enum import IntEnum
from enum import unique


@unique
class FloodStatus(IntEnum):
    """Codes for depression status"""

    _UNFLOODED = 0
    _PIT = 1
    _CURRENT_LAKE = 2
    _FLOODED = 3



================================================
File: depression_finder/lake_mapper.py
================================================
"""Find depressions on a topographic surface.

.. codeauthor:: gtucker, DEJH (Flow routing)
"""

# Routing by DEJH, Oct 15.


import numpy as np

from ...core.model_component import Component
from ...core.utils import as_id_array
from ...field import FieldError
from ...grid import RasterModelGrid
from ..flow_accum import flow_accum_bw
from .cfuncs import find_lowest_node_on_lake_perimeter_c
from .floodstatus import FloodStatus

_UNFLOODED = FloodStatus._UNFLOODED
_CURRENT_LAKE = FloodStatus._CURRENT_LAKE
_FLOODED = FloodStatus._FLOODED
_PIT = FloodStatus._PIT

use_cfuncs = True


class DepressionFinderAndRouter(Component):
    """Find depressions on a topographic surface.

    This component identifies depressions in a topographic surface, finds an
    outlet for each depression.  If directed to do so (default True), and the
    component is able to find existing routing fields output from the
    'route_flow_dn' component, it will then modify the drainage directions and
    accumulations already stored in the grid to route flow across these
    depressions.

    Note that in general properties of this class named "depression" identify
    each individual pit in the topography, including those that will merge
    once the fill is performed. Those named "lake" return the unique lakes
    created by the fill, and are probably the properties most users will
    want.

    Note also that the structure of drainage within the lakes is not
    guaranteed, and in particular, may not be symmetrical even if your
    boundary conditions are.
    However, the outputs from the lake will all still be correct.

    Note the routing part of this component may not yet be fully compatible with
    irregular grids.

    The prinary method of this class is
    *map_depressions()*.

    Examples
    --------
    Route flow across a depression in a sloped surface.

    >>> from landlab import RasterModelGrid
    >>> from landlab.components import FlowAccumulator, DepressionFinderAndRouter
    >>> mg = RasterModelGrid((7, 7), xy_spacing=0.5)
    >>> z = mg.add_field("topographic__elevation", mg.node_x.copy(), at="node")
    >>> z += 0.01 * mg.node_y
    >>> mg.at_node["topographic__elevation"].reshape(mg.shape)[2:5, 2:5] *= 0.1
    >>> fr = FlowAccumulator(mg, flow_director="D8")
    >>> fr.run_one_step()  # the flow "gets stuck" in the hole
    >>> mg.at_node["flow__receiver_node"].reshape(mg.shape)
    array([[ 0,  1,  2,  3,  4,  5,  6],
           [ 7,  7, 16, 17, 18, 18, 13],
           [14, 14, 16, 16, 17, 18, 20],
           [21, 21, 16, 23, 24, 25, 27],
           [28, 28, 23, 30, 31, 32, 34],
           [35, 35, 30, 31, 32, 32, 41],
           [42, 43, 44, 45, 46, 47, 48]])
    >>> mg.at_node["drainage_area"].reshape(mg.shape)
    array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],
           [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.  ],
           [0.25, 0.25, 5.  , 1.5 , 1.  , 0.25, 0.  ],
           [0.25, 0.25, 3.  , 0.75, 0.5 , 0.25, 0.  ],
           [0.25, 0.25, 2.  , 1.5 , 1.  , 0.25, 0.  ],
           [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.  ],
           [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]])
    >>> df = DepressionFinderAndRouter(mg)
    >>> df.map_depressions()  # reroute_flow defaults to True
    >>> mg.at_node["flow__receiver_node"].reshape(mg.shape)
    array([[ 0,  1,  2,  3,  4,  5,  6],
           [ 7,  7, 16, 17, 18, 18, 13],
           [14, 14,  8, 16, 17, 18, 20],
           [21, 21, 16, 16, 24, 25, 27],
           [28, 28, 23, 24, 24, 32, 34],
           [35, 35, 30, 31, 32, 32, 41],
           [42, 43, 44, 45, 46, 47, 48]])
    >>> mg.at_node["drainage_area"].reshape(mg.shape)
    array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],
           [5.25, 5.25, 0.25, 0.25, 0.25, 0.25, 0.  ],
           [0.25, 0.25, 5.  , 1.5 , 1.  , 0.25, 0.  ],
           [0.25, 0.25, 0.75, 2.25, 0.5 , 0.25, 0.  ],
           [0.25, 0.25, 0.5 , 0.5 , 1.  , 0.25, 0.  ],
           [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.  ],
           [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]])
    >>> df.lake_at_node.reshape(mg.shape)
    array([[False, False, False, False, False, False, False],
           [False, False, False, False, False, False, False],
           [False, False,  True,  True,  True, False, False],
           [False, False,  True,  True,  True, False, False],
           [False, False,  True,  True,  True, False, False],
           [False, False, False, False, False, False, False],
           [False, False, False, False, False, False, False]])
    >>> df.lake_map.reshape(mg.shape)
    array([[-1, -1, -1, -1, -1, -1, -1],
           [-1, -1, -1, -1, -1, -1, -1],
           [-1, -1, 16, 16, 16, -1, -1],
           [-1, -1, 16, 16, 16, -1, -1],
           [-1, -1, 16, 16, 16, -1, -1],
           [-1, -1, -1, -1, -1, -1, -1],
           [-1, -1, -1, -1, -1, -1, -1]])
    >>> df.lake_codes  # a unique code for each lake present on the grid
    array([16])
    >>> df.lake_outlets  # the outlet node of each lake in lake_codes
    array([8])
    >>> df.lake_areas  # the area of each lake in lake_codes
    array([2.25])

    Because ``rereoute_flow`` defaults to ``True``, the flow connectivity fields
    created by the :py:class:`~landlab.components.flow_accum.FlowAccumulator`
    will have now been modified to route flow over the depressions in the
    surface. The topogrphy itself is not modified.

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    Tucker, G. E., Lancaster, S. T., Gasparini, N. M., and Bras, R. L.: The
    Channel-Hillslope Integrated Landscape Development Model (CHILD), in:
    Landscape Erosion and Evolution Modeling, Springer US, Boston, MA, USA,
    349–388, 2001.

    """

    _name = "DepressionFinderAndRouter"

    _unit_agnostic = True

    _info = {
        "depression__depth": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of depression below its spillway point",
        },
        "depression__outlet_node": {
            "dtype": int,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": (
                "If a depression, the id of the outlet node for that depression, "
                "otherwise grid.BAD_INDEX"
            ),
        },
        "flood_status_code": {
            "dtype": int,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Map of flood status (_PIT, _CURRENT_LAKE, _UNFLOODED, or _FLOODED).",
        },
        "is_pit": {
            "dtype": bool,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Boolean flag indicating whether a node is a pit.",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
    }

    def __init__(self, grid, routing="D8", pits="flow__sink_flag", reroute_flow=True):
        """Create a DepressionFinderAndRouter.

        Constructor assigns a copy of the grid, sets the current time, and
        calls the initialize method.

        Parameters
        ----------
        grid : RasterModelGrid
            A landlab RasterModelGrid.
        routing : str
            If grid is a raster type, controls whether lake connectivity can
            occur on diagonals ('D8', default), or only orthogonally ('D4').
            Has no effect if grid is not a raster.
        pits : array or str or None, optional
            If a field name, the boolean field containing True where pits.
            If an array, either a boolean array of nodes of the pits, or an
            array of pit node IDs. It does not matter whether or not open
            boundary nodes are flagged as pits; they are never treated as such.
            Default is 'flow__sink_flag', the pit field output from the
            :py:mod:`FlowDirectors <landlab.components.flow_director>`.
        reroute_flow : bool, optional
            If True (default), and the component detects the output fields in
            the grid produced by the FlowAccumulator component, this component
            will modify the existing flow fields to route the flow across the
            lake surface(s).
        """
        super().__init__(grid)

        self._bc_set_code = self._grid.bc_set_code

        self._user_supplied_pits = pits
        self._reroute_flow = reroute_flow

        if routing != "D8":
            assert routing == "D4"
        self._routing = routing

        if isinstance(grid, RasterModelGrid) and (routing == "D8"):
            self._D8 = True
            self._num_nbrs = 8
            self._diag_link_length = np.sqrt(grid.dx**2 + grid.dy**2)
        else:
            self._D8 = False  # useful shorthand for thia test we do a lot
            if isinstance(grid, RasterModelGrid):
                self._num_nbrs = 4
            else:
                self._num_nbrs = self._grid.links_at_node.shape[1]

        if "flow__receiver_node" in self._grid.at_node and self._grid.at_node[
            "flow__receiver_node"
        ].size != self._grid.size("node"):
            raise NotImplementedError(
                "A route-to-multiple flow director has been "
                "run on this grid. The depression finder is "
                "not compatible with the grid anymore. Use "
                "DepressionFinderAndRouter with reroute_flow=True "
                "only with route-to-one methods. If using this "
                "component with such a flow directing method is desired "
                "please open a GitHub Issue/"
            )

        # Make sure the grid includes elevation data.
        self._elev = self._grid.at_node["topographic__elevation"]

        # Create output variables.
        #
        # Note that we initialize depression
        # outlet ID to self._grid.BAD_INDEX (which is a major clue!)
        self._depression_depth = self._grid.add_zeros(
            "depression__depth", at="node", clobber=True
        )
        self._depression_outlet_map = self._grid.add_zeros(
            "depression__outlet_node", at="node", dtype=int, clobber=True
        )
        self._depression_outlet_map += self._grid.BAD_INDEX

        # Later on, we'll need a number that's guaranteed to be larger than the
        # highest elevation in the grid.
        self._BIG_ELEV = 1.0e99

        self.updated_boundary_conditions()

        self._lake_outlets = []  # a list of each unique lake outlet
        # ^note this is nlakes-long

        self._is_pit = self._grid.add_ones(
            "is_pit", at="node", dtype=bool, clobber=True
        )
        self._flood_status = self._grid.add_zeros(
            "flood_status_code", at="node", dtype=int, clobber=True
        )
        self._lake_map = np.empty(self._grid.number_of_nodes, dtype=int)
        self._lake_map.fill(self._grid.BAD_INDEX)

    def updated_boundary_conditions(self):
        """Call this if boundary conditions on the grid are updated after the
        component is instantiated."""
        try:
            dx = self._grid.dx
            dy = self._grid.dy
        except AttributeError:
            pass
        # We'll also need a handy copy of the node neighbor lists
        # TODO: presently, this grid method seems to only exist for Raster
        # grids. We need it for *all* grids!
        self._node_nbrs = self._grid.active_adjacent_nodes_at_node
        if self._D8:
            diag_nbrs = self._grid.diagonal_adjacent_nodes_at_node.copy()
            # remove the inactive nodes:
            diag_nbrs[
                self._grid.status_at_node[diag_nbrs] == self._grid.BC_NODE_IS_CLOSED
            ] = -1
            self._node_nbrs = np.concatenate((self._node_nbrs, diag_nbrs), 1)
            self._link_lengths = np.empty(8, dtype=float)
            self._link_lengths[0] = dx
            self._link_lengths[2] = dx
            self._link_lengths[1] = dy
            self._link_lengths[3] = dy
            self._link_lengths[4:].fill(np.sqrt(dx * dx + dy * dy))
        elif isinstance(self._grid, RasterModelGrid) and (self._routing == "D4"):
            self._link_lengths = np.empty(4, dtype=float)
            self._link_lengths[0] = dx
            self._link_lengths[2] = dx
            self._link_lengths[1] = dy
            self._link_lengths[3] = dy
        else:
            self._link_lengths = self._grid.length_of_link

    @property
    def is_pit(self):
        """At node array indicating whether the node is a pit or not."""
        return self._is_pit

    @property
    def number_of_pits(self):
        """The number of pits on the grid."""
        return self._number_of_pits

    @property
    def pit_node_ids(self):
        """Node IDs of grid nodes identified as pits."""
        return self._pit_node_ids

    @property
    def flood_status(self):
        """Map of flood status (_PIT, _CURRENT_LAKE, _UNFLOODED, or
        _FLOODED)."""
        return self._flood_status

    @property
    def receivers(self):
        """At node array indicating which node receives flow."""
        return self._receivers

    @receivers.setter
    def receivers(self, receivers):
        self._receivers = receivers

    @property
    def depression_depth(self):
        """At node array of depression depths."""
        return self._depression_depth

    @property
    def depression_outlet_map(self):
        """At node array indicating the node-id of the depression outlet."""
        return self._depression_outlet_map

    def _find_pits(self):
        """Locate local depressions ("pits") in a gridded elevation field.

        Notes
        -----
        **Uses**:

        * ``self._elev``
        * ``self._grid``

        **Creates**:

        * ``is_pit`` (node array of booleans): Flag indicating whether
          the node is a pit.
        * ``number_of_pits`` (int): Number of pits found.
        * ``pit_node_ids`` (node array of ints): IDs of the nodes that
          are pits

        A node is defined as being a pit if and only if:

        1. All neighboring core nodes have equal or greater elevation, and
        2. Any neighboring open boundary nodes have a greater elevation.

        The algorithm starts off assuming that all core nodes are pits. We then
        loop through all active links. For each link, if one node is higher
        than the other, the higher one cannot be a pit, so we flag it False.
        We also look at cases in which an active link's nodes have equal
        elevations. If one is an open boundary, then the other must be a core
        node, and we declare the latter not to be a pit (via rule 2 above).
        """
        # Create the is_pit array, with all core nodes initialized to True and
        # all boundary nodes initialized to False.
        self._is_pit.fill(True)
        self._is_pit[self._grid.boundary_nodes] = False

        # Loop over all active links: if one of a link's two nodes is higher
        # than the other, the higher one is not a pit. Also, if they have
        # equal elevations and one is an open boundary, the other is not a pit.
        act_links = self._grid.active_links
        h_orth = self._grid.node_at_link_head[act_links]
        t_orth = self._grid.node_at_link_tail[act_links]

        # These two lines assign the False flag to any node that is higher
        # than its partner on the other end of its link
        self._is_pit[h_orth[np.where(self._elev[h_orth] > self._elev[t_orth])[0]]] = (
            False
        )
        self._is_pit[t_orth[np.where(self._elev[t_orth] > self._elev[h_orth])[0]]] = (
            False
        )

        # If we have a raster grid, handle the diagonal active links too
        # (At the moment, their data structure is a bit different)
        # TODO: update the diagonal link data structures
        # DEJH doesn't understand why this can't be vectorized as above...
        if self._D8:
            for h, t in self._grid.nodes_at_diagonal[self._grid.active_diagonals]:
                if self._elev[h] > self._elev[t]:
                    self._is_pit[h] = False
                elif self._elev[t] > self._elev[h]:
                    self._is_pit[t] = False
                elif self._elev[h] == self._elev[t]:
                    if (
                        self._grid.status_at_node[h]
                        == self._grid.BC_NODE_IS_FIXED_VALUE
                    ):
                        self._is_pit[t] = False
                    elif (
                        self._grid.status_at_node[t]
                        == self._grid.BC_NODE_IS_FIXED_VALUE
                    ):
                        self._is_pit[h] = False

        # Record the number of pits and the IDs of pit nodes.
        self._number_of_pits = np.count_nonzero(self._is_pit)
        self._pit_node_ids = as_id_array(np.where(self._is_pit)[0])

    def _links_and_nbrs_at_node(self, the_node):
        """Compile and return arrays with IDs of neighbor links and nodes.

        If D8 Raster, returns *diag_nbrs* containing the diagonal neighbors;
        otherwise, *diag_nbrs = None*.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import DepressionFinderAndRouter
        >>> rg = RasterModelGrid((3, 3))
        >>> z = rg.add_zeros("topographic__elevation", at="node")
        >>> z[4] = 2.0
        >>> df = DepressionFinderAndRouter(rg, routing="D4")
        >>> (links, nbrs, dnbrs) = df._links_and_nbrs_at_node(4)
        >>> links
        array([6, 8, 5, 3])
        >>> nbrs
        array([5, 7, 3, 1])
        >>> dnbrs
        >>> df = DepressionFinderAndRouter(rg, routing="D8")
        >>> (links, nbrs, dnbrs) = df._links_and_nbrs_at_node(4)
        >>> links
        array([6, 8, 5, 3])
        >>> nbrs
        array([5, 7, 3, 1])
        >>> dnbrs
        array([8, 6, 0, 2])
        """

        # Get the neighboring links (and, if applicable, the diagonals)
        links = self._grid.links_at_node[the_node]
        nbrs = self._grid.adjacent_nodes_at_node[the_node]
        if self._D8:
            diag_nbrs = self._grid.diagonal_adjacent_nodes_at_node[the_node]
        else:
            diag_nbrs = None

        return links, nbrs, diag_nbrs

    def assign_outlet_receiver(self, outlet_node):
        """Find drainage direction for outlet_node that does not flow into its
        own lake.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab.components import DepressionFinderAndRouter
        >>> from landlab.components import FlowAccumulator
        >>> from landlab import RasterModelGrid
        >>> rg = RasterModelGrid((7, 7))
        >>> rg.status_at_node[rg.nodes_at_right_edge] = rg.BC_NODE_IS_CLOSED
        >>> z = rg.add_zeros("topographic__elevation", at="node")
        >>> z[:] = rg.x_of_node + 0.01 * rg.y_of_node
        >>> lake_nodes = np.array([10, 16, 17, 18, 24, 32, 33, 38, 40])
        >>> z[lake_nodes] *= 0.1
        >>> fr = FlowAccumulator(rg, flow_director="D4")
        >>> fr.run_one_step()
        >>> rg.at_node["flow__receiver_node"].reshape(rg.shape)
        array([[ 0,  1,  2,  3,  4,  5,  6],
               [ 7,  7, 16, 10, 10, 11, 13],
               [14, 14, 16, 16, 17, 18, 20],
               [21, 21, 16, 17, 24, 33, 27],
               [28, 28, 29, 24, 32, 32, 34],
               [35, 35, 38, 38, 38, 33, 41],
               [42, 43, 44, 45, 46, 47, 48]])
        >>> df = DepressionFinderAndRouter(rg, routing="D4")
        >>> df.map_depressions()
        >>> rg.at_node["flow__receiver_node"].reshape(rg.shape)
        array([[ 0,  1,  2,  3,  4,  5,  6],
               [ 7,  7, 16, 17, 10, 11, 13],
               [14, 14, 15, 16, 17, 18, 20],
               [21, 21, 16, 17, 24, 33, 27],
               [28, 28, 29, 38, 31, 32, 34],
               [35, 35, 36, 37, 38, 33, 41],
               [42, 43, 44, 45, 46, 47, 48]])
        >>> fr = FlowAccumulator(rg, flow_director="D8")
        >>> fr.run_one_step()
        >>> rg.at_node["flow__receiver_node"].reshape(rg.shape)
        array([[ 0,  1,  2,  3,  4,  5,  6],
               [ 7,  7, 16, 16, 10, 18, 13],
               [14, 14, 16, 16, 17, 18, 20],
               [21, 21, 16, 16, 24, 33, 27],
               [28, 28, 24, 24, 24, 32, 34],
               [35, 35, 38, 38, 38, 32, 41],
               [42, 43, 44, 45, 46, 47, 48]])
        >>> df = DepressionFinderAndRouter(rg, routing="D8")
        >>> df.map_depressions()
        >>> rg.at_node["flow__receiver_node"].reshape(rg.shape)
        array([[ 0,  1,  2,  3,  4,  5,  6],
               [ 7,  7, 16, 16, 10, 18, 13],
               [14, 14,  8, 16, 17, 18, 20],
               [21, 21, 16, 16, 24, 33, 27],
               [28, 28, 24, 24, 24, 32, 34],
               [35, 35, 38, 32, 38, 32, 41],
               [42, 43, 44, 45, 46, 47, 48]])
        """

        (links, nbrs, diag_nbrs) = self._links_and_nbrs_at_node(outlet_node)

        # Sweep through them, identifying the neighbor with the greatest slope.
        # We are probably duplicating some gradient calculations, but this only
        # happens occasionally, when we have a candidate outlet node.

        # We're seeking the valid neighbor (*receiver*) in the direction of
        # steepest descent. Initially set *receiver* to the node itself, and
        # downhill-positive gradient to zero. If we don't find any neighbor
        # with a steeper path (or an open boundary), then we have failed.
        max_downhill_grad = 0.0
        receiver = outlet_node
        node_elev = self._elev[outlet_node]

        # Iterate over all "regular" neighbors
        for i in range(len(links)):
            lnk = links[i]
            nbr = nbrs[i]

            # To pass this first hurdle, the neighbor must:
            #   * not be part of the current lake
            #   * have a surface (if flooded, WATER surface)
            #     lower than our outlet node;
            #   * not be a closed boundary
            if (
                self._flood_status[nbr] != _CURRENT_LAKE
                and (
                    (self._elev[nbr] + self._depression_depth[nbr])
                    < self._elev[receiver]
                )
                and self._grid.status_at_node[nbr] != self._grid.BC_NODE_IS_CLOSED
            ):
                # Next test: is it the steepest downhill grad so far?
                # If so, we've found a candidate.
                grad = (node_elev - self._elev[nbr]) / self._grid.length_of_link[lnk]
                if grad > max_downhill_grad:
                    # Update the receiver and max grad: this is now the one
                    # to beat.
                    max_downhill_grad = grad
                    receiver = nbr

        # If we're on a D8 raster, iterate over all diagonal neighbors
        if self._D8:
            for nbr in diag_nbrs:
                # Again, to pass this first hurdle, the neighbor must:
                #   * not be part of the current lake
                #   * have a surface (if flooded, WATER surface)
                #     lower than our outlet node;
                #   * not be a closed boundary
                if (
                    self._flood_status[nbr] != _CURRENT_LAKE
                    and (
                        (self._elev[nbr] + self._depression_depth[nbr])
                        < self._elev[receiver]
                    )
                    and self._grid.status_at_node[nbr] != self._grid.BC_NODE_IS_CLOSED
                ):
                    # Next test: is it the steepest downhill grad so far?
                    # If so, we've found a candidate.
                    grad = (node_elev - self._elev[nbr]) / self._diag_link_length
                    if grad > max_downhill_grad:
                        # Update the receiver and max grad: this is now the one
                        # to beat.
                        max_downhill_grad = grad
                        receiver = nbr

        # We only call this method after is_valid_outlet has evaluated True,
        # so in theory it should NEVER be the case that we fail to find a
        # receiver. However, let's make sure.
        assert receiver != outlet_node, "failed to find receiver with ID: %r" % receiver

        # Finally, let's assign it

        self._grid.at_node["flow__receiver_node"][outlet_node] = receiver

    def node_can_drain(self, the_node):
        """Check if a node has drainage away from the current lake/depression.

        Parameters
        ----------
        the_node : int
            The node to test.
        nodes_this_depression : array_like of int
            Nodes that form a pit.

        Returns
        -------
        boolean
            ``True`` if the node can drain. Otherwise, ``False``.
        """
        nbrs = self._node_nbrs[the_node]
        not_bad = nbrs != self._grid.BAD_INDEX
        not_too_high = self._elev[nbrs] < self._elev[the_node]
        not_current_lake = np.not_equal(self._flood_status[nbrs], _CURRENT_LAKE)
        not_flooded = np.not_equal(self._flood_status[nbrs], _FLOODED)

        # The following logic block handles the case when a neighbor is
        # flooded but its outlet is LOWER than the_node, so the_node could
        # be an outlet that flows into a lower lake.
        #
        # We proceed only if there is at least one flooded node
        if np.any(np.logical_not(not_flooded)):
            # Examine each neighbor
            for i in range(len(nbrs)):
                # If the neighbor is flooded...
                if not not_flooded[i]:
                    # Check to see whether its own outlet is lower than
                    # the_node. If so, then it does not "count" as being
                    # flooded, because its water level is lower than our
                    # current potential lake outlet.
                    dep_out = self._depression_outlet_map[nbrs[i]]
                    if self._elev[the_node] > self._elev[dep_out]:
                        not_flooded[i] = True

        # Now combine all the issues: any neighbor(s) that is not "bad",
        # too high, part of the current lake, or flooded at a level equal to
        # or higher than the_node, is a potential outlet. So, if there are any
        # neighbor nodes that pass all these tests, then the_node can drain.
        all_probs = np.logical_and(
            np.logical_and(not_bad, not_too_high),
            np.logical_and(not_current_lake, not_flooded),
        )
        return np.any(all_probs)

    def is_valid_outlet(self, the_node):
        """Check if a node is a valid outlet for the depression.

        Parameters
        ----------
        the_node : int
            The node to test.
        nodes_this_depression : array_like of int
            Nodes that form a pit.

        Returns
        -------
        boolean
            ``True`` if the node is a valid outlet. Otherwise, ``False``.
        """
        if self._grid.status_at_node[the_node] == self._grid.BC_NODE_IS_FIXED_VALUE:
            return True

        if self.node_can_drain(the_node):
            return True

        return False

    def _record_depression_depth_and_outlet(
        self, nodes_this_depression, outlet_id, pit_node
    ):
        """Record information about a depression.

        Record information about this depression/lake in the flood_status,
        depression_depth, and depression_outlet arrays.

        Parameters
        ----------
        nodes_this_depression : iterable of int
            Nodes that form a pit.
        outlet_id : int
            Node that is the outlet of the pit.
        pit_node : int
            Node that is the deepest pit, uniquely associated with this
            depression.
        """
        n = nodes_this_depression

        # three cases possible - new lake is fresh; new lake is smaller than
        # an existing lake (subsumed, and unimportant), new lake is equal to
        # or bigger than old lake (or multiple old lakes). It SHOULDN'T be
        # possible to have two lakes overlapping... We can test this with an
        # assertion that out total # of *tracked* lakes matches the accumulated
        # total of unique vals in lake_map.
        fresh_nodes = np.equal(self._lake_map[n], self._grid.BAD_INDEX)
        if np.all(fresh_nodes):  # a new lake
            self._flood_status[n] = _FLOODED
            self._depression_depth[n] = self._elev[outlet_id] - self._elev[n]
            self._depression_outlet_map[n] = outlet_id
            self._lake_map[n] = pit_node
            self._pits_flooded += 1
            pit_node_where = np.searchsorted(self._pit_node_ids, pit_node)
            self._unique_pits[pit_node_where] = True
        elif np.any(fresh_nodes):  # lake is bigger than one or more existing
            self._flood_status[n] = _FLOODED
            depth_this_lake = self._elev[outlet_id] - self._elev[n]
            self._depression_depth[n] = depth_this_lake
            self._depression_outlet_map[n] = outlet_id
            # ^these two will just get stamped over as needed
            subsumed_lakes = np.unique(self._lake_map[n])  # IDed by pit_node
            # the final entry is self._grid.BAD_INDEX
            subs_lakes_where = np.searchsorted(self._pit_node_ids, subsumed_lakes[1:])
            pit_node_where = np.searchsorted(self._pit_node_ids, pit_node)
            self._unique_pits[subs_lakes_where] = False
            self._unique_pits[pit_node_where] = True
            self._pits_flooded -= subsumed_lakes.size - 2
            # -1 for the self._grid.BAD_INDEX that must be present; another -1
            # because a single lake is just replaced by a new lake.
            self._lake_map[n] = pit_node
        else:  # lake is subsumed within an existing lake
            print(" eaten lake")
            assert np.all(np.equal(self._flood_status[n], _CURRENT_LAKE))
            self._flood_status[n] = _FLOODED

    def find_depression_from_pit(self, pit_node, reroute_flow=True):
        """Find the extent of the nodes that form a pit.

        Identify extent of depression/lake whose lowest point is the node
        pit_node (which is a itself a pit, a.k.a., closed depression).

        Parameters
        ----------
        pit_node : int
            The node that is the lowest point of a pit.
        """
        # Flag the pit as being _CURRENT_LAKE (it's the first node in the
        # current lake)
        self._flood_status[pit_node] = _CURRENT_LAKE

        # This flag keeps track of when we're done with this depression
        found_outlet = False

        # Safety check
        count = 0
        max_count = self._grid.number_of_nodes + 1

        # Place pit_node at top of depression list
        nodes_this_depression = self.grid.zeros(at="node", dtype=int)
        nodes_this_depression[0] = pit_node
        pit_count = 1

        while not found_outlet:
            lowest_node_on_perimeter, pit_count = find_lowest_node_on_lake_perimeter_c(
                self._node_nbrs,
                self.flood_status,
                self._elev,
                nodes_this_depression,
                pit_count,
                self._BIG_ELEV,
            )

            # note this can return the supplied node, if - somehow - the
            # surrounding nodes are all self._grid.BAD_INDEX
            # I BELIEVE THE IS_VALID_OUTLET FN SHOULD ASSIGN FLOW DIR
            found_outlet = self.is_valid_outlet(lowest_node_on_perimeter)

            # If we haven't found an outlet, add lowest_node to the lake list
            # and flag it as being part of the current lake/depression
            if not found_outlet:
                nodes_this_depression[pit_count] = lowest_node_on_perimeter
                self._flood_status[lowest_node_on_perimeter] = _CURRENT_LAKE
                pit_count += 1

            # If we HAVE found an outlet, and we are re-routing flow, then
            # assign the proper flow direction to the outlet node. If it is an
            # open boundary, then it drains to itself. Otherwise, call
            # assign_outlet_receiver to find the correct receiver (so that it
            # doesn't simply drain back into the lake)
            elif ("flow__receiver_node" in self._grid.at_node) and reroute_flow:
                if (
                    self._grid.status_at_node[lowest_node_on_perimeter]
                    != self._grid.BC_NODE_IS_CORE
                ):
                    self._grid.at_node["flow__receiver_node"][
                        lowest_node_on_perimeter
                    ] = lowest_node_on_perimeter
                else:
                    self.assign_outlet_receiver(lowest_node_on_perimeter)

            # Safety check, in case a bug (ha!) puts us in an infinite loop
            assert count < max_count, "too many iterations in lake filler!"
            count += 1

        self._depression_outlets.append(lowest_node_on_perimeter)
        # Now that we've mapped this depression, record it in the arrays
        # depression_depth, depression_outlet, and flood_status
        self._record_depression_depth_and_outlet(
            nodes_this_depression[:pit_count], lowest_node_on_perimeter, pit_node
        )

        # TODO: ideally we need a way to keep track of the number, area extent,
        # and average depth of depressions. Tricky thing is that one might be
        # devoured by another, so would need to be removed from the list.

    def _identify_depressions_and_outlets(self, reroute_flow=True):
        """Find depression and lakes on a topographic surface.

        Find and map the depressions/lakes in a topographic surface,
        given a previously identified list of pits (if any) in the
        surface.
        """
        self._pits_flooded = 0
        self._unique_pits = np.zeros_like(self._pit_node_ids, dtype=bool)
        # debug_count = 0
        for pit_node in self._pit_node_ids:
            if self._flood_status[pit_node] != _PIT:
                self._depression_outlets.append(self._grid.BAD_INDEX)
            else:
                self.find_depression_from_pit(pit_node, reroute_flow)
                self._pits_flooded += 1

        assert len(self._depression_outlets) == self._unique_pits.size

        self._unique_lake_outlets = np.array(self._depression_outlets)[
            self._unique_pits
        ]

    def update(self):
        """Alias for map_depressions."""
        self.map_depressions()

    def map_depressions(self):
        """Map depressions/lakes in a topographic surface.

        Examples
        --------
        Test #1: 5x5 raster grid with a diagonal lake.

        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import DepressionFinderAndRouter

        >>> rg = RasterModelGrid((5, 5))
        >>> rg.at_node["topographic__elevation"] = [
        ...     [100.0, 100.0, 95.0, 100.0, 100.0],
        ...     [100.0, 101.0, 92.0, 1.0, 100.0],
        ...     [100.0, 101.0, 2.0, 101.0, 100.0],
        ...     [100.0, 3.0, 101.0, 101.0, 100.0],
        ...     [90.0, 95.0, 100.0, 100.0, 100.0],
        ... ]

        >>> df = DepressionFinderAndRouter(rg, reroute_flow=False)
        >>> df.map_depressions()
        >>> df.display_depression_map()
        . . . . .
        . . . ~ .
        . . ~ . .
        . ~ . . .
        o . . . .
        """
        if self._bc_set_code != self._grid.bc_set_code:
            self.updated_boundary_conditions()
            self._bc_set_code = self._grid.bc_set_code

        # verify that there is an outlet to the grid and
        if not np.any(
            self._grid.status_at_node[self._grid.boundary_nodes]
            != self._grid.BC_NODE_IS_CLOSED
        ):
            raise ValueError(
                "DepressionFinderAndRouter requires that there is at least one "
                "open boundary node."
            )

        self._lake_map.fill(self._grid.BAD_INDEX)
        self._depression_outlet_map.fill(self._grid.BAD_INDEX)
        self._depression_depth.fill(0.0)
        self._depression_outlets = []  # reset these
        # Locate nodes with pits
        if isinstance(self._user_supplied_pits, str):
            try:
                pits = self._grid.at_node[self._user_supplied_pits]
                supplied_pits = np.where(pits)[0]
                self._pit_node_ids = as_id_array(
                    np.setdiff1d(supplied_pits, self._grid.boundary_nodes)
                )
                self._number_of_pits = self._pit_node_ids.size
                self._is_pit.fill(False)
                self._is_pit[self._pit_node_ids] = True
            except FieldError:
                self._find_pits()
        elif self._user_supplied_pits is None:
            self._find_pits()
        else:  # hopefully an array or other sensible iterable
            if len(self._user_supplied_pits) == self._grid.number_of_nodes:
                supplied_pits = np.where(self._user_supplied_pits)[0]
            else:  # it's an array of node ids
                supplied_pits = self._user_supplied_pits
            # remove any boundary nodes from the supplied pit list
            self._pit_node_ids = as_id_array(
                np.setdiff1d(supplied_pits, self._grid.boundary_nodes)
            )

            self._number_of_pits = self._pit_node_ids.size
            self._is_pit.fill(False)
            self._is_pit[self._pit_node_ids] = True
        # Set up "lake code" array
        self._flood_status.fill(_UNFLOODED)
        self._flood_status[self._pit_node_ids] = _PIT

        self._identify_depressions_and_outlets(self._reroute_flow)

        if self._reroute_flow and ("flow__receiver_node" in self._grid.at_node):
            self._receivers = self._grid.at_node["flow__receiver_node"]
            self._sinks = self._grid.at_node["flow__sink_flag"]
            self._grads = self._grid.at_node["topographic__steepest_slope"]
            self._links = self._grid.at_node["flow__link_to_receiver_node"]
            self._route_flow()
            self._reaccumulate_flow()

    def _find_unresolved_neighbors(self, nbrs, receivers):
        """Make and return list of neighbors of node with unresolved flow dir.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab.components import DepressionFinderAndRouter
        >>> from landlab import RasterModelGrid
        >>> rg = RasterModelGrid((7, 8))
        >>> z = rg.add_zeros("topographic__elevation", at="node")
        >>> df = DepressionFinderAndRouter(rg)
        >>> rcvr = np.arange(56)
        >>> rcvr[13] = -1
        >>> rcvr[21] = -1
        >>> rcvr[29] = -1
        >>> rcvr[30] = -1
        >>> nbrs = np.array([23, 30, 21, 14], dtype=int)
        >>> df._find_unresolved_neighbors(nbrs, rcvr)
        array([30, 21])
        """
        # unresolved = np.where(receivers[nbrs] == -1)[0]
        # ur_nbrs = nbrs[unresolved]
        # ur_links = self._grid.links_at_node[unresolved]
        # return (ur_nbrs, ur_links)
        return nbrs[np.where(receivers[nbrs] == -1)[0]]

    def _find_unresolved_neighbors_new(self, nbrs, nbr_links, receivers):
        """Make and return list of neighbors of node with unresolved flow dir.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab.components import DepressionFinderAndRouter
        >>> from landlab import RasterModelGrid
        >>> rg = RasterModelGrid((7, 8))
        >>> z = rg.add_zeros("topographic__elevation", at="node")
        >>> df = DepressionFinderAndRouter(rg)
        >>> rcvr = np.arange(56)
        >>> rcvr[13] = -1
        >>> rcvr[21] = -1
        >>> rcvr[29] = -1
        >>> rcvr[30] = -1
        >>> nbrs = rg.adjacent_nodes_at_node[22]
        >>> nbr_links = rg.links_at_node[22]
        >>> df._find_unresolved_neighbors_new(nbrs, nbr_links, rcvr)
        (array([30, 21]), array([43, 35]))
        >>> nbrs = rg.diagonal_adjacent_nodes_at_node[22]
        >>> nbr_links = rg.d8s_at_node[22, 4:]
        >>> df._find_unresolved_neighbors_new(nbrs, nbr_links, rcvr)
        (array([29, 13]), array([136, 121]))
        """
        unresolved = np.where(receivers[nbrs] == -1)[0]
        ur_nbrs = nbrs[unresolved]
        ur_links = nbr_links[unresolved]
        return (ur_nbrs, ur_links)

    def _route_flow_for_one_lake(self, outlet, lake_nodes):
        """Route flow across a single lake. Alternative to part of _route_flow.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> import numpy as np
        >>> from landlab.components import DepressionFinderAndRouter
        >>> rg = RasterModelGrid((7, 8))
        >>> z = rg.add_zeros("topographic__elevation", at="node")
        >>> rcvr = rg.add_zeros("flow__receiver_node", at="node", dtype=int)
        >>> rcvr[:] = np.arange(rg.number_of_nodes)
        >>> lake_nodes = np.flatnonzero(
        ...     [
        ...         [0, 0, 0, 0, 0, 0, 0, 0],
        ...         [0, 0, 1, 0, 1, 1, 0, 0],
        ...         [0, 0, 0, 1, 1, 1, 0, 0],
        ...         [0, 1, 1, 1, 1, 1, 1, 0],
        ...         [0, 1, 1, 1, 1, 1, 1, 0],
        ...         [0, 0, 0, 0, 1, 1, 1, 0],
        ...         [0, 0, 0, 0, 0, 0, 0, 0],
        ...     ]
        ... )

        >>> rcvr[9] = 1
        >>> rcvr[11] = 3
        >>> rcvr[14] = 6
        >>> rcvr[17] = 16
        >>> rcvr[18] = 17
        >>> rcvr[22] = 14  # this is the outlet
        >>> rcvr[41] = 40
        >>> rcvr[42] = 50
        >>> rcvr[43] = 51
        >>> df = DepressionFinderAndRouter(rg)
        >>> df.receivers = rcvr
        >>> df._route_flow_for_one_lake(22, lake_nodes)
        >>> df.receivers
        array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  1, 19,  3, 13, 22,  6, 15, 16,
               16, 17, 20, 21, 22, 14, 23, 24, 26, 27, 28, 29, 22, 22, 31, 32, 34,
               35, 36, 29, 29, 30, 39, 40, 40, 50, 51, 36, 37, 38, 47, 48, 49, 50,
               51, 52, 53, 54, 55])
        """

        # Flag receiver nodes inside the lake as "unresolved"
        UNRESOLVED = -1
        self._receivers[lake_nodes] = UNRESOLVED

        # We work with two lists: the nodes currently being processed, and
        # the nodes that will be processed on the next iteration. We start with
        # the outlet node as the one being processed, and an empty list of
        # nodes to process next.
        nodes_being_processed = [outlet]
        nodes_to_proc_next = []

        # We must now iterate until we've taken care of all the nodes in the
        # lake. In each iteration, we:
        #  1 - find the unresolved neighbors of nodes being processed
        #  2 - point them toward the nodes being processed
        #  3 - place them on the nodes_to_proc_next list
        # We stop when there are no more nodes to process.
        #    Note that the nested looping will be slow, but could be sped up
        # by translating to cython.
        counter = 0  # counts # of times thru loop as fail-safe
        done = False
        while not done:
            # Get unresolved "regular" neighbors of the current nodes
            for cn in nodes_being_processed:
                # Get active and unresolved neighbors of cn
                (nbrs, lnks) = self._find_unresolved_neighbors_new(
                    self._grid.adjacent_nodes_at_node[cn],
                    self._grid.links_at_node[cn],
                    self._receivers,
                )
                # They will now flow to cn
                if nbrs.size > 0:
                    self._receivers[nbrs] = cn
                    if "flow__link_to_receiver_node" in self._grid.at_node:
                        self._links[nbrs] = lnks
                        slopes = (
                            self._elev[nbrs] - self._elev[cn]
                        ) / self._grid.length_of_link[lnks]
                        self._grads[nbrs] = np.maximum(slopes, 0.0)

                # Place them on the list of nodes to process next
                for n in nbrs:
                    nodes_to_proc_next.append(n)

            # If we're working with a raster that has diagonals, do the same
            # for the diagonal neighbors
            if self._D8:
                # Get unresolved "regular" neighbors of the current nodes
                for cn in nodes_being_processed:
                    (nbrs, diags) = self._find_unresolved_neighbors_new(
                        self._grid.diagonal_adjacent_nodes_at_node[cn],
                        self._grid.d8s_at_node[cn, 4:],
                        self._receivers,
                    )
                    # They will now flow to cn
                    if nbrs.size > 0:
                        self._receivers[nbrs] = cn
                        if "flow__link_to_receiver_node" in self._grid.at_node:
                            self._links[nbrs] = diags
                            slopes = (
                                self._elev[nbrs] - self._elev[cn]
                            ) / self._diag_link_length
                            self._grads[nbrs] = np.maximum(slopes, 0.0)

                    # Place them on the list of nodes to process next
                    for n in nbrs:
                        nodes_to_proc_next.append(n)

            # Move to the next set of nodes
            nodes_being_processed = nodes_to_proc_next
            nodes_to_proc_next = []
            if not nodes_being_processed:
                done = True

            # Just in case
            counter += 1
            assert counter < self._grid.number_of_nodes, "inf loop in lake"

    def _route_flow(self):
        """Route flow across lake flats.

        Route flow across lake flats, which have already been
        identified.
        """

        # Process each lake.
        for outlet_node, lake_code in zip(self.lake_outlets, self.lake_codes):
            # Get the nodes in the lake
            nodes_in_lake = np.where(self._lake_map == lake_code)[0]
            if len(nodes_in_lake) > 0:
                # find the correct outlet for the lake, if necessary
                if self._lake_map[self._receivers[outlet_node]] == lake_code:
                    nbrs = self._grid.active_adjacent_nodes_at_node[outlet_node]
                    not_lake = nbrs[np.where(self._lake_map[nbrs] != lake_code)[0]]
                    min_index = np.argmin(self._elev[not_lake])
                    new_receiver = not_lake[min_index]

                    # set receiver for new outlet.
                    self._receivers[outlet_node] = new_receiver

                # reset_link for new outlet
                outlet_receiver = self._receivers[outlet_node]
                if self._D8:
                    adjacent_links_and_diags = np.hstack(
                        (
                            self._grid.adjacent_nodes_at_node[outlet_node, :],
                            self._grid.diagonal_adjacent_nodes_at_node[outlet_node, :],
                        )
                    )
                    find_recs = outlet_receiver == adjacent_links_and_diags
                    new_link = self._grid.d8s_at_node[outlet_node, find_recs]
                else:
                    find_recs = (
                        outlet_receiver
                        == self._grid.adjacent_nodes_at_node[outlet_node, :]
                    )
                    new_link = self._grid.links_at_node[outlet_node, find_recs]

                if new_link.size == 0:
                    new_link = self._grid.BAD_INDEX
                if np.min(new_link) == np.max(new_link) and np.min(new_link) == -1:
                    self._links[outlet_node] = -1
                else:
                    assert len(new_link) == 1
                    self._links[outlet_node] = new_link[0]

                # make a check
                assert (
                    self._lake_map[self._receivers[outlet_node]] != lake_code
                ), "outlet of lake drains to itself!"

                # Route flow
                self._route_flow_for_one_lake(outlet_node, nodes_in_lake)

        self._sinks[self._pit_node_ids] = False

    def _reaccumulate_flow(self):
        """Update drainage area, discharge, upstream order, and flow link.

        Invoke the accumulator a second time to update drainage area,
        discharge, and upstream order.
        """
        # Calculate drainage area, discharge, and downstr->upstr order
        Q_in = self._grid.at_node["water__unit_flux_in"]
        areas = self._grid.cell_area_at_node.copy()
        areas[self._grid.closed_boundary_nodes] = 0.0

        self._a, q, s = flow_accum_bw.flow_accumulation(
            self._receivers, node_cell_area=areas, runoff_rate=Q_in
        )

        # finish the property updating:
        self._grid.at_node["drainage_area"][:] = self._a
        self._grid.at_node["surface_water__discharge"][:] = q
        self._grid.at_node["flow__upstream_node_order"][:] = s

    def _handle_outlet_node(self, outlet_node, nodes_in_lake):
        """Ensure the outlet node drains to the grid edge.

        Makes sure the outlet node is drains to the grid edge, not back
        into the depression.
        This exists because if the slope into the lake is steeper than the
        slope out from the (rim lowest) outlet node, the lake won't drain.

        Parameters
        ----------
        outlet_node : int
            The outlet node.
        nodes_in_lake : array_like of int
            The nodes that are contained within the lake.
        """
        if self._grid.status_at_node[outlet_node] == 0:  # it's not a BC
            if self._D8:
                outlet_neighbors = np.hstack(
                    (
                        self._grid.active_adjacent_nodes_at_node[outlet_node],
                        self._grid.diagonal_adjacent_nodes_at_node[outlet_node],
                    )
                )
            else:
                outlet_neighbors = self._grid.active_adjacent_nodes_at_node[
                    outlet_node
                ].copy()
            inlake = np.isin(outlet_neighbors.flat, nodes_in_lake)
            assert inlake.size > 0
            outlet_neighbors[inlake] = -1
            unique_outs, unique_indxs = np.unique(outlet_neighbors, return_index=True)
            out_draining = unique_outs[1:]
            if isinstance(self._grid, RasterModelGrid):
                link_l = self._link_lengths
            else:  # Voronoi
                link_l = self._link_lengths[self._grid.links_at_node[outlet_node, :]]
            eff_slopes = (self._elev[outlet_node] - self._elev[out_draining]) / link_l[
                unique_indxs[1:]
            ]
            lowest = np.argmax(eff_slopes)
            lowest_node = out_draining[lowest]
            # route the flow
            self._receivers[outlet_node] = lowest_node
        else:
            self._receivers[outlet_node] = outlet_node

    def display_depression_map(self):
        """Print a simple character-based map of depressions/lakes."""
        # Find the outlet nodes (just for display purposes)
        is_outlet = np.zeros(self._grid.number_of_nodes, dtype=bool)
        for i in self._grid.core_nodes:
            if self._flood_status[i] == _FLOODED:
                is_outlet[self._depression_outlet_map[i]] = True

        n = 0
        for _ in range(self._grid.number_of_node_rows):
            for _ in range(self._grid.number_of_node_columns):
                if is_outlet[n]:
                    print("o", end=" ")
                elif self._flood_status[n] == _UNFLOODED:
                    print(".", end=" ")
                else:
                    print("~", end=" ")
                n += 1
            print()

    @property
    def lake_outlets(self):
        """Returns the *unique* outlets for each lake, in same order as the
        return from lake_codes."""
        return np.array(self._depression_outlets)[self._unique_pits]

    @property
    def lake_codes(self):
        """Returns the *unique* code assigned to each unique lake.

        These are the values used to map the lakes in the property
        "lake_map".
        """
        return self._pit_node_ids[self._unique_pits]

    @property
    def number_of_lakes(self):
        """Return the number of individual lakes."""
        return self._unique_pits.sum()

    @property
    def lake_map(self):
        """Return an array of ints, where each node within a lake is labelled
        with a unique (non-consecutive) code corresponding to each unique lake.

        The codes used can be obtained with *lake_codes*. Nodes not in a
        lake are labelled with self._grid.BAD_INDEX
        """
        return self._lake_map

    @property
    def lake_at_node(self):
        """Return a boolean array, True if the node is flooded, False
        otherwise."""
        return self._lake_map != self._grid.BAD_INDEX

    @property
    def lake_areas(self):
        """A nlakes-long array of the area of each lake.

        The order is the same as that returned by *lake_codes*.
        """
        lake_areas = np.empty(self.number_of_lakes)
        for lake_counter, lake_code in enumerate(self.lake_codes):
            each_cell_in_lake = self._grid.cell_area_at_node[
                self._lake_map == lake_code
            ]
            lake_areas[lake_counter] = each_cell_in_lake.sum()
        return lake_areas

    @property
    def lake_volumes(self):
        """A nlakes-long array of the volume of each lake.

        The order is the same as that returned by *lake_codes*.
        """
        lake_vols = np.empty(self.number_of_lakes)
        col_vols = self._grid.cell_area_at_node * self._depression_depth
        for lake_counter, lake_code in enumerate(self.lake_codes):
            each_cell_in_lake = col_vols[self._lake_map == lake_code]
            lake_vols[lake_counter] = each_cell_in_lake.sum()
        return lake_vols



================================================
File: depth_dependent_diffusion/__init__.py
================================================
from .hillslope_depth_dependent_linear_flux import DepthDependentDiffuser

__all__ = ["DepthDependentDiffuser"]



================================================
File: depth_dependent_diffusion/hillslope_depth_dependent_linear_flux.py
================================================
"""Created on Fri Apr  8 08:32:48 2016.

@author: RCGlade
"""

import numpy as np

from landlab import Component
from landlab import LinkStatus


class DepthDependentDiffuser(Component):
    """This component implements a depth and slope dependent linear diffusion
    rule in the style of Johnstone and Hilley (2014).

    Hillslope sediment flux uses depth dependent component inspired by
    Johnstone and Hilley (2014). The flux :math:`q_s` is given as:

    .. math::

        q_s = - D S H^* (1.0 - exp( - H / H^*)

    where :math:`D` is is the diffusivity, :math:`S` is the slope (defined as
    negative downward), :math:`H` is the soil depth on links, and :math:`H^*`
    is the soil transport decay depth.

    This component will ignore soil thickness located at non-core nodes.

    Examples
    --------
    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import ExponentialWeatherer
    >>> from landlab.components import DepthDependentDiffuser
    >>> mg = RasterModelGrid((5, 5))
    >>> soilTh = mg.add_zeros("soil__depth", at="node")
    >>> z = mg.add_zeros("topographic__elevation", at="node")
    >>> BRz = mg.add_zeros("bedrock__elevation", at="node")
    >>> expweath = ExponentialWeatherer(mg)
    >>> DDdiff = DepthDependentDiffuser(mg)
    >>> expweath.calc_soil_prod_rate()
    >>> np.allclose(mg.at_node["soil_production__rate"][mg.core_nodes], 1.0)
    True
    >>> DDdiff.run_one_step(2.0)
    >>> np.allclose(mg.at_node["topographic__elevation"][mg.core_nodes], 0.0)
    True
    >>> np.allclose(mg.at_node["bedrock__elevation"][mg.core_nodes], -2.0)
    True
    >>> np.allclose(mg.at_node["soil__depth"][mg.core_nodes], 2.0)
    True

    Now with a slope:

    >>> mg = RasterModelGrid((3, 5))
    >>> soilTh = mg.add_zeros("soil__depth", at="node")
    >>> z = mg.add_zeros("topographic__elevation", at="node")
    >>> BRz = mg.add_zeros("bedrock__elevation", at="node")
    >>> z += mg.node_x.copy()
    >>> BRz += mg.node_x / 2.0
    >>> soilTh[:] = z - BRz
    >>> expweath = ExponentialWeatherer(mg)
    >>> DDdiff = DepthDependentDiffuser(mg)
    >>> expweath.calc_soil_prod_rate()
    >>> np.allclose(
    ...     mg.at_node["soil_production__rate"][mg.core_nodes],
    ...     np.array([0.60653066, 0.36787944, 0.22313016]),
    ... )
    True
    >>> DDdiff.run_one_step(2.0)
    >>> np.allclose(
    ...     mg.at_node["topographic__elevation"][mg.core_nodes],
    ...     np.array([1.47730244, 2.28949856, 3.17558975]),
    ... )
    True
    >>> np.allclose(
    ...     mg.at_node["bedrock__elevation"][mg.core_nodes],
    ...     np.array([-0.71306132, 0.26424112, 1.05373968]),
    ... )
    True
    >>> np.allclose(mg.at_node["soil__depth"], z - BRz)
    True

    Now, we'll test that changing the transport decay depth behaves as expected.

    >>> mg = RasterModelGrid((3, 5))
    >>> soilTh = mg.add_zeros("soil__depth", at="node")
    >>> z = mg.add_zeros("topographic__elevation", at="node")
    >>> BRz = mg.add_zeros("bedrock__elevation", at="node")
    >>> z += mg.node_x.copy() ** 0.5
    >>> BRz = z.copy() - 1.0
    >>> soilTh[:] = z - BRz
    >>> expweath = ExponentialWeatherer(mg)
    >>> DDdiff = DepthDependentDiffuser(mg, soil_transport_decay_depth=0.1)
    >>> DDdiff.run_one_step(1)
    >>> soil_decay_depth_point1 = mg.at_node["topographic__elevation"][mg.core_nodes]
    >>> z[:] = 0
    >>> z += mg.node_x.copy() ** 0.5
    >>> BRz = z.copy() - 1.0
    >>> soilTh[:] = z - BRz
    >>> DDdiff = DepthDependentDiffuser(mg, soil_transport_decay_depth=1.0)
    >>> DDdiff.run_one_step(1)
    >>> soil_decay_depth_1 = mg.at_node["topographic__elevation"][mg.core_nodes]
    >>> np.greater(soil_decay_depth_1[1], soil_decay_depth_point1[1])
    False

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Barnhart, K., Glade, R., Shobe, C., Tucker, G. (2019). Terrainbento 1.0: a
    Python package for multi-model analysis in long-term drainage basin
    evolution. Geoscientific Model Development  12(4), 1267--1297.
    https://dx.doi.org/10.5194/gmd-12-1267-2019

    **Additional References**

    Johnstone, S., Hilley, G. (2015). Lithologic control on the form of
    soil-mantled hillslopes Geology  43(1), 83-86.
    https://doi.org/10.1130/G36052.1

    """

    _name = "DepthDependentDiffuser"

    _unit_agnostic = True

    _cite_as = """
    @article{barnhart2019terrain,
      author = {Barnhart, Katherine R and Glade, Rachel C and Shobe, Charles M
                and Tucker, Gregory E},
      title = {{Terrainbento 1.0: a Python package for multi-model analysis in
                long-term drainage basin evolution}},
      doi = {10.5194/gmd-12-1267-2019},
      pages = {1267---1297},
      number = {4},
      volume = {12},
      journal = {Geoscientific Model Development},
      year = {2019},
    }
    """

    _info = {
        "bedrock__elevation": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "elevation of the bedrock surface",
        },
        "soil__depth": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of soil or weathered bedrock",
        },
        "soil__flux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m^2/yr",
            "mapping": "link",
            "doc": "flux of soil in direction of link",
        },
        "soil_production__rate": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m/yr",
            "mapping": "node",
            "doc": "rate of soil production at nodes",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__slope": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/m",
            "mapping": "link",
            "doc": "gradient of the ground surface",
        },
    }

    def __init__(self, grid, linear_diffusivity=1.0, soil_transport_decay_depth=1.0):
        """
        Parameters
        ----------
        grid: ModelGrid
            Landlab ModelGrid object
        linear_diffusivity: float
            Hillslope diffusivity, m**2/yr
        soil_transport_decay_depth: float
            Characteristic transport soil depth, m
        """
        super().__init__(grid)
        # Store grid and parameters

        self._K = linear_diffusivity
        self._soil_transport_decay_depth = soil_transport_decay_depth

        # get reference to inputs
        self._elev = self._grid.at_node["topographic__elevation"]
        self._soil_prod_rate = self._grid.at_node["soil_production__rate"]
        self._depth = self._grid.at_node["soil__depth"]

        # create outputs if necessary and get reference.
        self.initialize_output_fields()
        self._slope = self._grid.at_link["topographic__slope"]
        self._flux = self._grid.at_link["soil__flux"]
        self._bedrock = self._grid.at_node["bedrock__elevation"]

    def soilflux(self, dt):
        """Calculate soil flux for a time period 'dt'.

        Parameters
        ----------

        dt: float (time)
            The imposed timestep.
        """

        # update soil thickness
        self._grid.at_node["soil__depth"][:] = (
            self._grid.at_node["topographic__elevation"]
            - self._grid.at_node["bedrock__elevation"]
        )

        # Calculate soil depth at links.
        H_link = self._grid.map_value_at_max_node_to_link(
            "topographic__elevation", "soil__depth"
        )

        # Calculate gradients
        slope = self._grid.calc_grad_at_link(self._elev)
        slope[self._grid.status_at_link == LinkStatus.INACTIVE] = 0.0

        # Calculate flux
        self._flux[:] = (
            -self._K
            * slope
            * self._soil_transport_decay_depth
            * (1.0 - np.exp(-H_link / self._soil_transport_decay_depth))
        )

        # Calculate flux divergence
        dqdx = self._grid.calc_flux_div_at_node(self._flux)

        # Calculate change in soil depth
        dhdt = self._soil_prod_rate - dqdx

        # Calculate soil depth at nodes
        self._depth[self._grid.core_nodes] += dhdt[self._grid.core_nodes] * dt

        # prevent negative soil thickness
        self._depth[self._depth < 0.0] = 0.0

        # Calculate bedrock elevation
        self._bedrock[self._grid.core_nodes] -= (
            self._soil_prod_rate[self._grid.core_nodes] * dt
        )

        # Update topography
        self._elev[self._grid.core_nodes] = (
            self._depth[self._grid.core_nodes] + self._bedrock[self._grid.core_nodes]
        )

    def run_one_step(self, dt):
        """

        Parameters
        ----------
        dt: float (time)
            The imposed timestep.
        """

        self.soilflux(dt)



================================================
File: depth_dependent_taylor_soil_creep/__init__.py
================================================
from .hillslope_depth_dependent_taylor_flux import DepthDependentTaylorDiffuser

__all__ = ["DepthDependentTaylorDiffuser"]



================================================
File: depth_dependent_taylor_soil_creep/hillslope_depth_dependent_taylor_flux.py
================================================
"""DepthDependentTaylorNonLinearDiffuser Component.

@author: R Glade
@author: K Barnhart
@author: G Tucker
"""

import numpy as np

from landlab import Component
from landlab import LinkStatus
from landlab.core.messages import deprecation_message


class DepthDependentTaylorDiffuser(Component):
    r"""
    This component implements a depth-dependent Taylor series diffusion rule,
    combining concepts of Ganti et al. (2012) and Johnstone and Hilley (2014).

    Hillslope sediment flux uses a Taylor Series expansion of the Andrews-
    Bucknam formulation of nonlinear hillslope flux derived following following
    Ganti et al., 2012 with a depth dependent component inspired Johnstone and
    Hilley (2014). The flux :math:`q_s` is given as:

    .. math::

        q_s = - K H_* \nabla \eta (
                1 + (S/S_c)^2 + (S/S_c)^4 + .. + (S/S_c)^2(n-1)
            ) (1 - exp( - H / H_*)

    where :math:`K` is a transport velocity coefficient, :math:`\eta` is land
    surface elevation, :math:`S` is the slope gradient (defined as
    positive downward), :math:`S_c` is the critical slope, :math:`n` is the
    number of terms, :math:`H` is the soil depth on links, and :math:`H_*` is
    the soil transport decay depth.

    The default behavior uses two terms to produce a slope dependence as
    described by Equation 6 of Ganti et al. (2012).

    This component will ignore soil thickness located at non-core nodes.

    Examples
    --------
    First lets make a simple example with flat topography.

    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import ExponentialWeatherer
    >>> from landlab.components import DepthDependentTaylorDiffuser
    >>> mg = RasterModelGrid((5, 5))
    >>> soilTh = mg.add_zeros("soil__depth", at="node")
    >>> z = mg.add_zeros("topographic__elevation", at="node")
    >>> BRz = mg.add_zeros("bedrock__elevation", at="node")
    >>> expweath = ExponentialWeatherer(mg)
    >>> DDdiff = DepthDependentTaylorDiffuser(mg)
    >>> expweath.calc_soil_prod_rate()
    >>> np.allclose(mg.at_node["soil_production__rate"][mg.core_nodes], 1.0)
    True
    >>> DDdiff.run_one_step(2.0)
    >>> np.allclose(mg.at_node["topographic__elevation"][mg.core_nodes], 0.0)
    True
    >>> np.allclose(mg.at_node["bedrock__elevation"][mg.core_nodes], -2.0)
    True
    >>> np.allclose(mg.at_node["soil__depth"][mg.core_nodes], 2.0)
    True

    Now a more complicated example with a slope.

    >>> mg = RasterModelGrid((3, 5))
    >>> soilTh = mg.add_zeros("soil__depth", at="node")
    >>> z = mg.add_zeros("topographic__elevation", at="node")
    >>> BRz = mg.add_zeros("bedrock__elevation", at="node")
    >>> z += mg.node_x.copy()
    >>> BRz += mg.node_x / 2.0
    >>> soilTh[:] = z - BRz
    >>> expweath = ExponentialWeatherer(mg)
    >>> DDdiff = DepthDependentTaylorDiffuser(mg)
    >>> expweath.calc_soil_prod_rate()
    >>> np.allclose(
    ...     mg.at_node["soil_production__rate"][mg.core_nodes],
    ...     np.array([0.60653066, 0.36787944, 0.22313016]),
    ... )
    True
    >>> DDdiff.run_one_step(0.1)
    >>> np.allclose(
    ...     mg.at_node["topographic__elevation"][mg.core_nodes],
    ...     np.array([1.04773024, 2.02894986, 3.01755898]),
    ... )
    True
    >>> np.allclose(
    ...     mg.at_node["bedrock__elevation"][mg.core_nodes],
    ...     np.array([0.43934693, 0.96321206, 1.47768698]),
    ... )
    True
    >>> np.allclose(mg.at_node["soil__depth"], z - BRz)
    True

    The DepthDependentTaylorDiffuser makes and moves soil at a rate proportional
    to slope, this means that there is a characteristic time scale for soil
    transport and an associated stability criteria for the timestep. The
    maximum characteristic time scale, :math:`De_{max}`, is given as a function of the
    hillslope diffustivity, :math:`D`, the maximum slope, :math:`S_{max}`,
    and the critical slope :math:`S_c`.

    .. math::

        De_{max} = D
            \left(
            1 +
            \left( \frac{S_{max}{S_c}\right )^2 +
            \left( \frac{S_{max}{S_c}\right )^4 +
            \dots +
            \left( \frac{S_{max}{S_c}\right )^{( 2 * ( n - 1 ))}
            \right)

    The maximum stable time step is given by

    .. math::

        dtmax = courant_factor * dx * dx / Demax

    Where the courant factor is a user defined scale (default is 0.2), and
    dx is the length of the shortest link in the grid.

    The DepthDependentTaylorDiffuser has a boolean flag that permits a user
    to be warned if timesteps are too large for the slopes in the model grid
    (if_unstable = 'warn') and a boolean flag that turns on dynamic timestepping
    (dynamic_dt = False).

    >>> DDdiff = DepthDependentTaylorDiffuser(mg, if_unstable="warn")
    >>> DDdiff.run_one_step(2.0)
    Topographic slopes are high enough such that the Courant condition is
    exceeded AND you have not selected dynamic timestepping with
    dynamic_dt=True. This may lead to infinite and/or nan values for slope,
    elevation, and soil depth. Consider using a smaller time step or dynamic
    timestepping. The Courant condition recommends a timestep of
    0.09534076073069653 or smaller.

    Alternatively you can specify if_unstable='raise', and a Runtime Error will
    be raised if this condition is not met.

    Next, lets do an example with dynamic timestepping.

    >>> mg = RasterModelGrid((3, 5))
    >>> soilTh = mg.add_zeros("soil__depth", at="node")
    >>> z = mg.add_zeros("topographic__elevation", at="node")
    >>> BRz = mg.add_zeros("bedrock__elevation", at="node")

    We'll use a steep slope and very little soil.

    >>> z += mg.node_x.copy() ** 2
    >>> BRz = z.copy() - 1.0
    >>> soilTh[:] = z - BRz
    >>> expweath = ExponentialWeatherer(mg)

    Lets try to move the soil with a large timestep. Without dynamic time
    steps, this gives a warning that we've exceeded the dynamic timestep size
    and should use a smaller timestep. We could either use the smaller timestep,
    or specify that we want to use a dynamic timestep.

    >>> DDdiff = DepthDependentTaylorDiffuser(mg, if_unstable="warn", dynamic_dt=False)
    >>> expweath.calc_soil_prod_rate()
    >>> DDdiff.run_one_step(10)
    Topographic slopes are high enough such that the Courant condition is
    exceeded AND you have not selected dynamic timestepping with
    dynamic_dt=True. This may lead to infinite and/or nan values for slope,
    elevation, and soil depth. Consider using a smaller time step or dynamic
    timestepping. The Courant condition recommends a timestep of
    0.004 or smaller.

    Now, we'll re-build the grid and do the same example with dynamic timesteps.

    >>> mg = RasterModelGrid((3, 5))
    >>> soilTh = mg.add_zeros("soil__depth", at="node")
    >>> z = mg.add_zeros("topographic__elevation", at="node")
    >>> BRz = mg.add_zeros("bedrock__elevation", at="node")
    >>> z += mg.node_x.copy() ** 2
    >>> BRz = z.copy() - 1.0
    >>> soilTh[:] = z - BRz
    >>> expweath = ExponentialWeatherer(mg)
    >>> DDdiff = DepthDependentTaylorDiffuser(mg, if_unstable="warn", dynamic_dt=True)
    >>> expweath.calc_soil_prod_rate()
    >>> DDdiff.run_one_step(10)
    >>> np.any(np.isnan(z))
    False

    Now, we'll test that changing the transport decay depth behaves as expected.

    >>> mg = RasterModelGrid((3, 5))
    >>> soilTh = mg.add_zeros("soil__depth", at="node")
    >>> z = mg.add_zeros("topographic__elevation", at="node")
    >>> BRz = mg.add_zeros("bedrock__elevation", at="node")
    >>> z += mg.node_x.copy() ** 0.5
    >>> BRz = z.copy() - 1.0
    >>> soilTh[:] = z - BRz
    >>> expweath = ExponentialWeatherer(mg)
    >>> DDdiff = DepthDependentTaylorDiffuser(mg, soil_transport_decay_depth=0.1)
    >>> DDdiff.run_one_step(1)
    >>> soil_decay_depth_point1 = mg.at_node["topographic__elevation"][mg.core_nodes]
    >>> z[:] = 0
    >>> z += mg.node_x.copy() ** 0.5
    >>> BRz = z.copy() - 1.0
    >>> soilTh[:] = z - BRz
    >>> DDdiff = DepthDependentTaylorDiffuser(mg, soil_transport_decay_depth=1.0)
    >>> DDdiff.run_one_step(1)
    >>> soil_decay_depth_1 = mg.at_node["topographic__elevation"][mg.core_nodes]
    >>> np.greater(soil_decay_depth_1[1], soil_decay_depth_point1[1])
    False

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Barnhart, K., Glade, R., Shobe, C., Tucker, G. (2019). Terrainbento 1.0: a
    Python package for multi-model analysis in long-term drainage basin
    evolution. Geoscientific Model Development  12(4), 1267--1297.
    https://dx.doi.org/10.5194/gmd-12-1267-2019

    **Additional References**

    Ganti, V., Passalacqua, P., Foufoula-Georgiou, E. (2012). A sub-grid scale
    closure for nonlinear hillslope sediment transport models Journal of
    Geophysical Research: Earth Surface  117(F2).
    https://dx.doi.org/10.1029/2011jf002181

    Johnstone, S., Hilley, G. (2015). Lithologic control on the form of
    soil-mantled hillslopes Geology  43(1), 83-86.
    https://doi.org/10.1130/G36052.1

    """

    _name = "DepthDependentTaylorDiffuser"

    _unit_agnostic = True

    _cite_as = """
    @article{barnhart2019terrain,
      author = {Barnhart, Katherine R and Glade, Rachel C and Shobe, Charles M
                and Tucker, Gregory E},
      title = {{Terrainbento 1.0: a Python package for multi-model analysis in
                long-term drainage basin evolution}},
      doi = {10.5194/gmd-12-1267-2019},
      pages = {1267---1297},
      number = {4},
      volume = {12},
      journal = {Geoscientific Model Development},
      year = {2019},
    }
    """

    _info = {
        "bedrock__elevation": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "elevation of the bedrock surface",
        },
        "soil__depth": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of soil or weathered bedrock",
        },
        "soil__flux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m^2/yr",
            "mapping": "link",
            "doc": "flux of soil in direction of link",
        },
        "soil_production__rate": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m/yr",
            "mapping": "node",
            "doc": "rate of soil production at nodes",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__slope": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/m",
            "mapping": "link",
            "doc": "gradient of the ground surface",
        },
    }

    def __init__(
        self,
        grid,
        linear_diffusivity=None,
        slope_crit=1.0,
        soil_transport_decay_depth=1.0,
        nterms=2,
        dynamic_dt=False,
        if_unstable="pass",
        courant_factor=0.2,
        soil_transport_velocity=1.0,
    ):
        """Initialize the DepthDependentTaylorDiffuser.

        Parameters
        ----------
        grid: ModelGrid
            Landlab ModelGrid object
        linear_diffusivity: float, optional, DEPRECATED
            Hillslope diffusivity / decay depth, m/yr
            Default = 1.0
        slope_crit: float, optional
            Critical gradient parameter, m/m
            Default = 1.0
        soil_transport_decay_depth: float, optional
            characteristic transport soil depth, m
            Default = 1.0
        nterms: int, optional. default = 2
            number of terms in the Taylor expansion.
            Two terms (default) gives the behavior
            described in Ganti et al. (2012).
        dynamic_dt : bool, optional, default  = False
            Whether internal timestepping is used.
        if_unstable : str, optional, default = "pass"
            What to do if unstable (options are "pass",
            "raise", "warn")
        courant_factor : float, optional, default = 0.2
            Courant factor for timestep calculation.
        soil_transport_velocity : float, optional, default = 1.0
            Velocity parameter for soil transport, m/yr. Diffusivity is the
            product of this parameter and soil_transport_decay_depth.
        """
        super().__init__(grid)

        # Handle now-deprecated diffusivity argument
        if linear_diffusivity is None:
            self._K = soil_transport_velocity
        else:
            message = """Use of linear_diffusivity is deprecated, because the
                         name is misleading: it is actually a velocity;
                         diffusivity is obtained by multiplying by soil
                         transport decay depth. Use soil_transport_velocity
                         instead."""
            print(deprecation_message(message))
            self._K = linear_diffusivity
        self._soil_transport_decay_depth = soil_transport_decay_depth
        self._slope_crit = slope_crit
        self._nterms = nterms

        self._dynamic_dt = dynamic_dt
        self._if_unstable = if_unstable
        self._courant_factor = courant_factor
        self._shortest_link = np.amin(grid.length_of_link)  # for Courant

        # get reference to inputs
        self._elev = self._grid.at_node["topographic__elevation"]
        self._soil_prod_rate = self._grid.at_node["soil_production__rate"]
        self._depth = self._grid.at_node["soil__depth"]

        # create outputs if necessary and get reference.
        self.initialize_output_fields()
        self._slope = self._grid.at_link["topographic__slope"]
        self._flux = self._grid.at_link["soil__flux"]
        self._bedrock = self._grid.at_node["bedrock__elevation"]

    def soilflux(self, dt):
        """Calculate soil flux for a time period 'dt'.

        Parameters
        ----------
        dt: float (time)
            The imposed timestep.
        """
        # establish time left as all of dt
        time_left = dt

        # begin while loop for time left
        while time_left > 0.0:
            # calculate soil__depth
            self._grid.at_node["soil__depth"][:] = (
                self._grid.at_node["topographic__elevation"]
                - self._grid.at_node["bedrock__elevation"]
            )

            # Calculate soil depth at links.
            self._H_link = self._grid.map_value_at_max_node_to_link(
                "topographic__elevation", "soil__depth"
            )

            # Calculate gradients
            self._slope = self._grid.calc_grad_at_link(self._elev)
            self._slope[self._grid.status_at_link == LinkStatus.INACTIVE] = 0.0

            # Test for time stepping courant condition
            # Test for time stepping courant condition
            courant_slope_term = 0.0
            courant_s_over_scrit = self._slope.max() / self._slope_crit
            for i in range(0, 2 * self._nterms, 2):
                courant_slope_term += courant_s_over_scrit**i
                if np.any(np.isinf(courant_slope_term)):
                    message = (
                        "Soil flux term is infinite in Courant condition "
                        "calculation. This is likely due to "
                        "using too many terms in the Taylor expansion."
                    )
                    raise RuntimeError(message)
            # Calculate De Max
            de_max = self._K * self._soil_transport_decay_depth * courant_slope_term
            # Calculate longest stable timestep
            self._dt_max = self._courant_factor * self._shortest_link**2 / de_max

            # Test for the Courant condition and print warning if user intended
            # for it to be printed.
            if (
                (self._dt_max < dt)
                and (not self._dynamic_dt)
                and (self._if_unstable != "pass")
            ):
                message = (
                    "Topographic slopes are high enough such that the "
                    "Courant condition is exceeded AND you have not "
                    "selected dynamic timestepping with dynamic_dt=True. "
                    "This may lead to infinite and/or nan values for "
                    "slope, elevation, and soil depth. Consider using a "
                    "smaller time step or dynamic timestepping. The "
                    "Courant condition recommends a timestep of "
                    "" + str(self._dt_max) + " or smaller."
                )
                if self._if_unstable == "raise":
                    raise RuntimeError(message)
                if self._if_unstable == "warn":
                    print(message)

            # if dynamic dt is selected, use it, otherwise, use the entire time
            if self._dynamic_dt:
                self._sub_dt = np.min([dt, self._dt_max])
                time_left -= self._sub_dt
            else:
                self._sub_dt = dt
                time_left = 0

            # update sed flux, topography, soil, and bedrock based on the
            # current self._sub_dt
            self._update_flux_topography_soil_and_bedrock()

    def _update_flux_topography_soil_and_bedrock(self):
        """Calculate soil flux and update topography."""
        # Calculate flux
        slope_term = 0.0
        s_over_scrit = self._slope / self._slope_crit
        for i in range(0, 2 * self._nterms, 2):
            slope_term += s_over_scrit**i
            if np.any(np.isinf(slope_term)):
                message = (
                    "Soil flux term is infinite. This is likely due to "
                    "using too many terms in the Taylor expansion."
                )
                raise RuntimeError(message)

        self._flux[:] = -(
            (self._K * self._slope * self._soil_transport_decay_depth)
            * (slope_term)
            * (1.0 - np.exp(-self._H_link / self._soil_transport_decay_depth))
        )

        # Calculate flux divergence
        dqdx = self._grid.calc_flux_div_at_node(self._flux)

        # Calculate change in soil depth
        dhdt = self._soil_prod_rate - dqdx

        # Calculate soil depth at nodes
        self._depth[self._grid.core_nodes] += dhdt[self._grid.core_nodes] * self._sub_dt

        # prevent negative soil thickness
        self._depth[self._depth < 0.0] = 0.0

        # Calculate bedrock elevation
        self._bedrock[self._grid.core_nodes] -= (
            self._soil_prod_rate[self._grid.core_nodes] * self._sub_dt
        )

        # Update topography
        self._elev[self._grid.core_nodes] = (
            self._depth[self._grid.core_nodes] + self._bedrock[self._grid.core_nodes]
        )

    def run_one_step(self, dt):
        """

        Parameters
        ----------
        dt: float (time)
            The imposed timestep.
        """
        self.soilflux(dt)



================================================
File: detachment_ltd_erosion/__init__.py
================================================
from .generate_detachment_ltd_erosion import DetachmentLtdErosion
from .generate_erosion_by_depth_slope import DepthSlopeProductErosion

__all__ = ["DetachmentLtdErosion", "DepthSlopeProductErosion"]



================================================
File: detachment_ltd_erosion/generate_detachment_ltd_erosion.py
================================================
"""Landlab component that simulates detachment-limited river erosion.

This component calculates changes in elevation in response to
vertical incision.
"""

import numpy as np

from landlab import Component


class DetachmentLtdErosion(Component):
    """Simulate detachment limited sediment transport.

    Landlab component that simulates detachment limited sediment transport is more
    general than the stream power component. Doesn't require the upstream node
    order, links to flow receiver and flow receiver fields. Instead, takes in
    the discharge values on NODES calculated by the OverlandFlow class and
    erodes the landscape in response to the output discharge.

    As of right now, this component relies on the OverlandFlow component
    for stability. There are no stability criteria implemented in this class.
    To ensure model stability, use StreamPowerEroder or FastscapeEroder
    components instead.

    .. codeauthor:: Jordan Adams

    Examples
    --------
    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import DetachmentLtdErosion

    Create a grid on which to calculate detachment ltd sediment transport.

    >>> grid = RasterModelGrid((4, 5))

    The grid will need some data to provide the detachment limited sediment
    transport component. To check the names of the fields that provide input to
    the detachment ltd transport component, use the *input_var_names* class
    property.

    Create fields of data for each of these input variables.

    >>> grid.at_node["topographic__elevation"] = [
    ...     [0.0, 0.0, 0.0, 0.0, 0.0],
    ...     [1.0, 1.0, 1.0, 1.0, 1.0],
    ...     [2.0, 2.0, 2.0, 2.0, 2.0],
    ...     [3.0, 3.0, 3.0, 3.0, 3.0],
    ... ]

    Using the set topography, now we will calculate slopes on all nodes.

    >>> grid.at_node["topographic__slope"] = [
    ...     [0.0, 0.0, 0.0, 0.0, 0.0],
    ...     [0.70710678, 1.0, 1.0, 1.0, 0.70710678],
    ...     [0.70710678, 1.0, 1.0, 1.0, 0.70710678],
    ...     [0.70710678, 1.0, 1.0, 1.0, 0.70710678],
    ... ]


    Now we will arbitrarily add water discharge to each node for simplicity.

    >>> grid.at_node["surface_water__discharge"] = [
    ...     [30.0, 30.0, 30.0, 30.0, 30.0],
    ...     [20.0, 20.0, 20.0, 20.0, 20.0],
    ...     [10.0, 10.0, 10.0, 10.0, 10.0],
    ...     [5.0, 5.0, 5.0, 5.0, 5.0],
    ... ]

    Instantiate the `DetachmentLtdErosion` component to work on this grid, and
    run it. In this simple case, we need to pass it a time step ('dt')

    >>> dt = 10.0
    >>> dle = DetachmentLtdErosion(grid)
    >>> dle.run_one_step(dt=dt)

    After calculating the erosion rate, the elevation field is updated in the
    grid. Use the *output_var_names* property to see the names of the fields that
    have been changed.

    >>> dle.output_var_names
    ('topographic__elevation',)

    The `topographic__elevation` field is defined at nodes.

    >>> dle.var_loc("topographic__elevation")
    'node'


    Now we test to see how the topography changed as a function of the erosion
    rate.

    >>> grid.at_node["topographic__elevation"].reshape(grid.shape)
    array([[0.        , 0.        , 0.        , 0.        , 0.        ],
           [0.99936754, 0.99910557, 0.99910557, 0.99910557, 0.99936754],
           [1.99955279, 1.99936754, 1.99936754, 1.99936754, 1.99955279],
           [2.99968377, 2.99955279, 2.99955279, 2.99955279, 2.99968377]])

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    Howard, A. (1994). A detachment-limited model of drainage basin evolution. Water
    Resources Research  30(7), 2261-2285. https://dx.doi.org/10.1029/94wr00757
    """

    _name = "DetachmentLtdErosion"

    _unit_agnostic = True

    _info = {
        "surface_water__discharge": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m**3/s",
            "mapping": "node",
            "doc": "Volumetric discharge of surface water",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__slope": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "-",
            "mapping": "node",
            "doc": "gradient of the ground surface",
        },
    }

    def __init__(
        self,
        grid,
        K_sp=0.00002,
        m_sp=0.5,
        n_sp=1.0,
        uplift_rate=0.0,
        entrainment_threshold=0.0,
        slope="topographic__slope",
    ):
        """Calculate detachment limited erosion rate on nodes.

        Landlab component that generalizes the detachment limited erosion
        equation, primarily to be coupled to the the Landlab OverlandFlow
        component.

        This component adjusts topographic elevation.

        Parameters
        ----------
        grid : RasterModelGrid
            A landlab grid.
        K_sp : float, optional
            K in the stream power equation (units vary with other parameters -
            if used with the de Almeida equation it is paramount to make sure
            the time component is set to *seconds*, not *years*!)
        m_sp : float, optional
            Stream power exponent, power on discharge
        n_sp : float, optional
            Stream power exponent, power on slope
        uplift_rate : float, optional
            changes in topographic elevation due to tectonic uplift
        entrainment_threshold : float, optional
            threshold for sediment movement
        slope : str
            Field name of an at-node field that contains the slope.
        """
        super().__init__(grid)

        assert slope in grid.at_node

        self._K = K_sp
        self._m = m_sp
        self._n = n_sp

        self._I = self._grid.zeros(at="node")  # noqa: E741
        self._uplift_rate = uplift_rate
        self._entrainment_threshold = entrainment_threshold

        self._dzdt = self._grid.zeros(at="node")

    def run_one_step(self, dt):
        """Erode into grid topography.

        For one time step, this erodes into the grid topography using
        the water discharge and topographic slope.

        The grid field 'topographic__elevation' is altered each time step.

        Parameters
        ----------
        dt : float
            Time step.
        """

        S = self._grid.at_node["topographic__slope"]
        Q = self._grid.at_node["surface_water__discharge"]

        Q_to_m = np.power(Q, self._m)

        S_to_n = np.power(S, self._n)

        self._I = (
            self._K * Q_to_m * S_to_n
        ) - self._entrainment_threshold  # noqa: E741

        self._I[self._I < 0.0] = 0.0

        self._dz = (self._uplift_rate - self._I) * dt

        self._grid["node"]["topographic__elevation"] += self._dz



================================================
File: detachment_ltd_erosion/generate_erosion_by_depth_slope.py
================================================
"""Landlab component that simulates detachment-limited river erosion.

This component calculates changes in elevation in response to
vertical incision.
"""

import numpy as np
import scipy.constants

from landlab import Component


class DepthSlopeProductErosion(Component):
    """Calculate erosion rate as a function of the depth-slope product.

    Erosion rate is calculated as, ``erosion_rate = k_e * ((tau ** a - tau_crit ** a))``

    *k_e*
        Erodibility coefficient
    *tau*
        Bed shear stress: ``tau = rho * g * h * S``
    *rho*
        Density of fluid
    *g*
        Gravitational acceleration
    *h*
        Water depths
    *S*
        Slope
    *tau_crit*
        Critical shear stress
    *a*
        Positive exponent


    Note this equation was presented in Tucker, G.T., 2004, Drainage basin
    sensitivity to tectonic and climatic forcing: Implications of a stochastic
    model for the role of entrainment and erosion thresholds,
    Earth Surface Processes and Landforms.

    More generalized than other erosion components, as it doesn't require the
    upstream node order, links to flow receiver and flow receiver fields. Instead,
    takes in the water depth and slope fields on NODES calculated by the
    OverlandFlow class and erodes the landscape in response to the hydrograph
    generted by that method.

    As of right now, this component relies on the OverlandFlow component
    for stability. There are no stability criteria implemented in this class.
    To ensure model stability, use StreamPowerEroder or FastscapeEroder
    components instead.

    .. codeauthor:: Jordan Adams

    Examples
    --------
    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import DepthSlopeProductErosion

    Create a grid on which to calculate detachment ltd sediment transport.

    >>> grid = RasterModelGrid((5, 5))

    The grid will need some data to provide the detachment limited sediment
    transport component. To check the names of the fields that provide input to
    the detachment ltd transport component, use the *input_var_names* class
    property.

    Create fields of data for each of these input variables.

    First create topography. This is a flat surface of elevation 10 m.

    >>> grid.at_node["topographic__elevation"] = np.ones(grid.number_of_nodes)
    >>> grid.at_node["topographic__elevation"] *= 10.0
    >>> grid.at_node["topographic__elevation"] = [
    ...     [10.0, 10.0, 10.0, 10.0, 10.0],
    ...     [10.0, 10.0, 10.0, 10.0, 10.0],
    ...     [10.0, 10.0, 10.0, 10.0, 10.0],
    ...     [10.0, 10.0, 10.0, 10.0, 10.0],
    ...     [10.0, 10.0, 10.0, 10.0, 10.0],
    ... ]

    Now we'll add an arbitrary water depth field on top of that topography.

    >>> grid.at_node["surface_water__depth"] = [
    ...     [5.0, 5.0, 5.0, 5.0, 5.0],
    ...     [4.0, 4.0, 4.0, 4.0, 4.0],
    ...     [3.0, 3.0, 3.0, 3.0, 3.0],
    ...     [2.0, 2.0, 2.0, 2.0, 2.0],
    ...     [1.0, 1.0, 1.0, 1.0, 1.0],
    ... ]

    Using the set topography, now we will calculate slopes on all nodes.

    First calculating slopes on links

    >>> grid.at_link["water_surface__slope"] = grid.calc_grad_at_link(
    ...     "surface_water__depth"
    ... )

    Now putting slopes on nodes

    >>> grid.at_node["water_surface__slope"] = (
    ...     grid.at_link["water_surface__slope"][grid.links_at_node]
    ...     * grid.active_link_dirs_at_node
    ... ).max(axis=1)
    >>> grid.at_node["water_surface__slope"][grid.core_nodes]
    array([1., 1., 1., 1., 1., 1., 1., 1., 1.])


    Instantiate the `DepthSlopeProductErosion` component to work on this grid, and
    run it. In this simple case, we need to pass it a time step ('dt') and also
    an erodibility factor ('k_e').

    >>> dt = 1.0
    >>> dspe = DepthSlopeProductErosion(
    ...     grid, k_e=0.00005, g=9.81, slope="water_surface__slope"
    ... )
    >>> dspe.run_one_step(
    ...     dt=dt,
    ... )

    Now we test to see how the topography changed as a function of the erosion
    rate. First, we'll look at the erosion rate:

    >>> dspe.dz.reshape(grid.shape)
    array([[ 0.    , -2.4525, -2.4525, -2.4525,  0.    ],
           [ 0.    , -1.962 , -1.962 , -1.962 ,  0.    ],
           [ 0.    , -1.4715, -1.4715, -1.4715,  0.    ],
           [ 0.    , -0.981 , -0.981 , -0.981 ,  0.    ],
           [ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ]])

    Now, our updated topography...

    >>> grid.at_node["topographic__elevation"].reshape(grid.shape)
    array([[10.    ,  7.5475,  7.5475,  7.5475, 10.    ],
           [10.    ,  8.038 ,  8.038 ,  8.038 , 10.    ],
           [10.    ,  8.5285,  8.5285,  8.5285, 10.    ],
           [10.    ,  9.019 ,  9.019 ,  9.019 , 10.    ],
           [10.    , 10.    , 10.    , 10.    , 10.    ]])
    """

    _name = "DepthSlopeProductErosion"

    _unit_agnostic = True

    _info = {
        "surface_water__depth": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of water on the surface",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__slope": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "-",
            "mapping": "node",
            "doc": "gradient of the ground surface",
        },
    }

    def __init__(
        self,
        grid,
        k_e=0.001,
        fluid_density=1000.0,
        g=scipy.constants.g,
        a_exp=1.0,
        tau_crit=0.0,
        uplift_rate=0.0,
        slope="topographic__slope",
    ):
        """Calculate detachment limited erosion rate on nodes using the shear
        stress equation, solved using the depth slope product.

        Landlab component that generalizes the detachment limited erosion
        equation, primarily to be coupled to the the Landlab OverlandFlow
        component.

        This component adjusts topographic elevation and is contained in the
        landlab.components.detachment_ltd_erosion folder.

        Parameters
        ----------
        grid : RasterModelGrid
            A landlab grid.
        k_e : float
            Erodibility parameter, (m^(1+a_exp)*s^(2*a_exp-1)/kg^a_exp)
        fluid_density : float, optional
            Density of fluid, default set to water density of 1000 kg / m^3
        g : float, optional
            Acceleration due to gravity (m/s^2).
        a_exp : float, optional
            exponent on shear stress, positive, unitless
        tau_crit : float, optional
            threshold for sediment movement, (kg/m/s^2)
        uplift_rate : float, optional
            uplift rate applied to the topographic surface, m/s
        slope : str
            Field name of an at-node field that contains the slope.
        """
        super().__init__(grid)

        assert slope in grid.at_node

        self._slope = slope
        self._a = a_exp
        self._g = g
        self._rho = fluid_density
        self._E = self._grid.zeros(at="node")
        self._uplift_rate = uplift_rate
        self._tau_crit = tau_crit
        self._k_e = k_e

        self._dz = self._grid.zeros(at="node")

    def run_one_step(self, dt):
        """Erode into grid topography.

        For one time step, this erodes into the grid topography using
        the water discharge and topographic slope.

        The grid field 'topographic__elevation' is altered each time step.

        Parameters
        ----------
        dt : float
            Time step.
        """
        S = self._grid.at_node[self._slope]
        h = self._grid.at_node["surface_water__depth"]

        self._tau = self._rho * self._g * h * S

        (greater_than_tc,) = np.where(self._tau >= self._tau_crit)
        (less_than_tc,) = np.where(self._tau < self._tau_crit)

        self._E[less_than_tc] = 0.0

        self._E[greater_than_tc] = self._k_e * (
            (self._tau[greater_than_tc] ** self._a) - (self._tau_crit**self._a)
        )

        self._E[self._E < 0.0] = 0.0

        self._dz = (self._uplift_rate - self._E) * dt

        self._grid["node"]["topographic__elevation"] += self._dz

    @property
    def dz(self):
        """Magnitude of change of the topographic__elevation due to erosion
        [L]."""
        return self._dz



================================================
File: diffusion/__init__.py
================================================
from .diffusion import LinearDiffuser

__all__ = ["LinearDiffuser"]



================================================
File: diffusion/diffusion.py
================================================
#! /usr/env/python
"""Component that models 2D diffusion using an explicit finite-volume method.

Created July 2013 GT Last updated March 2016 DEJH with LL v1.0 component
style
"""


import numpy as np

from landlab import Component
from landlab import LinkStatus
from landlab import NodeStatus
from landlab import RasterModelGrid

_ALPHA = 0.15  # time-step stability factor
# ^0.25 not restrictive enough at meter scales w S~1 (possible cases)


class LinearDiffuser(Component):
    """Linear diffusion of a Landlab field.

    Component assumes grid does not deform. If the boundary conditions on the
    grid change after component instantiation, be sure to also call
    :func:`updated_boundary_conditions` to ensure these are reflected in the
    component (especially if fixed_links are present).

    The ``method`` keyword allows control of the way the solver works.
    Note that the option 'resolve_on_patches' can result in
    somewhat counterintuitive behavior - this option tells the component to
    treat the diffusivity as a field **with directionality to it** (i.e., that
    the diffusivites are defined on links). Thus if all links have the same
    diffusivity value, with this flag active "effective" diffusivities
    at the nodes will be *higher* than this value (by a factor of root 2) as
    the diffusivity at each patch will be the mean vector sum of that at the
    bounding links.

    The primary method of this class is :func:`run_one_step`.

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> import numpy as np

    >>> grid = RasterModelGrid((9, 9))
    >>> z = grid.add_zeros("topographic__elevation", at="node")
    >>> z.reshape((9, 9))[4, 4] = 1.0
    >>> grid.set_closed_boundaries_at_grid_edges(True, True, True, True)
    >>> ld = LinearDiffuser(grid, linear_diffusivity=1.0)
    >>> ld.run_one_step(1.0)
    >>> np.isclose(z[grid.core_nodes].sum(), 1.0)
    True

    >>> grid = RasterModelGrid((5, 30))
    >>> z2 = grid.add_zeros("topographic__elevation", at="node")
    >>> z2.reshape((5, 30))[2, 8] = 1.0
    >>> z2.reshape((5, 30))[2, 22] = 1.0
    >>> grid.set_closed_boundaries_at_grid_edges(True, True, True, True)
    >>> kd = grid.node_x / grid.node_x.mean()
    >>> ld2 = LinearDiffuser(grid, linear_diffusivity=kd)
    >>> for _ in range(10):
    ...     ld2.run_one_step(0.1)
    ...
    >>> z2[grid.core_nodes].sum() == 2.0
    True
    >>> z2.reshape((5, 30))[2, 8] > z2.reshape((5, 30))[2, 22]
    True

    An example using links:

    >>> grid1 = RasterModelGrid((10, 10), xy_spacing=100.0)
    >>> grid2 = RasterModelGrid((10, 10), xy_spacing=100.0)
    >>> z1 = grid1.add_zeros("topographic__elevation", at="node")
    >>> z2 = grid2.add_zeros("topographic__elevation", at="node")
    >>> dt = 1.0
    >>> nt = 10
    >>> grid2.at_link["surface_water__discharge"] = np.full(
    ...     grid2.number_of_links, 10000.0
    ... )
    >>> dfn1 = LinearDiffuser(grid1, linear_diffusivity=10000.0)
    >>> dfn2 = LinearDiffuser(grid2, linear_diffusivity="surface_water__discharge")
    >>> for i in range(nt):
    ...     z1[grid1.core_nodes] += 1.0
    ...     z2[grid2.core_nodes] += 1.0
    ...     dfn1.run_one_step(dt)
    ...     dfn2.run_one_step(dt)
    ...
    >>> np.allclose(z1, z2)
    True
    >>> z2.fill(0.0)
    >>> dfn2 = LinearDiffuser(
    ...     grid2,
    ...     linear_diffusivity="surface_water__discharge",
    ...     method="resolve_on_patches",
    ... )
    >>> for i in range(nt):
    ...     z2[grid2.core_nodes] += 1.0
    ...     dfn2.run_one_step(dt)
    ...
    >>> np.all(z2[grid2.core_nodes] < z1[grid2.core_nodes])
    True

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    Culling, W. (1963). Soil Creep and the Development of Hillside Slopes.
    The Journal of Geology  71(2), 127-161. https://dx.doi.org/10.1086/626891

    """

    _name = "LinearDiffuser"

    _unit_agnostic = True

    _info = {
        "hillslope_sediment__unit_volume_flux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m**2/s",
            "mapping": "link",
            "doc": "Volume flux per unit width along links",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__gradient": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "link",
            "doc": "Gradient of the ground surface",
        },
    }

    def __init__(self, grid, linear_diffusivity=0.01, method="simple", deposit=True):
        """
        Parameters
        ----------
        grid : ModelGrid
            A grid.
        linear_diffusivity : float, array, or field name (m**2/time)
            The diffusivity. If an array or field name, these must be the
            diffusivities on either nodes or links - the component will
            distinguish which based on array length. Values on nodes will be
            mapped to links using an upwind scheme in the simple case.
        method : {'simple', 'resolve_on_patches'}
            The method used to represent the fluxes. 'simple' solves a finite
            difference method with a simple staggered grid scheme onto the links.
            'resolve_on_patches' solves the scheme by mapping both slopes and
            diffusivities onto the patches and solving there before resolving
            values back to the nodes (and at the moment requires a raster grid).
            Note that this scheme enforces directionality in the diffusion field;
            it's no longer just a scalar field. Thus diffusivities must be defined
            *on links* when this option is chosen.
        deposit : {True, False}
            Whether diffusive material can be deposited. True means that diffusive
            material will be deposited if the divergence of sediment flux is
            negative. False means that even when the divergence of sediment flux is
            negative, no material is deposited. (No deposition ever.) The False
            case is a bit of a band-aid to account for cases when fluvial incision
            likely removes any material that would be deposited. If one couples
            fluvial detachment-limited incision with linear diffusion, the channels
            will not reach the predicted analytical solution unless deposit is set
            to False.
        """
        super().__init__(grid)

        self._bc_set_code = self._grid.bc_set_code
        method = self._validate_method(method)

        if method == "resolve_on_patches" and not isinstance(grid, RasterModelGrid):
            raise TypeError(
                "the resolve_on_patches method is only available for RasterModelGrid."
            )

        self._use_patches = method == "resolve_on_patches"

        self._current_time = 0.0
        self._run_before = False

        self._kd = self._validate_linear_diffusivity(grid, linear_diffusivity)

        if self._use_patches and np.ndim(self._kd) == 0:
            self._kd = np.broadcast_to(self._kd, grid.number_of_links)

        self._kd_on_links = np.size(self._kd) == grid.number_of_links

        if self._kd_on_links and not isinstance(grid, RasterModelGrid):
            raise TypeError(
                "linear_diffusivity defined at links is only available for"
                " RasterModelGrid."
            )

        # if we're using patches, it is VITAL that diffusivity is defined on
        # links. The whole point of this functionality is that we honour
        # *directionality* in the diffusivities.
        if self._use_patches and not self._kd_on_links:
            raise ValueError(
                "if using the resolve_on_patches method, linear_diffusivity"
                " must be defined at links."
            )

        # set _deposit flag to tell code whether or not diffusion can deposit.

        self._deposit = deposit

        self._values_to_diffuse = "topographic__elevation"

        # Set internal time step
        # ..todo:
        #   implement mechanism to compute time-steps dynamically if grid is
        #   adaptive/changing
        # as of modern componentization (Spring '16), this can take arrays
        # and irregular grids
        # CFL condition precalc:
        CFL_prefactor = (
            _ALPHA * self._grid.length_of_link[: self._grid.number_of_links] ** 2.0
        )

        self._CFL_actives_prefactor = CFL_prefactor[self._grid.active_links]
        # ^note we can do this as topology shouldn't be changing

        # Get a list of interior cells
        self._interior_cells = self._grid.node_at_core_cell

        self._z = self._grid.at_node[self._values_to_diffuse]
        self._dqsds = self._grid.zeros(at="node", dtype=float)

        for name in ("topographic__gradient", "hillslope_sediment__unit_volume_flux"):
            if name not in self._grid.at_link:
                self._grid.add_zeros(name, at="link")

        if self._use_patches or self._kd_on_links:
            mg = self._grid
            try:
                self._hoz = self.grid.horizontal_links
                self._vert = self.grid.vertical_links
            except AttributeError:
                pass
            self._x_link_patches = mg.patches_at_link[self._hoz]
            x_link_patch_pres = mg.patches_present_at_link[self._hoz]
            self._x_link_patch_mask = np.logical_not(x_link_patch_pres)
            self._y_link_patches = mg.patches_at_link[self._vert]
            y_link_patch_pres = mg.patches_present_at_link[self._vert]
            self._y_link_patch_mask = np.logical_not(y_link_patch_pres)
            self._hoz_link_neighbors = np.empty((self._hoz.size, 4), dtype=int)
            self._vert_link_neighbors = np.empty((self._vert.size, 4), dtype=int)

        # do some pre-work to make fixed grad BC updating faster in the loop:
        self.updated_boundary_conditions()

    @staticmethod
    def _validate_method(method):
        valid_methods = {"simple", "resolve_on_patches"}

        if method not in valid_methods:
            raise ValueError(
                f"method {method} not understood"
                f" (must be one of {', '.join(sorted(valid_methods))})."
            )
        return method

    @staticmethod
    def _validate_linear_diffusivity(grid, linear_diffusivity):
        if isinstance(linear_diffusivity, str):
            if linear_diffusivity in grid.at_link:
                k = grid.at_link[linear_diffusivity]
            elif linear_diffusivity in grid.at_node:
                k = grid.at_node[linear_diffusivity]
            else:
                raise ValueError(
                    f"linear_diffusivity {linear_diffusivity!r}, it must be defined "
                    "at either nodes, or links."
                )
        elif np.ndim(linear_diffusivity) == 0:
            k = float(linear_diffusivity)
        else:
            k = np.asarray(linear_diffusivity)
            if k.size not in (grid.number_of_nodes, grid.number_of_links):
                raise ValueError(
                    "linear_diffusivity must be defined at either nodes, or links."
                )

        return k

    @property
    def fixed_grad_nodes(self):
        """Fixed gradient nodes."""
        return self._fixed_grad_nodes

    @property
    def fixed_grad_anchors(self):
        """Fixed gradient anchors."""
        return self._fixed_grad_anchors

    @property
    def fixed_grad_offsets(self):
        """Fixed gradient offsets."""
        return self._fixed_grad_offsets

    def updated_boundary_conditions(self):
        """Call if grid BCs are updated after component instantiation.

        Sets `fixed_grad_nodes`, `fixed_grad_anchors`, & `fixed_grad_offsets`,
        such that::

            value[fixed_grad_nodes] = value[fixed_grad_anchors] + offset

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> import numpy as np
        >>> mg = RasterModelGrid((4, 5))
        >>> z = mg.add_zeros("topographic__elevation", at="node")
        >>> z[mg.core_nodes] = 1.0
        >>> ld = LinearDiffuser(mg, linear_diffusivity=1.0)
        >>> ld.fixed_grad_nodes.size == 0
        True
        >>> ld.fixed_grad_anchors.size == 0
        True
        >>> ld.fixed_grad_offsets.size == 0
        True
        >>> mg.at_link["topographic__slope"] = mg.calc_grad_at_link(
        ...     "topographic__elevation"
        ... )
        >>> mg.status_at_node[mg.perimeter_nodes] = mg.BC_NODE_IS_FIXED_GRADIENT
        >>> ld.updated_boundary_conditions()
        >>> ld.fixed_grad_nodes
        array([ 1,  2,  3,  5,  9, 10, 14, 16, 17, 18])
        >>> ld.fixed_grad_anchors
        array([ 6,  7,  8,  6,  8, 11, 13, 11, 12, 13])
        >>> ld.fixed_grad_offsets
        array([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1.])
        >>> np.allclose(
        ...     z[ld.fixed_grad_nodes], z[ld.fixed_grad_anchors] + ld.fixed_grad_offsets
        ... )
        True
        """
        fixed_grad_nodes = np.where(
            self._grid.status_at_node == NodeStatus.FIXED_GRADIENT
        )[0]
        heads = self._grid.node_at_link_head[self._grid.fixed_links]
        tails = self._grid.node_at_link_tail[self._grid.fixed_links]
        head_is_fixed = np.isin(heads, fixed_grad_nodes)
        self._fixed_grad_nodes = np.where(head_is_fixed, heads, tails)
        self._fixed_grad_anchors = np.where(head_is_fixed, tails, heads)
        vals = self._grid.at_node[self._values_to_diffuse]
        self._fixed_grad_offsets = (
            vals[self._fixed_grad_nodes] - vals[self._fixed_grad_anchors]
        )

        if self._kd_on_links or self._use_patches:
            mg = self._grid
            x_link_patch_pres = mg.patches_present_at_link[self._hoz]
            self._x_link_patch_mask = np.logical_not(x_link_patch_pres)
            y_link_patch_pres = mg.patches_present_at_link[self._vert]
            self._y_link_patch_mask = np.logical_not(y_link_patch_pres)
            self._hoz_link_neighbors[:, :2] = mg.links_at_node[
                mg.node_at_link_head[self._hoz], 1:4:2
            ]
            self._hoz_link_neighbors[:, 2:] = mg.links_at_node[
                mg.node_at_link_tail[self._hoz], 1:4:2
            ]
            self._vert_link_neighbors[:, :2] = mg.links_at_node[
                mg.node_at_link_head[self._vert], 0:3:2
            ]
            self._vert_link_neighbors[:, 2:] = mg.links_at_node[
                mg.node_at_link_tail[self._vert], 0:3:2
            ]
            self._vert_link_badlinks = np.logical_or(
                mg.status_at_link[self._vert_link_neighbors] == LinkStatus.INACTIVE,
                self._vert_link_neighbors == -1,
            )
            self._hoz_link_badlinks = np.logical_or(
                mg.status_at_link[self._hoz_link_neighbors] == LinkStatus.INACTIVE,
                self._hoz_link_neighbors == -1,
            )

    def run_one_step(self, dt):
        """Run the diffuser for one timestep, dt.

        If the imposed timestep dt is longer than the Courant-Friedrichs-Lewy
        condition for the diffusion, this timestep will be internally divided
        as the component runs, as needed.

        Parameters
        ----------
        dt : float (time)
            The imposed timestep.
        """
        gradient = self._grid.at_link["topographic__gradient"]
        sediment_flux = self._grid.at_link["hillslope_sediment__unit_volume_flux"]

        mg = self._grid
        z = self._grid.at_node[self._values_to_diffuse]

        if not self._run_before:
            self.updated_boundary_conditions()  # just in case
            self._run_before = True
        if self._bc_set_code != self._grid.bc_set_code:
            self.updated_boundary_conditions()
            self._bc_set_code = self._grid.bc_set_code

        core_nodes = self._grid.node_at_core_cell
        # do mapping of array kd here, in case it points at an updating
        # field:
        if isinstance(self._kd, np.ndarray):
            if not self._kd_on_links:
                kd_links = self._grid.map_max_of_link_nodes_to_link(self._kd)
                kd_activelinks = kd_links[self._grid.active_links]
                # re-derive CFL condition, as could change dynamically:
                dt_links = self._CFL_actives_prefactor / kd_activelinks
                self._dt = np.nanmin(dt_links)
            else:
                kd_links = self._kd
                kd_activelinks = self._kd[self._grid.active_links]
                dt_links = self._CFL_actives_prefactor / kd_activelinks
                self._dt_links = dt_links
                self._dt = np.nanmin(np.fabs(dt_links))
        else:
            kd_activelinks = self._kd
            # re-derive CFL condition, as could change dynamically:
            dt_links = self._CFL_actives_prefactor / kd_activelinks
            self._dt = np.nanmin(dt_links)

        if self._use_patches:
            # need this else diffusivities on inactive links deform off-angle
            # calculations
            kd_links = kd_links.copy()
            kd_links[self._grid.status_at_link == LinkStatus.INACTIVE] = 0.0

        # Take the smaller of delt or built-in time-step size self._dt
        self._tstep_ratio = dt / self._dt
        repeats = int(self._tstep_ratio // 1.0)
        extra_time = self._tstep_ratio - repeats

        # Can really get into trouble if no diffusivity happens but we run...
        if self._dt < np.inf:
            loops = repeats + 1
        else:
            loops = 0
        for i in range(loops):
            grads = mg.calc_grad_at_link(z)
            gradient[mg.active_links] = grads[mg.active_links]
            if not self._use_patches:  # currently forbidden
                # if diffusivity is an array, self._kd is already
                # active_links-long
                sediment_flux[mg.active_links] = (
                    -kd_activelinks * gradient[mg.active_links]
                )
                # Calculate the net deposition/erosion rate at each node
                mg.calc_flux_div_at_node(sediment_flux, out=self._dqsds)
            else:  # project onto patches
                slx = mg.zeros("link")
                sly = mg.zeros("link")
                slx[self._hoz] = gradient[self._hoz]
                sly[self._vert] = gradient[self._vert]
                patch_dx, patch_dy = mg.calc_grad_at_patch(z)
                xvecs_vert = np.ma.array(
                    patch_dx[self._y_link_patches], mask=self._y_link_patch_mask
                )
                slx[self._vert] = xvecs_vert.mean()
                yvecs_hoz = np.ma.array(
                    patch_dy[self._x_link_patches], mask=self._x_link_patch_mask
                )
                sly[self._hoz] = yvecs_hoz.mean()
                # now map diffusivities (already on links, but we want
                # more spatial averaging)
                Kx = mg.zeros("link")
                Ky = mg.zeros("link")
                Kx[self._hoz] = kd_links[self._hoz]
                Ky[self._vert] = kd_links[self._vert]
                vert_link_crosslink_K = np.ma.array(
                    kd_links[self._vert_link_neighbors],
                    mask=self._vert_link_badlinks,
                )
                hoz_link_crosslink_K = np.ma.array(
                    kd_links[self._hoz_link_neighbors], mask=self._hoz_link_badlinks
                )
                Kx[self._vert] = vert_link_crosslink_K.mean(axis=1)
                Ky[self._hoz] = hoz_link_crosslink_K.mean(axis=1)
                Cslope = np.sqrt(slx**2 + sly**2)
                v = np.sqrt(Kx**2 + Ky**2)
                flux_links = v * Cslope
                # NEW, to resolve issue with K being off angle to S:
                # in fact, no. Doing this just makes this equivalent
                # to the basic diffuser, but with a bunch more crap
                # involved.
                # flux_x = slx * Kx
                # flux_y = sly * Ky
                # flux_links = np.sqrt(flux_x*flux_x + flux_y*flux_y)
                theta = np.arctan(np.fabs(sly) / (np.fabs(slx) + 1.0e-10))
                flux_links[self._hoz] *= np.sign(slx[self._hoz]) * np.cos(
                    theta[self._hoz]
                )
                flux_links[self._vert] *= np.sign(sly[self._vert]) * np.sin(
                    theta[self._vert]
                )
                # zero out the inactive links
                sediment_flux[mg.active_links] = -flux_links[mg.active_links]

                self._grid.calc_flux_div_at_node(sediment_flux, out=self._dqsds)

            # Calculate the total rate of elevation change
            dzdt = -self._dqsds
            if not self._deposit:
                dzdt[np.where(dzdt > 0)] = 0.0
            # Update the elevations
            timestep = self._dt
            if i == (repeats):
                timestep *= extra_time
            else:
                pass
            self._grid.at_node[self._values_to_diffuse][core_nodes] += (
                dzdt[core_nodes] * timestep
            )

            # check the BCs, update if fixed gradient
            vals = self._grid.at_node[self._values_to_diffuse]
            vals[self._fixed_grad_nodes] = (
                vals[self._fixed_grad_anchors] + self._fixed_grad_offsets
            )

    @property
    def time_step(self):
        """Returns internal time-step size (as a property)."""
        return self._dt



================================================
File: dimensionless_discharge/__init__.py
================================================
#!/usr/bin/env python
"""
.. codeauthor:: S Lundell

.. sectionauthor:: S Lundell
"""

from .dimensionless_discharge import DimensionlessDischarge

__all__ = ["DimensionlessDischarge"]



================================================
File: dimensionless_discharge/dimensionless_discharge.py
================================================
#!/usr/bin/env python3
"""
Calculate dimensionless discharge of stream sections based on Tang et
al. (2019)
"""

import numpy as np
from scipy import constants

from landlab import Component
from landlab.utils.return_array import return_array_at_node


class DimensionlessDischarge(Component):
    r"""Component that calculates dimensionless discharge of stream
     segments.

    The dimensionless discharge model calculates the unitless  discharge
    value of streams and can be used to help determine locations of
    debris flows. It uses an equation from Tang et al. (2019) to
    calculate the dimensionless discharge as well as the threshold for
    whether a debris flow will occur for a specified location.

    .. math::

        q* = \frac{q}{\sqrt{\frac{\rho_s-\rho}{\rho}*g*D_{50}^3}}

     where :math:`q*` is the dimensionless discharge value for a stream
     segment, :math:`q` is flux in a stream segment,  :math:`\rho_s`
     is soil density, :math:`\rho` is water density, :math:`g` is the
     gravitational constant, and :math:`D_50` is the average sediment
     partical size in the stream segment area.

     Constants C and N are coefficients used in the slope-dependent
     equation

     .. math::

        q*_{thresold} = \frac{C}{(tan(\theta))^N}

     to determine whether the dimensionless discharge calculated
     exceeds thresholds for a sediment-laden (Upper Limit) or
     water-producing only (Lower Limit) debris flow.  C and N are
     empirically-derived by comparing dimensionless discharge estimates
     against previous debris flow events.  The values will vary based on
     local geology and soil type. In southern California values of C=12
     and N=0.85 (upper limits, Tang et al, 2019) and C=4.29, N=0.78
     (lower limits, Tang et al, 2019) have been used, while values of
     C=0.195 and N=1.27 have been in used in the Italian Dolomites
     (Gregoretti and Fontana, 2008). Default values are C=12 and N=0.85.

     Examples
     --------
     >>> from landlab.components import DimensionlessDischarge
     >>> from landlab import RasterModelGrid
     >>> import random
     >>> watershed_grid = RasterModelGrid((3, 3))
     >>> surface_water__unit_discharge = watershed_grid.add_ones(
     ...     "surface_water__unit_discharge", at="node"
     ... )
     >>> d50 = watershed_grid.add_ones(
     ...     "channel_bottom_sediment_grain__d50_diameter", at="node"
     ... )
     >>> watershed_grid.at_node["topographic__elevation"] = np.array(
     ...     [[1.1, 2, 3, 4, 2, 3, 4, 5, 3]]
     ... )
     >>> dd = DimensionlessDischarge(watershed_grid, gravity=9.8)
     >>> dd.run_one_step()
     >>> watershed_grid.at_node["dimensionless_discharge"]
     array([0.55372743, 0.55372743, 0.55372743, 0.55372743, 0.55372743,
            0.55372743, 0.55372743, 0.55372743, 0.55372743])

     References
     ----------
     Tang, H., McGuire, L. A., Rengers, F. K., Kean, J. W., Staley,
     D. M., & Smith, J. B. (2019). Developing and Testing Physically
     Based Triggering Thresholds for Runoff-Generated Debris Flows.
     Geophysical Research Letters, 46(15), 8830–8839.
     https://doi.org/10.1029/2019GL083623

    """

    _name = "DimensionlessDischargeModel"

    _unit_agnostic = False

    _info = {
        "dimensionless_discharge": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "none",
            "mapping": "node",
            "doc": "Dimensionless discharge value for a stream segment.",
        },
        "dimensionless_discharge_above_threshold": {
            "dtype": bool,
            "intent": "out",
            "optional": False,
            "units": "none",
            "mapping": "node",
            "doc": (
                "True if dimensionless discharge value is above threshold value, "
                "false otherwise."
            ),
        },
        "dimensionless_discharge_threshold": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "none",
            "mapping": "node",
            "doc": "Dimensionless discharge threshold for each stream segment.",
        },
        "surface_water__unit_discharge": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m**2/s",
            "mapping": "node",
            "doc": "Volumetric discharge of surface water per unit width",
        },
        "channel_bottom_sediment_grain__d50_diameter": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "m",
            "mapping": "node",
            "doc": "soil grain size average in stream segment",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
    }

    def __init__(
        self,
        grid,
        soil_density=1330,
        water_density=997.9,
        C=12.0,
        N=0.85,
        gravity=constants.g,
        channel_bottom_sediment_grain__d50_diameter="channel_bottom_sediment_grain__d50_diameter",
    ):
        """Initialize the DimensionlessDischarge.

        Parameters
        ----------
        soil_density : float, field name, or array, optional
            density of soil (kg/m^3) (defaults to 1330)
        water_density : float, optional
            density of water (kg/m^3) (defaults to 997.9)
        C : float, optional
            Numerator of the debris flow threshold equation; Empirically
            derived constant (See Tang et al. 2019). (defaults to 12.0)
        N : float, optional
            Exponent for slope in the denominator of the debris flow
            threshold equation; Empirically derived constant (See Tang
            et al. 2019). (defaults to 0.85)
        gravity : float, optional
            (defaults to scipy's standard acceleration of gravity)
        channel_bottom_sediment_grain__d50_diameter : float, field_name, or array
            defaults to the field name
            "channel_bottom_sediment_grain__d50_diameter"
        """

        super().__init__(grid)

        # Store parameters

        # scalar parameters.
        self._c = C
        self._n = N
        self._water_density = water_density
        self._gravity = gravity

        # get topographic__elevation values and change into slope of a
        # stream segment
        self._stream_slopes = self._elevationToSlope()

        # both soil density and d50 can be float, array, or field name.
        self._soil_density = return_array_at_node(self.grid, soil_density)
        self._channel_bottom_sediment_grain__d50_diameter = return_array_at_node(
            self.grid, channel_bottom_sediment_grain__d50_diameter
        )

        # create output fields
        self.grid.add_zeros("dimensionless_discharge", at="node")
        self.grid.add_full(
            "dimensionless_discharge_above_threshold", False, at="node", dtype=bool
        )
        self.grid.add_zeros("dimensionless_discharge_threshold", at="node", dtype=float)

        # calculate threshold values for each segment
        self._calc_threshold()

    def _calc_threshold(self):
        self.grid.at_node["dimensionless_discharge_threshold"] = self._c / (
            self._stream_slopes**self._n
        )

    def _elevationToSlope(self):
        return self.grid.calc_slope_at_node(elevs="topographic__elevation")

    def run_one_step(self):
        # update slopes
        self._stream_slopes = self._elevationToSlope()

        # recalculate threshold with new slopes.
        self._calc_threshold()

        self.grid.at_node["dimensionless_discharge"] = self.grid.at_node[
            "surface_water__unit_discharge"
        ] / np.sqrt(
            ((self._soil_density - self._water_density) / self._water_density)
            * self._gravity
            * (self._channel_bottom_sediment_grain__d50_diameter**3)
        )

        self.grid.at_node["dimensionless_discharge_above_threshold"] = (
            self.grid.at_node["dimensionless_discharge"]
            > self.grid.at_node["dimensionless_discharge_threshold"]
        )



================================================
File: discharge_diffuser/__init__.py
================================================
from .diffuse_by_discharge import DischargeDiffuser

__all__ = ["DischargeDiffuser"]



================================================
File: discharge_diffuser/diffuse_by_discharge.py
================================================
"""This is an implementation of Vaughan Voller's experimental boundary method
reduced complexity flow router. Credit: Voller, Hobley, Paola.

Created on Fri Feb 20 09:32:27 2015

@author: danhobley (SiccarPoint), after volle001@umn.edu
"""

import numpy as np

from landlab import Component
from landlab import RasterModelGrid


class DischargeDiffuser(Component):
    """Diffuse sediment proportional to an implicit water discharge value.

    This class implements Voller, Hobley, and Paola's scheme for sediment
    diffusion, where the diffusivity of the sediment is proportional to the
    local discharge of water. The method works by solving for a potential
    field describing the water discharge at all nodes on the grid, which
    enforces both mass conservation and flow downhill along topographic
    gradients. This routine is designed to construct sediment fans.

    Note that both the water and sediment discharges are calculated together
    within the component.

    The algorithm uses a rule that looks like:

        q_sed = q_water * (S - S_crit)

    where S_crit is a critical slope threshold. [MODIFY THIS]

    It is VITAL you initialize this component AFTER setting boundary
    conditions.

    The primary method of this class is :func:`run_one_step`.

    Notes
    -----
    This is a "research grade" component, and is subject to dramatic change
    with little warning. No guarantees are made regarding its accuracy or
    utility. It is not recommended for user use yet!

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    None Listed

    """

    _name = "DischargeDiffuser"

    _unit_agnostic = True

    _info = {
        "flow__potential": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m**3/s",
            "mapping": "node",
            "doc": (
                "Value of the hypothetical field 'K', used to force water "
                "flux to flow downhill"
            ),
        },
        "sediment__discharge_in": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m**3/s",
            "mapping": "node",
            "doc": "Sediment discharge into a node.",
        },
        "surface_water__discharge": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m**3/s",
            "mapping": "node",
            "doc": "Volumetric discharge of surface water",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "water__discharge_in": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m**3/s",
            "mapping": "node",
            "doc": "Incoming water discharge at node.",
        },
    }

    _min_slope_thresh = 1.0e-24
    # if your flow isn't connecting up, this probably needs to be reduced

    def __init__(self, grid, slope=0.25, flat_thresh=1.0e-4):
        """
        Parameters
        ----------
        grid : ModelGrid
            A grid.
        """
        super().__init__(grid)

        if isinstance(grid, RasterModelGrid):
            assert grid.number_of_node_rows >= 3
            assert grid.number_of_node_columns >= 3
            self._raster = True
        else:
            self._raster = False

        assert self._raster is True  # ...for now

        self._slope = slope
        self._flat_thresh = flat_thresh

        # hacky fix because water__discharge is defined on both links and nodes
        self.initialize_output_fields()

        ni = grid.number_of_node_rows
        nj = grid.number_of_node_columns

        self._K = grid.zeros(at="node", dtype=float)
        self._Knew = grid.zeros(at="node", dtype=float)
        self._prevK = grid.zeros(at="node", dtype=float)
        self._znew = grid.zeros(at="node", dtype=float)
        # discharge across north, south, west, and east face of control volume
        self._Qn = np.zeros((ni, nj), dtype="float")
        self._Qs = np.zeros((ni, nj), dtype="float")
        self._Qw = np.zeros((ni, nj), dtype="float")
        self._Qe = np.zeros((ni, nj), dtype="float")

        # coefficenst used in solition of flow conductivity K
        self._app = np.zeros((ni, nj), dtype="float")
        self._apz = np.zeros((ni, nj), dtype="float")
        self._aww = np.zeros((ni, nj), dtype="float")
        self._awp = np.zeros((ni, nj), dtype="float")
        self._awz = np.zeros((ni, nj), dtype="float")
        self._aee = np.zeros((ni, nj), dtype="float")
        self._aep = np.zeros((ni, nj), dtype="float")
        self._aez = np.zeros((ni, nj), dtype="float")
        self._ass = np.zeros((ni, nj), dtype="float")
        self._asp = np.zeros((ni, nj), dtype="float")
        self._asz = np.zeros((ni, nj), dtype="float")
        self._ann = np.zeros((ni, nj), dtype="float")
        self._anp = np.zeros((ni, nj), dtype="float")
        self._anz = np.zeros((ni, nj), dtype="float")

        self._slx = np.empty((ni, nj), dtype=float)
        self._sly = np.empty((ni, nj), dtype=float)
        self._Qsed_w = np.empty((ni, nj), dtype=float)
        self._Qsed_e = np.empty((ni, nj), dtype=float)
        self._Qsed_n = np.empty((ni, nj), dtype=float)
        self._Qsed_s = np.empty((ni, nj), dtype=float)

    def run_one_step(self, dt):
        """Run forward a duration of time, dt.

        Parameters
        ----------
        dt: float
        """
        grid = self._grid
        ni = grid.number_of_node_rows
        nj = grid.number_of_node_columns
        z = grid.at_node["topographic__elevation"]
        Qsp = grid.at_node["water__discharge_in"].reshape(
            (grid.number_of_node_rows, grid.number_of_node_columns)
        )
        Qsource = grid.at_node["sediment__discharge_in"].reshape(
            (grid.number_of_node_rows, grid.number_of_node_columns)
        )

        # #####STABILITY ANALYSIS GOES HERE
        dt_stab = dt

        # elevation at current and new time
        # Note a horizonal surface is the initial condition
        eta = z.reshape((ni, nj))
        K = self._K.reshape((ni, nj))
        Knew = self._Knew.reshape((ni, nj))
        # etan = self._znew.reshape((grid.number_of_node_rows,
        #                            grid.number_of_node_columns))

        # pad eta
        pad_eta = np.pad(eta, ((1, 1), (1, 1)), "edge")
        # do the sediment diffusion
        for dir in ("W", "E", "S", "N"):
            self._grad_on_link(pad_eta, dir)
            Cslope = np.sqrt(self._slx**2 + self._sly**2)
            self._link_sed_flux_from_slope(Cslope, self._slope, dir)

        try:
            Qin = Qsource.reshape((ni, nj))
        except AttributeError:
            Qin = float(Qsource)  # if both fail, we're in trouble
        eta[:] += (
            dt_stab
            * (self._Qsed_e + self._Qsed_n + self._Qsed_w + self._Qsed_s + Qin)
            / grid.dx
            / grid.dy
        )

        # do the water routing on links
        # These calculations are based on assuming that the flow is a sheet
        # flow that can be characterized with a potential equation. If this
        # flow is isotropic (an assumption we should revisit) with this model
        # the flow discharge in the x-direction (for example) can be calculated
        # as a constant (K the 'flow conductivity') times the component of the
        # sediment slope in that direction. It helps to define a 'slope
        # velocity' u, with components ustar=-deta/dx and vstar=-deta/dx which
        # allows us to write down the following advection like gov. eq. for
        # the flow  ----div(Ku)+Q=0---where Q represents external flow inputs

        # Since we can readily determine u from the current sediment topography
        # We solve this equation for K using an upwind scheme

        # Build upwinded coefficients. Vals only 0 if if flow is in upwind dir
        # note cols/rows which don't get updated will always remain as 0,
        # which is right provided we want no flow BCs
        eta_diff = -eta[:-1, :] + eta[1:, :]
        self._ann[:-1, :] = eta_diff.clip(0.0)
        self._anp[:-1, :] = (-eta_diff).clip(0.0)
        eta_diff = -eta[1:, :] + eta[:-1, :]
        self._ass[1:, :] = eta_diff.clip(0.0)
        self._asp[1:, :] = (-eta_diff).clip(0.0)
        eta_diff = -eta[:, :-1] + eta[:, 1:]
        self._aee[:, :-1] = eta_diff.clip(0.0)
        self._aep[:, :-1] = (-eta_diff).clip(0.0)
        eta_diff = -eta[:, 1:] + eta[:, :-1]
        self._aww[:, 1:] = eta_diff.clip(0.0)
        self._awp[:, 1:] = (-eta_diff).clip(0.0)

        self._app[:] = self._awp + self._aep + self._asp + self._anp

        apz = self._app.copy()
        awz = self._aww.copy()
        aez = self._aee.copy()
        asz = self._ass.copy()
        anz = self._ann.copy()
        # zero elevation treatment
        # at a zero elevation we use a simple averaging approach
        # this rationale is questionable - a propagation across flats may be
        # preferable
        flats = np.abs(self._app) < self._flat_thresh
        apz[flats] = 4
        for NSEW in (awz, aez, asz, anz):
            NSEW[flats] = 1
        # NOTE when we do not have a zero elevation condition the
        # coefficients a*z are the upwind coefficents

        # Solve upwind equations for nodal K
        # this involves iteration to a stable solution
        # #####IMPLEMENT IT
        # calc the new K based on incoming discharges
        for _ in range(1):
            Knew[:, 1:] += awz[:, 1:] + K[:, :-1]
            Knew[:, 0] += awz[:, 0] + K[:, 0]
            Knew[:, :-1] += aez[:, :-1] + K[:, 1:]
            Knew[:, -1] += awz[:, -1] + K[:, -1]
            Knew[1:, :] += asz[1:, :] + K[:-1, :]
            Knew[0, :] += asz[0, :] + K[0, :]
            Knew[:-1, :] += anz[:-1, :] + K[1:, :]
            Knew[-1, :] += asz[-1, :] + K[-1, :]
            Knew += Qsp
            Knew /= apz
            K[:] = Knew

        Kpad = np.pad(K, ((1, 1), (1, 1)), "edge")
        self._Qw += self._aww * Kpad[1:-1, :-2]
        self._Qw -= self._awp * K
        self._Qe += self._aee * Kpad[1:-1, 2:]
        self._Qe -= self._aep * K
        self._Qs += self._ass * Kpad[:-2, 1:-1]
        self._Qs -= self._asp * K
        self._Qn += self._ann * Kpad[2:, 1:-1]
        self._Qn -= self._anp * K

    @property
    def discharges_at_links(self):
        """Return the discharges at links.

        Note that if diagonal routing, this will return
        number_of_d8_links. Otherwise, it will be number_of_links.
        """
        return self._discharges_at_link

    def _grad_on_link(self, padded_eta, direction):
        """Updates slx and sly with link gradient values according to
        `direction`.

        eta = elevations in grid form
        direction = {'E', 'N', 'S', 'W'}

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> mg = RasterModelGrid((3, 4), xy_spacing=(1.0, 0.5))
        >>> z = mg.add_zeros("topographic__elevation", at="node")
        >>> z = mg.add_zeros("water__discharge_in", at="node")
        >>> z = mg.add_zeros("sediment__discharge_in", at="node")
        >>> z[:] = np.array([[1, 2, 3, 4, 2, 3, 4, 5, 3, 4, 5, 6]])
        >>> zpad = np.pad(z.reshape((3, 4)), ((1, 1), (1, 1)), "edge")
        >>> dd = DischargeDiffuser(mg, 0.25)
        >>> dd._grad_on_link(zpad, "W")
        """
        core = (slice(1, -1, 1), slice(1, -1, 1))
        if direction == "W":
            self._slx[:] = (padded_eta[1:-1, :-2] - padded_eta[core]) / self._grid.dx
            self._sly[:] = padded_eta[:-2, :-2]
            self._sly -= padded_eta[2:, :-2]
            self._sly += padded_eta[:-2, 1:-1]
            self._sly -= padded_eta[2:, 1:-1]
            self._sly *= 0.25
            self._sly /= self._grid.dy

        elif direction == "E":
            self._slx[:] = (padded_eta[1:-1, 2:] - padded_eta[core]) / self._grid.dx
            self._sly[:] = padded_eta[:-2, 2:]
            self._sly -= padded_eta[2:, 2:]
            self._sly += padded_eta[:-2, 1:-1]
            self._sly -= padded_eta[2:, 1:-1]
            self._sly *= 0.25
            self._sly /= self._grid.dy

        elif direction == "S":
            self._sly[:] = (padded_eta[:-2, 1:-1] - padded_eta[core]) / self._grid.dy
            self._slx[:] = padded_eta[:-2, :-2]
            self._slx -= padded_eta[:-2, 2:]
            self._slx += padded_eta[1:-1, :-2]
            self._slx -= padded_eta[1:-1, 2:]
            self._slx *= 0.25
            self._slx /= self._grid.dx

        elif direction == "N":
            self._sly[:] = (padded_eta[2:, 1:-1] - padded_eta[core]) / self._grid.dy
            self._slx[:] = padded_eta[2:, :-2]
            self._slx -= padded_eta[2:, 2:]
            self._slx += padded_eta[1:-1, :-2]
            self._slx -= padded_eta[1:-1, 2:]
            self._slx *= 0.25
            self._slx /= self._grid.dx

        else:
            raise NameError("direction must be {'E', 'N', 'S', 'W'}")

    def _link_sed_flux_from_slope(self, S_val, S_thresh, direction):
        """Update the sed flux array for a given link dir, assuming a critical
        S."""
        if direction == "W":
            dir_sed_flux = self._Qsed_w
            dir_water_flux = self._Qw
            thisslice = (slice(0, -1, 1), slice(1, -1, 1))
            deadedge = (slice(0, -1, 1), slice(0, 1, 1))
        elif direction == "E":
            dir_sed_flux = self._Qsed_e
            dir_water_flux = self._Qe
            thisslice = (slice(0, -1, 1), slice(1, -2, 1))
            deadedge = (slice(0, -1, 1), slice(-2, -1, 1))
        elif direction == "N":
            dir_sed_flux = self._Qsed_n
            dir_water_flux = self._Qn
            thisslice = (slice(0, -2, 1), slice(0, -1, 1))
            deadedge = (slice(-2, -1, 1), slice(0, -1, 1))
        elif direction == "S":
            dir_sed_flux = self._Qsed_s
            dir_water_flux = self._Qs
            thisslice = (slice(1, -1, 1), slice(0, -1, 1))
            deadedge = (slice(0, 1, 1), slice(0, -1, 1))
        else:
            raise NameError("direction must be {'E', 'N', 'S', 'W'}")
        slope_diff = (S_val - S_thresh).clip(0.0)
        dir_sed_flux[thisslice] = dir_water_flux[thisslice] * slope_diff[thisslice]
        dir_sed_flux[deadedge] = 0.0

    def diffuse_sediment(self, Qw_in, Qsed_in):
        """ """
        pass


if __name__ == "__main__":
    from landlab import imshow_grid_at_node

    S_crit = 0.25
    mg = RasterModelGrid((20, 20), 0.5)
    mg.add_zeros("topographic__elevation", at="node")
    Qw_in = mg.add_zeros("water__discharge_in", at="node")
    Qs_in = mg.add_zeros("sediment__discharge_in", at="node")
    Qw_in[0] = 0.5 * np.pi
    Qs_in[0] = (1.0 - S_crit) * 0.5 * np.pi
    dd = DischargeDiffuser(mg, S_crit)
    for _ in range(5):  # 501
        dd.run_one_step(0.01)  # 0.08
    imshow_grid_at_node(mg, "topographic__elevation")



================================================
File: drainage_density/__init__.py
================================================
from .drainage_density import DrainageDensity

__all__ = ["DrainageDensity"]



================================================
File: drainage_density/cfuncs.pyx
================================================
cimport cython
from libc.stdint cimport uint8_t

# https://cython.readthedocs.io/en/stable/src/userguide/fusedtypes.html
ctypedef fused id_t:
    cython.integral
    long long


@cython.boundscheck(False)
def _calc_dists_to_channel(
    uint8_t [:] ch_network,
    id_t [:] flow_receivers,
    id_t [:] upstream_order,
    const cython.floating [:] link_lengths,
    id_t [:] stack_links,
    cython.floating [:] dist_to_ch,
    long num_nodes,
):
    """Calculate distance to nearest channel.

    Calculate the distances to the closest channel node for all nodes in the
    grid.

    Parameters
    ----------
    ch_network : node array
        integer logical map of which nodes contain channels.
    flow_receivers : node array
        ID of the next downstream node.
    link_lengths : num_d8_links-length array of floats
        The length of all links on the grid, including diagonals if present.
    stack_links : node array
        The ID of the link that leads to the downstream node.
    dists_to_ch : number_of_nodes-length array of floats
        The output array; the distance to the nearest channel node.
    num_nodes : int
        The number of nodes.
    """
    cdef long node
    cdef long node_iter
    cdef long flag
    cdef double distance

    for i in range(num_nodes):
        node = upstream_order[i]
        if ch_network[node] == 1:
            # ^we're standing in a channel
            dist_to_ch[node] = 0.
        else:
            distance = 0
            flag = 0
            node_iter = node
            while flag == 0:  # as long as we haven't hit a channel yet...
                if flow_receivers[node_iter] == node_iter:
                    # ^if no flow receiver (boundary probably)
                    ch_network[node_iter] = 1
                    # ^convince the node it's a channel
                else:
                    distance += link_lengths[stack_links[node_iter]]
                    node_iter = flow_receivers[node_iter]
                if ch_network[node_iter] == 1:
                    # ^we've hit a channel
                    dist_to_ch[node] = distance
                    # ^save distance to channel
                    flag = 1



================================================
File: drainage_density/drainage_density.py
================================================
"""Landlab component to calculate drainage density."""

from warnings import warn

import numpy as np

from landlab import Component


class DrainageDensity(Component):
    r"""
    Calculate drainage density over a DEM.

    Landlab component that implements the distance to channel algorithm of
    Tucker et al., 2001.

    This component requires EITHER a ``channel__mask array`` with 1's
    where channels exist and 0's elsewhere, OR a set of coefficients
    and exponents for a slope-area relationship and a
    channelization threshold to compare against that relationship.

    If an array is provided it MUST be of type ``np.uint8``. See the example
    below for how to make such an array.

    The ``channel__mask`` array will be assigned to an at-node field with the
    name ``channel__mask``. If the channel__mask was originaly created from a
    passed array, a user can update this array to change the mask.

    If the ``channel__mask`` is created using an area coefficent,
    slope coefficient, area exponent, slope exponent, and channelization
    threshold, the location of the mask will be re-update when
    calculate_drainage_density is called.

    If an area coefficient, :math:`C_A`, a slope coefficent, :math:`C_S`, an
    area exponent, :math:`m_r`, a slope exponent, :math:`n_r`, and
    channelization threshold :math:`T_C` are provided, nodes that meet the
    criteria

    .. math::

       C_A A^{m_r} C_s S^{n_r} > T_c

    where :math:`A` is the drainage density and :math:`S` is the local slope,
    will be marked as channel nodes.

    The ``calculate_drainage_density`` function returns drainage density for the
    model domain. This function calculates the distance from every node to the
    nearest channel node :math:`L` along the flow line of steepest descent
    (assuming D8 routing if the grid is a RasterModelGrid).

    This component stores this distance a field, called:
    ``surface_to_channel__minimum_distance``. The drainage density is then
    calculated (after Tucker et al., 2001):

    .. math::

       D_d = \frac{1}{2\overline{L}}

    where :math:`\overline{L}` is the mean L for the model domain.

    Examples
    --------
    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import FlowAccumulator, FastscapeEroder
    >>> mg = RasterModelGrid((10, 10))
    >>> _ = mg.add_zeros("node", "topographic__elevation")
    >>> np.random.seed(50)
    >>> noise = np.random.rand(100)
    >>> mg.at_node["topographic__elevation"] += noise
    >>> mg.at_node["topographic__elevation"].reshape(mg.shape)
    array([[0.49460165, 0.2280831 , 0.25547392, 0.39632991, 0.3773151 ,
            0.99657423, 0.4081972 , 0.77189399, 0.76053669, 0.31000935],
           [0.3465412 , 0.35176482, 0.14546686, 0.97266468, 0.90917844,
            0.5599571 , 0.31359075, 0.88820004, 0.67457307, 0.39108745],
           [0.50718412, 0.5241035 , 0.92800093, 0.57137307, 0.66833757,
            0.05225869, 0.3270573 , 0.05640164, 0.17982769, 0.92593317],
           [0.93801522, 0.71409271, 0.73268761, 0.46174768, 0.93132927,
            0.40642024, 0.68320577, 0.64991587, 0.59876518, 0.22203939],
           [0.68235717, 0.8780563 , 0.79671726, 0.43200225, 0.91787822,
            0.78183368, 0.72575028, 0.12485469, 0.91630845, 0.38771099],
           [0.29492955, 0.61673141, 0.46784623, 0.25533891, 0.83899589,
            0.1786192 , 0.22711417, 0.65987645, 0.47911625, 0.07344734],
           [0.13896007, 0.11230718, 0.47778497, 0.54029623, 0.95807105,
            0.58379231, 0.52666409, 0.92226269, 0.91925702, 0.25200886],
           [0.68263261, 0.96427612, 0.22696165, 0.7160172 , 0.79776011,
            0.9367512 , 0.8537225 , 0.42154581, 0.00543987, 0.03486533],
           [0.01390537, 0.58890993, 0.3829931 , 0.11481895, 0.86445401,
            0.82165703, 0.73749168, 0.84034417, 0.4015291 , 0.74862   ],
           [0.55962945, 0.61323757, 0.29810165, 0.60237917, 0.42567684,
            0.53854438, 0.48672986, 0.49989164, 0.91745948, 0.26287702]])
    >>> fr = FlowAccumulator(mg, flow_director="D8")
    >>> fsc = FastscapeEroder(mg, K_sp=0.01, m_sp=0.5, n_sp=1)
    >>> for x in range(100):
    ...     fr.run_one_step()
    ...     fsc.run_one_step(dt=10.0)
    ...     mg.at_node["topographic__elevation"][mg.core_nodes] += 0.01
    ...
    >>> channels = np.array(mg.at_node["drainage_area"] > 5, dtype=np.uint8)
    >>> dd = DrainageDensity(mg, channel__mask=channels)
    >>> mean_drainage_density = dd.calculate_drainage_density()
    >>> np.isclose(mean_drainage_density, 0.3831100571)
    True

    Alternatively you can pass a set of coefficients to identify the channel
    mask. Next shows the same example as above, but with these coefficients
    provided.

    >>> mg = RasterModelGrid((10, 10))
    >>> _ = mg.add_zeros("node", "topographic__elevation")
    >>> np.random.seed(50)
    >>> noise = np.random.rand(100)
    >>> mg.at_node["topographic__elevation"] += noise
    >>> fr = FlowAccumulator(mg, flow_director="D8")
    >>> fsc = FastscapeEroder(mg, K_sp=0.01, m_sp=0.5, n_sp=1)
    >>> for x in range(100):
    ...     fr.run_one_step()
    ...     fsc.run_one_step(dt=10.0)
    ...     mg.at_node["topographic__elevation"][mg.core_nodes] += 0.01
    ...
    >>> channels = np.array(mg.at_node["drainage_area"] > 5, dtype=np.uint8)
    >>> dd = DrainageDensity(
    ...     mg,
    ...     area_coefficient=1.0,
    ...     slope_coefficient=1.0,
    ...     area_exponent=1.0,
    ...     slope_exponent=0.0,
    ...     channelization_threshold=5,
    ... )
    >>> mean_drainage_density = dd.calculate_drainage_density()
    >>> np.isclose(mean_drainage_density, 0.3831100571)
    True

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    Tucker, G., Catani, F., Rinaldo, A., Bras, R. (2001). Statistical analysis
    of drainage density from digital terrain data. Geomorphology 36(3-4),
    187-202. https://dx.doi.org/10.1016/s0169-555x(00)00056-8

    """

    _name = "DrainageDensity"

    _unit_agnostic = True

    _info = {
        "area_coefficient": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "-",
            "mapping": "node",
            "doc": "Area coefficient to define channels.",
        },
        "area_exponent": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "-",
            "mapping": "node",
            "doc": "Area exponent to define channels.",
        },
        "channel__mask": {
            "dtype": np.uint8,
            "intent": "in",
            "optional": True,
            "units": "-",
            "mapping": "node",
            "doc": "Logical map of at which grid nodes channels are present",
        },
        "channelization_threshold": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "-",
            "mapping": "node",
            "doc": (
                "Channelization threshold for use with area and slope "
                "coefficients and exponents."
            ),
        },
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__upstream_node_order": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array containing downstream-to-upstream ordered list of node IDs",
        },
        "slope_coefficient": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "-",
            "mapping": "node",
            "doc": "Slope coefficient to define channels.",
        },
        "slope_exponent": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "-",
            "mapping": "node",
            "doc": "Slope exponent to define channels.",
        },
        "surface_to_channel__minimum_distance": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Distance from each node to the nearest channel",
        },
        "topographic__steepest_slope": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "The steepest *downhill* slope",
        },
    }

    def __init__(
        self,
        grid,
        channel__mask=None,
        area_coefficient=None,
        slope_coefficient=None,
        area_exponent=None,
        slope_exponent=None,
        channelization_threshold=None,
    ):
        """Initialize the DrainageDensity component.

        Parameters
        ----------
        grid : ModelGrid
        channel__mask : Array that holds 1's where
            channels exist and 0's elsewhere
        area_coefficient : coefficient to multiply drainage area by,
            for calculating channelization threshold
        slope_coefficient : coefficient to multiply slope by,
            for calculating channelization threshold
        area_exponent : exponent to raise drainage area to,
            for calculating channelization threshold
        slope_exponent : exponent to raise slope to,
            for calculating channelization threshold
        channelization_threshold : threshold value above
            which channels exist
        """
        super().__init__(grid)

        if grid.at_node["flow__receiver_node"].size != grid.size("node"):
            raise NotImplementedError(
                "A route-to-multiple flow director has been "
                "run on this grid. The landlab development team has not "
                "verified that DrainageDensity is compatible with "
                "route-to-multiple methods. Please open a GitHub Issue "
                "to start this process."
            )

        if channel__mask is not None:
            if area_coefficient is not None:
                warn(
                    "Channel mask and area "
                    "coefficient supplied. Defaulting "
                    "to channel mask, ignoring area "
                    "coefficient."
                )
            if slope_coefficient is not None:
                warn(
                    "Channel mask and slope "
                    "coefficient supplied. Defaulting "
                    "to channel mask, ignoring slope "
                    "coefficient."
                )
            if area_exponent is not None:
                warn(
                    "Channel mask and area "
                    "exponent supplied. Defaulting "
                    "to channel mask, ignoring area "
                    "exponent."
                )
            if slope_exponent is not None:
                warn(
                    "Channel mask and slope "
                    "exponent supplied. Defaulting "
                    "to channel mask, ignoring slope "
                    "exponent."
                )
            if channelization_threshold is not None:
                warn(
                    "Channel mask and channelization "
                    "threshold supplied. Defaulting "
                    "to channel mask, ignoring "
                    "threshold."
                )
            if grid.number_of_nodes != len(channel__mask):
                raise ValueError(
                    "Length of channel mask is not equal to " "number of grid nodes"
                )

            if "channel__mask" in grid.at_node:
                warn("Existing channel__mask grid field was overwritten.")

            if channel__mask.dtype.type is not np.uint8:
                raise ValueError("mask must by np.uint8")

            self._mask_as_array = True
            self._update_channel_mask = self._update_channel_mask_array
            grid.at_node["channel__mask"] = channel__mask

        if channel__mask is None:
            if area_coefficient is None:
                raise ValueError(
                    "No channel mask and no area "
                    "coefficient supplied. Either "
                    "a channel mask or all 5 threshold "
                    "parameters are needed."
                )
            if slope_coefficient is None:
                raise ValueError(
                    "No channel mask and no slope "
                    "coefficient supplied. Either "
                    "a channel mask or all 5 threshold "
                    "parameters are needed."
                )
            if area_exponent is None:
                raise ValueError(
                    "No channel mask and no area "
                    "exponent supplied. Either "
                    "a channel mask or all 5 threshold "
                    "parameters are needed."
                )
            if slope_exponent is None:
                raise ValueError(
                    "No channel mask and no slope "
                    "exponent supplied. Either "
                    "a channel mask or all 5 threshold "
                    "parameters are needed."
                )
            if channelization_threshold is None:
                raise ValueError(
                    "No channel mask and no channelization "
                    "threshold supplied. Either "
                    "a channel mask or all 5 threshold "
                    "parameters are needed."
                )

            self._mask_as_array = False
            self._update_channel_mask = self._update_channel_mask_values
            self._area_coefficient = area_coefficient
            self._slope_coefficient = slope_coefficient
            self._area_exponent = area_exponent
            self._slope_exponent = slope_exponent
            self._channelization_threshold = channelization_threshold

            self._update_channel_mask()

        # for this component to work with Cython acceleration,
        # the channel_network must be uint8, not bool...
        self._channel_network = grid.at_node["channel__mask"]

        # Flow receivers
        self._flow_receivers = grid.at_node["flow__receiver_node"]

        # Links to receiver nodes
        self._stack_links = grid.at_node["flow__link_to_receiver_node"]

        # Upstream node order
        self._upstream_order = grid.at_node["flow__upstream_node_order"]

        # Distance to channel
        if "surface_to_channel__minimum_distance" not in grid.at_node:
            grid.add_zeros(
                "surface_to_channel__minimum_distance", at="node", dtype=float
            )
        self._distance_to_channel = grid.at_node["surface_to_channel__minimum_distance"]

        # Use the appropriate array for link or d8 lengths
        try:
            self._length_of_link = self._grid.length_of_d8
        except AttributeError:
            self._length_of_link = self._grid.length_of_link

    def _update_channel_mask_array(self):
        raise NotImplementedError(
            "If you provided a channel mask to "
            "DrainageDensity, update it by updating the "
            "model grid field channel__mask"
        )

    def _update_channel_mask_values(self):
        channel__mask = (
            self._area_coefficient
            * np.power(self._grid.at_node["drainage_area"], self._area_exponent)
            * self._slope_coefficient
            * np.power(
                self._grid.at_node["topographic__steepest_slope"], self._slope_exponent
            )
        ) > self._channelization_threshold
        self._grid.at_node["channel__mask"] = channel__mask.astype(np.uint8)

    def calculate_drainage_density(self):
        """Calculate drainage density.

        If the channel mask is defined based on slope and area coefficients,
        it will be update based on the current drainage area and slope fields.

        Returns
        -------
        landscape_drainage_density : float (1/m)
            Drainage density over the model domain.
        """
        from .cfuncs import _calc_dists_to_channel

        if self._mask_as_array is False:
            self._update_channel_mask()

        _calc_dists_to_channel(
            self._channel_network,
            self._flow_receivers,
            self._upstream_order,
            self._length_of_link,
            self._stack_links,
            self._distance_to_channel,
            self._grid.number_of_nodes,
        )
        landscape_drainage_density = 1.0 / (
            2.0
            * np.mean(
                self._grid.at_node["surface_to_channel__minimum_distance"][
                    self._grid.core_nodes
                ]
            )
        )
        # this is THE drainage density
        return landscape_drainage_density



================================================
File: erosion_deposition/__init__.py
================================================
from .erosion_deposition import ErosionDeposition
from .shared_stream_power import SharedStreamPower

__all__ = [
    "ErosionDeposition",
    "SharedStreamPower",
]



================================================
File: erosion_deposition/cfuncs.pyx
================================================
cimport cython

# https://cython.readthedocs.io/en/stable/src/userguide/fusedtypes.html
ctypedef fused id_t:
    cython.integral
    long long


cdef extern from "math.h":
    double exp(double x) nogil


@cython.boundscheck(False)
@cython.wraparound(False)
cpdef void calculate_qs_in(
    const id_t [:] stack_flip_ud,
    const id_t [:] flow_receivers,
    const cython.floating [:] cell_area_at_node,
    const cython.floating [:] q,
    cython.floating [:] qs,
    cython.floating [:] qs_in,
    const cython.floating [:] Es,
    const cython.floating [:] v_s,
    const double F_f,
) noexcept nogil:
    """Calculate and qs and qs_in."""
    cdef unsigned int n_nodes = len(stack_flip_ud)
    cdef unsigned int node
    cdef unsigned int i

    # iterate top to bottom through the stack, calculate qs and adjust qs_in
    for i in range(n_nodes):

        # choose the node id
        node = stack_flip_ud[i]

        # If q at current node is greather than zero, calculate qs based on a
        # local analytical solution. This local analytical solution depends on
        # qs_in, the sediment flux coming into the node from upstream (hence
        # the upstream to downstream node ordering).

        # Because calculation of qs requires qs_in, this operation must be done
        # in an upstream to downstream loop, and cannot be vectorized.
        #
        # there is water flux (q) and this node is not a pit then calculate qs.

        if q[node] > 0 and flow_receivers[node] != node:
            qs[node] = (
                (
                    qs_in[node]
                    + (1.0 - F_f) * Es[node] * cell_area_at_node[node]
                ) / (1.0 + v_s[node] * cell_area_at_node[node] / q[node])
            )

            # finally, add this node's qs to recieiving nodes qs_in.
            # if qs[node] == 0, then there is no need for this line to be
            # evaluated.
            qs_in[flow_receivers[node]] += qs[node]

        else:
            # if q at the current node is zero, set qs at that node is zero.
            qs[node] = 0



================================================
File: erosion_deposition/erosion_deposition.py
================================================
import numpy as np

from landlab.components.erosion_deposition.cfuncs import calculate_qs_in
from landlab.components.erosion_deposition.generalized_erosion_deposition import (
    DEFAULT_MINIMUM_TIME_STEP,
)
from landlab.components.erosion_deposition.generalized_erosion_deposition import (
    _GeneralizedErosionDeposition,
)

ROOT2 = np.sqrt(2.0)  # syntactic sugar for precalculated square root of 2
TIME_STEP_FACTOR = 0.5  # factor used in simple subdivision solver


class ErosionDeposition(_GeneralizedErosionDeposition):
    """Erosion-Deposition model in the style of Davy and Lague (2009).

    Erosion-Deposition model in the style of Davy and Lague (2009). It uses a
    mass balance approach across the total sediment mass both in the bed and
    in transport coupled with explicit representation of the sediment
    transport lengthscale (the "xi-q" model) to derive a range of erosional
    and depositional responses in river channels.

    This implementation is close to the Davy & Lague scheme, with a few
    deviations:

    * A fraction of the eroded sediment is permitted to enter the wash load,
      and lost to the mass balance (``F_f``).
    * Here an incision threshold ``omega`` is permitted, where it was not by Davy &
      Lague. It is implemented with an exponentially smoothed form to prevent
      discontinuities in the parameter space. See the
      :py:class:`~landlab.components.StreamPowerSmoothThresholdEroder`
      for more documentation.
    * This component uses an "effective" settling velocity, ``v_s``, as one of its
      inputs. This parameter is simply equal to Davy & Lague's ``d_star * V``
      dimensionless number.

    Erosion of the bed follows a stream power formulation, i.e.::

        E = K * q**m_sp * S**n_sp - omega

    Note that the transition between transport-limited and detachment-limited
    behavior is controlled by the dimensionless ratio (``v_s / r``) where ``r`` is the
    runoff ratio (``Q=Ar``). ``r`` can be changed in the flow accumulation component
    but is not changed within ErosionDeposition. Because the runoff ratio ``r``
    is not changed within the ErosionDeposition component, ``v_s`` becomes the
    parameter that fundamentally controls response style. Very small ``v_s`` will
    lead to a detachment-limited response style, very large ``v_s`` will lead to a
    transport-limited response style. ``v_s == 1`` means equal contributions from
    transport and erosion, and a hybrid response as described by Davy & Lague.

    Unlike other some other fluvial erosion componets in Landlab, in this
    component (and :py:class:`~landlab.components.SPACE`) no erosion occurs
    in depressions or in areas with adverse slopes. There is no ability to
    pass a keyword argument ``erode_flooded_nodes``.

    If depressions are handled (as indicated by the presence of the field
    ``"flood_status_code"`` at nodes), then deposition occurs throughout the
    depression and sediment is passed out of the depression. Where pits are
    encountered, then all sediment is deposited at that node only.

    Component written by C. Shobe, K. Barnhart, and G. Tucker.

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Barnhart, K., Glade, R., Shobe, C., Tucker, G. (2019). Terrainbento 1.0: a
    Python package for multi-model analysis in long-term drainage basin
    evolution. Geoscientific Model Development  12(4), 1267--1297.
    https://dx.doi.org/10.5194/gmd-12-1267-2019

    **Additional References**

    Davy, P., Lague, D. (2009). Fluvial erosion/transport equation of landscape
    evolution models revisited Journal of Geophysical Research  114(F3),
    F03007. https://dx.doi.org/10.1029/2008jf001146

    Examples
    ---------
    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import FlowAccumulator
    >>> from landlab.components import DepressionFinderAndRouter
    >>> from landlab.components import ErosionDeposition
    >>> from landlab.components import FastscapeEroder
    >>> np.random.seed(seed=5000)

    Define grid and initial topography:

    * 5x5 grid with baselevel in the lower left corner
    * All other boundary nodes closed
    * Initial topography is plane tilted up to the upper right + noise

    >>> nr = 5
    >>> nc = 5
    >>> dx = 10
    >>> grid = RasterModelGrid((nr, nc), xy_spacing=10.0)
    >>> grid.at_node["topographic__elevation"] = (
    ...     grid.node_y / 10
    ...     + grid.node_x / 10
    ...     + np.random.rand(grid.number_of_nodes) / 10
    ... )
    >>> grid.set_closed_boundaries_at_grid_edges(
    ...     bottom_is_closed=True,
    ...     left_is_closed=True,
    ...     right_is_closed=True,
    ...     top_is_closed=True,
    ... )
    >>> grid.set_watershed_boundary_condition_outlet_id(
    ...     0, grid.at_node["topographic__elevation"], -9999.0
    ... )
    >>> fsc_dt = 100.0
    >>> ed_dt = 1.0

    Check initial topography

    >>> grid.at_node["topographic__elevation"].reshape(grid.shape)
    array([[0.02290479, 1.03606698, 2.0727653 , 3.01126678, 4.06077707],
           [1.08157495, 2.09812694, 3.00637448, 4.07999597, 5.00969486],
           [2.04008677, 3.06621577, 4.09655859, 5.04809001, 6.02641123],
           [3.05874171, 4.00585786, 5.0595697 , 6.04425233, 7.05334077],
           [4.05922478, 5.0409473 , 6.07035008, 7.0038935 , 8.01034357]])

    Instantiate Fastscape eroder, flow router, and depression finder

    >>> fr = FlowAccumulator(grid, flow_director="D8")
    >>> df = DepressionFinderAndRouter(grid)
    >>> fsc = FastscapeEroder(grid, K_sp=0.001, m_sp=0.5, n_sp=1)

    Burn in an initial drainage network using the Fastscape eroder:

    >>> for _ in range(100):
    ...     fr.run_one_step()
    ...     df.map_depressions()
    ...     flooded = np.where(df.flood_status == 3)[0]
    ...     fsc.run_one_step(dt=fsc_dt)
    ...     grid.at_node["topographic__elevation"][0] -= 0.001  # uplift
    ...

    Instantiate the E/D component:

    >>> ed = ErosionDeposition(
    ...     grid, K=0.00001, v_s=0.001, m_sp=0.5, n_sp=1.0, sp_crit=0
    ... )

    Now run the E/D component for 2000 short timesteps:

    >>> for _ in range(2000):  # E/D component loop
    ...     fr.run_one_step()
    ...     df.map_depressions()
    ...     ed.run_one_step(dt=ed_dt)
    ...     grid.at_node["topographic__elevation"][0] -= 2e-4 * ed_dt
    ...

    Now we test to see if topography is right:

    >>> np.around(grid.at_node["topographic__elevation"], decimals=3).reshape(
    ...     grid.shape
    ... )
    array([[-0.477,  1.036,  2.073,  3.011,  4.061],
           [ 1.082, -0.08 , -0.065, -0.054,  5.01 ],
           [ 2.04 , -0.065, -0.065, -0.053,  6.026],
           [ 3.059, -0.054, -0.053, -0.035,  7.053],
           [ 4.059,  5.041,  6.07 ,  7.004,  8.01 ]])
    """

    _name = "ErosionDeposition"

    _unit_agnostic = True

    _cite_as = """
    @article{barnhart2019terrain,
      author = {Barnhart, Katherine R and Glade, Rachel C and Shobe, Charles M
                and Tucker, Gregory E},
      title = {{Terrainbento 1.0: a Python package for multi-model analysis in
                long-term drainage basin evolution}},
      doi = {10.5194/gmd-12-1267-2019},
      pages = {1267---1297},
      number = {4},
      volume = {12},
      journal = {Geoscientific Model Development},
      year = {2019},
    }
    """

    _info = {
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__upstream_node_order": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array containing downstream-to-upstream ordered list of node IDs",
        },
        "sediment__influx": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m3/s",
            "mapping": "node",
            "doc": "Sediment flux (volume per unit time of sediment entering each node)",
        },
        "sediment__outflux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m3/s",
            "mapping": "node",
            "doc": "Sediment flux (volume per unit time of sediment leaving each node)",
        },
        "surface_water__discharge": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m**2/s",
            "mapping": "node",
            "doc": "Volumetric discharge of surface water",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__steepest_slope": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "The steepest *downhill* slope",
        },
    }

    def __init__(
        self,
        grid,
        K=0.002,
        v_s=1.0,
        m_sp=0.5,
        n_sp=1.0,
        sp_crit=0.0,
        F_f=0.0,
        discharge_field="surface_water__discharge",
        solver="basic",
        dt_min=DEFAULT_MINIMUM_TIME_STEP,
    ):
        """Initialize the ErosionDeposition model.

        Parameters
        ----------
        grid : ModelGrid
            Landlab ModelGrid object
        K : str or array_like, optional
            Erodibility for substrate (units vary).
        v_s : str or array_like, optional
            Effective settling velocity for chosen grain size metric [L/T].
        m_sp : float, optional
            Discharge exponent (units vary)
        n_sp : float, optional
            Slope exponent (units vary)
        sp_crit : str or array_like, optional
            Critical stream power to erode substrate [E/(TL^2)]
        F_f : float, optional
            Fraction of eroded material that turns into "fines" that do not
            contribute to (coarse) sediment load. Defaults to zero.
        discharge_field : str or array_like, optional
            Discharge [L^2/T]. The default is to use the grid field
            'surface_water__discharge', which is simply drainage area
            multiplied by the default rainfall rate (1 m/yr). To use custom
            spatially/temporally varying rainfall, use 'water__unit_flux_in'
            to specify water input to the FlowAccumulator.
        solver : {"basic", "adaptive"}, optional
            Solver to use. Options at present include:

            1. "basic" (default): explicit forward-time extrapolation.
               Simple but will become unstable if time step is too large.
            2. "adaptive": adaptive time-step solver that estimates a
               stable step size based on the shortest time to "flattening"
               among all upstream-downstream node pairs.
        """
        if solver not in {"basic", "adaptive"}:
            raise ValueError(f"unknown solver ({solver} not one of 'basic', 'adaptive'")

        super().__init__(
            grid,
            m_sp=m_sp,
            n_sp=n_sp,
            F_f=F_f,
            v_s=v_s,
            dt_min=dt_min,
            discharge_field=discharge_field,
        )

        if grid.at_node["flow__receiver_node"].size != grid.size("node"):
            raise NotImplementedError(
                "A route-to-multiple flow director has been"
                " run on this grid. The landlab development team has not"
                " verified that ErosionDeposition is compatible with"
                " route-to-multiple methods. Please open a GitHub Issue"
                " to start this process."
            )

        # E/D specific inits.

        # K's and critical values can be floats, grid fields, or arrays
        # use setter for K defined below
        self._K = K
        self._sp_crit = sp_crit

        # Handle option for solver
        if solver == "basic":
            self.run_one_step = self.run_one_step_basic
        elif solver == "adaptive":
            self.run_one_step = self.run_with_adaptive_time_step_solver
            self._time_to_flat = np.zeros(grid.number_of_nodes)

    @property
    def K(self):
        """Erodibility of substrate (units depend on m_sp)."""
        if isinstance(self._K, str):
            return self.grid.at_node[self._K]
        else:
            return self._K

    @property
    def sp_crit(self):
        """Critical stream power to erode substrate [E/(TL^2)]"""
        if isinstance(self._sp_crit, str):
            return self.grid.at_node[self._sp_crit]
        else:
            return self._sp_crit

    def _calc_erosion_rates(self):
        """Calculate erosion rates."""
        omega = self.K * self._Q_to_the_m * np.power(self._slope, self._n_sp)
        omega_over_sp_crit = np.divide(
            omega, self.sp_crit, out=np.zeros_like(omega), where=self.sp_crit != 0
        )

        self._erosion_term = omega - self.sp_crit * (1.0 - np.exp(-omega_over_sp_crit))

    def _calc_qs_in_and_depo_rate(self):
        self._calc_hydrology()
        self._calc_erosion_rates()

        is_flooded_core_node = self._get_flooded_core_nodes()
        self._erosion_term[is_flooded_core_node] = 0.0

        self.sediment_influx[:] = 0.0
        self._depo_rate[:] = 0.0

        v_s = np.broadcast_to(self.v_s, self._q.shape)

        # iterate top to bottom through the stack, calculate qs
        # cythonized version of calculating qs_in
        calculate_qs_in(
            np.flipud(self._stack),
            self._flow_receivers,
            self._cell_area_at_node,
            self._q,
            self._qs,
            self.sediment_influx,
            self._erosion_term,
            v_s,
            self._F_f,
        )

        positive_q = self._q > 0.0

        self._depo_rate[positive_q] = self._qs[positive_q] * (
            v_s[positive_q] / self._q[positive_q]
        )
        if not self._depressions_are_handled():  # all sed dropped here
            self._depo_rate[is_flooded_core_node] = (
                self.sediment_influx[is_flooded_core_node]
                / self._cell_area_at_node[is_flooded_core_node]
            )

    def run_one_step_basic(self, dt=1.0):
        """Calculate change in rock and alluvium thickness for a time period
        'dt'.

        Parameters
        ----------
        dt : float
            Model timestep [T]
        """
        self._calc_qs_in_and_depo_rate()

        # topo elev is old elev + deposition - erosion
        cores = self._grid.core_nodes
        dzdt = self._depo_rate - self._erosion_term
        self._topographic__elevation[cores] += dzdt[cores] * dt

    def run_with_adaptive_time_step_solver(self, dt=1.0):
        """CHILD-like solver that adjusts time steps to prevent slope flattening.

        Parameters
        ----------
        dt : float
            Model timestep [T]
        """

        # Initialize remaining_time, which records how much of the global time
        # step we have yet to use up.
        remaining_time = dt

        z = self._grid.at_node["topographic__elevation"]
        r = self._flow_receivers
        dzdt = np.zeros(len(z))
        cores = self._grid.core_nodes

        first_iteration = True

        is_flooded_core_node = self._get_flooded_core_nodes()

        # Outer WHILE loop: keep going until time is used up
        while remaining_time > 0.0:
            # Update all the flow-link slopes.
            #
            # For the first iteration, we assume this has already been done
            # outside the component (e.g., by flow router), but we need to do
            # it ourselves on subsequent iterations.
            if not first_iteration:
                # update the link slopes
                self._update_flow_link_slopes()
                # update where nodes are flooded. This shouldn't happen bc
                # of the dynamic timestepper, but just in case, we update here.
                is_flooded_core_node[self._slope < 0] = True
            else:
                first_iteration = False

            self._calc_qs_in_and_depo_rate()

            # Rate of change of elevation at core nodes:
            dzdt[cores] = self._depo_rate[cores] - self._erosion_term[cores]

            # Difference in elevation between each upstream-downstream pair
            zdif = z - z[r]

            # Rate of change of the *difference* in elevation between each
            # upstream-downstream pair.
            rocdif = dzdt - dzdt[r]

            # (Re)-initialize the array that will contain "time to (almost)
            # flat" for each node (relative to its downstream neighbor).
            self._time_to_flat[:] = remaining_time

            # Find locations where the upstream and downstream node elevations
            # are converging (e.g., the upstream one is eroding faster than its
            # downstream neighbor)
            converging = np.nonzero(rocdif < 0.0)[0]

            # Find the time to (almost) flat by dividing difference by rate of
            # change of difference, and then multiplying by a "safety factor"
            self._time_to_flat[converging] = -(
                TIME_STEP_FACTOR * zdif[converging] / rocdif[converging]
            )

            # Mask out pairs where the source at the same or lower elevation
            # as its downstream neighbor (e.g., because it's a pit or a lake).
            # Here, masking out means simply assigning the remaining time in
            # the global time step.
            self._time_to_flat[np.nonzero(zdif <= 0.0)[0]] = remaining_time
            self._time_to_flat[is_flooded_core_node] = remaining_time

            # From this, find the maximum stable time step. If it is smaller
            # than our tolerance, report and quit.
            dt_max = max(np.amin(self._time_to_flat), self._dt_min)

            # Finally, apply dzdt to all nodes for a (sub)step of duration
            # dt_max
            z[cores] += dzdt[cores] * dt_max

            # Update remaining time and continue the loop
            remaining_time -= dt_max



================================================
File: erosion_deposition/generalized_erosion_deposition.py
================================================
import numpy as np

from landlab import Component
from landlab import RasterModelGrid
from landlab.utils.return_array import return_array_at_node

from ..depression_finder.lake_mapper import _FLOODED

DEFAULT_MINIMUM_TIME_STEP = 0.001  # default minimum time step duration


class _GeneralizedErosionDeposition(Component):
    """Base class for erosion-deposition type components.

    More documenation here.
    """

    _name = "_GeneralizedErosionDeposition"

    _unit_agnostic = True

    _info = {
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__upstream_node_order": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array containing downstream-to-upstream ordered list of node IDs",
        },
        "sediment__influx": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m3/s",
            "mapping": "node",
            "doc": "Sediment flux (volume per unit time of sediment entering each node)",
        },
        "sediment__outflux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m3/s",
            "mapping": "node",
            "doc": "Sediment flux (volume per unit time of sediment leaving each node)",
        },
        "surface_water__discharge": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m**2/s",
            "mapping": "node",
            "doc": "Volumetric discharge of surface water",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__steepest_slope": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "The steepest *downhill* slope",
        },
    }

    def __init__(
        self,
        grid,
        m_sp,
        n_sp,
        F_f,
        v_s,
        discharge_field="surface_water__discharge",
        dt_min=DEFAULT_MINIMUM_TIME_STEP,
    ):
        """Initialize the GeneralizedErosionDeposition model.

        Parameters
        ----------
        grid : ModelGrid
            Landlab ModelGrid object
        m_sp : float
            Discharge exponent (units vary)
        n_sp : float
            Slope exponent (units vary)
        F_f : float
            Fraction of eroded material that turns into "fines" that do not
            contribute to (coarse) sediment load. Defaults to zero.
        v_s : array_like of float
            Effective settling velocity for chosen grain size metric [L/T].
        discharge_field : float, field name, or array
            Discharge [L^2/T].
        dt_min : float, optional
            Only applies when adaptive solver is used. Minimum timestep that
            adaptive solver will use when subdividing unstable timesteps.
            Default values is 0.001. [T].
        """
        if F_f > 1.0 or F_f < 0.0:
            raise ValueError(f"fraction of fines must be between 0.0 and 1.0 ({F_f})")

        super().__init__(grid)

        self._flow_receivers = grid.at_node["flow__receiver_node"]
        self._stack = grid.at_node["flow__upstream_node_order"]
        self._topographic__elevation = grid.at_node["topographic__elevation"]
        self._slope = grid.at_node["topographic__steepest_slope"]
        self._link_to_reciever = grid.at_node["flow__link_to_receiver_node"]
        self._cell_area_at_node = grid.cell_area_at_node

        if isinstance(grid, RasterModelGrid):
            self._link_lengths = grid.length_of_d8
        else:
            self._link_lengths = grid.length_of_link

        self.initialize_output_fields()

        self._qs = grid.at_node["sediment__outflux"]
        self._q = return_array_at_node(grid, discharge_field)

        # For backward compatibility (remove in 3.0.0+)
        grid.at_node["sediment__flux"] = grid.at_node["sediment__outflux"]

        self._Q_to_the_m = np.zeros(grid.number_of_nodes)
        self._S_to_the_n = np.zeros(grid.number_of_nodes)
        self._depo_rate = np.zeros(grid.number_of_nodes)

        # store other constants
        self._m_sp = float(m_sp)
        self._n_sp = float(n_sp)
        self._v_s = v_s
        self._dt_min = dt_min
        self._F_f = float(F_f)

    @property
    def v_s(self):
        """Effective settling velocity for chosen grain size metric [L/T]."""
        if isinstance(self._v_s, str):
            return self.grid.at_node[self._v_s]
        else:
            return self._v_s

    @property
    def sediment_influx(self):
        """Volumetric sediment influx to each node."""
        return self.grid.at_node["sediment__influx"]

    @property
    def m_sp(self):
        """Discharge exponent (units vary)."""
        return self._m_sp

    @property
    def n_sp(self):
        """Slope exponent (units vary)."""
        return self._n_sp

    def _update_flow_link_slopes(self):
        """Updates gradient between each core node and its receiver.

        Used to update slope values between sub-time-steps, when we do not
        re-run flow routing.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator

        >>> rg = RasterModelGrid((3, 4))
        >>> z = rg.add_zeros("node", "topographic__elevation")
        >>> z[:] = rg.x_of_node + rg.y_of_node
        >>> fa = FlowAccumulator(rg, flow_director="FlowDirectorD8")
        >>> fa.run_one_step()
        >>> rg.at_node["topographic__steepest_slope"][5:7]
        array([1.41421356, 1.41421356])
        >>> sp = _GeneralizedErosionDeposition(rg, v_s=0.001, m_sp=0.5, n_sp=1.0, F_f=0)
        >>> z *= 0.1
        >>> sp._update_flow_link_slopes()
        >>> rg.at_node["topographic__steepest_slope"][5:7]
        array([0.14142136, 0.14142136])
        """
        self._slope[:] = (
            self._topographic__elevation
            - self._topographic__elevation[self._flow_receivers]
        ) / self._link_lengths[self._link_to_reciever]

    def _calc_hydrology(self):
        self._Q_to_the_m[:] = np.power(self._q, self._m_sp)

    def _depressions_are_handled(self):
        """Return True if a depression-handling component is present."""
        return "flood_status_code" in self._grid.at_node

    def _get_flooded_core_nodes(self):
        """Return boolean node array

        True where core node is flooded or self-draining.
        """
        is_core = self._grid.status_at_node == self._grid.BC_NODE_IS_CORE
        if self._depressions_are_handled():
            is_flooded_core = is_core & (
                self._grid.at_node["flood_status_code"] == _FLOODED
            )
        else:
            is_flooded_core = is_core & (
                self._flow_receivers == self._grid.nodes.flatten()
            )
        return np.asarray(is_flooded_core)



================================================
File: erosion_deposition/shared_stream_power.py
================================================
import numpy as np

from landlab.components.erosion_deposition.erosion_deposition import ErosionDeposition
from landlab.components.erosion_deposition.generalized_erosion_deposition import (
    DEFAULT_MINIMUM_TIME_STEP,
)
from landlab.utils import return_array_at_node

TIME_STEP_FACTOR = 0.5


class SharedStreamPower(ErosionDeposition):
    """Shared Stream Power Model in the style of Hergarten (2021).

    Implements the Shared Stream Power Model in the style of Hergarten (2021).
    Designed to simultaneously model
    river incision and sediment transport in order to seamlessly
    transition between detachment limited to transport limited erosion.
    Mathematically equivalent to the linear decline model from Davy and Lague (2009),
    which is used by the base class, ErosionDeposition. In addition, this component is
    designed to work with varying runoff rates, and can update the discharge
    and other parameters effected by discharge with each timestep.

    Here is the equation for erosion without a threshold::

        E = k_bedrock * A**m_sp * S**n_sp - k_bedrock / k_transport * Qs / A

    where ``Q`` is water discharge, ``Qs`` is sediment flux, ``S`` is slope, ``m_sp``
    and ``n_sp`` are scaling exponents, coefficient ``k_bedrock`` is the erodibility of
    the bedrock and coefficient ``k_transport`` is the ability to transport sediment.

    The first term, ``k_bedrock * A**m_sp * S**n_sp``, is the incision term, and the
    second term, ``k_bedrock / k_transport * Qs / A``, is the transport term. Note that
    ``k_bedrock / k_transport`` determines the relative amount of incision and
    sediment transport. ``k_bedrock`` modifies the incision term.

    The equivalent equation used by ErosionDeposition from Davy & Lague (2009) is::

        E = K * q**m_sp * S**n_sp - v_s * Qs / q

    where ``K`` is sediment erodibility, ``v_s`` is the settling velocity for sediment,
    and ``q`` is the water discharge.

    To translate the shared stream power input for ErosionDeposition, we use the
    equations::

        q = Ar
        K = k_bedrock / r**m_sp
        v_s = k_bedrock / k_transport

    It is important to note that the second two equations were derived only
    for calibrating the model, and do not necessarily correlate to landscape evolution
    in nature.

    To write the final equation we define the incision term as omega::

        omega = k_bedrock * A**m_sp * S**n_sp

    and incorporate ``sp_crit``, the critical stream power needed to erode bedrock,
    giving::

        E = omega * (1 - exp(omega / sp_crit) ) - k_bedrock / k_transport * Qs / A


    Written by A. Thompson.

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Hergarten, S. (2021). The influence of sediment transport on stationary
    and mobile knickpoints in river profiles. Journal of Geophysical Research:
    Earth Surface, 126, e2021JF006218. https://doi.org/10.1029/2021JF006218

    **Additional References**

    Barnhart, K., Glade, R., Shobe, C., Tucker, G. (2019). Terrainbento 1.0: a
    Python package for multi-model analysis in long-term drainage basin
    evolution. Geoscientific Model Development  12(4), 1267--1297.
    https://dx.doi.org/10.5194/gmd-12-1267-2019

    Davy, P., Lague, D. (2009). Fluvial erosion/transport equation of landscape
    evolution models revisited Journal of Geophysical Research  114(F3),
    F03007. https://dx.doi.org/10.1029/2008jf001146

    Examples
    ---------
    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import FlowAccumulator
    >>> from landlab.components import DepressionFinderAndRouter
    >>> from landlab.components import ErosionDeposition
    >>> from landlab.components import FastscapeEroder
    >>> np.random.seed(seed=5000)

    Define grid and initial topography:

    * 5x5 grid with baselevel in the lower left corner
    * All other boundary nodes closed
    * Initial topography is plane tilted up to the upper right + noise

    >>> grid = RasterModelGrid((5, 5), xy_spacing=10.0)
    >>> grid.at_node["topographic__elevation"] = (
    ...     grid.y_of_node / 10
    ...     + grid.x_of_node / 10
    ...     + np.random.rand(grid.number_of_nodes) / 10
    ... )
    >>> grid.set_closed_boundaries_at_grid_edges(
    ...     bottom_is_closed=True,
    ...     left_is_closed=True,
    ...     right_is_closed=True,
    ...     top_is_closed=True,
    ... )
    >>> grid.set_watershed_boundary_condition_outlet_id(
    ...     0, grid.at_node["topographic__elevation"], -9999.0
    ... )
    >>> fsc_dt = 100.0
    >>> ed_dt = 1.0

    Check initial topography

    >>> grid.at_node["topographic__elevation"].reshape(grid.shape)
    array([[0.02290479, 1.03606698, 2.0727653 , 3.01126678, 4.06077707],
           [1.08157495, 2.09812694, 3.00637448, 4.07999597, 5.00969486],
           [2.04008677, 3.06621577, 4.09655859, 5.04809001, 6.02641123],
           [3.05874171, 4.00585786, 5.0595697 , 6.04425233, 7.05334077],
           [4.05922478, 5.0409473 , 6.07035008, 7.0038935 , 8.01034357]])

    Instantiate Fastscape eroder, flow router, and depression finder

    >>> fr = FlowAccumulator(grid, flow_director="D8")
    >>> df = DepressionFinderAndRouter(grid)
    >>> fsc = FastscapeEroder(grid, K_sp=0.001, m_sp=0.5, n_sp=1)

    Burn in an initial drainage network using the Fastscape eroder:

    >>> for _ in range(100):
    ...     fr.run_one_step()
    ...     df.map_depressions()
    ...     flooded = np.where(df.flood_status == 3)[0]
    ...     fsc.run_one_step(dt=fsc_dt)
    ...     grid.at_node["topographic__elevation"][0] -= 0.001  # uplift
    ...

    Instantiate the SharedStreamPower component:

    >>> ssp = SharedStreamPower(
    ...     grid,
    ...     k_bedrock=0.00001,
    ...     k_transport=0.001,
    ...     m_sp=0.5,
    ...     n_sp=1.0,
    ...     sp_crit=0,
    ... )

    Now run the E/D component for 2000 short timesteps:

    >>> for _ in range(2000):  # E/D component loop
    ...     fr.run_one_step()
    ...     df.map_depressions()
    ...     ssp.run_one_step(dt=ed_dt)
    ...     grid.at_node["topographic__elevation"][0] -= 2e-4 * ed_dt
    ...

    Now we test to see if topography is right:

    >>> np.around(grid.at_node["topographic__elevation"], decimals=3).reshape(
    ...     grid.shape
    ... )
    array([[-0.477,  1.036,  2.073,  3.011,  4.061],
           [ 1.082, -0.08 , -0.065, -0.054,  5.01 ],
           [ 2.04 , -0.065, -0.065, -0.053,  6.026],
           [ 3.059, -0.054, -0.053, -0.035,  7.053],
           [ 4.059,  5.041,  6.07 ,  7.004,  8.01 ]])
    """

    _name = "SharedStreamPower"

    _unit_agnostic = True

    def __init__(
        self,
        grid,
        k_bedrock=0.001,
        k_transport=0.001,
        runoff_rate=1.0,
        m_sp=0.5,
        n_sp=1.0,
        sp_crit=0.0,
        F_f=0.0,
        discharge_field="surface_water__discharge",
        solver="basic",
    ):
        """Initialize the Shared Stream Power model.

        Parameters
        ----------
        grid : ModelGrid
            Landlab ModelGrid object
        k_bedrock : str, or array_like, optional
            Erodibility for bedrock (units vary).
        k_transport : str, or array_like, optional
            Ability to transport sediment (units vary).
        runoff_rate : float, optional
            Runoff rate. Scales Q = Ar. [m/yr]
        m_sp : float, optional
            Discharge exponent (units vary).
        n_sp : float, optional
            Slope exponent (units vary).
        sp_crit : str or array_like
            Critical stream power to erode substrate [E/(TL^2)]
        F_f : float, optional
            Fraction of eroded material that turns into "fines" that do not
            contribute to (coarse) sediment load.
        discharge_field : str or array_like, optional
            Discharge [L^2/T]. The default is to use the grid field
            ``"surface_water__discharge"``, which is simply drainage area
            multiplied by the default rainfall rate (1 m/yr). To use custom
            spatially/temporally varying rainfall, use 'water__unit_flux_in'
            to specify water input to the FlowAccumulator.
        solver : {"basic", "adaptive"}, optional
            Solver to use. Options at present include:

            1. ``"basic"`` (default): explicit forward-time extrapolation.
               Simple but will become unstable if time step is too large.
            2. ``"adaptive"``: adaptive time-step solver that estimates a
               stable step size based on the shortest time to "flattening"
               among all upstream-downstream node pairs.
        """
        self._discharge_field = discharge_field
        self._runoff_rate = runoff_rate
        self._k_bedrock = k_bedrock
        self._k_transport = k_transport

        # convert shared stream power inputs to erosion deposition inputs
        v_s = self.k_bedrock * self.runoff_rate / self.k_transport
        K_s = self.k_bedrock / self.runoff_rate**m_sp

        # instantiate ErosionDeposition
        super().__init__(
            grid,
            K=K_s,
            v_s=v_s,
            m_sp=m_sp,
            n_sp=n_sp,
            sp_crit=sp_crit,
            F_f=F_f,
            discharge_field=discharge_field,
            solver=solver,
            dt_min=DEFAULT_MINIMUM_TIME_STEP,
        )

    @property
    def k_bedrock(self):
        """Erodibility for bedrock (units vary)."""
        if isinstance(self._k_bedrock, str):
            return self.grid.at_node[self._k_bedrock]
        else:
            return self._k_bedrock

    @property
    def k_transport(self):
        """Ability to transport sediment (units vary)."""
        if isinstance(self._k_transport, str):
            return self.grid.at_node[self._k_transport]
        else:
            return self._k_transport

    @property
    def runoff_rate(self):
        """Runoff rate. Scales Q = Ar. [m/yr]"""
        if isinstance(self._runoff_rate, str):
            return self.grid.at_node[self._runoff_rate]
        else:
            return self._runoff_rate

    def update_runoff(self, new_runoff=1.0):
        """Update runoff variables.

        Updates ``runoff_rate``, ``K``, ``v_s``, and ``"water__unit_flux_in"`` for a new
        runoff rate. Works only if discharge field is set to ``"water__unit_flux_in"``.

        Parameters
        ----------
        new_runoff : str or array_like
            New runoff rate.
        """
        if self._discharge_field != "water__unit_flux_in":
            ValueError(
                "The SharedStreamPower's update_runoff method can only be used"
                "when discharge field is set to water__unit_flux_in (got"
                f" {self._discharge_field})"
            )

        self._runoff_rate = new_runoff

        self._K = self.k_bedrock / self.runoff_rate**self.m_sp
        self._v_s = self.k_bedrock * self.runoff_rate / self.k_transport
        np.multiply(
            self.runoff_rate,
            self._grid.at_node["drainage_area"],
            out=self._grid.at_node["water__unit_flux_in"],
        )
        self._q = return_array_at_node(self._grid, self._discharge_field)



================================================
File: fire_generator/__init__.py
================================================
from .generate_fire import FireGenerator

__all__ = ["FireGenerator"]



================================================
File: fire_generator/generate_fire.py
================================================
"""Landlab component that generates a random fire event in time.

This component generates a random fire event or fire time series from the
Weibull statistical distribution.

.. codeauthor:: Jordan Adams

This component generates random numbers using the Weibull distribution
(Weibull, 1951). No particular units must be used, but it was written with
the fire recurrence units in time (yrs).

Using the Weibull Distribution assumes two things: All elements within the
study area have the same fire regime. Each element must have (on average) a
constant fire regime during the time span of the study.

As of Sept. 2013, fires are considered instantaneous events independent of
other fire events in the time series.

Written by Jordan M. Adams, 2013. Updated April 2016.


Examples
--------
>>> from landlab.components.fire_generator import FireGenerator
>>> from landlab import RasterModelGrid

Create an instance of the FireGenerator component

>>> mg = RasterModelGrid((10, 10))
>>> fg = FireGenerator(mg, mean_fire_recurrence=15.0, shape_parameter=4.5)

This creates an instance of the component that has a mean_fire_recurrence, or
average interval between fires of 15 years. We gave it a shape parameter of
4.5, suggesting the cumulative distribution function that is skewed right.

Since we didn't pass it a scale parameter, the component calculates it for you.
Testing this...

>>> fg.scale_parameter
16.437036931437866

To get a time to next fire:

>>> fg.generate_fire_recurrence()  # doctest: +SKIP
10.68

References
----------
**Required Software Citation(s) Specific to this Component**

None Listed

**Additional References**

Polakow, D., Dunne, T. (1999). Modelling fire-return interval T: stochasticity
and censoring in the two-parameter Weibull model Ecological Modelling  121(1),
79-102. https://dx.doi.org/10.1016/s0304-3800(99)00074-5

"""

from random import weibullvariate

from scipy import special

from landlab import Component


class FireGenerator(Component):
    """Generate a random fire event or time series.

    Parameters
    ----------
    mean_fire_recurrence : float
        Average time between fires for a given location
    shape_parameter : float
        Describes the skew of the Weibull distribution.
        If shape < 3.5, data skews left.
        If shape == 3.5, data is normal.
        If shape > 3.5, data skews right.
        To approximate a normal bell curve, use a value of 3.5
    scale_parameter : float, optional
        Describes the peak of the Weibull distribution, located at the
        63.5% value of the cumulative distribution function. If unknown,
        it can be found using mean fire recurrence value and the
        get_scale_parameter() method described later.
    """

    _name = "FireGenerator"

    _unit_agnostic = True

    _info = {}

    def __init__(
        self, grid, mean_fire_recurrence=1.0, shape_parameter=3.5, scale_parameter=None
    ):
        """Generate a random fire event in time.

        Parameters
        ----------
        grid: landlab model grid
        mean_fire_recurrence : float
            Average time between fires for a given location
        shape_parameter : float
            Describes the skew of the Weibull distribution.
            If shape < 3.5, data skews left.
            If shape == 3.5, data is normal.
            If shape > 3.5, data skews right.
        scale_parameter : float, optional
            Describes the peak of the Weibull distribution, located at the
            63.5% value of the cumulative distribution function. If unknown,
            it can be found using mean fire recurrence value and the
            get_scale_parameter().
        """
        super().__init__(grid)
        self._mean_fire_recurrence = mean_fire_recurrence

        self._shape_parameter = shape_parameter

        if scale_parameter is None:
            self.get_scale_parameter()

        else:
            self._scale_parameter = scale_parameter

    @property
    def scale_parameter(self):
        """Scale parameter for the random distribution."""
        return self._scale_parameter

    def get_scale_parameter(self):
        """Get the scale parameter.

        ::
            mean_fire_recurrence = (scale_parameter * (
                                    special.gamma(1 + (1 / shape))))

        sets the scale parameter.
        """

        shape_in_gamma_func = float(1 + (1 / self._shape_parameter))
        gamma_func = special.gamma(shape_in_gamma_func)
        self._scale_parameter = self._mean_fire_recurrence / gamma_func

    def generate_fire_recurrence(self):
        """Get time to next fire.

        Finds the time to next fire (fire recurrence) based on the scale
        parameter (63.5% of fire Weibull distribution) and the shape parameter
        (describes the skew of the histogram, shape = 3.5
        represents a normal distribution).

        Rounds the time to next fire to 4 significant figures, for neatness.

        Returns
        -------
        float
            Updated value for the time to next fire.
        """
        self._time_to_next_fire = round(
            weibullvariate(self._scale_parameter, self._shape_parameter), 2
        )
        return self._time_to_next_fire



================================================
File: flexure/__init__.py
================================================
#!/usr/bin/env python
""".. codeauthor:: Eric Hutton <huttone@colorado.edu>

.. sectionauthor:: Eric Hutton <huttone@colorado.edu>
"""

from landlab.components.flexure.flexure import Flexure
from landlab.components.flexure.flexure_1d import Flexure1D
from landlab.components.flexure.funcs import get_flexure_parameter
from landlab.components.flexure.funcs import subside_point_load

__all__ = ["Flexure", "Flexure1D", "get_flexure_parameter", "subside_point_load"]



================================================
File: flexure/flexure.py
================================================
#!/usr/bin/env python
"""Deform the lithosphere with 1D or 2D flexure.

Landlab component that implements a 1 and 2D lithospheric flexure
model.

Examples
--------

Create a grid on which we will run the flexure calculations.

>>> from landlab import RasterModelGrid
>>> from landlab.components.flexure import Flexure
>>> grid = RasterModelGrid((5, 4), xy_spacing=(1.0e4, 1.0e4))
>>> lith_press = grid.add_zeros("lithosphere__overlying_pressure_increment", at="node")

Check the fields that are used as input to the flexure component.

>>> Flexure.input_var_names
('lithosphere__overlying_pressure_increment',)

Check the units for the fields.

>>> Flexure.var_units("lithosphere__overlying_pressure_increment")
'Pa'

If you are not sure about one of the input or output variables, you can
get help for specific variables.

>>> Flexure.var_help("lithosphere__overlying_pressure_increment")
name: lithosphere__overlying_pressure_increment
description:
  Applied pressure to the lithosphere over a time step
units: Pa
unit agnostic: True
at: node
intent: in

>>> flex = Flexure(grid)

In creating the component, a field (initialized with zeros) was added to the
grid. Reset the interior nodes for the loading.

>>> dh = grid.at_node["lithosphere__overlying_pressure_increment"]
>>> dh = dh.reshape(grid.shape)
>>> dh[1:-1, 1:-1] = flex.gamma_mantle

>>> flex.update()

>>> flex.output_var_names
('lithosphere_surface__elevation_increment',)
>>> flex.grid.at_node["lithosphere_surface__elevation_increment"].reshape(grid.shape)
array([[0., 0., 0., 0.],
       [0., 1., 1., 0.],
       [0., 1., 1., 0.],
       [0., 1., 1., 0.],
       [0., 0., 0., 0.]])
"""

import numpy as np

from landlab import Component
from landlab.components.flexure._ext.flexure2d import subside_loads as _subside_loads
from landlab.components.flexure._ext.flexure2d_slow import subside_grid_in_parallel
from landlab.components.flexure.funcs import get_flexure_parameter


class Flexure(Component):
    """Deform the lithosphere with 1D or 2D flexure.

    Landlab component that implements a 1 and 2D lithospheric flexure
    model.

    Examples
    --------

    >>> from landlab import RasterModelGrid
    >>> from landlab.components.flexure import Flexure
    >>> grid = RasterModelGrid((5, 4), xy_spacing=(1.0e4, 1.0e4))
    >>> lith_press = grid.add_zeros(
    ...     "lithosphere__overlying_pressure_increment", at="node"
    ... )

    >>> flex = Flexure(grid)
    >>> flex.name
    'Flexure'
    >>> flex.input_var_names
    ('lithosphere__overlying_pressure_increment',)
    >>> flex.output_var_names
    ('lithosphere_surface__elevation_increment',)
    >>> sorted(flex.units)
    [('lithosphere__overlying_pressure_increment', 'Pa'),
     ('lithosphere_surface__elevation_increment', 'm')]

    >>> flex.grid.number_of_node_rows
    5
    >>> flex.grid.number_of_node_columns
    4
    >>> flex.grid is grid
    True

    >>> np.all(grid.at_node["lithosphere_surface__elevation_increment"] == 0.0)
    True

    >>> np.all(grid.at_node["lithosphere__overlying_pressure_increment"] == 0.0)
    True
    >>> flex.update()
    >>> np.all(grid.at_node["lithosphere_surface__elevation_increment"] == 0.0)
    True

    >>> load = grid.at_node["lithosphere__overlying_pressure_increment"]
    >>> load[4] = 1e9
    >>> dz = grid.at_node["lithosphere_surface__elevation_increment"]
    >>> np.all(dz == 0.0)
    True

    >>> flex.update()
    >>> np.all(grid.at_node["lithosphere_surface__elevation_increment"] == 0.0)
    False

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Hutton, E., Syvitski, J. (2008). Sedflux 2.0: An advanced process-response
    model that generates three-dimensional stratigraphy. Computers &
    Geosciences.  34(10), 1319-1337.
    https://dx.doi.org/10.1016/j.cageo.2008.02.013

    **Additional References**

    Lambeck, K.: Geophysical Geodesy, The Slow Deformations of the Earth,
    Clarendon Press, Oxford, UK, 718 pp., 1988.

    """

    _name = "Flexure"

    _unit_agnostic = True

    _cite_as = r"""
    @article{hutton2008sedflux,
        title={Sedflux 2.0: An advanced process-response model that generates
               three-dimensional stratigraphy},
        author={Hutton, Eric WH and Syvitski, James PM},
        journal={Computers \& Geosciences},
        volume={34},
        number={10},
        pages={1319--1337},
        year={2008},
        publisher={Pergamon}
        }"""

    _info = {
        "lithosphere__overlying_pressure_increment": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "Pa",
            "mapping": "node",
            "doc": "Applied pressure to the lithosphere over a time step",
        },
        "lithosphere_surface__elevation_increment": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": (
                "The change in elevation of the top of the lithosphere (the "
                "land surface) in one timestep"
            ),
        },
    }

    def __init__(
        self,
        grid,
        eet=65e3,
        youngs=7e10,
        method="airy",
        rho_mantle=3300.0,
        gravity=9.80665,
        n_procs=1,
    ):
        """Initialize the flexure component.

        Parameters
        ----------
        grid : RasterModelGrid
            A grid.
        eet : float, optional
            Effective elastic thickness (m).
        youngs : float, optional
            Young's modulus.
        method : {'airy', 'flexure'}, optional
            Method to use to calculate deflections.
        rho_mantle : float, optional
            Density of the mantle (kg / m^3).
        gravity : float, optional
            Acceleration due to gravity (m / s^2).
        n_procs : int, optional
            Number of processors to use for calculations.
        """
        if method not in ("airy", "flexure"):
            raise ValueError(f"{method}: method not understood")

        super().__init__(grid)

        self._youngs = youngs
        self._method = method
        self._rho_mantle = rho_mantle
        self._gravity = gravity
        self.eet = eet
        self._n_procs = n_procs

        self.initialize_output_fields()

        self._r = self._create_kei_func_grid(
            self._grid.shape, (self._grid.dy, self._grid.dx), self.alpha
        )

    @property
    def eet(self):
        """Effective elastic thickness (m)."""
        return self._eet

    @eet.setter
    def eet(self, new_val):
        if new_val <= 0:
            raise ValueError("Effective elastic thickness must be positive.")
        self._eet = new_val
        self._r = self._create_kei_func_grid(
            self._grid.shape, (self._grid.dy, self._grid.dx), self.alpha
        )

    @property
    def youngs(self):
        """Young's modulus of lithosphere (Pa)."""
        return self._youngs

    @property
    def rho_mantle(self):
        """Density of mantle (kg/m^3)."""
        return self._rho_mantle

    @property
    def gamma_mantle(self):
        """Specific density of mantle (N/m^3)."""
        return self._rho_mantle * self._gravity

    @property
    def gravity(self):
        """Acceleration due to gravity (m/s^2)."""
        return self._gravity

    @property
    def method(self):
        """Name of method used to calculate deflections."""
        return self._method

    @property
    def alpha(self):
        """Flexure parameter (m)."""
        return get_flexure_parameter(
            self._eet, self._youngs, 2, gamma_mantle=self.gamma_mantle
        )

    @staticmethod
    def _create_kei_func_grid(shape, xy_spacing, alpha):
        from scipy.special import kei

        dx, dy = np.meshgrid(
            np.arange(shape[1]) * xy_spacing[1], np.arange(shape[0]) * xy_spacing[0]
        )

        return kei(np.sqrt(dx**2 + dy**2) / alpha)

    def update(self):
        """Update fields with current loading conditions."""
        load = self._grid.at_node["lithosphere__overlying_pressure_increment"]
        deflection = self._grid.at_node["lithosphere_surface__elevation_increment"]

        new_load = load.copy()

        deflection.fill(0.0)

        if self.method == "airy":
            deflection[:] = new_load / self.gamma_mantle
        else:
            self.subside_loads(new_load, out=deflection)

    def subside_loads_slow(self, loads, out=None):
        """Subside surface due to multiple loads.

        Parameters
        ----------
        loads : ndarray of float
            Loads applied to each grid node.
        out : ndarray of float, optional
            Buffer to place resulting deflection values.

        Returns
        -------
        ndarray of float
            Deflections caused by the loading.
        """
        if out is None:
            out = np.zeros(self._grid.shape, dtype=float)
        dz = out.reshape(self._grid.shape)
        load = loads.reshape(self._grid.shape)

        subside_grid_in_parallel(
            dz,
            load * self._grid.dx * self._grid.dy,
            self._r,
            self.alpha,
            self.gamma_mantle,
            self._n_procs,
        )

        return out

    def subside_loads(self, loads, row_col_of_load=None, out=None):
        """Subside surface due to multiple loads.

        Parameters
        ----------
        loads : ndarray of float
            Loads applied to grid node. ``loads`` can be either an array
            of size ``n_nodes``, in which case the load values are applied
            at their corresponding nodes, or an array of arbitray length,
            in which case loads are applied at locations supplied through
            the ``row_col_of_load`` keyword.
        row_col_of_load: tuple of ndarray of int, optional
            If provided, the row and column indices where loads are applied.
            The first element of the tuple is an array of rows while the
            seconds element is an array of columns.
        out : ndarray of float, optional
            Buffer to place resulting deflection values. If not provided,
            deflections will be placed into a newly-allocated array.

        Returns
        -------
        ndarray of float
            Deflections caused by the loading.
        """
        loads = np.asarray(loads)
        if out is None:
            out = self.grid.zeros(at="node")
        dz = out.reshape(self.grid.shape)

        if row_col_of_load is None:
            loads, row_col_of_load = self._handle_no_row_col(loads)
        row_of_load, col_of_load = row_col_of_load

        _subside_loads(
            dz,
            self._r.reshape(self.grid.shape),
            loads * self.grid.dx * self.grid.dy,
            np.asarray(row_of_load),
            np.asarray(col_of_load),
            self.alpha,
            self.gamma_mantle,
        )

        return out

    def _handle_no_row_col(self, loads, tol=1e-6):
        """Handle the case where the row_col_of_load keyword is not provided."""
        loads = loads.reshape(self.grid.shape)
        row_col_of_load = np.unravel_index(
            np.flatnonzero(np.abs(loads) > tol), self.grid.shape
        )
        loads = loads[row_col_of_load]

        return loads, row_col_of_load



================================================
File: flexure/flexure_1d.py
================================================
#!/usr/bin/env python
"""Deform the lithosphere with 1D or 2D flexure.

Landlab component that implements a 1D lithospheric flexure.

Examples
--------

Create a grid on which we will run the flexure calculations.

>>> from landlab import RasterModelGrid
>>> from landlab.components.flexure import Flexure1D
>>> grid = RasterModelGrid((3, 4), xy_spacing=(1.0e4, 1.0e4))
>>> lith_press = grid.add_zeros("node", "lithosphere__increment_of_overlying_pressure")

Because `Flexure1D` is a one-dimensional component, it operates
*independently* on each row of grid nodes. By default, it will
calculate deflections on every row but you can also specify
which rows to operate on by passing an indexing array to the
`rows` keyword.

In this example, we'll just calculate deflections along the
middle row or nodes (that is, row 1)

>>> flex = Flexure1D(grid, rows=1)

In creating the component, a field (initialized with zeros) was
added to the grid that represents the current loading distribution.
If the grid already had this field, the component would use the
existing field. This can be accessed either through the *grid*
attribute in the same way as other landlab fields,

>>> flex.grid.at_node["lithosphere__increment_of_overlying_pressure"]
array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])

or through the `load_at_node` attribute of `Flexure1D`,

>>> flex.load_at_node
array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.]])

Notice that `load_at_node` returns a reshaped view of the array
whereas the field returns a flattened array. Change values in this
array to add loads to the grid,

>>> flex.load_at_node[1, 2:4] = flex.gamma_mantle
>>> flex.run_one_step()

The output deflections can be retrieved either using landlab fields
as,

>>> flex.grid.at_node["lithosphere_surface__increment_of_elevation"]
array([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.])

or through the `dz_at_node` attribute,

>>> flex.dz_at_node
array([[0., 0., 0., 0.],
       [0., 0., 1., 1.],
       [0., 0., 0., 0.]])
"""
import contextlib

import numpy as np

from landlab import Component
from landlab.components.flexure._ext.flexure1d import subside_load_1d


class Flexure1D(Component):
    """Deform the lithosphere with 1D flexure.

    Landlab component that implements a 1D lithospheric flexure model.

    Parameters
    ----------
    grid : RasterModelGrid
        A grid.
    eet : float, optional
        Effective elastic thickness (m).
    youngs : float, optional
        Young's modulus.
    method : {'airy', 'flexure'}, optional
        Method to use to calculate deflections.
    rho_mantle : float, optional
        Density of the mantle (kg / m^3).
    rho_water : float, optional
        Density of the sea water (kg / m^3).
    gravity : float, optional
        Acceleration due to gravity (m / s^2).
    rows : int, optional
        Node rows that this component will operate on (default is to
        operate on *all* rows).

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> from landlab.components.flexure import Flexure1D
    >>> grid = RasterModelGrid((5, 4), xy_spacing=(1.0e4, 1.0e4))
    >>> lith_press = grid.add_zeros(
    ...     "node", "lithosphere__increment_of_overlying_pressure"
    ... )
    >>> flex = Flexure1D(grid)
    >>> flex.name
    '1D Flexure Equation'
    >>> flex.input_var_names
    ('lithosphere__increment_of_overlying_pressure',)
    >>> flex.output_var_names
    ('lithosphere_surface__increment_of_elevation',)
    >>> sorted(flex.units)
    [('lithosphere__increment_of_overlying_pressure', 'Pa'),
     ('lithosphere_surface__increment_of_elevation', 'm')]

    >>> flex.grid.number_of_node_rows
    5
    >>> flex.grid.number_of_node_columns
    4
    >>> flex.grid is grid
    True

    >>> np.all(grid.at_node["lithosphere_surface__increment_of_elevation"] == 0.0)
    True

    >>> np.all(grid.at_node["lithosphere__increment_of_overlying_pressure"] == 0.0)
    True
    >>> flex.update()
    >>> np.all(grid.at_node["lithosphere_surface__increment_of_elevation"] == 0.0)
    True

    >>> load = grid.at_node["lithosphere__increment_of_overlying_pressure"]
    >>> load[4] = 1e9
    >>> dz = grid.at_node["lithosphere_surface__increment_of_elevation"]
    >>> np.all(dz == 0.0)
    True

    >>> flex.update()
    >>> np.all(grid.at_node["lithosphere_surface__increment_of_elevation"] == 0.0)
    False
    """

    _name = "1D Flexure Equation"

    _unit_agnostic = True

    _info = {
        "lithosphere__increment_of_overlying_pressure": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "Pa",
            "mapping": "node",
            "doc": "Applied pressure to the lithosphere over a time step",
        },
        "lithosphere_surface__increment_of_elevation": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": (
                "The change in elevation of the top of the lithosphere (the "
                "land surface) in one timestep"
            ),
        },
    }

    _POISSON = 0.25

    def __init__(
        self,
        grid,
        eet=65e3,
        youngs=7e10,
        method="airy",
        rho_mantle=3300.0,
        rho_water=1030.0,
        gravity=9.80665,
        rows=None,
    ):
        """Initialize the flexure component.

        Parameters
        ----------
        grid : RasterModelGrid
            A grid.
        eet : float, optional
            Effective elastic thickness (m).
        youngs : float, optional (Pa)
            Young's modulus.
        method : {'airy', 'flexure'}, optional
            Method to use to calculate deflections.
        rho_mantle : float, optional
            Density of the mantle (kg / m^3).
        rho_water : float, optional
            Density of the sea water (kg / m^3).
        gravity : float, optional
            Acceleration due to gravity (m / s^2).
        rows : int, optional
            Node rows that this component will operate on (default is to
            operate on *all* rows).
        """
        if method not in ("airy", "flexure"):
            raise ValueError(f"{method}: method not understood")

        super().__init__(grid)

        self._method = method
        self.youngs = youngs
        self.rho_mantle = rho_mantle
        self.rho_water = rho_water
        self.gravity = gravity
        self.eet = eet

        self.initialize_output_fields()

        self._rows = (rows,) or Ellipsis

        self._x_at_node = self._grid.x_of_node.reshape(self._grid.shape).copy()

    @property
    def eet(self):
        """Effective elastic thickness (m)."""
        return self._eet

    @eet.setter
    def eet(self, new_val):
        if new_val <= 0:
            raise ValueError("Effective elastic thickness must be positive.")
        with contextlib.suppress(AttributeError):
            del self._rigidity, self._alpha
        self._eet = float(new_val)

    @property
    def youngs(self):
        """Young's modulus of lithosphere (Pa)."""
        return self._youngs

    @youngs.setter
    def youngs(self, new_val):
        if new_val <= 0:
            raise ValueError("Young's modulus must be positive.")
        with contextlib.suppress(AttributeError):
            del self._rigidity, self._alpha
        self._youngs = float(new_val)

    @property
    def rho_water(self):
        """Density of water (kg/m^3)."""
        return self._rho_water

    @rho_water.setter
    def rho_water(self, new_val):
        if new_val <= 0:
            raise ValueError("Water density must be positive.")
        with contextlib.suppress(AttributeError):
            del self._gamma_mantle, self._alpha
        self._rho_water = float(new_val)

    @property
    def rho_mantle(self):
        """Density of mantle (kg/m^3)."""
        return self._rho_mantle

    @rho_mantle.setter
    def rho_mantle(self, new_val):
        if new_val <= 0:
            raise ValueError("Mantle density must be positive.")
        with contextlib.suppress(AttributeError):
            del self._gamma_mantle, self._alpha
        self._rho_mantle = float(new_val)

    @property
    def gravity(self):
        """Acceleration due to gravity (m/s^2)."""
        return self._gravity

    @gravity.setter
    def gravity(self, new_val):
        if new_val <= 0:
            raise ValueError("Acceleration due to gravity must be positive.")
        with contextlib.suppress(AttributeError):
            del self._gamma_mantle, self._alpha
        self._gravity = float(new_val)

    @property
    def alpha(self):
        """Flexure parameter (m)."""
        try:
            self._alpha
        except AttributeError:
            self._alpha = np.power(4 * self.rigidity / self.gamma_mantle, 0.25)
        return self._alpha

    @property
    def rigidity(self):
        """Flexural rigidity (N m)."""
        try:
            self._rigidity
        except AttributeError:
            self._rigidity = (
                self._eet**3.0 * self._youngs / (12.0 * (1.0 - self._POISSON**2.0))
            )
        return self._rigidity

    @property
    def gamma_mantle(self):
        """Specific density of mantle (N/m^3)."""
        try:
            self._gamma_mantle
        except AttributeError:
            self._gamma_mantle = (self._rho_mantle - self._rho_water) * self._gravity
        return self._gamma_mantle

    @property
    def method(self):
        """Name of method used to calculate deflections."""
        return self._method

    @property
    def x_at_node(self):
        return self._x_at_node

    @property
    def load_at_node(self):
        return self._grid.at_node[
            "lithosphere__increment_of_overlying_pressure"
        ].reshape(self._grid.shape)

    @property
    def dz_at_node(self):
        return self._grid.at_node[
            "lithosphere_surface__increment_of_elevation"
        ].reshape(self._grid.shape)

    def update(self):
        """Update fields with current loading conditions."""
        load = self.load_at_node[self._rows]
        deflection = self.dz_at_node[self._rows]

        if self._method == "airy":
            deflection[:] = load / self.gamma_mantle
        else:
            Flexure1D.calc_flexure(
                self.x_at_node[0], load, self.alpha, self.rigidity, out=deflection
            )

        if not np.may_share_memory(deflection, self.dz_at_node):
            self.dz_at_node[self._rows] = deflection

    def run_one_step(self):
        self.update()

    def subside_loads(self, loads, out=None):
        """Subside surface due to multiple loads.

        Parameters
        ----------
        loads : ndarray of float
            Loads applied to each grid node (Pa).
        out : ndarray of float, optional
            Buffer to place resulting deflection values (m).

        Returns
        -------
        ndarray of float
            Deflections caused by the loading.
        """
        if out is None:
            out = np.zeros(self._grid.shape)
        loads = np.asarray(loads)
        if self._method == "airy":
            out[:] = loads / self.gamma_mantle
        else:
            Flexure1D.calc_flexure(
                self.x_at_node[0], loads, self.alpha, self.rigidity, out=out
            )

        return out

    @staticmethod
    def calc_flexure(x, loads, alpha, rigidity, out=None):
        """Subside surface due to multiple loads.

        Parameters
        ----------
        x : ndarray of float
            Position of grid nodes (m).
        loads : ndarray of float
            Loads applied to each grid node (Pa).
        alpha : float
            Flexure parameter of the lithosphere (m).
        rigidity : float
            Flexural rigidity of the lithosphere (N m).
        out : ndarray of float, optional
            Buffer to place resulting deflection values (m).

        Returns
        -------
        ndarray of float
            Deflections caused by the loading.
        """
        if out is None:
            out = np.zeros_like(loads, dtype=float)

        loads = loads.reshape((-1, loads.shape[-1]))
        dz = out[..., :].reshape((-1, out.shape[-1]))
        subside_load_1d(x, loads, alpha, rigidity, dz[..., :])

        return out



================================================
File: flexure/funcs.py
================================================
#!/usr/bin/env python

import numpy as np
import scipy.special

_POISSON = 0.25

_N_PROCS = 4


def get_flexure_parameter(eet, youngs, n_dim, gamma_mantle=33000.0):
    """Calculate the flexure parameter based on some physical constants.

    Parameters
    ----------
    eet : float
        Effective elastic thickness of Earth's crust [m].
    youngs : float
        Young's modulus.
    n_dim: int
        Number of spatial dimensions (1 or 2).
    gamma_mantle: float
        Speific weight of the mantle [N/m^3].

    Returns
    -------
    float
        The flexure parameter.

    Examples
    --------
    >>> from landlab.components.flexure import get_flexure_parameter

    >>> eet = 65000.0
    >>> youngs = 7e10
    >>> alpha = get_flexure_parameter(eet, youngs, 1)
    >>> print("%.3f" % round(alpha, 3))
    119965.926

    >>> alpha = get_flexure_parameter(eet, youngs, 2)
    >>> print("%.2f" % alpha)
    84828.72
    """
    D = youngs * pow(eet, 3) / 12.0 / (1.0 - pow(_POISSON, 2))

    if n_dim not in (1, 2):
        raise ValueError("n_dim must be either 1 or 2")

    if n_dim == 2:
        alpha = pow(D / gamma_mantle, 0.25)
    else:
        alpha = pow(4.0 * D / gamma_mantle, 0.25)

    return alpha


def _calculate_distances(locs, coords):
    r = pow(coords[0][:, np.newaxis] - locs[0], 2)
    r += pow(coords[1][:, np.newaxis] - locs[1], 2)
    return np.sqrt(r, out=r)


def _calculate_deflections(load, locs, coords, alpha, out=None, gamma_mantle=33000.0):
    c = -load / (2.0 * np.pi * gamma_mantle * pow(alpha, 2.0))
    r = _calculate_distances(locs, coords) / alpha

    scipy.special.kei(r, out=r)
    np.multiply(r, c[np.newaxis, :], out=r)
    return np.sum(r, axis=1, out=out)


def subside_point_load(load, loc, coords, params=None, out=None):
    """Calculate deflection at points due a point load.

    Calculate deflections on a grid, defined by the points in the *coords*
    tuple, due to a point load of magnitude *load* applied at *loc*.

    *x* and *y* are the x and y coordinates of each node of the solution
    grid (in meters). The scalars *eet* and *youngs* define the crustal
    properties.

    Parameters
    ----------
    load : float
        Magnitude of the point load.
    loc : float or tuple
        Location of the load as either a scalar or as (*x*, *y*)
    coords : ndarray
        Array of points to calculate deflections at
    params : dict-like
        Physical parameters used for deflection calculation. Valid keys are
        - *eet*: Effective elastic thickness
        - *youngs*: Young's modulus
    out : ndarray, optional
        Array to put deflections into.

    Returns
    -------
    out : ndarray
        Array of deflections.

    Examples
    --------

    >>> from landlab.components.flexure import subside_point_load

    >>> params = dict(eet=65000.0, youngs=7e10)
    >>> load = 1e9

    Define a unifrom rectilinear grid.

    >>> x = np.arange(0, 10000, 100.0)
    >>> y = np.arange(0, 5000, 100.0)
    >>> (x, y) = np.meshgrid(x, y)
    >>> x.shape = (x.size,)
    >>> y.shape = (y.size,)

    Calculate deflections due to a load applied at position (5000., 2500.).

    >>> x = np.arange(0, 10000, 1000.0)
    >>> y = np.arange(0, 5000, 1000.0)
    >>> (x, y) = np.meshgrid(x, y)
    >>> x.shape = (x.size,)
    >>> y.shape = (y.size,)
    >>> dz = subside_point_load(load, (5000.0, 2500.0), (x, y), params=params)
    >>> print("%.5g" % round(dz.sum(), 9))
    2.6267e-05
    >>> print(round(dz.min(), 9))
    5.24e-07
    >>> print(round(dz.max(), 9))
    5.26e-07

    >>> dz = subside_point_load(
    ...     (1e9, 1e9), ((5000.0, 5000.0), (2500.0, 2500.0)), (x, y), params=params
    ... )
    >>> print(round(dz.min(), 9) / 2.0)
    5.235e-07
    >>> print(round(dz.max(), 9) / 2.0)
    5.265e-07
    """
    params = params or {"eet": 6500.0, "youngs": 7.0e10}
    eet, youngs = params["eet"], params["youngs"]
    gamma_mantle = params.get("gamma_mantle", 33000.0)

    load = np.asarray(load).reshape((-1,))
    loc = np.asarray(loc).reshape((-1, len(load)))
    coords = np.asarray(coords)
    if coords.ndim == 1:
        coords = np.expand_dims(coords, axis=0)

    n_dim = len(loc)
    if n_dim not in (1, 2):
        raise ValueError("number of dimension must be 1 or 2")
    if len(coords) != n_dim:
        raise ValueError("number of dimensions in coordinates doesn't match loc")

    if out is None:
        out = np.empty(coords[0].size, dtype=float)

    alpha = get_flexure_parameter(eet, youngs, n_dim, gamma_mantle=gamma_mantle)

    if n_dim == 2:
        _calculate_deflections(
            load, loc, coords, alpha, out=out, gamma_mantle=gamma_mantle
        )
    else:
        x, x0 = np.meshgrid(loc[0], coords[0])
        c = load / (2.0 * alpha * gamma_mantle)
        r = abs(x - x0) / alpha
        out[:] = (c * np.exp(-r) * (np.cos(r) + np.sin(r))).sum(axis=1)

    return out



================================================
File: flexure/_ext/__init__.py
================================================
from .flexure1d import subside_load_1d

__all__ = ["subside_load_1d"]



================================================
File: flexure/_ext/flexure1d.pyx
================================================
cimport cython
from cython.parallel cimport prange
from libc.math cimport cos
from libc.math cimport exp
from libc.math cimport fabs
from libc.math cimport sin


@cython.boundscheck(False)
@cython.wraparound(False)
def subside_load_1d(
    cython.floating [:] x,
    cython.floating [:, :] loads,
    double alpha,
    double rigidity,
    cython.floating [:, :] out,
):
    cdef long n_rows = loads.shape[0]
    cdef long row

    for row in prange(n_rows, nogil=True, schedule="static"):
        _subside_row(x, loads[row], alpha, rigidity, out[row])


@cython.cdivision(True)
@cython.boundscheck(False)
@cython.wraparound(False)
cdef void _subside_row(
    cython.floating [:] x,
    cython.floating [:] loads,
    double alpha,
    double rigidity,
    cython.floating [:] out,
) noexcept nogil:
    cdef long n_points = len(x)
    cdef long col

    for col in range(n_points):
        _subside_point_load(x, x[col], loads[col], alpha, rigidity, out)


@cython.cdivision(True)
@cython.boundscheck(False)
@cython.wraparound(False)
cdef void _subside_point_load(
    cython.floating [:] x,
    double x_at_load,
    double load,
    double alpha,
    double rigidity,
    cython.floating [:] out,
) noexcept nogil:
    cdef long n_points = len(x)
    cdef float c = load * alpha ** 3. / (8. * rigidity)
    cdef float dx
    cdef long col

    for col in range(n_points):
        dx = fabs(x[col] - x_at_load) / alpha
        out[col] += c * exp(- dx) * (cos(dx) + sin(dx))



================================================
File: flexure/_ext/flexure2d.pyx
================================================
cimport cython
from cython.parallel cimport prange
from libc.math cimport M_PI
from libc.stdlib cimport labs

# https://cython.readthedocs.io/en/stable/src/userguide/fusedtypes.html
ctypedef fused index_t:
    cython.integral
    long long
    unsigned int
    unsigned long
    unsigned long long


@cython.boundscheck(False)
@cython.wraparound(False)
def subside_loads(
    cython.floating [:, :] w,
    const cython.floating [:, :] r,
    const cython.floating [:] loads,
    const index_t [:] row_of_load,
    const index_t [:] col_of_load,
    const double alpha,
    const double gamma_mantle,
):
    cdef long n_rows = w.shape[0]
    cdef long n_cols = w.shape[1]
    cdef long n_loads = loads.shape[0]
    cdef long load_row
    cdef long load_col
    cdef long row
    cdef long col
    cdef long d_row
    cdef long load
    cdef double c
    cdef double inv_c = 1. / (2. * M_PI * gamma_mantle * alpha ** 2.)

    for row in prange(n_rows, nogil=True, schedule="static"):
        for load in range(n_loads):
            c = loads[load] * inv_c
            load_row = row_of_load[load]
            load_col = col_of_load[load]
            d_row = labs(load_row - row)
            for col in range(n_cols):
                w[row, col] = w[row, col] - c * r[d_row, abs(load_col - col)]



================================================
File: flexure/_ext/flexure2d_slow.pyx
================================================
from multiprocessing import Pool

import numpy as np

cimport cython
cimport numpy as np
from libc.math cimport fabs
from libc.stdlib cimport abs

DTYPE = np.double
ctypedef np.double_t DTYPE_t


@cython.boundscheck(False)
def subside_parallel_row(
    np.ndarray[DTYPE_t, ndim=1] w,
    np.ndarray[DTYPE_t, ndim=1] load,
    np.ndarray[DTYPE_t, ndim=1] r,
    DTYPE_t alpha,
    DTYPE_t gamma_mantle,
):
    cdef int ncols = w.shape[0]
    cdef double inv_c = 1. / (2. * np.pi * gamma_mantle * alpha ** 2.)
    cdef double c
    cdef int i
    cdef int j

    for i in range(ncols):
        if fabs(load[i]) > 1e-6:
            c = load[i] * inv_c
            for j in range(ncols):
                w[j] += - c * r[abs(j - i)]


def subside_grid(
    np.ndarray[DTYPE_t, ndim=2] w,
    np.ndarray[DTYPE_t, ndim=2] load,
    np.ndarray[DTYPE_t, ndim=2] r,
    DTYPE_t alpha, DTYPE_t gamma_mantle,
):
    cdef int nrows = w.shape[0]
    cdef int i
    cdef int j

    for i in range(nrows):
        for j in range(nrows):
            subside_parallel_row(w[j], load[i], r[abs(j - i)], alpha, gamma_mantle)


def subside_grid_strip(
    np.ndarray[DTYPE_t, ndim=2] load,
    np.ndarray[DTYPE_t, ndim=2] r,
    DTYPE_t alpha, DTYPE_t gamma_mantle, strip_range,
):
    (start, stop) = strip_range

    cdef np.ndarray w = np.zeros((stop - start, load.shape[1]), dtype=DTYPE)
    cdef i

    for i in range(load.shape[0]):
        for j in range(start, stop):
            subside_parallel_row(
                w[j - start], load[i], r[abs(j - i)], alpha, gamma_mantle
            )

    return w, strip_range


def tile_grid_into_strips(grid, n_strips):
    rows_per_strip = grid.shape[0] // n_strips

    starts = np.arange(0, grid.shape[0], rows_per_strip)
    stops = starts + rows_per_strip
    stops[-1] = grid.shape[0]

    return zip(starts, stops)


def _subside_grid_strip_helper(args):
    return subside_grid_strip(*args)


def subside_grid_in_parallel(
    np.ndarray[DTYPE_t, ndim=2] w,
    np.ndarray[DTYPE_t, ndim=2] load,
    np.ndarray[DTYPE_t, ndim=2] r,
    DTYPE_t alpha, DTYPE_t gamma_mantle, n_procs,
):
    if n_procs == 1:
        return subside_grid(w, load, r, alpha, gamma_mantle)

    strips = tile_grid_into_strips(w, n_procs)

    args = [(load, r, alpha, gamma_mantle, strip) for strip in strips]

    pool = Pool(processes=n_procs)

    results = pool.map(_subside_grid_strip_helper, args)
    for dz, strip in results:
        start, stop = strip
        w[start:stop] += dz



================================================
File: flow_accum/__init__.py
================================================
from .flow_accum_bw import find_drainage_area_and_discharge
from .flow_accum_bw import flow_accumulation
from .flow_accum_bw import make_ordered_node_array
from .flow_accumulator import FlowAccumulator
from .lossy_flow_accumulator import LossyFlowAccumulator

__all__ = [
    "FlowAccumulator",
    "LossyFlowAccumulator",
    "make_ordered_node_array",
    "find_drainage_area_and_discharge",
    "flow_accumulation",
]



================================================
File: flow_accum/cfuncs.pyx
================================================
cimport cython

# https://cython.readthedocs.io/en/stable/src/userguide/fusedtypes.html
ctypedef fused id_t:
    cython.integral
    long long


@cython.boundscheck(False)
cpdef _add_to_stack(
    long l,
    long j,
    id_t [:] s,
    id_t [:] delta,
    id_t [:] donors,
):
    """
    Adds node l to the stack and increments the current index (j).
    """
    cdef int m, n, delta_l, delta_lplus1

    s[j] = l
    j += 1
    delta_l = delta[l]
    delta_lplus1 = delta[l+1]

    for n in range(delta_l, delta_lplus1):
        m = donors[n]
        if m != l:
            j = _add_to_stack(m, j, s, delta, donors)

    return j


@cython.boundscheck(False)
cpdef _accumulate_to_n(
    long size,
    long q,
    id_t [:] s,
    id_t [:, :] r,
    cython.floating [:, :] p,
    cython.floating [:] drainage_area,
    cython.floating [:] discharge,
):
    """
    Accumulates drainage area and discharge, permitting transmission losses.
    """
    cdef int donor, recvr, i, v
    cdef float accum, proportion

    # Iterate backward through the list, which means we work from upstream to
    # downstream.
    for i in range(size - 1, -1, -1):
        donor = s[i]
        for v in range(q):
            recvr = r[donor, v]
            proportion = p[donor, v]
            if proportion > 0.:
                if donor != recvr:
                    drainage_area[recvr] += proportion*drainage_area[donor]
                    accum = discharge[recvr] + proportion*discharge[donor]
                    if accum < 0.:
                        accum = 0.
                    discharge[recvr] = accum


@cython.boundscheck(False)
cpdef _accumulate_bw(
    long size,
    id_t [:] s,
    id_t [:] r,
    cython.floating [:] drainage_area,
    cython.floating [:] discharge,
):
    """
    Accumulates drainage area and discharge, permitting transmission losses.
    """
    cdef int donor, recvr, i
    cdef float accum

    # Iterate backward through the list, which means we work from upstream to
    # downstream.
    for i in range(size - 1, -1, -1):
        donor = s[i]
        recvr = r[donor]
        if donor != recvr:
            drainage_area[recvr] += drainage_area[donor]
            accum = discharge[recvr] + discharge[donor]
            if accum < 0.:
                accum = 0.
            discharge[recvr] = accum


@cython.boundscheck(False)
cpdef _make_donors(
    long size,
    id_t [:] w,
    id_t [:] D,
    id_t [:] delta,
    id_t [:] r,
):
    """Determines number of donors"""
    cdef int ri, i
    for i in range(size):
        ri = r[i]
        D[delta[ri] + w[ri]] = i
        w[ri] += 1


@cython.boundscheck(False)
cpdef _make_donors_to_n(
    long size,
    long q,
    id_t [:] w,
    id_t [:] D,
    id_t [:] delta,
    id_t [:, :] r,
    cython.floating [:, :] p,
):
    """Determines number of donors for route to n"""
    cdef int ri, i, v, ind
    for v in range(q):
        for i in range(size):
            ri = r[i, v]
            if p[i, v] > 0:
                ind = delta[ri] + w[ri]
                D[ind] = i
                w[ri] += 1



================================================
File: flow_accum/flow_accum_bw.py
================================================
#!/usr/env/python

"""
flow_accum_bw.py: Implementation of the Braun & Willet (2012) stack alorithm.

Implementation of Braun & Willett (2012) algorithm for calculating drainage
area and (optionally) water discharge. Assumes each node has only one
downstream receiver. If water discharge is calculated, the result assumes
steady flow (that is, hydrologic equilibrium).

The main public function is::

    a, q, s = flow_accumulation(r)

which takes an array of receiver-node IDs, r (the nodes that "receive" the flow
from a each node; this array would be returned by the flow_routing component's
calc_flowdirs() method). It returns Numpy
arrays with the drainage area (a) and discharge (q) at each node, along with an
array (s) that contains the IDs of the nodes in downstream-to-upstream order.

If you simply want the ordered list by itself, use::

    s = make_ordered_node_array(r)

Created: GT Nov 2013
"""
import numpy

from landlab.core.utils import as_id_array

from .cfuncs import _accumulate_bw
from .cfuncs import _add_to_stack
from .cfuncs import _make_donors


class _DrainageStack:
    """Implements Braun & Willett's add_to_stack function.

    The _DrainageStack() class implements Braun & Willett's add_to_stack
    function (as a method) and also keeps track of the counter (j) and
    the stack (s). It is used by the make_ordered_node_array() function.
    """

    def __init__(self, delta, D):
        """Initializes the _Drainage_Stack class.

        Initializes the index counter j to zero, creates the stack array
        s, and stores references to delta and D.
        """
        self.j = 0
        self.s = numpy.zeros(len(D), dtype=int)
        self.delta = delta
        self.D = D

    def add_to_stack(self, node):
        """Adds *node* to the stack and increments the current index (j).

        Examples
        --------
        >>> import numpy as np
        >>> from landlab.components.flow_accum.flow_accum_bw import _DrainageStack
        >>> delta = np.array([0, 0, 2, 2, 2, 6, 7, 9, 10, 10, 10])
        >>> D = np.array([0, 2, 1, 4, 5, 7, 6, 3, 8, 9])
        >>> ds = _DrainageStack(delta, D)
        >>> ds.add_to_stack(4)
        >>> ds.s
        array([4, 1, 0, 2, 5, 6, 3, 8, 7, 9])
        """
        # we invoke cython here to attempt to suppress Python's RecursionLimit
        self.j = _add_to_stack(node, self.j, self.s, self.delta, self.D)


def _make_number_of_donors_array(r):
    """Number of donors for each node.

    Creates and returns an array containing the number of donors for each node.

    Parameters
    ----------
    r : ndarray
        ID of receiver for each node.

    Returns
    -------
    ndarray
        Number of donors for each node.

    Examples
    --------
    The example below is from Braun and Willett (2012); nd corresponds to their
    d_i in Table 1.

    >>> import numpy as np
    >>> from landlab.components.flow_accum.flow_accum_bw import (
    ...     _make_number_of_donors_array,
    ... )
    >>> r = np.array([2, 5, 2, 7, 5, 5, 6, 5, 7, 8]) - 1
    >>> nd = _make_number_of_donors_array(r)
    >>> nd
    array([0, 2, 0, 0, 4, 1, 2, 1, 0, 0])
    """
    nd = numpy.zeros(r.size, dtype=int)
    max_index = numpy.max(r)
    nd[: (max_index + 1)] = numpy.bincount(r)
    return nd


def _make_delta_array(nd):
    r"""
    Delta array.

    Creates and returns the "delta" array, which is a list containing, for each
    node, the array index where that node's donor list begins.

    Parameters
    ----------
    nd : ndarray of int
        Number of donors for each node

    Returns
    -------
    ndarray of int
        Delta array

    Examples
    --------
    The example below is from Braun and Willett (2012), and represents
    \delta_i in their Table 1. Here, the numbers are all one less than in their
    table because here we number indices from 0 rather than 1.

    >>> import numpy as np
    >>> from landlab.components.flow_accum.flow_accum_bw import _make_delta_array
    >>> nd = np.array([0, 2, 0, 0, 4, 1, 2, 1, 0, 0])
    >>> delta = _make_delta_array(nd)
    >>> delta
    array([ 0,  0,  2,  2,  2,  6,  7,  9, 10, 10, 10])
    """
    np = len(nd)
    delta = numpy.zeros(np + 1, dtype=int)
    delta.fill(np)
    delta[-2::-1] -= numpy.cumsum(nd[::-1])
    return delta


def _make_array_of_donors(r, delta):
    """Creates and returns an array containing the IDs of donors for each node.

    Essentially, the array is a series of lists (not in the Python list object
    sense) of IDs for each node. See Braun & Willett (2012) for details.

    The example below is from Braun & Willett (2012), and produces D_i in their
    Table 1 (except that here the ID numbers are one less, because we number
    indices from zero).

    Examples
    --------
    >>> import numpy as np
    >>> from landlab.components.flow_accum.flow_accum_bw import _make_array_of_donors
    >>> r = np.array([2, 5, 2, 7, 5, 5, 6, 5, 7, 8]) - 1
    >>> delta = np.array([0, 0, 2, 2, 2, 6, 7, 9, 10, 10, 10])
    >>> D = _make_array_of_donors(r, delta)
    >>> D
    array([0, 2, 1, 4, 5, 7, 6, 3, 8, 9])
    """
    np = len(r)
    w = numpy.zeros(np, dtype=r.dtype)
    D = numpy.zeros(np, dtype=r.dtype)

    _make_donors(np, w, D, delta, r)

    return D


def make_ordered_node_array(receiver_nodes, nd=None, delta=None, D=None):
    """Create an array of node IDs that is arranged in order from.

    Creates and returns an array of node IDs that is arranged in order from
    downstream to upstream.

    The lack of a leading underscore is meant to signal that this operation
    could be useful outside of this module!

    Examples
    --------
    >>> import numpy as np
    >>> from landlab.components.flow_accum import make_ordered_node_array
    >>> r = np.array([2, 5, 2, 7, 5, 5, 6, 5, 7, 8]) - 1
    >>> s = make_ordered_node_array(r)
    >>> s
    array([4, 1, 0, 2, 5, 6, 3, 8, 7, 9])
    """
    node_id = numpy.arange(receiver_nodes.size)
    baselevel_nodes = numpy.where(node_id == receiver_nodes)[0]
    if nd is None:
        nd = _make_number_of_donors_array(receiver_nodes)
    if delta is None:
        delta = _make_delta_array(nd)
    if D is None:
        D = _make_array_of_donors(receiver_nodes, delta)
    dstack = _DrainageStack(delta, D)
    add_it = dstack.add_to_stack
    for k in baselevel_nodes:
        add_it(k)  # don't think this is a bottleneck, so no C++

    return dstack.s


def find_drainage_area_and_discharge(
    s, r, node_cell_area=1.0, runoff=1.0, boundary_nodes=None
):
    """Calculate the drainage area and water discharge at each node.

    Parameters
    ----------
    s : ndarray of int
        Ordered (downstream to upstream) array of node IDs
    r : ndarray of int
        Receiver IDs for each node
    node_cell_area : float or ndarray
        Cell surface areas for each node. If it's an array, must have same
        length as s (that is, the number of nodes).
    runoff : float or ndarray
        Local runoff rate at each cell (in water depth per time). If it's an
        array, must have same length as s (that is, the number of nodes).
    boundary_nodes: list, optional
        Array of boundary nodes to have discharge and drainage area set to zero.
        Default value is None.
    Returns
    -------
    tuple of ndarray
        drainage area and discharge

    Notes
    -----
    -  If node_cell_area not given, the output drainage area is equivalent
       to the number of nodes/cells draining through each point, including
       the local node itself.
    -  Give node_cell_area as a scalar when using a regular raster grid.
    -  If runoff is not given, the discharge returned will be the same as
       drainage area (i.e., drainage area times unit runoff rate).
    -  If using an unstructured Landlab grid, make sure that the input
       argument for node_cell_area is the cell area at each NODE rather than
       just at each CELL. This means you need to include entries for the
       perimeter nodes too. They can be zeros.

    Examples
    --------
    >>> import numpy as np
    >>> from landlab.components.flow_accum import find_drainage_area_and_discharge
    >>> r = np.array([2, 5, 2, 7, 5, 5, 6, 5, 7, 8]) - 1
    >>> s = np.array([4, 1, 0, 2, 5, 6, 3, 8, 7, 9])
    >>> a, q = find_drainage_area_and_discharge(s, r)
    >>> a
    array([  1.,   3.,   1.,   1.,  10.,   4.,   3.,   2.,   1.,   1.])
    >>> q
    array([  1.,   3.,   1.,   1.,  10.,   4.,   3.,   2.,   1.,   1.])
    """
    # Number of points
    np = len(s)

    # Initialize the drainage_area and discharge arrays. Drainage area starts
    # out as the area of the cell in question, then (unless the cell has no
    # donors) grows from there. Discharge starts out as the cell's local runoff
    # rate times the cell's surface area.
    drainage_area = numpy.zeros(np, dtype=int) + node_cell_area
    discharge = numpy.zeros(np, dtype=int) + node_cell_area * runoff

    # Optionally zero out drainage area and discharge at boundary nodes
    if boundary_nodes is not None:
        drainage_area[boundary_nodes] = 0
        discharge[boundary_nodes] = 0

    # Call the cfunc to work accumulate from upstream to downstream, permitting
    # transmission losses
    _accumulate_bw(np, s, r, drainage_area, discharge)
    # nodes at channel heads can still be negative with this method, so...
    discharge = discharge.clip(0.0)

    return drainage_area, discharge


def find_drainage_area_and_discharge_lossy(
    s,
    r,
    link_to_receiver,
    loss_function,
    grid,
    node_cell_area=1.0,
    runoff=1.0,
    boundary_nodes=None,
):
    """Calculate the drainage area and water discharge at each node, permitting
    discharge to fall (or gain) as it moves downstream according to some
    function. Note that only transmission creates loss, so water sourced
    locally within a cell is always retained. The loss on each link is recorded
    in the 'surface_water__discharge_loss' link field on the grid; ensure this
    exists before running the function.

    Parameters
    ----------
    s : ndarray of int
        Ordered (downstream to upstream) array of node IDs
    r : ndarray of int
        Receiver node IDs for each node
    link_to_receiver : ndarray of int
        Link to receiver node IDs for each node
    loss_function : Python function(Qw, nodeID, linkID, grid)
        Function dictating how to modify the discharge as it leaves each node.
        nodeID is the current node; linkID is the downstream link, grid is a
        ModelGrid. Returns a float.
    grid : Landlab ModelGrid (or None)
        A grid to enable spatially variable parameters to be used in the loss
        function. If no spatially resolved parameters are needed, this can be
        a dummy variable, e.g., None.
    node_cell_area : float or ndarray
        Cell surface areas for each node. If it's an array, must have same
        length as s (that is, the number of nodes).
    runoff : float or ndarray
        Local runoff rate at each cell (in water depth per time). If it's an
        array, must have same length as s (that is, the number of nodes).
    boundary_nodes: list, optional
        Array of boundary nodes to have discharge and drainage area set to zero.
        Default value is None.

    Returns
    -------
    tuple of ndarray
        drainage area and discharge

    Notes
    -----
    -  If node_cell_area not given, the output drainage area is equivalent
       to the number of nodes/cells draining through each point, including
       the local node itself.
    -  Give node_cell_area as a scalar when using a regular raster grid.
    -  If runoff is not given, the discharge returned will be the same as
       drainage area (i.e., drainage area times unit runoff rate).
    -  If using an unstructured Landlab grid, make sure that the input
       argument for node_cell_area is the cell area at each NODE rather than
       just at each CELL. This means you need to include entries for the
       perimeter nodes too. They can be zeros.
    -  Loss cannot go negative.

    Examples
    --------
    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components.flow_accum import find_drainage_area_and_discharge
    >>> r = np.array([2, 5, 2, 7, 5, 5, 6, 5, 7, 8]) - 1
    >>> s = np.array([4, 1, 0, 2, 5, 6, 3, 8, 7, 9])
    >>> l = np.ones(10, dtype=int)  # dummy
    >>> nodes_wo_outlet = np.array([0, 1, 2, 3, 5, 6, 7, 8, 9])

    >>> def lossfunc(Qw, dummyn, dummyl, dummygrid):
    ...     return 0.5 * Qw
    ...
    >>> mg = RasterModelGrid((3, 4))  # some grid big enough to make go
    >>> _ = mg.add_zeros("node", "surface_water__discharge_loss", dtype=float)
    >>> a, q = find_drainage_area_and_discharge_lossy(s, r, l, lossfunc, mg)
    >>> a
    array([ 1.,  3.,  1.,  1., 10.,  4.,  3.,  2.,  1.,  1.])
    >>> q
    array([1.  , 2.  , 1.  , 1.  , 3.75, 2.  , 2.  , 1.5 , 1.  , 1.  ])
    >>> np.allclose(
    ...     mg.at_node["surface_water__discharge_loss"][nodes_wo_outlet],
    ...     0.5 * q[nodes_wo_outlet],
    ... )
    True
    >>> np.isclose(mg.at_node["surface_water__discharge_loss"][4], 0.0)
    True

    >>> lossfield = mg.add_ones("loss_field", at="node", dtype=float)
    >>> lossfield *= 0.5
    >>> def lossfunc2(Qw, nodeID, dummyl, grid):
    ...     return grid.at_node["loss_field"][nodeID] * Qw
    ...
    >>> a, q = find_drainage_area_and_discharge_lossy(s, r, l, lossfunc2, mg)
    >>> a
    array([ 1.,  3.,  1.,  1., 10.,  4.,  3.,  2.,  1.,  1.])
    >>> q
    array([1.  , 2.  , 1.  , 1.  , 3.75, 2.  , 2.  , 1.5 , 1.  , 1.  ])
    >>> np.allclose(
    ...     mg.at_node["surface_water__discharge_loss"][nodes_wo_outlet],
    ...     lossfield[nodes_wo_outlet] * q[nodes_wo_outlet],
    ... )
    True

    >>> def lossfunc3(Qw, nodeID, dummyl, dummygrid):
    ...     return Qw - 100.0  # a huge loss
    ...
    >>> a, q = find_drainage_area_and_discharge_lossy(s, r, l, lossfunc3, mg)
    >>> a
    array([ 1.,  3.,  1.,  1., 10.,  4.,  3.,  2.,  1.,  1.])
    >>> q
    array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])
    """
    # Number of points
    np = len(s)

    # Initialize the drainage_area and discharge arrays. Drainage area starts
    # out as the area of the cell in question, then (unless the cell has no
    # donors) grows from there. Discharge starts out as the cell's local runoff
    # rate times the cell's surface area.
    drainage_area = numpy.zeros(np, dtype=int) + node_cell_area
    discharge = numpy.zeros(np, dtype=int) + node_cell_area * runoff
    # note no loss occurs at a node until the water actually moves along a link

    # Optionally zero out drainage area and discharge at boundary nodes
    if boundary_nodes is not None:
        drainage_area[boundary_nodes] = 0
        discharge[boundary_nodes] = 0

    # Iterate backward through the list, which means we work from upstream to
    # downstream.
    for i in range(np - 1, -1, -1):
        donor = s[i]
        recvr = r[donor]
        lrec = link_to_receiver[donor]
        if donor != recvr:
            drainage_area[recvr] += drainage_area[donor]
            discharge_remaining = numpy.clip(
                loss_function(discharge[donor], donor, lrec, grid), 0.0, float("inf")
            )
            grid.at_node["surface_water__discharge_loss"][donor] = (
                discharge[donor] - discharge_remaining
            )
            discharge[recvr] += discharge_remaining

    return drainage_area, discharge


def flow_accumulation(
    receiver_nodes, node_cell_area=1.0, runoff_rate=1.0, boundary_nodes=None
):
    """Calculate drainage area and (steady) discharge.

    Calculates and returns the drainage area and (steady) discharge at each
    node, along with a downstream-to-upstream ordered list (array) of node IDs.

    Examples
    --------
    >>> import numpy as np
    >>> from landlab.components.flow_accum import flow_accumulation
    >>> r = np.array([2, 5, 2, 7, 5, 5, 6, 5, 7, 8]) - 1
    >>> a, q, s = flow_accumulation(r)
    >>> a
    array([  1.,   3.,   1.,   1.,  10.,   4.,   3.,   2.,   1.,   1.])
    >>> q
    array([  1.,   3.,   1.,   1.,  10.,   4.,   3.,   2.,   1.,   1.])
    >>> s
    array([4, 1, 0, 2, 5, 6, 3, 8, 7, 9])
    """

    s = as_id_array(make_ordered_node_array(receiver_nodes))
    # Note that this ordering of s DOES INCLUDE closed nodes. It really shouldn't!
    # But as we don't have a copy of the grid accessible here, we'll solve this
    # problem as part of route_flow_dn.

    a, q = find_drainage_area_and_discharge(
        s, receiver_nodes, node_cell_area, runoff_rate, boundary_nodes
    )

    return a, q, s


if __name__ == "__main__":  # pragma: no cover
    import doctest

    doctest.testmod()



================================================
File: flow_accum/flow_accum_to_n.py
================================================
#!/usr/env/python

"""Short description.

flow_accum_to_n.py: Implementation a route-to-multiple drainage stack alorithm.


Algorithm for route to multiple (N) flow accumulation. Inspiration for data
structures and attempting O(n) efficiency taken from Braun and Willet(2013).

Algorithm constructs drainage area and (optionally) water discharge. Can
handle the case in which each node has more than one downstream receiver.

Computationally, for a grid of the same size this algorithm will take about

    1.5 x (avg number of downstream nodes per cell)
        x (duration of flow_accum_bw for same grid using route-to-one method)

So under route-to-one direction schemes, using the Braun and Willet method is
recommended.

If water discharge is calculated, the result assumes steady flow (that is,
hydrologic equilibrium).

The main public function is::

    a, q, s = flow_accumulation_to_n(r, p)

which takes the following inputs:

    r, an (np, q) array of receiver-node IDs, where np is the total number of
    nodes and q is the maximum number of receivers any node in the grid has.
    This array would be returned by the flow_routing component.

    p, an (np, q) array that identifies the proportion of flow going to each
    receiver. For each q elements along the np axis, sum(p(i, :)) must equal
    1. This array would be returned by the flow_routing component.

It returns Numpy arrays with the drainage area (a) and discharge (q) at each
node, along with an array (s) that contains the IDs of the nodes in downstream-
to-upstream order.

If you simply want the ordered list by itself, use::

    s = make_ordered_node_array_to_n(r, p, b)

Created: KRB Oct 2016 (modified from flow_accumu_bw)
"""
import numpy

from landlab.core.utils import as_id_array

from .cfuncs import _accumulate_to_n
from .cfuncs import _make_donors_to_n


class _DrainageStack_to_n:
    """Implementation of the DrainageStack_to_n class.

    The _DrainageStack_to_n() class implements a set based approach to
    constructing a stack with similar properties to the stack constructed by
    Braun & Willet (2013). It constructs an list, s, of all nodes in the grid
    such that a given node is always located earlier in the list than all
    upstream nodes that contribute to it.

    It is used by the make_ordered_node_array_to_n() function.
    """

    def __init__(self, delta, D, num_receivers):
        """Creates the stack array s and stores references to delta and D.

        Initialization of the _DrainageStack_to_n() class including
        storing delta and D.
        """

        self.num_receivers = num_receivers
        self.s = []
        self.delta = delta
        self.D = D

    def construct__stack(self, nodes):
        """Function to construct the drainage stack.

        Function to add all nodes upstream of a set of base level nodes given
        by list *nodes* in an order
        such that downstream nodes always occur before upstream nodes.

        This function contains the major algorithmic difference between the
        route to 1 method of Braun and Willet (2013) and the route to N method
        presented here.

        Rather than recursively moving up the tributary tree this method uses
        sets test that a node is downstream and add it to the stack. Both
        methods are functionally depth first searches. The method that Braun
        and Willet (2013) implement is optimized given that each node only has
        one receiver. This method is optimized to visit more than one vertex/
        node of the graph at a time.

        An important note: Since sets are un-ordered, we cannot expect the
        stack to be exactly the same each time. It will always put nodes that
        are downstream before those that are upstream, but because it will move
        up multiple branches at the same time, it may put three nodes into the
        stack at the same time that are on different branches of the flow
        network. Because these nodes are in different parts of the network,
        the relative order of them does not matter.

        For example, in the example below, the nodes 1 and 7 must be added
        after 5 but before 2 and 6.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab.components.flow_accum.flow_accum_to_n import (
        ...     _DrainageStack_to_n,
        ... )
        >>> delta = np.array([0, 0, 2, 4, 4, 8, 12, 14, 17, 18, 18])
        >>> num_receivers = np.array([2, 2, 2, 2, 1, 1, 2, 2, 2, 2])
        >>> D = np.array([0, 2, 0, 3, 1, 4, 5, 7, 6, 1, 2, 7, 3, 8, 9, 6, 8, 9])
        >>> ds = _DrainageStack_to_n(delta, D, num_receivers)
        >>> ds.construct__stack(4)
        >>> ds.s[0] == 4
        True
        >>> ds.s[1] == 5
        True
        >>> ds.s[9] == 9
        True
        >>> len(set([1, 7]) - set(ds.s[2:4]))
        0
        >>> len(set([2, 6]) - set(ds.s[4:6]))
        0
        >>> len(set([0, 3, 8]) - set(ds.s[6:9]))
        0
        """
        # create base nodes set
        try:
            base = set(nodes)
        except TypeError:
            base = {nodes}

        # instantiate the time keeping variable i, and a variable to keep track
        # of the visit time. Using visit time allows us to itterate through
        # the entire graph and make sure that only put a node in the stack
        # the last time it is visited.

        i = 0
        visit_time = -1 * numpy.ones(self.delta.size - 1)
        num_visits = numpy.zeros(self.delta.size - 1)

        # deal with the first node, which goes to it
        visit_time[list(base)] = i
        num_visits[list(base)] += 1

        i = 1
        visited = set()
        for node_i in base:
            # select the nodes to visit
            visit = set(self.D[self.delta[node_i] : self.delta[node_i + 1]])
            visit = visit - base

            # record the visit time.
            visit_time[list(visit)] = i

            # record that they have been visited.
            num_visits[list(visit)] += 1

            visited.update(list(visit))

        visited = numpy.array(list(visited))
        if visited.size > 0:
            visited_enough = num_visits[visited] == self.num_receivers[visited]
            completed = set(visited[visited_enough])
        else:
            completed = {}
        # recurse through the remainder. Only look above completed nodes,
        # this prevents repeat link walking.
        while len(completed) > 0:
            # increase counter
            i += 1

            visited = set()
            new_completes = set()

            for node_i in completed:
                # select the nodes to visit
                visit = self.D[self.delta[node_i] : self.delta[node_i + 1]]
                # record the visit time.
                visit_time[visit] = i

                # record that they have been visited.
                num_visits[visit] += 1

                # add nodes that have been visited enough times to complete
                # to the upstream stack. We can ignore the rest, they will
                # be re-visited. This should reduce the number of times each
                # link is walked to the number of active links.
                visited_enough = (
                    num_visits[numpy.array(visit)]
                    == self.num_receivers[numpy.array(visit)]
                )

                visited.update(visit)
                new_completes.update(visit[visited_enough])
            completed = new_completes

        # the stack is the argsort of visit time.
        self.s = numpy.argsort(visit_time, kind="stable")


def _make_number_of_donors_array_to_n(r, p):
    """Number of donors for each node.

    Creates and returns an array containing the number of donors for each node.

    Parameters
    ----------
    r : ndarray size (np, q) where r[i,:] gives all receivers of node i. Each
        node recieves flow fom up to q donors.

    p : ndarray size (np, q) where p[i,v] give the proportion of flow going
        from node i to the receiver listed in r[i,v].

    Returns
    -------
    ndarray size (np)
        Number of donors for each node.

    Examples
    --------

    >>> import numpy as np
    >>> from landlab.components.flow_accum.flow_accum_to_n import (
    ...     _make_number_of_donors_array_to_n,
    ... )
    >>> r = np.array(
    ...     [
    ...         [1, 2],
    ...         [4, 5],
    ...         [1, 5],
    ...         [6, 2],
    ...         [4, -1],
    ...         [4, -1],
    ...         [5, 7],
    ...         [4, 5],
    ...         [6, 7],
    ...         [7, 8],
    ...     ]
    ... )
    >>> p = np.array(
    ...     [
    ...         [0.6, 0.4],
    ...         [0.85, 0.15],
    ...         [0.65, 0.35],
    ...         [0.9, 0.1],
    ...         [1.0, 0.0],
    ...         [1.0, 0.0],
    ...         [0.75, 0.25],
    ...         [0.55, 0.45],
    ...         [0.8, 0.2],
    ...         [0.95, 0.05],
    ...     ]
    ... )
    >>> nd = _make_number_of_donors_array_to_n(r, p)
    >>> nd
    array([0, 2, 2, 0, 4, 4, 2, 3, 1, 0])
    """
    nd = numpy.zeros(r.shape[0], dtype=int)

    # filter r based on p and flatten
    r_filter_flat = r.flatten()[p.flatten() > 0]

    max_index = numpy.amax(r_filter_flat)

    nd[: (max_index + 1)] = numpy.bincount(r_filter_flat)
    return nd


def _make_delta_array_to_n(nd):
    r"""
    Function to create the delta array.

    Creates and returns the "delta" array, which is a list containing, for each
    node, the array index where that node's donor list begins.

    Parameters
    ----------
    nd : ndarray of int
        Number of donors for each node

    Returns
    -------
    ndarray of int
        Delta array

    Examples
    --------
    >>> import numpy as np
    >>> from landlab.components.flow_accum.flow_accum_to_n import _make_delta_array_to_n
    >>> nd = np.array([0, 2, 2, 0, 4, 4, 2, 3, 1, 0])
    >>> delta = _make_delta_array_to_n(nd)
    >>> delta
    array([ 0,  0,  2,  4,  4,  8,  12,  14, 17, 18, 18])
    >>> sum(nd) == max(delta)
    True
    """
    nt = sum(nd)
    np = len(nd)
    delta = numpy.zeros(np + 1, dtype=int)
    delta.fill(nt)
    delta[-2::-1] -= numpy.cumsum(nd[::-1])

    return delta


def _make_array_of_donors_to_n(r, p, delta):
    """Creates and returns an array containing the IDs of donors for each node.

    Essentially, the array is a series of lists (not in the Python list object
    sense) of IDs for each node. See Braun & Willett (2012) for details.

    The example below is from Braun & Willett (2012), and produces D_i in their
    Table 1 (except that here the ID numbers are one less, because we number
    indices from zero).

    Vectorized - inefficiently! - DEJH, 5/20/14

    Examples
    --------
    >>> import numpy as np
    >>> from landlab.components.flow_accum.flow_accum_to_n import (
    ...     _make_array_of_donors_to_n,
    ... )
    >>> r = np.array(
    ...     [
    ...         [1, 2],
    ...         [4, 5],
    ...         [1, 5],
    ...         [6, 2],
    ...         [4, -1],
    ...         [4, -1],
    ...         [5, 7],
    ...         [4, 5],
    ...         [6, 7],
    ...         [7, 8],
    ...     ]
    ... )
    >>> p = np.array(
    ...     [
    ...         [0.6, 0.4],
    ...         [0.85, 0.15],
    ...         [0.65, 0.35],
    ...         [0.9, 0.1],
    ...         [1.0, 0.0],
    ...         [1.0, 0.0],
    ...         [0.75, 0.25],
    ...         [0.55, 0.45],
    ...         [0.8, 0.2],
    ...         [0.95, 0.05],
    ...     ]
    ... )
    >>> delta = np.array([0, 0, 2, 4, 4, 8, 12, 14, 17, 18, 18])
    >>> D = _make_array_of_donors_to_n(r, p, delta)
    >>> D
    array([0, 2, 0, 3, 1, 4, 5, 7, 6, 1, 2, 7, 3, 8, 9, 6, 8, 9])
    """
    np = r.shape[0]
    q = r.shape[1]
    nt = delta[-1]

    w = numpy.zeros(np, dtype=int)
    D = numpy.zeros(nt, dtype=int)

    _make_donors_to_n(np, q, w, D, delta, r, p)

    return D


def make_ordered_node_array_to_n(
    receiver_nodes, receiver_proportion, nd=None, delta=None, D=None
):
    """Create an array of node IDs.

    Creates and returns an array of node IDs that is arranged in order from
    downstream to upstream.

    The lack of a leading underscore is meant to signal that this operation
    could be useful outside of this module!

    Examples
    --------
    >>> import numpy as np
    >>> from landlab.components.flow_accum.flow_accum_to_n import (
    ...     make_ordered_node_array_to_n,
    ... )
    >>> r = np.array(
    ...     [
    ...         [1, 2],
    ...         [4, 5],
    ...         [1, 5],
    ...         [6, 2],
    ...         [4, -1],
    ...         [4, -1],
    ...         [5, 7],
    ...         [4, 5],
    ...         [6, 7],
    ...         [7, 8],
    ...     ]
    ... )
    >>> p = np.array(
    ...     [
    ...         [0.6, 0.4],
    ...         [0.85, 0.15],
    ...         [0.65, 0.35],
    ...         [0.9, 0.1],
    ...         [1.0, 0.0],
    ...         [1.0, 0.0],
    ...         [0.75, 0.25],
    ...         [0.55, 0.45],
    ...         [0.8, 0.2],
    ...         [0.95, 0.05],
    ...     ]
    ... )
    >>> s = make_ordered_node_array_to_n(r, p)
    >>> s[0] == 4
    True
    >>> s[1] == 5
    True
    >>> s[9] == 9
    True
    >>> len(set([1, 7]) - set(s[2:4]))
    0
    >>> len(set([2, 6]) - set(s[4:6]))
    0
    >>> len(set([0, 3, 8]) - set(s[6:9]))
    0
    """
    node_id = numpy.arange(receiver_nodes.shape[0])
    baselevel_nodes = numpy.where(node_id == receiver_nodes[:, 0])[0]
    if nd is None:
        nd = _make_number_of_donors_array_to_n(receiver_nodes, receiver_proportion)
    if delta is None:
        delta = _make_delta_array_to_n(nd)
    if D is None:
        D = _make_array_of_donors_to_n(receiver_nodes, receiver_proportion, delta)

    num_receivers = numpy.sum(receiver_nodes >= 0, axis=1)

    dstack = _DrainageStack_to_n(delta, D, num_receivers)
    construct_it = dstack.construct__stack

    construct_it(baselevel_nodes)  # don't think this is a bottleneck, so no C++
    return dstack.s


def find_drainage_area_and_discharge_to_n(
    s, r, p, node_cell_area=1.0, runoff=1.0, boundary_nodes=None
):
    """Calculate the drainage area and water discharge at each node.

    Parameters
    ----------
    s : ndarray of int
        Ordered (downstream to upstream) array of node IDs
    r : ndarray size (np, q) where r[i, :] gives all receivers of node i. Each
        node recieves flow fom up to q donors.
    p : ndarray size (np, q) where p[i, v] give the proportion of flow going
        from node i to the receiver listed in r[i, v].
    node_cell_area : float or ndarray
        Cell surface areas for each node. If it's an array, must have same
        length as s (that is, the number of nodes).
    runoff : float or ndarray
        Local runoff rate at each cell (in water depth per time). If it's an
        array, must have same length as s (that is, the number of nodes).
        runoff *is* permitted to be negative, in which case it performs as a
        transmission loss.
    boundary_nodes: list, optional
        Array of boundary nodes to have discharge and drainage area set to
        zero. Default value is None.

    Returns
    -------
    tuple of ndarray
        drainage area and discharge

    Notes
    -----
    -  If node_cell_area not given, the output drainage area is equivalent
       to the number of nodes/cells draining through each point, including
       the local node itself.
    -  Give node_cell_area as a scalar when using a regular raster grid.
    -  If runoff is not given, the discharge returned will be the same as
       drainage area (i.e., drainage area times unit runoff rate).
    -  If using an unstructured Landlab grid, make sure that the input
       argument for node_cell_area is the cell area at each NODE rather than
       just at each CELL. This means you need to include entries for the
       perimeter nodes too. They can be zeros.

    Examples
    --------
    >>> import numpy as np
    >>> from landlab.components.flow_accum.flow_accum_to_n import (
    ...     find_drainage_area_and_discharge_to_n,
    ... )
    >>> r = np.array(
    ...     [
    ...         [1, 2],
    ...         [4, 5],
    ...         [1, 5],
    ...         [6, 2],
    ...         [4, -1],
    ...         [4, -1],
    ...         [5, 7],
    ...         [4, 5],
    ...         [6, 7],
    ...         [7, 8],
    ...     ]
    ... )
    >>> p = np.array(
    ...     [
    ...         [0.6, 0.4],
    ...         [0.85, 0.15],
    ...         [0.65, 0.35],
    ...         [0.9, 0.1],
    ...         [1.0, 0.0],
    ...         [1.0, 0.0],
    ...         [0.75, 0.25],
    ...         [0.55, 0.45],
    ...         [0.8, 0.2],
    ...         [0.95, 0.05],
    ...     ]
    ... )
    >>> s = np.array([4, 5, 1, 7, 2, 6, 0, 8, 3, 9])
    >>> a, q = find_drainage_area_and_discharge_to_n(s, r, p)
    >>> a.round(4)
    array([  1.    ,   2.575 ,   1.5   ,   1.    ,  10.    ,   5.2465,
             2.74  ,   2.845 ,   1.05  ,   1.    ])
    >>> q.round(4)
    array([  1.    ,   2.575 ,   1.5   ,   1.    ,  10.    ,   5.2465,
             2.74  ,   2.845 ,   1.05  ,   1.    ])
    """
    # Number of points
    np = r.shape[0]
    q = r.shape[1]

    # Initialize the drainage_area and discharge arrays. Drainage area starts
    # out as the area of the cell in question, then (unless the cell has no
    # donors) grows from there. Discharge starts out as the cell's local runoff
    # rate times the cell's surface area.
    drainage_area = numpy.zeros(np) + node_cell_area
    discharge = numpy.zeros(np) + node_cell_area * runoff

    # Optionally zero out drainage area and discharge at boundary nodes
    if boundary_nodes is not None:
        drainage_area[boundary_nodes] = 0
        discharge[boundary_nodes] = 0

    # Call the cfunc to work accumulate from upstream to downstream, permitting
    # transmission losses
    _accumulate_to_n(np, q, s, r, p, drainage_area, discharge)
    # nodes at channel heads can still be negative with this method, so...
    discharge = discharge.clip(0.0)

    return drainage_area, discharge


def find_drainage_area_and_discharge_to_n_lossy(
    s,
    r,
    link_to_receiver,
    p,
    loss_function,
    grid,
    node_cell_area=1.0,
    runoff=1.0,
    boundary_nodes=None,
):
    """Calculate the drainage area and water discharge at each node, permitting
    discharge to fall (or gain) as it moves downstream according to some
    function. Note that only transmission creates loss, so water sourced
    locally within a cell is always retained. The loss on each link is recorded
    in the 'surface_water__discharge_loss' link field on the grid; ensure this
    exists before running the function.

    Parameters
    ----------
    s : ndarray of int
        Ordered (downstream to upstream) array of node IDs
    r : ndarray size (np, q) where r[i, :] gives all receivers of node i. Each
        node receives flow fom up to q donors.
    link_to_receiver : ndarray size (np, q) where l[i, :] gives all links to receivers of
        node i.
    p : ndarray size (np, q) where p[i, v] give the proportion of flow going
        from node i to the receiver listed in r[i, v].
    loss_function : Python function(Qw, nodeID, linkID)
        Function dictating how to modify the discharge as it leaves each node.
        nodeID is the current node; linkID is the downstream link. Returns a
        float.
    grid : Landlab ModelGrid (or None)
        A grid to enable spatially variable parameters to be used in the loss
        function. If no spatially resolved parameters are needed, this can be
        a dummy variable, e.g., None.
    node_cell_area : float or ndarray
        Cell surface areas for each node. If it's an array, must have same
        length as s (that is, the number of nodes).
    runoff : float or ndarray
        Local runoff rate at each cell (in water depth per time). If it's an
        array, must have same length as s (that is, the number of nodes).
    boundary_nodes: list, optional
        Array of boundary nodes to have discharge and drainage area set to
        zero. Default value is None.

    Returns
    -------
    tuple of ndarray
        drainage area and discharge

    Notes
    -----
    -  If node_cell_area not given, the output drainage area is equivalent
       to the number of nodes/cells draining through each point, including
       the local node itself.
    -  Give node_cell_area as a scalar when using a regular raster grid.
    -  If runoff is not given, the discharge returned will be the same as
       drainage area (i.e., drainage area times unit runoff rate).
    -  If using an unstructured Landlab grid, make sure that the input
       argument for node_cell_area is the cell area at each NODE rather than
       just at each CELL. This means you need to include entries for the
       perimeter nodes too. They can be zeros.
    -  Loss cannot go negative.

    Examples
    --------
    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components.flow_accum.flow_accum_to_n import (
    ...     find_drainage_area_and_discharge_to_n_lossy,
    ... )
    >>> r = np.array([[1, 2], [3, -1], [3, 1], [3, -1]])
    >>> p = np.array([[0.5, 0.5], [1.0, 0.0], [0.2, 0.8], [1.0, 0.0]])
    >>> s = np.array([3, 1, 2, 0])
    >>> l = np.ones_like(r, dtype=int)  # dummy

    Make here a grid that contains (too many!) links holding values for loss.
    We're only going to use the first 4 links, but illustrates the use of the
    grid for link input.

    >>> mg = RasterModelGrid((3, 3))
    >>> _ = mg.add_zeros("node", "surface_water__discharge_loss", dtype=float)
    >>> lossy = mg.add_ones("lossy", at="link", dtype=float)
    >>> lossy *= 0.5
    >>> def lossfunc(Qw, dummyn, linkID, grid):
    ...     return grid.at_link["lossy"][linkID] * Qw
    ...

    >>> a, q = find_drainage_area_and_discharge_to_n_lossy(s, r, l, p, lossfunc, mg)
    >>> a
    array([1. , 2.7, 1.5, 4. ])
    >>> q
    array([1.  , 1.75, 1.25, 2.  ])
    >>> np.allclose(mg.at_node["surface_water__discharge_loss"][:3], 0.5 * q[:3])
    True

    Note by definition no loss is occuring at the outlet node, as there are no
    nodes downstream.

    Final example of total transmission loss:

    >>> def lossfunc(Qw, dummyn, dummyl, dummygrid):
    ...     return Qw - 100.0  # huge loss
    ...
    >>> a, q = find_drainage_area_and_discharge_to_n_lossy(s, r, l, p, lossfunc, mg)
    >>> a
    array([1. , 2.7, 1.5, 4. ])
    >>> q
    array([1., 1., 1., 1.])
    """
    # Number of points
    np = r.shape[0]
    q = r.shape[1]

    # Initialize the drainage_area and discharge arrays. Drainage area starts
    # out as the area of the cell in question, then (unless the cell has no
    # donors) grows from there. Discharge starts out as the cell's local runoff
    # rate times the cell's surface area.
    drainage_area = numpy.zeros(np) + node_cell_area
    discharge = numpy.zeros(np) + node_cell_area * runoff

    # grab the field to ouput loss to

    # Optionally zero out drainage area and discharge at boundary nodes
    if boundary_nodes is not None:
        drainage_area[boundary_nodes] = 0
        discharge[boundary_nodes] = 0

    # Iterate backward through the list, which means we work from upstream to
    # downstream.
    for i in range(np - 1, -1, -1):
        donor = s[i]
        for v in range(q):
            recvr = r[donor, v]
            lrec = link_to_receiver[donor, v]
            proportion = p[donor, v]
            if proportion > 0 and donor != recvr:
                drainage_area[recvr] += proportion * drainage_area[donor]
                discharge_head = proportion * discharge[donor]
                discharge_remaining = numpy.clip(
                    loss_function(discharge_head, donor, lrec, grid),
                    0.0,
                    float("inf"),
                )
                grid.at_node["surface_water__discharge_loss"][donor] += (
                    discharge_head - discharge_remaining
                )
                discharge[recvr] += discharge_remaining

    return drainage_area, discharge


def flow_accumulation_to_n(
    receiver_nodes,
    receiver_proportions,
    node_cell_area=1.0,
    runoff_rate=1.0,
    boundary_nodes=None,
):
    """Calculate drainage area and (steady) discharge.

    Calculates and returns the drainage area and (steady) discharge at each
    node, along with a downstream-to-upstream ordered list (array) of node IDs.

    Examples
    --------
    >>> import numpy as np
    >>> from landlab.components.flow_accum.flow_accum_to_n import flow_accumulation_to_n
    >>> r = np.array(
    ...     [
    ...         [1, 2],
    ...         [4, 5],
    ...         [1, 5],
    ...         [6, 2],
    ...         [4, -1],
    ...         [4, -1],
    ...         [5, 7],
    ...         [4, 5],
    ...         [6, 7],
    ...         [7, 8],
    ...     ]
    ... )
    >>> p = np.array(
    ...     [
    ...         [0.6, 0.4],
    ...         [0.85, 0.15],
    ...         [0.65, 0.35],
    ...         [0.9, 0.1],
    ...         [1.0, 0.0],
    ...         [1.0, 0.0],
    ...         [0.75, 0.25],
    ...         [0.55, 0.45],
    ...         [0.8, 0.2],
    ...         [0.95, 0.05],
    ...     ]
    ... )
    >>> a, q, s = flow_accumulation_to_n(r, p)
    >>> a.round(4)
    array([  1.    ,   2.575 ,   1.5   ,   1.    ,  10.    ,   5.2465,
             2.74  ,   2.845 ,   1.05  ,   1.    ])
    >>> q.round(4)
    array([  1.    ,   2.575 ,   1.5   ,   1.    ,  10.    ,   5.2465,
             2.74  ,   2.845 ,   1.05  ,   1.    ])
    >>> s[0] == 4
    True
    >>> s[1] == 5
    True
    >>> s[9] == 9
    True
    >>> len(set([1, 7]) - set(s[2:4]))
    0
    >>> len(set([2, 6]) - set(s[4:6]))
    0
    >>> len(set([0, 3, 8]) - set(s[6:9]))
    0
    """

    assert (
        receiver_nodes.shape == receiver_proportions.shape
    ), "r and p arrays are not the same shape"

    s = as_id_array(make_ordered_node_array_to_n(receiver_nodes, receiver_proportions))
    # Note that this ordering of s DOES INCLUDE closed nodes. It really
    # shouldn't!
    # But as we don't have a copy of the grid accessible here, we'll solve this
    # problem as part of route_flow_dn.

    a, q = find_drainage_area_and_discharge_to_n(
        s,
        receiver_nodes,
        receiver_proportions,
        node_cell_area,
        runoff_rate,
        boundary_nodes,
    )

    return a, q, s


if __name__ == "__main__":  # pragma: no cover
    import doctest

    doctest.testmod()



================================================
File: flow_accum/flow_accumulator.py
================================================
#!/usr/env/python

"""
flow_accumulator.py: Component to accumulate flow and calculate drainage area.

Provides the FlowAccumulator component which accumulates flow and calculates
drainage area. FlowAccumulator supports multiple methods for calculating flow
direction. Optionally a depression finding component can be specified and flow
directing, depression finding, and flow routing can all be accomplished
together.
"""


import numpy as np

from landlab import Component  # for type tests
from landlab import FieldError
from landlab import NetworkModelGrid
from landlab import RasterModelGrid
from landlab import VoronoiDelaunayGrid
from landlab.components.flow_accum import flow_accum_bw
from landlab.components.flow_accum import flow_accum_to_n
from landlab.core.messages import warning_message
from landlab.core.utils import as_id_array
from landlab.utils.return_array import return_array_at_node

from ..depression_finder.floodstatus import FloodStatus

_UNFLOODED = FloodStatus._UNFLOODED


class FlowAccumulator(Component):
    """Component to accumulate flow and calculate drainage area.

    This is accomplished by first finding flow directions by a user-specified
    method and then calculating the drainage area and discharge.

    Optionally, spatially variable runoff can be set either by the model grid
    field 'water__unit_flux_in' or the input variable *runoff_rate**.

    Optionally a depression finding component can be specified and flow
    directing, depression finding, and flow routing can all be accomplished
    together.

    NOTE: The perimeter nodes  NEVER contribute to the accumulating flux, even
    if the  gradients from them point inwards to the main body of the grid.
    This is because under Landlab definitions, perimeter nodes lack cells, so
    cannot accumulate any discharge.

    FlowAccumulator stores as ModelGrid fields:

    -  Node array of drainage areas: *'drainage_area'*
    -  Node array of discharges: *'surface_water__discharge'*
    -  Node array containing downstream-to-upstream ordered list of node
        IDs: *'flow__upstream_node_order'*
    -  Node array of all but the first element of the delta data structure:
        *flow__data_structure_delta*. The first element is always zero.

    The FlowDirector component will add additional ModelGrid fields.
    DirectToOne methods(Steepest/D4 and D8) and DirectToMany(DINF and MFD) use
    the same model grid field names. Some of these fields will be different
    shapes if a DirectToOne or a DirectToMany method is used.

    The FlowDirectors store the following as ModelGrid fields:

    -  Node array of receivers (nodes that receive flow), or ITS OWN ID if
       there is no receiver: *'flow__receiver_node'*. This array is 2D for
       RouteToMany methods and has the shape
       (n-nodes x max number of receivers).
    -  Node array of flow proportions: *'flow__receiver_proportions'*. This
       array is 2D, for RouteToMany methods and has the shape
       (n-nodes x max number of receivers).
    -  Node array of links carrying flow:  *'flow__link_to_receiver_node'*.
       This array is 2D for RouteToMany methods and has the shape
       (n-nodes x max number of receivers).
    -  Node array of downhill slopes from each receiver:
       *'topographic__steepest_slope'* This array is 2D for RouteToMany
       methods and has the shape (n-nodes x max number of receivers).
    -  Boolean node array of all local lows: *'flow__sink_flag'*
    -  Link array identifing if flow goes with (1) or against (-1) the link
       direction: *'flow__link_direction'*

    The primary method of this class is :func:`run_one_step`.

    `run_one_step` takes the optional argument update_flow_director (default is
    True) that determines if the flow_director is re-run before flow is
    accumulated.

    Parameters
    ----------
    grid : ModelGrid
        A Landlab grid.
    surface : field name at node or array of length node
        The surface to direct flow across.
    flow_director : string, class, instance of class.
        A string of method or class name (e.g. 'D8' or 'FlowDirectorD8'), an
        uninstantiated FlowDirector class, or an instance of a FlowDirector
        class. This sets the method used to calculate flow directions.
        Default is 'FlowDirectorSteepest'
    runoff_rate : field name, array, or float, optional (m/time)
        If provided, sets the runoff rate and will be assigned to the grid field
        'water__unit_flux_in'. If a spatially and and temporally variable runoff
        rate is desired, pass this field name and update the field through model
        run time. If both the field and argument are present at the time of
        initialization, runoff_rate will *overwrite* the field. If neither are
        set, defaults to spatially constant unit input.
        Both a runoff_rate array and the 'water__unit_flux_in' field are
        permitted to contain negative values, in which case they mimic
        transmission losses rather than e.g. rain inputs.
    depression_finder : string, class, instance of class, optional
         A string of class name (e.g., 'DepressionFinderAndRouter'), an
         uninstantiated DepressionFinder class, or an instance of a
         DepressionFinder class.
         This sets the method for depression finding.
    **kwargs : any additional parameters to pass to a FlowDirector or
         DepressionFinderAndRouter instance (e.g., partion_method for
         FlowDirectorMFD). This will have no effect if an instantiated component
         is passed using the flow_director or depression_finder keywords.

    Examples
    --------
    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import FlowAccumulator
    >>> mg = RasterModelGrid((3, 3))
    >>> mg.set_closed_boundaries_at_grid_edges(True, True, True, False)
    >>> _ = mg.add_field(
    ...     "topographic__elevation",
    ...     mg.node_x + mg.node_y,
    ...     at="node",
    ... )

    The FlowAccumulator component accumulates flow and calculates drainage using
    all of the different methods for directing flow in Landlab. These include
    steepest descent (also known as D4 for the case of a raster grid) and D8 (on
    raster grids only). The method for flow director can be specified as a
    string (e.g., 'D8' or 'FlowDirectorD8'), as an uninstantiated FlowDirector
    component or as an instantiated FlowDirector component.

    The default method is to use FlowDirectorSteepest.

    First let's look at the three ways to instantiate a FlowAccumulator. The
    following four methods are all equivalent. First, we can pass the entire
    name of a flow director as a string to the argument `flow_director`:

    >>> fa = FlowAccumulator(
    ...     mg, "topographic__elevation", flow_director="FlowDirectorSteepest"
    ... )

    Second, we can pass just the method name as a string to the argument
    `flow_director`:

    >>> fa = FlowAccumulator(mg, "topographic__elevation", flow_director="Steepest")

    Third, we can import a FlowDirector component from Landlab and pass it to
    `flow_director`:

    >>> from landlab.components import FlowDirectorSteepest
    >>> fa = FlowAccumulator(
    ...     mg, "topographic__elevation", flow_director=FlowDirectorSteepest
    ... )

    Finally, we can instantiate a FlowDirector component and pass this
    instantiated version to `flow_director`. You might want to do this if you
    used a FlowDirector in order to set up something before starting a
    time loop and then want to use the same flow director within the loop.

    >>> fd = FlowDirectorSteepest(mg, "topographic__elevation")
    >>> fa = FlowAccumulator(
    ...     mg, "topographic__elevation", flow_director=FlowDirectorSteepest
    ... )

    Now let's look at what FlowAccumulator does. Even before we run
    FlowAccumulator it has the property `surface_values` that stores the values
    of the surface over which flow is directed and accumulated.

    >>> fa.surface_values
    array([0., 1., 2., 1., 2., 3., 2., 3., 4.])

    Now let's make a more complicated elevation grid for the next examples.

    >>> mg = RasterModelGrid((5, 4))
    >>> topographic__elevation = [
    ...     [0.0, 0.0, 0.0, 0.0],
    ...     [0.0, 21.0, 10.0, 0.0],
    ...     [0.0, 31.0, 20.0, 0.0],
    ...     [0.0, 32.0, 30.0, 0.0],
    ...     [0.0, 0.0, 0.0, 0.0],
    ... ]
    >>> _ = mg.add_field("topographic__elevation", topographic__elevation, at="node")
    >>> mg.set_closed_boundaries_at_grid_edges(True, True, True, False)
    >>> fa = FlowAccumulator(
    ...     mg, "topographic__elevation", flow_director=FlowDirectorSteepest
    ... )
    >>> fa.run_one_step()
    >>> mg.at_node["flow__receiver_node"].reshape(mg.shape)
    array([[ 0,  1,  2,  3],
           [ 4,  1,  2,  7],
           [ 8, 10,  6, 11],
           [12, 14, 10, 15],
           [16, 17, 18, 19]])
    >>> mg.at_node["drainage_area"].reshape(mg.shape)
    array([[0., 1., 5., 0.],
           [0., 1., 5., 0.],
           [0., 1., 4., 0.],
           [0., 1., 2., 0.],
           [0., 0., 0., 0.]])

    Now let's change the cell area (100.) and the runoff rates:

    >>> mg = RasterModelGrid((5, 4), xy_spacing=(10.0, 10))

    Put the data back into the new grid.

    >>> _ = mg.add_field("topographic__elevation", topographic__elevation, at="node")
    >>> mg.set_closed_boundaries_at_grid_edges(True, True, True, False)
    >>> fa = FlowAccumulator(
    ...     mg, "topographic__elevation", flow_director=FlowDirectorSteepest
    ... )
    >>> runoff_rate = np.arange(mg.number_of_nodes, dtype=float)
    >>> rnff = mg.add_field("water__unit_flux_in", runoff_rate, at="node", clobber=True)
    >>> fa.run_one_step()
    >>> mg.at_node["surface_water__discharge"].reshape(mg.shape)
    array([[   0.,  500.,  5200.,    0.],
           [   0.,  500.,  5200.,    0.],
           [   0.,  900.,  4600.,    0.],
           [   0., 1300.,  2700.,    0.],
           [   0.,    0.,     0.,    0.]])

    The flow accumulator will happily work with a negative runoff rate, which
    could be used to allow, e.g., transmission losses:

    >>> runoff_rate.fill(1.0)
    >>> fa.run_one_step()
    >>> mg.at_node["surface_water__discharge"].reshape(mg.shape)
    array([[  0., 100., 500.,   0.],
           [  0., 100., 500.,   0.],
           [  0., 100., 400.,   0.],
           [  0., 100., 200.,   0.],
           [  0.,   0.,   0.,   0.]])
    >>> runoff_rate[:8] = -0.5
    >>> fa.run_one_step()
    >>> mg.at_node["surface_water__discharge"].reshape(mg.shape)
    array([[  0.,   0., 350.,   0.],
           [  0.,   0., 350.,   0.],
           [  0., 100., 400.,   0.],
           [  0., 100., 200.,   0.],
           [  0.,   0.,   0.,   0.]])

    The drainage area array is unaffected, as you would expect:

    >>> mg.at_node["drainage_area"].reshape(mg.shape)
    array([[  0., 100., 500.,   0.],
           [  0., 100., 500.,   0.],
           [  0., 100., 400.,   0.],
           [  0., 100., 200.,   0.],
           [  0.,   0.,   0.,   0.]])

    The FlowAccumulator component will work for both raster grids and irregular
    grids. For the example we will use a Hexagonal Model Grid, a special type
    of Voroni Grid that has regularly spaced hexagonal cells.

    >>> from landlab import HexModelGrid
    >>> hmg = HexModelGrid((5, 3), xy_of_lower_left=(-1.0, 0.0))
    >>> _ = hmg.add_field(
    ...     "topographic__elevation",
    ...     hmg.node_x + np.round(hmg.node_y),
    ...     at="node",
    ... )
    >>> fa = FlowAccumulator(
    ...     hmg, "topographic__elevation", flow_director=FlowDirectorSteepest
    ... )
    >>> fa.surface_values
    array([0. , 1. , 2. ,
           0.5, 1.5, 2.5, 3.5,
           1. , 2. , 3. , 4. , 5. ,
           2.5, 3.5, 4.5, 5.5,
           3. , 4. , 5. ])

    If the FlowDirector you want to use takes keyword arguments and you want
    to specify it using a string or uninstantiated FlowDirector class, include
    those keyword arguments when you create FlowAccumulator.

    For example, in the case of a raster grid, FlowDirectorMFD can use only
    orthogonal links, or it can use both orthogonal and diagonal links.

    >>> mg = RasterModelGrid((5, 5))
    >>> topographic__elevation = mg.node_y + mg.node_x
    >>> _ = mg.add_field("topographic__elevation", topographic__elevation, at="node")
    >>> fa = FlowAccumulator(
    ...     mg, "topographic__elevation", flow_director="MFD", diagonals=True
    ... )
    >>> fa.run_one_step()
    >>> mg.at_node["flow__receiver_node"]
    array([[ 0, -1, -1, -1, -1, -1, -1, -1],
           [ 1, -1, -1, -1, -1, -1, -1, -1],
           [ 2, -1, -1, -1, -1, -1, -1, -1],
           [ 3, -1, -1, -1, -1, -1, -1, -1],
           [ 4, -1, -1, -1, -1, -1, -1, -1],
           [ 5, -1, -1, -1, -1, -1, -1, -1],
           [-1, -1,  5,  1, -1, -1,  0, -1],
           [-1, -1,  6,  2, -1, -1,  1, -1],
           [-1, -1,  7,  3, -1, -1,  2, -1],
           [ 9, -1, -1, -1, -1, -1, -1, -1],
           [10, -1, -1, -1, -1, -1, -1, -1],
           [-1, -1, 10,  6, -1, -1,  5, -1],
           [-1, -1, 11,  7, -1, -1,  6, -1],
           [-1, -1, 12,  8, -1, -1,  7, -1],
           [14, -1, -1, -1, -1, -1, -1, -1],
           [15, -1, -1, -1, -1, -1, -1, -1],
           [-1, -1, 15, 11, -1, -1, 10, -1],
           [-1, -1, 16, 12, -1, -1, 11, -1],
           [-1, -1, 17, 13, -1, -1, 12, -1],
           [19, -1, -1, -1, -1, -1, -1, -1],
           [20, -1, -1, -1, -1, -1, -1, -1],
           [21, -1, -1, -1, -1, -1, -1, -1],
           [22, -1, -1, -1, -1, -1, -1, -1],
           [23, -1, -1, -1, -1, -1, -1, -1],
           [24, -1, -1, -1, -1, -1, -1, -1]])
    >>> mg.at_node["drainage_area"].round(4).reshape(mg.shape)
    array([[1.4117, 2.065 , 1.3254, 0.4038, 0.    ],
           [2.065 , 3.4081, 2.5754, 1.3787, 0.    ],
           [1.3254, 2.5754, 2.1716, 1.2929, 0.    ],
           [0.4038, 1.3787, 1.2929, 1.    , 0.    ],
           [0.    , 0.    , 0.    , 0.    , 0.    ]])

    It may seem odd that there are no round numbers in the drainage area field.
    This is because flow is directed to all downhill boundary nodes and
    partitioned based on slope.

    To check that flow is conserved, sum along all boundary nodes.

    >>> round(sum(mg.at_node["drainage_area"][mg.boundary_nodes]), 4)
    9.0

    This should be the same as the number of core nodes --- as boundary nodes
    in landlab do not have area.

    >>> len(mg.core_nodes)
    9

    Next, let's set the dx spacing such that each cell has an area of one.

    >>> dx = (2.0 / (3.0**0.5)) ** 0.5
    >>> hmg = HexModelGrid((5, 3), spacing=dx, xy_of_lower_left=(-1.0745, 0.0))
    >>> _ = hmg.add_field(
    ...     "topographic__elevation",
    ...     hmg.node_x**2 + np.round(hmg.node_y) ** 2,
    ...     at="node",
    ... )
    >>> fa = FlowAccumulator(
    ...     hmg, "topographic__elevation", flow_director=FlowDirectorSteepest
    ... )
    >>> fa.run_one_step()
    >>> hmg.at_node["flow__receiver_node"]
    array([ 0,  1,  2,
            3,  0,  1,  6,
            7,  3,  4,  5, 11,
           12,  8,  9, 15,
           16, 17, 18])
    >>> np.round(hmg.at_node["drainage_area"])
    array([3., 2., 0.,
           2., 3., 2., 0.,
           0., 2., 2., 1., 0.,
           0., 1., 1., 0.,
           0., 0., 0.])

    Now let's change the cell area (100.) and the runoff rates:

    >>> hmg = HexModelGrid((5, 3), spacing=dx * 10.0, xy_of_lower_left=(-10.745, 0.0))

    Put the data back into the new grid.

    >>> _ = hmg.add_field(
    ...     "topographic__elevation",
    ...     hmg.node_x**2 + np.round(hmg.node_y) ** 2,
    ...     at="node",
    ... )
    >>> fa = FlowAccumulator(
    ...     hmg, "topographic__elevation", flow_director=FlowDirectorSteepest
    ... )
    >>> fa.run_one_step()
    >>> np.round(hmg.at_node["surface_water__discharge"])
    array([500.,   0.,   0.,
           200., 500., 200.,   0.,
             0., 200., 200., 100.,   0.,
             0., 100., 100.,   0.,
             0.,   0.,   0.])

    Next, let's see what happens to a raster grid when there is a depression.

    >>> mg = RasterModelGrid((7, 7), xy_spacing=0.5)
    >>> z = mg.add_field("topographic__elevation", mg.node_x.copy(), at="node")
    >>> z += 0.01 * mg.node_y
    >>> mg.at_node["topographic__elevation"].reshape(mg.shape)[2:5, 2:5] *= 0.1
    >>> mg.set_closed_boundaries_at_grid_edges(True, True, False, True)

    This model grid has a depression in the center.

    >>> mg.at_node["topographic__elevation"].reshape(mg.shape)
    array([[0.    , 0.5   , 1.    , 1.5   , 2.    , 2.5   , 3.    ],
           [0.005 , 0.505 , 1.005 , 1.505 , 2.005 , 2.505 , 3.005 ],
           [0.01  , 0.51  , 0.101 , 0.151 , 0.201 , 2.51  , 3.01  ],
           [0.015 , 0.515 , 0.1015, 0.1515, 0.2015, 2.515 , 3.015 ],
           [0.02  , 0.52  , 0.102 , 0.152 , 0.202 , 2.52  , 3.02  ],
           [0.025 , 0.525 , 1.025 , 1.525 , 2.025 , 2.525 , 3.025 ],
           [0.03  , 0.53  , 1.03  , 1.53  , 2.03  , 2.53  , 3.03  ]])
    >>> fa = FlowAccumulator(
    ...     mg, "topographic__elevation", flow_director=FlowDirectorSteepest
    ... )
    >>> fa.run_one_step()  # the flow "gets stuck" in the hole
    >>> mg.at_node["flow__receiver_node"].reshape(mg.shape)
    array([[ 0,  1,  2,  3,  4,  5,  6],
           [ 7,  7, 16, 17, 18, 11, 13],
           [14, 14, 16, 16, 17, 18, 20],
           [21, 21, 16, 23, 24, 25, 27],
           [28, 28, 23, 30, 31, 32, 34],
           [35, 35, 30, 31, 32, 39, 41],
           [42, 43, 44, 45, 46, 47, 48]])
    >>> mg.at_node["drainage_area"].reshape(mg.shape)
    array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],
           [0.25, 0.25, 0.25, 0.25, 0.5 , 0.25, 0.  ],
           [0.25, 0.25, 5.  , 1.5 , 1.  , 0.25, 0.  ],
           [0.25, 0.25, 3.  , 0.75, 0.5 , 0.25, 0.  ],
           [0.25, 0.25, 2.  , 1.5 , 1.  , 0.25, 0.  ],
           [0.25, 0.25, 0.25, 0.25, 0.5 , 0.25, 0.  ],
           [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]])

    Because of the depression, the flow 'got stuck' in the hole in the center
    of the grid. We can fix this by using a depression finder, such as
    DepressionFinderAndRouter.

    >>> from landlab.components import DepressionFinderAndRouter

    We can either run the depression finder separately from the flow
    accumulator or we can specify the depression finder and router when we
    instantiate the accumulator and it will run automatically. Similar to
    specifying the FlowDirector we can provide a depression finder in multiple
    three ways.

    First let's try running them separately.

    >>> df_4 = DepressionFinderAndRouter(mg)
    >>> df_4.map_depressions()
    >>> mg.at_node["flow__receiver_node"].reshape(mg.shape)
    array([[ 0,  1,  2,  3,  4,  5,  6],
           [ 7,  7, 16, 17, 18, 11, 13],
           [14, 14,  8, 16, 17, 18, 20],
           [21, 21, 16, 16, 24, 25, 27],
           [28, 28, 23, 24, 24, 32, 34],
           [35, 35, 30, 31, 32, 39, 41],
           [42, 43, 44, 45, 46, 47, 48]])
    >>> mg.at_node["drainage_area"].reshape(mg.shape)
    array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],
           [5.25, 5.25, 0.25, 0.25, 0.5 , 0.25, 0.  ],
           [0.25, 0.25, 5.  , 1.5 , 1.  , 0.25, 0.  ],
           [0.25, 0.25, 0.75, 2.25, 0.5 , 0.25, 0.  ],
           [0.25, 0.25, 0.5 , 0.5 , 1.  , 0.25, 0.  ],
           [0.25, 0.25, 0.25, 0.25, 0.5 , 0.25, 0.  ],
           [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]])

    Now the flow is routed correctly. The depression finder has properties that
    including whether there is a lake at the node, which lake is at each node,
    the outlet node of each lake, and the area of each lake.

    >>> df_4.lake_at_node.reshape(mg.shape)
    array([[False, False, False, False, False, False, False],
           [False, False, False, False, False, False, False],
           [False, False,  True,  True,  True, False, False],
           [False, False,  True,  True,  True, False, False],
           [False, False,  True,  True,  True, False, False],
           [False, False, False, False, False, False, False],
           [False, False, False, False, False, False, False]])
    >>> df_4.lake_map.reshape(mg.shape)
    array([[-1, -1, -1, -1, -1, -1, -1],
           [-1, -1, -1, -1, -1, -1, -1],
           [-1, -1, 16, 16, 16, -1, -1],
           [-1, -1, 16, 16, 16, -1, -1],
           [-1, -1, 16, 16, 16, -1, -1],
           [-1, -1, -1, -1, -1, -1, -1],
           [-1, -1, -1, -1, -1, -1, -1]])
    >>> df_4.lake_codes  # a unique code for each lake present on the grid
    array([16])
    >>> df_4.lake_outlets  # the outlet node of each lake in lake_codes
    array([8])
    >>> df_4.lake_areas  # the area of each lake in lake_codes
    array([2.25])

    Alternatively, we can initialize a flow accumulator with a depression
    finder specified. Calling run_one_step() will run both the accumulator
    and the depression finder with one call. For this example, we will pass the
    class DepressionFinderAndRouter to the parameter `depression_finder`.

    >>> mg = RasterModelGrid((7, 7), xy_spacing=0.5)
    >>> z = mg.add_field("topographic__elevation", mg.node_x.copy(), at="node")
    >>> z += 0.01 * mg.node_y
    >>> mg.at_node["topographic__elevation"].reshape(mg.shape)[2:5, 2:5] *= 0.1
    >>> fa = FlowAccumulator(
    ...     mg,
    ...     "topographic__elevation",
    ...     flow_director="FlowDirectorD8",
    ...     depression_finder=DepressionFinderAndRouter,
    ... )
    >>> fa.run_one_step()

    This has the same effect of first calling the accumulator and then calling
    the depression finder.

    >>> mg.at_node["flow__receiver_node"].reshape(mg.shape)
    array([[ 0,  1,  2,  3,  4,  5,  6],
           [ 7,  7, 16, 17, 18, 18, 13],
           [14, 14,  8, 16, 17, 18, 20],
           [21, 21, 16, 16, 24, 25, 27],
           [28, 28, 23, 24, 24, 32, 34],
           [35, 35, 30, 31, 32, 32, 41],
           [42, 43, 44, 45, 46, 47, 48]])
    >>> mg.at_node["drainage_area"].reshape(mg.shape)
    array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],
           [5.25, 5.25, 0.25, 0.25, 0.25, 0.25, 0.  ],
           [0.25, 0.25, 5.  , 1.5 , 1.  , 0.25, 0.  ],
           [0.25, 0.25, 0.75, 2.25, 0.5 , 0.25, 0.  ],
           [0.25, 0.25, 0.5 , 0.5 , 1.  , 0.25, 0.  ],
           [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.  ],
           [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]])

    The depression finder is stored as part of the flow accumulator, so its
    properties can be accessed through the depression finder.

    >>> fa.depression_finder.lake_at_node.reshape(mg.shape)
    array([[False, False, False, False, False, False, False],
           [False, False, False, False, False, False, False],
           [False, False,  True,  True,  True, False, False],
           [False, False,  True,  True,  True, False, False],
           [False, False,  True,  True,  True, False, False],
           [False, False, False, False, False, False, False],
           [False, False, False, False, False, False, False]])
    >>> fa.depression_finder.lake_map.reshape(mg.shape)
    array([[-1, -1, -1, -1, -1, -1, -1],
           [-1, -1, -1, -1, -1, -1, -1],
           [-1, -1, 16, 16, 16, -1, -1],
           [-1, -1, 16, 16, 16, -1, -1],
           [-1, -1, 16, 16, 16, -1, -1],
           [-1, -1, -1, -1, -1, -1, -1],
           [-1, -1, -1, -1, -1, -1, -1]])
    >>> fa.depression_finder.lake_codes  # a unique code for each lake present on the grid
    array([16])
    >>> fa.depression_finder.lake_outlets  # the outlet node of each lake in lake_codes
    array([8])
    >>> fa.depression_finder.lake_areas  # the area of each lake in lake_codes
    array([2.25])

    Finally, note that the DepressionFinderAndRouter takes a keyword argument
    *routing* ('D8', default; 'D4') that sets how connectivity is set between
    nodes. Similar to our ability to pass keyword arguments to the FlowDirector
    through FlowAccumulator, we can pass this keyword argument to the
    DepressionFinderAndRouter component.

    >>> fa = FlowAccumulator(
    ...     mg,
    ...     "topographic__elevation",
    ...     flow_director=FlowDirectorSteepest,
    ...     depression_finder=DepressionFinderAndRouter,
    ...     routing="D4",
    ... )

    FlowAccumulator was designed to work with all types of grids. However,
    NetworkModelGrid's have no cell area. Thus, in order for FlowAccumulator to
    this type of grid, an at-node array called ``cell_area_at_node`` must be
    present.

    >>> from landlab.grid.network import NetworkModelGrid
    >>> y_of_node = (0, 1, 2, 2)
    >>> x_of_node = (0, 0, -1, 1)
    >>> nodes_at_link = ((1, 0), (2, 1), (3, 1))
    >>> nmg = NetworkModelGrid((y_of_node, x_of_node), nodes_at_link)
    >>> area = nmg.add_ones("cell_area_at_node", at="node")
    >>> z = nmg.add_field(
    ...     "topographic__elevation",
    ...     nmg.x_of_node + nmg.y_of_node,
    ...     at="node",
    ... )
    >>> fa = FlowAccumulator(nmg)
    >>> fa.run_one_step()
    >>> nmg.at_node["flow__receiver_node"]
    array([0, 0, 2, 1])

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    Braun, J., Willett, S. (2013). A very efficient O(n), implicit and parallel
    method to solve the stream power equation governing fluvial incision and
    landscape evolution. Geomorphology  180-181(C), 170-179.
    https://dx.doi.org/10.1016/j.geomorph.2012.10.008

    """

    _name = "FlowAccumulator"

    _unit_agnostic = True

    _info = {
        "drainage_area": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m**2",
            "mapping": "node",
            "doc": "Upstream accumulated surface area contributing to the node's discharge",
        },
        "flow__data_structure_delta": {
            "dtype": int,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": (
                "Node array containing the elements delta[1:] of the data "
                "structure 'delta' used for construction of the downstream-to-upstream "
                "node array"
            ),
        },
        "flow__upstream_node_order": {
            "dtype": int,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array containing downstream-to-upstream ordered list of node IDs",
        },
        "surface_water__discharge": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m**3/s",
            "mapping": "node",
            "doc": "Volumetric discharge of surface water",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "water__unit_flux_in": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "m/s",
            "mapping": "node",
            "doc": (
                "External volume water per area per time input to each node "
                "(e.g., rainfall rate)"
            ),
        },
    }

    def __init__(
        self,
        grid,
        surface="topographic__elevation",
        flow_director="FlowDirectorSteepest",
        runoff_rate=None,
        depression_finder=None,
        **kwargs,
    ):
        """Initialize the FlowAccumulator component.

        Saves the grid, tests grid type, tests imput types and
        compatability for the flow_director and depression_finder
        keyword arguments, tests the argument of runoff_rate, and
        initializes new fields.
        """
        super().__init__(grid)
        # Keep a local reference to the grid

        # Grid type testing
        self._is_raster = isinstance(self._grid, RasterModelGrid)
        self._is_Voroni = isinstance(self._grid, VoronoiDelaunayGrid)
        self._is_Network = isinstance(self._grid, NetworkModelGrid)
        self._kwargs = kwargs

        # STEP 1: Testing of input values, supplied either in function call or
        # as part of the grid.
        self._test_water_inputs(grid, runoff_rate)

        # save elevations and node_cell_area to class properites.
        self._surface = surface
        self._surface_values = return_array_at_node(grid, surface)

        if self._is_Network:
            try:
                node_cell_area = self._grid.at_node["cell_area_at_node"]
            except FieldError as exc:
                raise FieldError(
                    "In order for the FlowAccumulator to work, the "
                    "grid must have an at-node field called "
                    "cell_area_at_node."
                ) from exc
        else:
            node_cell_area = self._grid.cell_area_at_node.copy()
            node_cell_area[self._grid.closed_boundary_nodes] = 0.0

        self._node_cell_area = node_cell_area

        # STEP 2:
        # This component will track the following variables.
        # Attempt to create each, if they already exist, assign the existing
        # version to the local copy.

        #   - drainage area at each node
        #   - receiver of each node
        #   - delta array

        self.initialize_output_fields()

        self._drainage_area = grid.at_node["drainage_area"]
        self._discharges = grid.at_node["surface_water__discharge"]

        self._upstream_ordered_nodes = grid.at_node["flow__upstream_node_order"]
        if np.all(self._upstream_ordered_nodes == 0):
            self._upstream_ordered_nodes.fill(self._grid.BAD_INDEX)

        self._delta_structure = grid.at_node["flow__data_structure_delta"]
        if np.all(self._delta_structure == 0):
            self._delta_structure[:] = self._grid.BAD_INDEX

        self._D_structure = self._grid.BAD_INDEX * grid.ones(at="link", dtype=int)
        self._nodes_not_in_stack = True

        # STEP 3:
        # identify Flow Director method, save name, import and initialize the
        # correct flow director component if necessary; same with
        # lake/depression handler, if specified.
        self._add_director(flow_director)
        self._add_depression_finder(depression_finder)

        if len(self._kwargs) > 0:
            kwdstr = " ".join(list(self._kwargs.keys()))
            raise ValueError(f"Extra kwargs passed to FlowAccumulator:{kwdstr}")

    @property
    def surface_values(self):
        """Values of the surface over which flow is accumulated."""
        return self._surface_values

    @property
    def flow_director(self):
        """The FlowDirector used internally."""
        return self._flow_director

    @property
    def depression_finder(self):
        """The DepressionFinder used internally."""
        return self._depression_finder

    @property
    def node_drainage_area(self):
        """Return the drainage area."""
        return self._grid["node"]["drainage_area"]

    @property
    def node_water_discharge(self):
        """Return the surface water discharge."""
        return self._grid["node"]["surface_water__discharge"]

    @property
    def node_order_upstream(self):
        """Return the upstream node order (drainage stack)."""
        return self._grid["node"]["flow__upstream_node_order"]

    def link_order_upstream(self):
        """Return the upstream order of active links.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> mg = RasterModelGrid((5, 5))
        >>> mg.set_closed_boundaries_at_grid_edges(True, True, True, False)
        >>> _ = mg.add_field(
        ...     "topographic__elevation",
        ...     mg.node_x + mg.node_y,
        ...     at="node",
        ... )
        >>> fa = FlowAccumulator(mg, "topographic__elevation")
        >>> fa.run_one_step()
        >>> fa.link_order_upstream()
        array([ 5, 14, 23,  6, 15, 24,  7, 16, 25])

        This also works for route-to-many methods

        >>> mg = RasterModelGrid((5, 5))
        >>> mg.set_closed_boundaries_at_grid_edges(True, True, True, False)
        >>> np.flipud(
        ...     mg.add_field(
        ...         "topographic__elevation",
        ...         mg.node_x + mg.node_y,
        ...         at="node",
        ...     ).reshape(mg.shape)
        ... )
        array([[4., 5., 6., 7., 8.],
               [3., 4., 5., 6., 7.],
               [2., 3., 4., 5., 6.],
               [1., 2., 3., 4., 5.],
               [0., 1., 2., 3., 4.]])
        >>> fa = FlowAccumulator(mg, "topographic__elevation", flow_director="MFD")
        >>> fa.run_one_step()
        >>> link_order = fa.link_order_upstream()
        >>> link_order  # doctest: +SKIP
        array([ 5, 14, 10,  6, 11,  7, 23, 19, 15, 20, 16, 28, 24, 29, 25])
        >>> link_order[0]
        5
        >>> sorted(link_order[1:4])
        [6, 10, 14]
        >>> sorted(link_order[4:9])
        [7, 11, 15, 19, 23]
        >>> sorted(link_order[9:13])
        [16, 20, 24, 28]
        >>> sorted(link_order[13:])
        [25, 29]
        >>> np.all(sorted(link_order) == mg.active_links)
        True
        """
        downstream_links = self._grid["node"]["flow__link_to_receiver_node"][
            self._upstream_ordered_nodes
        ]
        out = downstream_links.flatten()
        return out[out != self._grid.BAD_INDEX]

    def headwater_nodes(self):
        """Return the headwater nodes.

        These are nodes that contribute flow and have no upstream nodes.

        Examples
        --------
        >>> from numpy.testing import assert_array_equal
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> mg = RasterModelGrid((5, 5))
        >>> mg.set_closed_boundaries_at_grid_edges(True, True, True, False)
        >>> _ = mg.add_field(
        ...     "topographic__elevation",
        ...     mg.node_x + mg.node_y,
        ...     at="node",
        ... )
        >>> fa = FlowAccumulator(mg, "topographic__elevation")
        >>> fa.run_one_step()
        >>> assert_array_equal(fa.headwater_nodes(), np.array([16, 17, 18]))
        """
        delta = np.concatenate(([0], self._delta_structure))
        num_donors = np.diff(delta)
        # note closed nodes have a value of 1 here since they flow to
        # themselves
        source_nodes = np.where(num_donors == 0)[0]
        return source_nodes

    def _test_water_inputs(self, grid, runoff_rate):
        """Test inputs for runoff_rate and water__unit_flux_in."""
        if "water__unit_flux_in" not in grid.at_node:
            if runoff_rate is None:
                # assume that if runoff rate is not supplied, that the value
                # should be set to one everywhere.
                grid.add_ones("water__unit_flux_in", at="node", dtype=float)
            else:
                runoff_rate = return_array_at_node(grid, runoff_rate)
                grid.at_node["water__unit_flux_in"] = runoff_rate
        else:
            if runoff_rate is not None:
                print(
                    "FlowAccumulator found both the field "
                    + "'water__unit_flux_in' and a provided float or "
                    + "array for the runoff_rate argument. THE FIELD IS "
                    + "BEING OVERWRITTEN WITH THE SUPPLIED RUNOFF_RATE!"
                )
                runoff_rate = return_array_at_node(grid, runoff_rate)
                grid.at_node["water__unit_flux_in"] = runoff_rate

    def _add_director(self, flow_director):
        """Test and add the flow director component."""
        PERMITTED_DIRECTORS = [
            "FlowDirectorSteepest",
            "FlowDirectorD8",
            "FlowDirectorMFD",
            "FlowDirectorDINF",
        ]

        potential_kwargs = ["partition_method", "diagonals"]
        kw = {}
        for p_k in potential_kwargs:
            if p_k in self._kwargs:
                kw[p_k] = self._kwargs.pop(p_k)

        # flow director is provided as a string.
        if isinstance(flow_director, str):
            if flow_director[:12] == "FlowDirector":
                flow_director = flow_director[12:]

            from landlab.components.flow_director import FlowDirectorD8
            from landlab.components.flow_director import FlowDirectorDINF
            from landlab.components.flow_director import FlowDirectorMFD
            from landlab.components.flow_director import FlowDirectorSteepest

            DIRECTOR_METHODS = {
                "D4": FlowDirectorSteepest,
                "Steepest": FlowDirectorSteepest,
                "D8": FlowDirectorD8,
                "MFD": FlowDirectorMFD,
                "DINF": FlowDirectorDINF,
            }

            try:
                FlowDirector = DIRECTOR_METHODS[flow_director]
            except KeyError as exc:
                raise ValueError(
                    "String provided in flow_director is not a "
                    "valid method or component name. The following"
                    "components are valid imputs:\n" + str(PERMITTED_DIRECTORS)
                ) from exc
            self._flow_director = FlowDirector(self._grid, self._surface, **kw)
        # flow director is provided as an instantiated flow director
        elif isinstance(flow_director, Component):
            if flow_director._name in PERMITTED_DIRECTORS:
                self._flow_director = flow_director
            else:
                raise ValueError(
                    "String provided in flow_director is not a "
                    "valid method or component name. The following"
                    "components are valid imputs:\n" + str(PERMITTED_DIRECTORS)
                )

            if len(kw) > 0:
                raise ValueError(
                    "flow_director provided as an instantiated ",
                    "component and keyword arguments provided. ",
                    "These kwargs would be ignored.",
                )

        # flow director is provided as an uninstantiated flow director
        else:
            if flow_director._name in PERMITTED_DIRECTORS:
                FlowDirector = flow_director
                self._flow_director = FlowDirector(self._grid, self._surface, **kw)
            else:
                raise ValueError(
                    "String provided in flow_director is not a "
                    "valid method or component name. The following"
                    "components are valid imputs:\n" + str(PERMITTED_DIRECTORS)
                )

        # save method as attribute
        self._method = self._flow_director._method

    def _add_depression_finder(self, depression_finder):
        """Test and add the depression finder component."""
        PERMITTED_DEPRESSION_FINDERS = ["DepressionFinderAndRouter", "LakeMapperBarnes"]

        # now do a similar thing for the depression finder.
        self._depression_finder_provided = depression_finder
        if self._depression_finder_provided is not None:
            # collect potential kwargs to pass to depression_finder
            # instantiation
            potential_kwargs = [
                "routing",
                "pits",
                "reroute_flow",
                "surface",
                "method",
                "fill_flat",
                "fill_surface",
                "redirect_flow_steepest_descent",
                "reaccumulate_flow",
                "ignore_overfill",
                "track_lakes",
            ]
            kw = {}
            for p_k in potential_kwargs:
                if p_k in self._kwargs:
                    kw[p_k] = self._kwargs.pop(p_k)

            # NEED TO TEST WHICH FLOWDIRECTOR WAS PROVIDED.
            if self._flow_director._name in ("FlowDirectorMFD", "FlowDirectorDINF"):
                raise NotImplementedError(
                    "The depression finder only works with route "
                    "to one FlowDirectors such as "
                    "FlowDirectorSteepest and  FlowDirectorD8. "
                    "Provide a different FlowDirector."
                )

            # depression finder is provided as a string.
            if isinstance(self._depression_finder_provided, str):
                from landlab.components.depression_finder.lake_mapper import (
                    DepressionFinderAndRouter,
                )
                from landlab.components.lake_fill.lake_fill_barnes import (
                    LakeMapperBarnes,
                )

                DEPRESSION_METHODS = {
                    "DepressionFinderAndRouter": DepressionFinderAndRouter,
                    "LakeMapperBarnes": LakeMapperBarnes,
                }

                try:
                    DepressionFinder = DEPRESSION_METHODS[
                        self._depression_finder_provided
                    ]
                except KeyError as exc:
                    raise ValueError(
                        "Component provided in depression_finder "
                        "is not a valid component. The following "
                        "components are valid imputs: "
                        f"{', '.join(repr(x) for x in PERMITTED_DEPRESSION_FINDERS)}."
                    ) from exc

                self._depression_finder = DepressionFinder(self._grid, **kw)
            # flow director is provided as an instantiated depression finder
            elif isinstance(self._depression_finder_provided, Component):
                if (
                    self._depression_finder_provided._name
                    in PERMITTED_DEPRESSION_FINDERS
                ):
                    self._depression_finder = self._depression_finder_provided
                else:
                    raise ValueError(
                        "Component provided in depression_finder "
                        "is not a valid component. The following "
                        "components are valid imputs:\n"
                        + str(PERMITTED_DEPRESSION_FINDERS)
                    )

                if len(kw) > 0:
                    raise ValueError(
                        "flow_director provided as an instantiated ",
                        "component and keyword arguments provided. ",
                        "These kwargs would be ignored.",
                    )

            # depression_finder is provided as an uninstantiated depression finder
            else:
                if (
                    self._depression_finder_provided._name
                    in PERMITTED_DEPRESSION_FINDERS
                ):
                    DepressionFinder = self._depression_finder_provided
                    self._depression_finder = DepressionFinder(self._grid, **kw)
                else:
                    raise ValueError(
                        "Component provided in depression_finder "
                        "is not a valid component. The following "
                        "components are valid imputs:\n"
                        + str(PERMITTED_DEPRESSION_FINDERS)
                    )

            # Make sure direction methods are consistent between the director
            # and the depression handler
            if isinstance(self._grid, RasterModelGrid):
                flow_director_method = self.flow_director_raster_method()
                depression_finder_method = (
                    self.depression_handler_raster_direction_method()
                )
                if flow_director_method != depression_finder_method:
                    message = (
                        "Incompatibility between flow-director routing method\n"
                        + "which is "
                        + flow_director_method
                        + ", and depression-handler method,\n"
                        + "which is "
                        + depression_finder_method
                    )
                    raise ValueError(warning_message(message))
        else:
            self._depression_finder = None

    def flow_director_raster_method(self):
        """Return 'D8' or 'D4' depending on the direction method used.

        (Note: only call this function for a raster gird;
        does not handle multiple-flow directors)
        """
        assert isinstance(self._grid, RasterModelGrid)
        if self._flow_director._name in ("FlowDirectorD8"):
            return "D8"
        else:
            return "D4"

    def depression_handler_raster_direction_method(self):
        """Return 'D8' or 'D4' depending on the direction method used.

        (Note: only call this function for a raster gird;
        does not handle multiple-flow directors)
        """
        assert isinstance(self._grid, RasterModelGrid)
        if self._depression_finder._name in ("DepressionFinderAndRouter"):
            return self._depression_finder._routing
        elif self._depression_finder._name in ("LakeMapperBarnes"):
            if (
                self._depression_finder._allneighbors.size
                > self.grid.adjacent_nodes_at_node.size
            ):
                return "D8"
            else:
                return "D4"
        else:
            raise ValueError("Depression finder type not recognized.")

    def pits_present(self):
        return np.any(self._grid.at_node["flow__sink_flag"][self._grid.core_nodes])

    def flooded_nodes_present(self):
        # flooded node status may not exist if no depression finder was used.
        if "flood_status_code" in self._grid.at_node:
            return np.all(self._grid.at_node["flood_status_code"] == _UNFLOODED)
        else:
            return False

    def accumulate_flow(self, update_flow_director=True, update_depression_finder=True):
        """Function to make FlowAccumulator calculate drainage area and
        discharge.

        Running accumulate_flow() results in the following to occur:

            1. Flow directions are updated (unless update_flow_director is set
               as False). This incluldes checking for updated boundary
               conditions.
            2. The depression finder, if present is updated (unless
               update_depression_finder is set as False).
            3. Intermediate steps that analyse the drainage network topology
               and create datastructures for efficient drainage area and
               discharge calculations.
            4. Calculation of drainage area and discharge.
            5. Return of drainage area and discharge.

        Parameters
        ----------
        update_flow_director : optional, bool
            Whether to update the flow director. Default is True.
        update_depression_finder : optional, bool
            Whether to update the depression finder, if present.
            Default is True.

        Returns
        -------
        drainage_area : array
            At node array which points to the field
            grid.at_node["drainage_area"].
        surface_water__discharge
            At node array which points to the field
            grid.at_node["surface_water__discharge"].
        """
        # set a couple of aliases
        a = self._grid["node"]["drainage_area"]
        q = self._grid["node"]["surface_water__discharge"]

        # step 1. Find flow directions by specified method
        if update_flow_director:
            self._flow_director.run_one_step()

        # further steps vary depending on how many recievers are present
        # one set of steps is for route to one (D8, Steepest/D4)

        # step 2. Get r
        r = as_id_array(self._grid["node"]["flow__receiver_node"])

        if self._flow_director._to_n_receivers == "one":
            # step 2b. Run depression finder if passed
            # Depression finder reaccumulates flow at the end of its routine.
            # At the moment, no depression finders work with to-many, so it
            # lives here
            if (
                self._depression_finder_provided is not None
                and update_depression_finder
            ):
                # only update depression finder if requested AND if there
                # are pits, or there were flooded nodes from last timestep.
                if self.pits_present or self.flooded_nodes_present:
                    self._depression_finder.update()

                # if FlowDirectorSteepest is used, update the link directions
                if self._flow_director._name == "FlowDirectorSteepest":
                    self._flow_director._determine_link_directions()

            # step 3. Stack, D, delta construction
            nd = as_id_array(flow_accum_bw._make_number_of_donors_array(r))
            delta = as_id_array(flow_accum_bw._make_delta_array(nd))
            D = as_id_array(flow_accum_bw._make_array_of_donors(r, delta))
            s = as_id_array(flow_accum_bw.make_ordered_node_array(r, nd, delta, D))

            # put these in grid so that depression finder can use it.
            # store the generated data in the grid
            self._grid.at_node["flow__data_structure_delta"][:] = as_id_array(delta[1:])
            self._D_structure = as_id_array(D)
            self._grid.at_node["flow__upstream_node_order"][:] = as_id_array(s)

            # step 4. Accumulate (to one or to N depending on direction method)
            a[:], q[:] = self._accumulate_A_Q_to_one(s, r)

        else:
            # Get p
            p = self._grid["node"]["flow__receiver_proportions"]

            # step 3. Stack, D, delta construction
            nd = as_id_array(flow_accum_to_n._make_number_of_donors_array_to_n(r, p))
            delta = as_id_array(flow_accum_to_n._make_delta_array_to_n(nd))
            D = as_id_array(flow_accum_to_n._make_array_of_donors_to_n(r, p, delta))
            s = as_id_array(
                flow_accum_to_n.make_ordered_node_array_to_n(r, p, nd, delta, D)
            )

            # put theese in grid so that depression finder can use it.
            # store the generated data in the grid
            self._grid["node"]["flow__data_structure_delta"][:] = delta[1:]
            self._D_structure = D

            self._grid["node"]["flow__upstream_node_order"][:] = s
            self._grid["node"]["flow__upstream_node_order"][:] = s

            # step 4. Accumulate (to one or to N depending on direction method)
            a[:], q[:] = self._accumulate_A_Q_to_n(s, r, p)

        return (a, q)

    def _accumulate_A_Q_to_one(self, s, r):
        """Accumulate area and discharge for a route-to-one scheme.

        Note this can be overridden in inherited components.
        """
        a, q = flow_accum_bw.find_drainage_area_and_discharge(
            s, r, self._node_cell_area, self._grid.at_node["water__unit_flux_in"]
        )
        return (a, q)

    def _accumulate_A_Q_to_n(self, s, r, p):
        """Accumulate area and discharge for a route-to-many scheme.

        Note this can be overridden in inherited components.
        """
        a, q = flow_accum_to_n.find_drainage_area_and_discharge_to_n(
            s, r, p, self._node_cell_area, self._grid.at_node["water__unit_flux_in"]
        )
        return (a, q)

    def run_one_step(self):
        """Accumulate flow and save to the model grid.

        1. Flow directions are updated. This incluldes checking for updated
           boundary conditions.
        2. The depression finder, if present is updated.
        3. Intermediate steps that analyse the drainage network topology
           and create datastructures for efficient drainage area and
           discharge calculations.
        4. Calculation of drainage area and discharge.
        5. Return of drainage area and discharge.

        An alternative to run_one_step() is accumulate_flow() which does the
        same things but also returns the drainage area and discharge.
        accumulate_flow() additionally provides the ability to turn off updating
        the flow director or the depression finder.
        """
        self.accumulate_flow()


if __name__ == "__main__":  # pragma: no cover
    import doctest

    doctest.testmod()



================================================
File: flow_accum/lossy_flow_accumulator.py
================================================
"""Accumulate flow and calc drainage area, while permitting gain or loss
of discharge during flow.

DEJH, late 2018
"""

from inspect import signature

from landlab.components.flow_accum import FlowAccumulator
from landlab.components.flow_accum import flow_accum_bw
from landlab.components.flow_accum import flow_accum_to_n


class LossyFlowAccumulator(FlowAccumulator):
    """Component to calculate drainage area and accumulate flow, while
    permitting dynamic loss or gain of flow downstream.

    This component is closely related to the :class:`.FlowAccumulator`,
    in that this is accomplished by first finding flow directions by a user-specified
    method and then calculating the drainage area and discharge. However,
    this component additionally requires the passing of a function that
    describes how discharge is lost or gained downstream::

        f(Qw, nodeID, linkID, grid)

    See the Examples below to see how this works in practice.

    Optionally, spatially variable runoff can be set either by the model grid
    field ``"water__unit_flux_in"`` or the input variable ``runoff_rate``.

    Optionally a depression finding component can be specified and flow
    directing, depression finding, and flow routing can all be accomplished
    together. Note that the :class:`.DepressionFinderAndRouter`
    is not particularly intelligent when running on lossy streams, and in particular,
    it will reroute flow around pits even when they are in fact not filled due to loss.

    .. note::

        The perimeter nodes *NEVER* contribute to the accumulating flux, even
        if the  gradients from them point inwards to the main body of the grid.
        This is because under Landlab definitions, perimeter nodes lack cells, so
        cannot accumulate any discharge.

    :class:`~.LossyFlowAccumulator` stores as :class:`~.ModelGrid` fields:

    -  Node array of drainage areas: ``"drainage_area"``
    -  Node array of discharges: ``"surface_water__discharge"``
    -  Node array of discharge loss in transit (vol / sec). This is the
       total loss across all of the downstream links:
       ``"surface_water__discharge_loss"``
    -  Node array containing downstream-to-upstream ordered list of node
       IDs: ``"flow__upstream_node_order"``
    -  Node array of all but the first element of the delta data structure:
       ``"flow__data_structure_delta"``. The first element is always zero.

    The :class:`FlowDirector` component will add additional
    :class:`~.ModelGrid` fields; see the
    :class:`~.FlowAccumulator` for full details. These are:

    -  Node array of receivers (nodes that receive flow), or *ITS OWN ID* if
       there is no receiver: ``"flow__receiver_node"``.
    -  Node array of flow proportions: ``"flow__receiver_proportions"``.
    -  Node array of links carrying flow:  ``"flow__link_to_receiver_node"``.
    -  Node array of downhill slopes from each receiver:
       ``"topographic__steepest_slope"``.
    -  Boolean node array of all local lows: ``"flow__sink_flag"``.

    The primary method of this class is
    :meth:`~landlab.components.LossyFlowAccumulator.run_one_step`.

    Examples
    --------
    These examples pertain only to the :class:`~.LossyFlowAccumulator`.
    See the main :class:`~.FlowAccumulator` documentation for more
    generic and comprehensive examples.

    First, a very simple example. Here's a 50% loss of discharge every time
    flow moves along a node:

    >>> import numpy as np
    >>> from landlab import RasterModelGrid, HexModelGrid
    >>> from landlab.components import FlowDirectorSteepest
    >>> from landlab.components import DepressionFinderAndRouter

    >>> mg = RasterModelGrid((3, 5), xy_spacing=(2, 1))
    >>> mg.set_closed_boundaries_at_grid_edges(True, True, False, True)
    >>> z = mg.add_field("topographic__elevation", mg.node_x + mg.node_y, at="node")

    >>> def mylossfunction(qw):
    ...     return 0.5 * qw
    ...

    >>> fa = LossyFlowAccumulator(
    ...     mg,
    ...     "topographic__elevation",
    ...     flow_director=FlowDirectorSteepest,
    ...     loss_function=mylossfunction,
    ... )
    >>> fa.run_one_step()

    >>> mg.at_node["drainage_area"].reshape(mg.shape)
    array([[0., 0., 0., 0., 0.],
           [6., 6., 4., 2., 0.],
           [0., 0., 0., 0., 0.]])
    >>> mg.at_node["surface_water__discharge"].reshape(mg.shape)
    array([[0.  , 0.  , 0.  , 0.  , 0.  ],
           [1.75, 3.5 , 3.  , 2.  , 0.  ],
           [0.  , 0.  , 0.  , 0.  , 0.  ]])
    >>> mg.at_node["surface_water__discharge_loss"].reshape(mg.shape)
    array([[0.  , 0.  , 0.  , 0.  , 0.  ],
           [0.  , 1.75, 1.5 , 1.  , 0.  ],
           [0.  , 0.  , 0.  , 0.  , 0.  ]])

    Here we use a spatially distributed field to derive loss terms, and also
    use a filled, non-raster grid.

    >>> dx = (2.0 / (3.0**0.5)) ** 0.5  # area to be 100.0
    >>> hmg = HexModelGrid((5, 3), spacing=dx, xy_of_lower_left=(-1.0745, 0.0))
    >>> z = hmg.add_field(
    ...     "topographic__elevation",
    ...     hmg.node_x**2 + np.round(hmg.node_y) ** 2,
    ...     at="node",
    ... )
    >>> z[9] = -10.0  # poke a hole
    >>> lossy = hmg.add_zeros("mylossterm", dtype=float, at="node")
    >>> lossy[14] = 1.0  # suppress all flow from node 14

    Without loss looks like this:

    >>> fa = LossyFlowAccumulator(
    ...     hmg,
    ...     "topographic__elevation",
    ...     flow_director=FlowDirectorSteepest,
    ...     depression_finder=DepressionFinderAndRouter,
    ... )
    >>> fa.run_one_step()
    >>> hmg.at_node["flow__receiver_node"]
    array([ 0,  1,  2,
            3,  0,  9,  6,
            7,  9,  4,  9, 11,
           12,  9,  9, 15,
           16, 17, 18])
    >>> np.round(hmg.at_node["drainage_area"])
    array([7., 0., 0.,
           0., 7., 1., 0.,
           0., 1., 6., 1., 0.,
           0., 1., 1., 0.,
           0., 0., 0.])
    >>> np.round(hmg.at_node["surface_water__discharge"])
    array([7., 0., 0.,
           0., 7., 1., 0.,
           0., 1., 6., 1., 0.,
           0., 1., 1., 0.,
           0., 0., 0.])

    With loss looks like this:

    >>> def mylossfunction2(Qw, nodeID, linkID, grid):
    ...     return (1.0 - grid.at_node["mylossterm"][nodeID]) * Qw
    ...
    >>> fa = LossyFlowAccumulator(
    ...     hmg,
    ...     "topographic__elevation",
    ...     flow_director=FlowDirectorSteepest,
    ...     depression_finder=DepressionFinderAndRouter,
    ...     loss_function=mylossfunction2,
    ... )
    >>> fa.run_one_step()
    >>> np.round(hmg.at_node["drainage_area"])
    array([7., 0., 0.,
           0., 7., 1., 0.,
           0., 1., 6., 1., 0.,
           0., 1., 1., 0.,
           0., 0., 0.])
    >>> np.round(hmg.at_node["surface_water__discharge"])
    array([6., 0., 0.,
           0., 6., 1., 0.,
           0., 1., 5., 1., 0.,
           0., 1., 1., 0.,
           0., 0., 0.])
    >>> np.allclose(
    ...     hmg.at_node["surface_water__discharge_loss"],
    ...     lossy * hmg.at_node["surface_water__discharge"],
    ... )
    True

    (Loss is only happening from the node, 14, that we set it to happen at.)

    Finally, note we can use the *linkIDs* to create flow-length-dependent
    effects:

    >>> from landlab.components import FlowDirectorMFD
    >>> mg = RasterModelGrid((4, 6), xy_spacing=(1, 2))
    >>> mg.set_closed_boundaries_at_grid_edges(True, True, False, True)
    >>> z = mg.add_field("topographic__elevation", 2.0 * mg.node_x, at="node")
    >>> z[9] = 8.0
    >>> z[16] = 6.5  # force the first node sideways

    >>> L = mg.add_zeros("spatialloss", at="node")
    >>> mg.at_node["spatialloss"][9] = 1.0
    >>> mg.at_node["spatialloss"][13] = 1.0
    >>> def fancyloss(Qw, nodeID, linkID, grid):
    ...     # now a true transmission loss:
    ...     Lt = 1.0 - 1.0 / grid.length_of_link[linkID] ** 2
    ...     Lsp = grid.at_node["spatialloss"][nodeID]
    ...     return Qw * (1.0 - Lt) * (1.0 - Lsp)
    ...

    >>> fa = LossyFlowAccumulator(
    ...     mg,
    ...     "topographic__elevation",
    ...     flow_director=FlowDirectorMFD,
    ...     loss_function=fancyloss,
    ... )
    >>> fa.run_one_step()

    >>> mg.at_node["drainage_area"].reshape(mg.shape)
    array([[ 0. ,  0. , 0. ,  0. ,  0. ,  0. ],
           [ 5.6,  5.6, 3.6,  2. ,  2. ,  0. ],
           [10.4, 10.4, 8.4,  6.4,  4. ,  0. ],
           [ 0. ,  0. , 0. ,  0. ,  0. ,  0. ]])
    >>> mg.at_node["surface_water__discharge"].reshape(mg.shape)
    array([[0. , 0. , 0. , 0. , 0. , 0. ],
           [4. , 4. , 2. , 2. , 2. , 0. ],
           [0. , 8.5, 6.5, 4.5, 2.5, 0. ],
           [0. , 0. , 0. , 0. , 0. , 0. ]])

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    Braun, J., Willett, S. (2013). A very efficient O(n), implicit and parallel
    method to solve the stream power equation governing fluvial incision and
    landscape evolution. Geomorphology  180-181(C), 170-179.
    https://dx.doi.org/10.1016/j.geomorph.2012.10.008
    """

    _name = "LossyFlowAccumulator"

    _info = {
        "drainage_area": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m**2",
            "mapping": "node",
            "doc": "Upstream accumulated surface area contributing to the node's discharge",
        },
        "flow__data_structure_delta": {
            "dtype": int,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": (
                "Node array containing the elements delta[1:] of the data "
                "structure 'delta' used for construction of the "
                "downstream-to-upstream node array"
            ),
        },
        "flow__upstream_node_order": {
            "dtype": int,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array containing downstream-to-upstream ordered list of node IDs",
        },
        "surface_water__discharge": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m**3/s",
            "mapping": "node",
            "doc": "Volumetric discharge of surface water",
        },
        "surface_water__discharge_loss": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m**3/s",
            "mapping": "node",
            "doc": "Total volume of water per second lost during all flow out of the node",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "water__unit_flux_in": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "m/s",
            "mapping": "node",
            "doc": (
                "External volume water per area per time input to each node "
                "(e.g., rainfall rate)"
            ),
        },
    }

    def __init__(
        self,
        grid,
        surface="topographic__elevation",
        flow_director="FlowDirectorSteepest",
        runoff_rate=None,
        depression_finder=None,
        loss_function=None,
        **kwargs,
    ):
        """Initialize the FlowAccumulator component.

        Saves the grid, tests grid type, tests imput types and
        compatability for the flow_director and depression_finder
        keyword arguments, tests the argument of runoff_rate, and
        initializes new fields.

        Parameters
        ----------
        grid : ModelGrid
            A Landlab grid.
        surface : str, int or array of int
            The surface to direct flow across.  Can be a field name at node or
            an array of length *node*.
        flow_director : str, class or instance of class.
            A string of method or class name (e.g. ``'D8'`` or ``'FlowDirectorD8'``), an
            uninstantiated FlowDirector class, or an instance of a
            :class:`FlowDirector` class. This sets the method
            used to calculate flow directions.
        runoff_rate : field name, array, or float, optional (m/time)
            If provided, sets the runoff rate and will be assigned to the grid
            field ``'water__unit_flux_in'``. If a spatially and and temporally variable
            runoff rate is desired, pass this field name and update the field
            through model run time. If both the field and argument are present at
            the time of initialization, runoff_rate will *overwrite* the field. If
            neither are set, defaults to spatially constant unit input.
        depression_finder : str, class, instance of class, optional
            A string of class name (e.g., ``'DepressionFinderAndRouter'``), an
            uninstantiated :class:`~.DepressionFinder` class,
            or an instance of a :class:`~.DepressionFinder` class.
            This sets the method for depression finding.
        loss_function : func, optional
            A function of the form ``f(Qw, [node_ID, [linkID, [grid]]])``, where Qw is
            the discharge at a node, node_ID the ID of the node at which the loss
            is to be calculated, linkID is the ID of the link down which the
            outflow drains (or a d8 ID if the routing is d8), and grid is a Landlab
            ModelGrid. The function then returns the new discharge at the node
            after the function is applied.

            Note that if a linkID is needed, a nodeID must also be specified, even
            if only as a dummy parameter; similarly, if a grid is to be passed, all
            of the preceding parameters must be specified. Both nodeID and linkID
            are required to permit spatially variable losses, and also losses
            dependent on flow path geometry (e.g., flow length). The grid is passed
            to allow fields or grid properties describing values across the grid
            to be accessed for the loss calculation (see examples).
            This function expects (float, [int, [int, [ModelGrid]]]), and
            return a single float, the new discharge value. This behavior is
            verified during component instantiation.
        **kwargs : optional
            Any additional parameters to pass to a FlowDirector or
            DepressionFinderAndRouter instance (e.g., partion_method for
            FlowDirectorMFD). This will have no effect if an instantiated
            component is passed using the flow_director or depression_finder
            keywords.
        """

        # add the new loss discharge field if necessary:
        if "surface_water__discharge_loss" not in grid.at_node:
            grid.add_zeros(
                "surface_water__discharge_loss", at="node", dtype=float, clobber=True
            )

        super().__init__(
            grid,
            surface=surface,
            flow_director=flow_director,
            runoff_rate=runoff_rate,
            depression_finder=depression_finder,
            **kwargs,
        )

        if loss_function is not None:
            sig = signature(loss_function)
            num_params = len(sig.parameters)

            # save the func for loss, and do a quick test on its inputs:
            if num_params == 1:
                # check the func takes a single value and turns it into a new
                # single value:
                if not isinstance(loss_function(1.0), float):
                    raise TypeError(
                        "The loss_function should take a float, and return " "a float."
                    )
                # now, for logical consistency in our calls to
                # find_drainage_area_and_discharge, wrap the func so it has two
                # arguments:

                def lossfunc(Qw, dummyn, dummyl, dummygrid):
                    return float(loss_function(Qw))

                self._lossfunc = lossfunc

            elif num_params == 2:
                # check the func takes a single value and turns it into a new
                # single value:
                if not isinstance(loss_function(1.0, 0), float):
                    raise TypeError(
                        "The loss_function should take (float, int), and "
                        "return a float."
                    )
                # now, for logical consistency in our calls to
                # find_drainage_area_and_discharge, wrap the func so it has two
                # arguments:

                def lossfunc(Qw, nodeID, dummyl, dummygrid):
                    return float(loss_function(Qw, nodeID))

                self._lossfunc = lossfunc

            elif num_params == 3:
                # check the func takes (float, int) and turns it into a new
                # single value:
                if not isinstance(loss_function(1.0, 0, 0), float):
                    raise TypeError(
                        "The loss_function should take (float, int, int), "
                        "and return a float."
                    )

                def lossfunc(Qw, nodeID, linkID, dummygrid):
                    return float(loss_function(Qw, nodeID, linkID))

                self._lossfunc = lossfunc

            elif num_params == 4:
                # this time, the test is too hard to implement cleanly so just
                self._lossfunc = loss_function
            else:
                raise ValueError(
                    "The loss_function must have only a single argument, "
                    "which should be the discharge at a node; a pair of "
                    "arguments, which should be the discharge at a node and "
                    "the node ID; or three arguments, which should be the "
                    "discharge at a node, the node ID, and the link along "
                    "which that discharge will flow."
                )
        else:
            # make a dummy
            def lossfunc(Qw, dummyn, dummyl, dummygrid):
                return float(Qw)

            self._lossfunc = lossfunc

    def _accumulate_A_Q_to_one(self, s, r):
        """Accumulate area and discharge for a route-to-one scheme."""
        link = self._grid.at_node["flow__link_to_receiver_node"]
        a, q = flow_accum_bw.find_drainage_area_and_discharge_lossy(
            s,
            r,
            link,
            self._lossfunc,
            self._grid,
            self._node_cell_area,
            self._grid.at_node["water__unit_flux_in"],
        )
        return a, q

    def _accumulate_A_Q_to_n(self, s, r, p):
        """Accumulate area and discharge for a route-to-one scheme."""
        link = self._grid.at_node["flow__link_to_receiver_node"]
        a, q = flow_accum_to_n.find_drainage_area_and_discharge_to_n_lossy(
            s,
            r,
            link,
            p,
            self._lossfunc,
            self._grid,
            self._node_cell_area,
            self._grid.at_node["water__unit_flux_in"],
        )
        return a, q



================================================
File: flow_director/__init__.py
================================================
from ..flow_director import flow_direction_DN
from ..flow_director.flow_direction_DN import flow_directions
from .flow_director_d8 import FlowDirectorD8
from .flow_director_dinf import FlowDirectorDINF
from .flow_director_mfd import FlowDirectorMFD
from .flow_director_steepest import FlowDirectorSteepest

__all__ = [
    "FlowDirectorD8",
    "FlowDirectorSteepest",
    "FlowDirectorMFD",
    "FlowDirectorDINF",
    "flow_directions",
    "flow_direction_DN",
]



================================================
File: flow_director/cfuncs.pyx
================================================
cimport cython

# https://cython.readthedocs.io/en/stable/src/userguide/fusedtypes.html
ctypedef fused id_t:
    cython.integral
    long long


@cython.boundscheck(False)
def adjust_flow_receivers(
    const id_t [:] src_nodes,
    const id_t [:] dst_nodes,
    const cython.floating [:] z,
    const cython.floating [:] link_slope,
    const id_t [:] active_links,
    id_t [:] receiver,
    id_t [:] receiver_link,
    cython.floating [:] steepest_slope,
):
    """Adjust flow receivers based on link slopes and steepest gradients.

    Parameters
    ----------
    src_nodes : array_like
        Ordered upstream node ids.
    dst_nodes : array_like
        Node ids of nodes receiving flow.
    z : array_like
        Node elevations.
    link_slope : array_like
        Link gradients.
    active_links : array_like
        Link IDs for active links.
    receiver : array_like
        Flow-receiver link IDs.
    steepest_slope : array_like
        Gradient of steepest descent from nodes.
    """
    cdef unsigned int n_nodes = src_nodes.shape[0]
    cdef int src_id
    cdef int dst_id

    for i in range(n_nodes):
        src_id = src_nodes[i]
        dst_id = dst_nodes[i]

        if (z[src_id] > z[dst_id]) and (link_slope[i] > steepest_slope[src_id]):
            receiver[src_id] = dst_id
            steepest_slope[src_id] = link_slope[i]
            receiver_link[src_id] = active_links[i]
        elif (z[dst_id] > z[src_id]) and (-link_slope[i] > steepest_slope[dst_id]):
            receiver[dst_id] = src_id
            steepest_slope[dst_id] = - link_slope[i]
            receiver_link[dst_id] = active_links[i]



================================================
File: flow_director/flow_direction_DN.py
================================================
#! /usr/env/python

"""
flow_direction_DN.py: calculates single-direction flow directions.

Works on both a regular or irregular grid.

GT Nov 2013
Modified Feb 2014
"""
import numpy as np

from landlab.core.utils import as_id_array
from landlab.grid.base import BAD_INDEX_VALUE

from .cfuncs import adjust_flow_receivers


def flow_directions(
    elev,
    active_links,
    tail_node,
    head_node,
    link_slope,
    grid=None,
    baselevel_nodes=None,
):
    """Find flow directions on a grid.

    Finds and returns flow directions for a given elevation grid. Each node is
    assigned a single direction, toward one of its N neighboring nodes (or
    itself, if none of its neighbors are lower).

    Parameters
    ----------
    elev : array_like
        Elevations at nodes.
    active_links : array_like
        IDs of active links.
    tail_node : array_like
        IDs of the tail node for each link.
    head_node : array_like
        IDs of the head node for each link.
    link_slope : array_like
        slope of each link, defined POSITIVE DOWNHILL (i.e., a negative value
        means the link runs uphill from the fromnode to the tonode).
    baselevel_nodes : array_like, optional
        IDs of open boundary (baselevel) nodes.

    Returns
    -------
    receiver : ndarray
        For each node, the ID of the node that receives its flow. Defaults to
        the node itself if no other receiver is assigned.
    steepest_slope : ndarray
        The slope value (positive downhill) in the direction of flow
    sink : ndarray
        IDs of nodes that are flow sinks (they are their own receivers)
    receiver_link : ndarray
        ID of link that leads from each node to its receiver, or
        BAD_INDEX_VALUE if none.

    Examples
    --------
    The example below assigns elevations to the 10-node example network in
    Braun and Willett (2012), so that their original flow pattern should be
    re-created.

    >>> import numpy as np
    >>> from landlab.components.flow_director import flow_directions
    >>> z = np.array([2.4, 1.0, 2.2, 3.0, 0.0, 1.1, 2.0, 2.3, 3.1, 3.2])
    >>> fn = np.array([1, 4, 4, 0, 1, 2, 5, 1, 5, 6, 7, 7, 8, 6, 3, 3, 2, 0])
    >>> tn = np.array([4, 5, 7, 1, 2, 5, 6, 5, 7, 7, 8, 9, 9, 8, 8, 6, 3, 3])
    >>> s = z[fn] - z[tn]  # slope with unit link length, positive downhill
    >>> active_links = np.arange(len(fn))
    >>> r, ss, snk, rl = flow_directions(z, active_links, fn, tn, s)
    >>> r
    array([1, 4, 1, 6, 4, 4, 5, 4, 6, 7])
    >>> ss
    array([1.4, 1. , 1.2, 1. , 0. , 1.1, 0.9, 2.3, 1.1, 0.9])
    >>> snk
    array([4])
    >>> rl[3:8]
    array([15, -1,  1,  6,  2])
    """
    # OK, the following are rough notes on design: we want to work with just
    # the active links. Ways to do this:
    # *  Pass active_links in as argument
    # *  In calling code, only refer to receiver_links for active nodes

    # Setup
    num_nodes = len(elev)
    steepest_slope = np.zeros(num_nodes)
    receiver = np.arange(num_nodes, dtype=active_links.dtype)
    receiver_link = np.full(num_nodes, BAD_INDEX_VALUE, dtype=active_links.dtype)

    # For each link, find the higher of the two nodes. The higher is the
    # potential donor, and the lower is the potential receiver. If the slope
    # from donor to receiver is steeper than the steepest one found so far for
    # the donor, then assign the receiver to the donor and record the new slope.
    # (Note the minus sign when looking at slope from "t" to "f").
    #
    # NOTE: MAKE SURE WE ARE ONLY LOOKING AT ACTIVE LINKS
    # THIS REMAINS A PROBLEM AS OF DEJH'S EFFORTS, MID MARCH 14.
    # overridden as part of fastscape_stream_power
    adjust_flow_receivers(
        tail_node,
        head_node,
        elev,
        link_slope,
        active_links,
        receiver,
        receiver_link,
        steepest_slope,
    )

    node_id = np.arange(num_nodes)

    # Optionally, handle baselevel nodes: they are their own receivers
    if baselevel_nodes is not None:
        receiver[baselevel_nodes] = node_id[baselevel_nodes]
        receiver_link[baselevel_nodes] = BAD_INDEX_VALUE
        steepest_slope[baselevel_nodes] = 0.0

    # The sink nodes are those that are their own receivers (this will normally
    # include boundary nodes as well as interior ones; "pits" would be sink
    # nodes that are also interior nodes).
    (sink,) = np.where(node_id == receiver)
    sink = as_id_array(sink)

    return receiver, steepest_slope, sink, receiver_link



================================================
File: flow_director/flow_direction_dinf.py
================================================
# def direct_dinf(grid, elevs='topographic_elevation', baselevel_nodes=None):


"""
flow_direction_dinf.py: calculate Dinfinity flow direction on raster grids.

Calculates flow direction and proportion on a raster grid by the Dinfinity
algorithm of Tarboton 1997.

KRB Feb 2017
"""

import numpy as np

from landlab.core.utils import as_id_array
from landlab.utils.return_array import return_array_at_node


def flow_directions_dinf(grid, elevs="topographic__elevation", baselevel_nodes=None):
    """Find Dinfinity flow directions and proportions on a raster grid.

    Finds and returns flow directions and proportions for a given elevation
    grid by the D infinity method (Tarboton, 1997). Each node is assigned two
    flow directions, toward the two neighboring nodes that are on the steepest
    subtriangle. Partitioning of flow is done based on the aspect of the
    subtriangle.

    This method does not support irregular grids.

    Parameters
    ----------
    grid : ModelGrid
        A grid of type Voroni.
    elevs : field name at node or array of length node
        The surface to direct flow across.
    baselevel_nodes : array_like, optional
        IDs of open boundary (baselevel) nodes.

    Returns
    -------
    receivers : ndarray of size (num nodes, max neighbors at node)
        For each node, the IDs of the nodes that receive its flow. For nodes
        that do not direct flow to all neighbors, grid.BAD_INDEX is given
        as a placeholder. The ID of the node itself is given if no other
        receiver is assigned.
    proportions : ndarray of size (num nodes, max neighbors at node)
        For each receiver, the proportion of flow (between 0 and 1) is given.
        A proportion of zero indicates that the link does not have flow along
        it.
    slopes: ndarray of size (num nodes, max neighbors at node)
        For each node in the array ``recievers``, the slope value (positive
        downhill) in the direction of flow. If no flow occurs (value of
        ``recievers`` is -1), then this array is set to 0.
    steepest_slope : ndarray
        The slope value (positive downhill) in the direction of flow.
    steepest_receiver : ndarray
        For each node, the node ID of the node connected by the steepest link.
        grid.BAD_INDEX is given if no flow emmanates from the node.
    sink : ndarray
        IDs of nodes that are flow sinks (they are their own receivers)
    receiver_links : ndarray of size (num nodes, max neighbors at node)
        ID of links that leads from each node to its receiver, or
        grid.BAD_INDEX if no flow occurs on this link.
    steepest_link : ndarray
        For each node, the link ID of the steepest link.
        grid.BAD_INDEX is given if no flow emmanates from the node.

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> from landlab.components.flow_director.flow_direction_dinf import (
    ...     flow_directions_dinf,
    ... )

    Dinfinity routes flow based on the relative proportion of flow along the
    triangular facets around a central raster node.

    >>> grid = RasterModelGrid((3, 3), xy_spacing=(1, 1))
    >>> _ = grid.add_field(
    ...     "topographic__elevation",
    ...     2.0 * grid.node_x + grid.node_y,
    ...     at="node",
    ... )
    >>> (
    ...     receivers,
    ...     proportions,
    ...     slopes,
    ...     steepest_slope,
    ...     steepest_receiver,
    ...     sink,
    ...     receiver_links,
    ...     steepest_link,
    ... ) = flow_directions_dinf(grid)
    >>> receivers
    array([[ 0, -1],
           [ 0, -1],
           [ 1, -1],
           [ 0, -1],
           [ 3,  0],
           [ 4,  1],
           [ 3, -1],
           [ 6,  3],
           [ 7,  4]])
    >>> proportions
    array([[ 1.        ,  0.        ],
           [ 1.        , -0.        ],
           [ 1.        , -0.        ],
           [ 1.        ,  0.        ],
           [ 0.40966553,  0.59033447],
           [ 0.40966553,  0.59033447],
           [ 1.        ,  0.        ],
           [ 0.40966553,  0.59033447],
           [ 0.40966553,  0.59033447]])

    This method also works if the elevations are passed as an array instead of
    the (implied) field name 'topographic__elevation'.

    >>> z = grid["node"]["topographic__elevation"]
    >>> (
    ...     receivers,
    ...     proportions,
    ...     slopes,
    ...     steepest_slope,
    ...     steepest_receiver,
    ...     sink,
    ...     receiver_links,
    ...     steepest_link,
    ... ) = flow_directions_dinf(grid, z)
    >>> receivers
    array([[ 0, -1],
           [ 0, -1],
           [ 1, -1],
           [ 0, -1],
           [ 3,  0],
           [ 4,  1],
           [ 3, -1],
           [ 6,  3],
           [ 7,  4]])
    >>> slopes
    array([[-1.        , -2.12132034],
           [ 2.        ,  0.70710678],
           [ 2.        ,  0.70710678],
           [ 1.        , -0.70710678],
           [ 2.        ,  2.12132034],
           [ 2.        ,  2.12132034],
           [ 1.        , -0.70710678],
           [ 2.        ,  2.12132034],
           [ 2.        ,  2.12132034]])
    >>> proportions
    array([[ 1.        ,  0.        ],
           [ 1.        , -0.        ],
           [ 1.        , -0.        ],
           [ 1.        ,  0.        ],
           [ 0.40966553,  0.59033447],
           [ 0.40966553,  0.59033447],
           [ 1.        ,  0.        ],
           [ 0.40966553,  0.59033447],
           [ 0.40966553,  0.59033447]])
    """
    try:
        grid.d8s_at_node
    except AttributeError as exc:
        raise NotImplementedError(
            "Dinfinity is currently implemented for Raster grids only"
        ) from exc
    # get elevs
    elevs = np.copy(return_array_at_node(grid, elevs))

    # find where there are closed nodes.
    closed_nodes = grid.status_at_node == grid.BC_NODE_IS_CLOSED

    closed_elevation = np.max(elevs[~closed_nodes]) + 1000

    elevs[closed_nodes] = closed_elevation

    # Step 1, some basic set-up, gathering information about the grid.

    # Calculate the number of nodes.
    num_nodes = len(elevs)

    # Set the number of receivers and facets.
    num_receivers = 2
    num_facets = 8

    # Create a node array
    node_id = np.arange(num_nodes)

    # create an array of the triangle numbers
    tri_numbers = np.arange(num_facets)

    # Step 3, create some triangle datastructures because landlab (smartly)
    # makes it hard to deal with diagonals.

    # create list of triangle neighbors at node. Use orientation associated
    # with tarboton's 1997 algorithm, orthogonal link first, then diagonal.
    # has shape, (nnodes, 8 triangles, 2 neighbors)
    n_at_node = grid.adjacent_nodes_at_node
    dn_at_node = grid.diagonal_adjacent_nodes_at_node

    triangle_neighbors_at_node = np.stack(
        [
            np.vstack((n_at_node[:, 0], dn_at_node[:, 0])),
            np.vstack((n_at_node[:, 1], dn_at_node[:, 0])),
            np.vstack((n_at_node[:, 1], dn_at_node[:, 1])),
            np.vstack((n_at_node[:, 2], dn_at_node[:, 1])),
            np.vstack((n_at_node[:, 2], dn_at_node[:, 2])),
            np.vstack((n_at_node[:, 3], dn_at_node[:, 2])),
            np.vstack((n_at_node[:, 3], dn_at_node[:, 3])),
            np.vstack((n_at_node[:, 0], dn_at_node[:, 3])),
        ],
        axis=-1,
    )
    triangle_neighbors_at_node = triangle_neighbors_at_node.swapaxes(0, 1)

    # next create, triangle links at node
    l_at_node = grid.d8s_at_node[:, :4]
    dl_at_node = grid.d8s_at_node[:, 4:]
    triangle_links_at_node = np.stack(
        [
            np.vstack((l_at_node[:, 0], dl_at_node[:, 0])),
            np.vstack((l_at_node[:, 1], dl_at_node[:, 0])),
            np.vstack((l_at_node[:, 1], dl_at_node[:, 1])),
            np.vstack((l_at_node[:, 2], dl_at_node[:, 1])),
            np.vstack((l_at_node[:, 2], dl_at_node[:, 2])),
            np.vstack((l_at_node[:, 3], dl_at_node[:, 2])),
            np.vstack((l_at_node[:, 3], dl_at_node[:, 3])),
            np.vstack((l_at_node[:, 0], dl_at_node[:, 3])),
        ],
        axis=-1,
    )
    triangle_links_at_node = triangle_links_at_node.swapaxes(0, 1)

    # next create link directions and active link directions at node
    # link directions
    ld_at_node = grid.link_dirs_at_node
    dld_at_node = grid.diagonal_dirs_at_node
    triangle_link_dirs_at_node = np.stack(
        [
            np.vstack((ld_at_node[:, 0], dld_at_node[:, 0])),
            np.vstack((ld_at_node[:, 1], dld_at_node[:, 0])),
            np.vstack((ld_at_node[:, 1], dld_at_node[:, 1])),
            np.vstack((ld_at_node[:, 2], dld_at_node[:, 1])),
            np.vstack((ld_at_node[:, 2], dld_at_node[:, 2])),
            np.vstack((ld_at_node[:, 3], dld_at_node[:, 2])),
            np.vstack((ld_at_node[:, 3], dld_at_node[:, 3])),
            np.vstack((ld_at_node[:, 0], dld_at_node[:, 3])),
        ],
        axis=-1,
    )
    triangle_link_dirs_at_node = triangle_link_dirs_at_node.swapaxes(0, 1)

    # need to create a list of diagonal links since it doesn't exist.
    diag_links = np.sort(np.unique(grid.d8s_at_node[:, 4:]))
    diag_links = diag_links[diag_links > 0]

    # calculate graidents across diagonals and orthogonals
    diag_grads = grid.calc_grad_at_diagonal(elevs)
    ortho_grads = grid.calc_grad_at_link(elevs)

    # finally compile link slopes
    link_slope = np.hstack((ortho_grads, diag_grads))

    # Construct the array of slope to triangles at node. This also will adjust
    # for the slope convention based on the direction of the links.
    # this is a (nnodes, 2, 8) array
    slopes_to_triangles_at_node = (
        link_slope[triangle_links_at_node] * triangle_link_dirs_at_node
    )

    # Step 3: make arrays necessary for the specific tarboton algorithm.
    # create a arrays
    ac = np.array([0.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0])
    af = np.array([1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0])

    # construct d1 and d2, we know these because we know where the orthogonal
    # links are
    diag_length = ((grid.dx) ** 2 + (grid.dy) ** 2) ** 0.5

    # for irregular grids, d1 and d2 will need to be matricies
    d1 = np.array(
        [grid.dx, grid.dy, grid.dy, grid.dx, grid.dx, grid.dy, grid.dy, grid.dy]
    )
    d2 = np.array(
        [grid.dx, grid.dx, grid.dy, grid.dy, grid.dx, grid.dx, grid.dy, grid.dy]
    )

    thresh = np.arctan(d2 / d1)

    # Step 4, Initialize receiver and proportion arrays
    receivers = grid.BAD_INDEX * np.ones((num_nodes, num_receivers), dtype=int)
    receiver_closed = grid.BAD_INDEX * np.ones((num_nodes, num_receivers), dtype=int)
    proportions = np.zeros((num_nodes, num_receivers), dtype=float)
    receiver_links = grid.BAD_INDEX * np.ones((num_nodes, num_receivers), dtype=int)
    slopes_to_receivers = np.zeros((num_nodes, num_receivers), dtype=float)

    # Step  5  begin the algorithm in earnest

    # construct e0, e1, e2 for all triangles at all nodes.
    # will be (nnodes, nfacets=8 for raster or nfacets = max number of patches
    # for irregular grids.

    # e0 is origin point of the facet
    e0 = elevs[node_id]

    # e1 is the point on the orthogoanal edges
    e1 = elevs[triangle_neighbors_at_node[:, 0, :]]
    # e2 is the point on the diagonal edges
    e2 = elevs[triangle_neighbors_at_node[:, 1, :]]

    # modification of original algorithm to address Landlab boundary conditions.
    # first,
    # for e1 and e2, mask out where nodes do not exits (e.g.
    # triangle_neighbors_at_node == -1)
    e1[triangle_neighbors_at_node[:, 0, :] == -1] = np.nan
    e2[triangle_neighbors_at_node[:, 1, :] == -1] = np.nan

    # loop through and calculate s1 and s2
    # this will only loop nfacets times.
    s1 = np.empty_like(e1)
    s2 = np.empty_like(e2)

    for i in range(num_facets):
        s1[:, i] = (e0 - e1[:, i]) / d1[i]
        s2[:, i] = (e1[:, i] - e2[:, i]) / d2[i]

    # calculate r and s, the direction and magnitude
    r = np.arctan2(s2, s1)
    s = ((s1**2) + (s2**2)) ** 0.5

    r[np.isnan(r)] = 0
    # adjust r if it doesn't sit in the realm of (0, arctan(d2,d1))
    too_small = r < 0
    radj = r.copy()
    radj[too_small] = 0
    s[too_small] = s1[too_small]

    # to consider two big, we need to look by triangle.
    for i in range(num_facets):
        too_big = r[:, i] > thresh[i]
        radj[too_big, i] = thresh[i]
        s[too_big, i] = (e0[too_big] - e2[too_big, i]) / diag_length

    # calculate the geospatial version of r based on radj
    rg = np.empty_like(r)
    for i in range(num_facets):
        rg[:, i] = (af[i] * radj[:, i]) + (ac[i] * np.pi / 2.0)

    # set slopes that are nan to below zero
    # if there is a flat slope, it should be chosen over the closed or non-existant
    # triangles that are represented by the nan values.
    s[np.isnan(s)] = -999.0

    # sort slopes
    # we've set slopes going to closed or non-existant triangles to -999.0, so
    # we shouldn't ever choose these.
    steepest_sort = np.argsort(s, kind="stable")

    # determine the steepest triangle
    steepest_triangle = tri_numbers[steepest_sort[:, -1]]

    # initialize arrays for the steepest rg and steepest s
    steepest_rg = np.empty_like(node_id, dtype=float)
    steepest_s = np.empty_like(node_id, dtype=float)

    closed_triangle_neighbors = closed_nodes[triangle_neighbors_at_node]
    for n in node_id:
        steepest_rg[n] = rg[n, steepest_sort[n, -1]]
        receiver_closed[n] = closed_triangle_neighbors[n, :, steepest_sort[n, -1]]
        steepest_s[n] = s[n, steepest_sort[n, -1]]
        receivers[n, :] = triangle_neighbors_at_node[n, :, steepest_sort[n, -1]]
        receiver_links[n, :] = triangle_links_at_node[n, :, steepest_sort[n, -1]]
        slopes_to_receivers[n, :] = slopes_to_triangles_at_node[
            n, :, steepest_sort[n, -1]
        ]

    # construct the baseline for proportions
    rg_baseline = np.array([0.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4]) * np.pi / 2.0

    # calculate alpha1 and alpha 2
    alpha2 = (steepest_rg - rg_baseline[steepest_triangle]) * af[steepest_triangle]
    alpha1 = thresh[steepest_triangle] - alpha2

    # calculate proportions from alpha
    proportions[:, 0] = (alpha1) / (alpha1 + alpha2)
    proportions[:, 1] = (alpha2) / (alpha1 + alpha2)

    # where proportions == 0, set reciever  to -1
    receivers[proportions == 0] = -1

    # END OF THE Tarboton algorithm, start of work to make this code mesh
    # with other landlab flow directing algorithms.

    # identify what drains to itself, and set proportion and id values based on
    # that.

    # if proportions is nan, drain to self
    drains_to_self = np.isnan(proportions[:, 0])

    # if all slopes are leading out or flat, drain to self
    drains_to_self[steepest_s <= 0] = True

    # if both receiver nodes are closed, drain to self
    drains_to_two_closed = receiver_closed.sum(axis=1) == num_receivers
    drains_to_self[drains_to_two_closed] = True

    # if drains to one closed receiver, check that the open receiver actually
    # gets flow. If so, route all to the open receiver. If the receiver getting
    # all the flow is closed, then drain to self.
    all_flow_to_closed = np.sum(receiver_closed * proportions, axis=1) == 1
    drains_to_self[all_flow_to_closed] = True

    drains_to_one_closed = receiver_closed.sum(axis=1) == 1
    fix_flow = drains_to_one_closed * (~all_flow_to_closed)
    first_column_has_closed = np.array(receiver_closed[:, 0] * fix_flow, dtype=bool)
    second_column_has_closed = np.array(receiver_closed[:, 1] * fix_flow, dtype=bool)

    # remove the link to the closed node
    receivers[first_column_has_closed, 0] = -1
    receivers[second_column_has_closed, 1] = -1

    # change the proportions
    proportions[first_column_has_closed, 0] = 0.0
    proportions[first_column_has_closed, 1] = 1.0

    proportions[second_column_has_closed, 0] = 1.0
    proportions[second_column_has_closed, 1] = 0.0

    # set properties of drains to self.
    receivers[drains_to_self, 0] = node_id[drains_to_self]
    receivers[drains_to_self, 1] = -1

    proportions[drains_to_self, 0] = 1.0
    proportions[drains_to_self, 1] = 0.0

    # set properties of closed
    receivers[closed_nodes, 0] = node_id[closed_nodes]
    receivers[closed_nodes, 1] = -1
    proportions[closed_nodes, 0] = 1.0
    proportions[closed_nodes, 1] = 0.0

    # mask the receiver_links by where flow doesn't occur to return
    receiver_links[drains_to_self, :] = grid.BAD_INDEX

    # identify the steepest link so that the steepest receiver, link, and slope
    # can be returned.
    slope_sort = np.argsort(
        np.argsort(slopes_to_receivers, axis=1, kind="stable"), axis=1, kind="stable"
    ) == (num_receivers - 1)
    steepest_slope = slopes_to_receivers[slope_sort]
    steepest_slope[drains_to_self] = 0.0

    # identify the steepest link and steepest receiever.
    steepest_link = receiver_links[slope_sort]
    steepest_link[drains_to_self] = grid.BAD_INDEX

    steepest_receiver = receivers[slope_sort]
    steepest_receiver[drains_to_self] = node_id[drains_to_self]

    # Optionally, handle baselevel nodes: they are their own receivers
    if baselevel_nodes is not None:
        receivers[baselevel_nodes, 0] = node_id[baselevel_nodes]
        receivers[baselevel_nodes, 1:] = -1
        proportions[baselevel_nodes, 0] = 1
        proportions[baselevel_nodes, 1:] = 0
        receiver_links[baselevel_nodes, :] = grid.BAD_INDEX
        steepest_slope[baselevel_nodes] = 0.0

    # ensure that if there is a -1, it is in the second column.
    order_reversed = receivers[:, 0] == -1

    receivers_out = receivers.copy()
    receivers_out[order_reversed, 1] = receivers[order_reversed, 0]
    receivers_out[order_reversed, 0] = receivers[order_reversed, 1]

    proportions_out = proportions.copy()
    proportions_out[order_reversed, 1] = proportions[order_reversed, 0]
    proportions_out[order_reversed, 0] = proportions[order_reversed, 1]

    slopes_to_receivers_out = slopes_to_receivers.copy()
    slopes_to_receivers_out[order_reversed, 1] = slopes_to_receivers[order_reversed, 0]
    slopes_to_receivers_out[order_reversed, 0] = slopes_to_receivers[order_reversed, 1]

    receiver_links_out = receiver_links.copy()
    receiver_links_out[order_reversed, 1] = receiver_links[order_reversed, 0]
    receiver_links_out[order_reversed, 0] = receiver_links[order_reversed, 1]

    # The sink nodes are those that are their own receivers (this will normally
    # include boundary nodes as well as interior ones; "pits" would be sink
    # nodes that are also interior nodes).
    (sink,) = np.where(node_id == receivers[:, 0])
    sink = as_id_array(sink)

    return (
        receivers_out,
        proportions_out,
        slopes_to_receivers_out,
        steepest_slope,
        steepest_receiver,
        sink,
        receiver_links_out,
        steepest_link,
    )


if __name__ == "__main__":  # pragma: no cover
    import doctest

    doctest.testmod()



================================================
File: flow_director/flow_direction_mfd.py
================================================
#! /usr/env/python

"""
flow_direction_mfd.py: calculate multiple-flow-direction flow directions.

Works on both a regular or irregular grid. Also calculates flow proportions.

KRB Jan 2017
"""

import numpy as np

from landlab.core.utils import as_id_array
from landlab.grid.base import BAD_INDEX_VALUE


def flow_directions_mfd(
    elev,
    neighbors_at_node,
    links_at_node,
    active_link_dir_at_node,
    link_slope,
    baselevel_nodes=None,
    partition_method="slope",
):
    """Find multiple-flow-direction flow directions on a grid.

    Finds and returns flow directions and proportions for a given elevation
    grid. Each node is assigned multiple flow directions, toward all of the N
    neighboring nodes that are lower than it. If none of the neighboring nodes
    are lower, it is assigned to itself. Flow proportions can be calculated as
    proportional to slope (default) or proportional to the square root of
    slope, which is the solution to a steady kinematic wave.

    Parameters
    ----------
    elev : array_like
        Elevations at nodes.
    neighbors_at_node : array_like (num nodes, max neighbors at node)
        For each node, the link IDs of active links.
    links_at_node : array_like (num nodes, max neighbors at node)

    link_dir_at_node: array_like (num nodes, max neighbors at node)

        IDs of the head node for each link.
    link_slope : array_like
        slope of each link, defined POSITIVE DOWNHILL (i.e., a negative value
        means the link runs uphill from the fromnode to the tonode).
    baselevel_nodes : array_like, optional
        IDs of open boundary (baselevel) nodes.
    partition_method: string, optional
        Method for partitioning flow. Options include 'slope' (default) and
        'square_root_of_slope'.

    Returns
    -------
    receivers : ndarray of size (num nodes, max neighbors at node)
        For each node, the IDs of the nodes that receive its flow. For nodes
        that do not direct flow to all neighbors, BAD_INDEX_VALUE is given as
        a placeholder. The ID of the node itself is given if no other receiver
        is assigned.
    proportions : ndarray of size (num nodes, max neighbors at node)
        For each receiver, the proportion of flow (between 0 and 1) is given.
        A proportion of zero indicates that the link does not have flow along
        it.
    slopes: ndarray of size (num nodes, max neighbors at node)
        For each node in the array ``recievers``, the slope value (positive
        downhill) in the direction of flow. If no flow occurs (value of
        ``recievers`` is -1), then this array is set to 0.
    steepest_slope : ndarray
        The slope value (positive downhill) in the direction of flow.
    steepest_receiver : ndarray
        For each node, the node ID of the node connected by the steepest link.
        BAD_INDEX_VALUE is given if no flow emmanates from the node.
    sink : ndarray
        IDs of nodes that are flow sinks (they are their own receivers)
    receiver_links : ndarray of size (num nodes, max neighbors at node)
        ID of links that leads from each node to its receiver, or
        BAD_INDEX_VALUE if no flow occurs on this link.
    steepest_link : ndarray
        For each node, the link ID of the steepest link.
        BAD_INDEX_VALUE is given if no flow emmanates from the node.

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> import numpy as np
    >>> from landlab.components.flow_director.flow_direction_mfd import (
    ...     flow_directions_mfd,
    ... )
    >>> grid = RasterModelGrid((3, 3), xy_spacing=(1, 1))
    >>> elev = grid.add_field(
    ...     "topographic__elevation",
    ...     grid.node_x + grid.node_y,
    ...     at="node",
    ... )

    For the first example, we will not pass any diagonal elements to the flow
    direction algorithm.

    >>> neighbors_at_node = grid.adjacent_nodes_at_node
    >>> links_at_node = grid.links_at_node
    >>> active_link_dir_at_node = grid.active_link_dirs_at_node
    >>> link_slope = np.arctan(grid.calc_grad_at_link(elev))
    >>> slopes_to_neighbors_at_node = (
    ...     link_slope[links_at_node] * active_link_dir_at_node
    ... )
    >>> (
    ...     receivers,
    ...     proportions,
    ...     slopes,
    ...     steepest_slope,
    ...     steepest_receiver,
    ...     sink,
    ...     receiver_links,
    ...     steepest_link,
    ... ) = flow_directions_mfd(
    ...     elev,
    ...     neighbors_at_node,
    ...     links_at_node,
    ...     active_link_dir_at_node,
    ...     link_slope,
    ...     baselevel_nodes=None,
    ...     partition_method="slope",
    ... )
    >>> receivers
    array([[ 0, -1, -1, -1],
           [ 1, -1, -1, -1],
           [ 2, -1, -1, -1],
           [ 3, -1, -1, -1],
           [-1, -1,  3,  1],
           [-1, -1,  4, -1],
           [ 6, -1, -1, -1],
           [-1, -1, -1,  4],
           [ 8, -1, -1, -1]])
    >>> proportions
    array([[1. , 0. , 0. , 0. ],
           [1. , 0. , 0. , 0. ],
           [1. , 0. , 0. , 0. ],
           [1. , 0. , 0. , 0. ],
           [0. , 0. , 0.5, 0.5],
           [0. , 0. , 1. , 0. ],
           [1. , 0. , 0. , 0. ],
           [0. , 0. , 0. , 1. ],
           [1. , 0. , 0. , 0. ]])
    >>> proportions.sum(axis=-1)
    array([1., 1., 1., 1., 1., 1., 1., 1., 1.])

    In the second example, we will pass diagonal elements to the flow direction
    algorithm.

    >>> dal = grid.active_d8
    >>> neighbors_at_node = np.hstack(
    ...     (grid.adjacent_nodes_at_node, grid.diagonal_adjacent_nodes_at_node)
    ... )
    >>> links_at_node = grid.d8s_at_node
    >>> active_link_dir_at_node = grid.active_d8_dirs_at_node

    We need to create a list of diagonal links since it doesn't exist.

    >>> diag_links = np.sort(np.unique(grid.d8s_at_node[:, 4:]))
    >>> diag_links = diag_links[diag_links > 0]
    >>> diag_grads = np.zeros(diag_links.shape)
    >>> where_active_diag = dal >= diag_links.min()
    >>> active_diags_inds = dal[where_active_diag] - diag_links.min()
    >>> diag_grads = grid.calc_grad_at_diagonal(elev)
    >>> ortho_grads = grid.calc_grad_at_link(elev)
    >>> link_slope = np.hstack((np.arctan(ortho_grads), np.arctan(diag_grads)))
    >>> (
    ...     receivers,
    ...     proportions,
    ...     slopes,
    ...     steepest_slope,
    ...     steepest_receiver,
    ...     sink,
    ...     receiver_links,
    ...     steepest_link,
    ... ) = flow_directions_mfd(
    ...     elev,
    ...     neighbors_at_node,
    ...     links_at_node,
    ...     active_link_dir_at_node,
    ...     link_slope,
    ...     baselevel_nodes=None,
    ...     partition_method="slope",
    ... )
    >>> receivers
    array([[ 0, -1, -1, -1, -1, -1, -1, -1],
           [ 1, -1, -1, -1, -1, -1, -1, -1],
           [ 2, -1, -1, -1, -1, -1, -1, -1],
           [ 3, -1, -1, -1, -1, -1, -1, -1],
           [-1, -1,  3,  1, -1, -1,  0, -1],
           [-1, -1,  4, -1, -1, -1, -1, -1],
           [ 6, -1, -1, -1, -1, -1, -1, -1],
           [-1, -1, -1,  4, -1, -1, -1, -1],
           [-1, -1, -1, -1, -1, -1,  4, -1]])
    >>> proportions
    array([[1.        , 0.        , 0.        , 0.        , 0.        ,
            0.        , 0.        , 0.        ],
           [1.        , 0.        , 0.        , 0.        , 0.        ,
            0.        , 0.        , 0.        ],
           [1.        , 0.        , 0.        , 0.        , 0.        ,
            0.        , 0.        , 0.        ],
           [1.        , 0.        , 0.        , 0.        , 0.        ,
            0.        , 0.        , 0.        ],
           [0.        , 0.        , 0.31091174,  0.31091174, 0.        ,
            0.        , 0.37817653, 0.        ],
           [0.        , 0.        , 1.        , 0.        , 0.        ,
            0.        , 0.        , 0.        ],
           [1.        , 0.        , 0.        , 0.        , 0.        ,
            0.        , 0.        , 0.        ],
           [0.        , 0.        , 0.        , 1.        , 0.        ,
            0.        , 0.        , 0.        ],
           [0.        , 0.        , 0.        , 0.        , 0.        ,
            0.        , 1.        , 0.        ]])
    >>> slopes
    array([[0.        , 0.        , 0.        , 0.        , 0.        ,
            0.        , 0.        , 0.        ],
           [0.        , 0.        , 0.        , 0.        , 0.        ,
            0.        , 0.        , 0.        ],
           [0.        , 0.        , 0.        , 0.        , 0.        ,
            0.        , 0.        , 0.        ],
           [0.        , 0.        , 0.        , 0.        , 0.        ,
            0.        , 0.        , 0.        ],
           [0.        , 0.        , 0.78539816, 0.78539816, 0.        ,
            0.        , 0.95531662, 0.        ],
           [0.        , 0.        , 0.78539816, 0.        , 0.        ,
            0.        , 0.        , 0.        ],
           [0.        , 0.        , 0.        , 0.        , 0.        ,
            0.        , 0.        , 0.        ],
           [0.        , 0.        , 0.        , 0.78539816, 0.        ,
            0.        , 0.        , 0.        ],
           [0.        , 0.        , 0.        , 0.        , 0.        ,
            0.        , 0.95531662, 0.        ]])
    >>> proportions.sum(axis=-1)
    array([1., 1., 1., 1., 1., 1., 1., 1., 1.])
    """
    # Calculate the number of nodes.
    num_nodes = len(elev)

    # Create a node array
    node_id = np.arange(num_nodes)

    # Calculate the maximum number of neighbors at node.
    max_number_of_neighbors = neighbors_at_node.shape[1]

    # Make a copy of neighbors_at_node so we can change it into the receiver
    # array.
    receivers = neighbors_at_node.copy()

    # Construct the array of slope to neighbors at node. This also will adjust
    # for the slope convention based on the direction of the link.
    slopes_to_neighbors_at_node = link_slope[links_at_node] * active_link_dir_at_node

    # Make a copy so this can be changed based on where no flow occurs.
    receiver_links = links_at_node.copy()

    # some of these potential recievers may have already been assigned as
    # BAD_INDEX_VALUE because the link was inactive. Make a mask of these for
    # future use. Also find the close nodes.
    inactive_link_to_neighbor = active_link_dir_at_node == 0
    closed_nodes = np.sum(np.abs(active_link_dir_at_node), 1) == 0
    # Now calculate where flow occurs.
    # First, make an elevation array of potential receivers.
    potential_receiver_elev = elev[neighbors_at_node]

    # now make an array of the same shape (for direct comparison) of the source
    # node elevation.
    source_node_elev = elev[np.tile(node_id, (max_number_of_neighbors, 1)).T]

    # find where flow does not occur (source is lower that receiver)
    flow_does_not_occur = source_node_elev <= potential_receiver_elev

    # Where the source is lower, set receivers to BAD_INDEX_VALUE
    receivers[flow_does_not_occur] = BAD_INDEX_VALUE

    # Where the link is not active, set receivers to BAD_INDEX_VALUE
    receivers[inactive_link_to_neighbor] = BAD_INDEX_VALUE

    # Next, find where a node drains to itself
    drains_to_self = receivers.sum(1) == -1 * max_number_of_neighbors

    # Where this occurs, set the receiver ID in the first column of receivers
    # to the node ID.
    receivers[drains_to_self, 0] = node_id[drains_to_self]

    # Finally, set the first element of the closed nodes to themselves.
    receivers[closed_nodes, 0] = node_id[closed_nodes]

    # Next, calculate flow proportions.
    # Copy slope array and mask by where flow is not occuring and where the
    # link is inactive.
    flow_slopes = slopes_to_neighbors_at_node.copy()
    flow_slopes[flow_does_not_occur] = 0.0
    flow_slopes[inactive_link_to_neighbor] = 0.0

    if partition_method == "square_root_of_slope":
        values_for_partitioning = flow_slopes**0.5
    elif partition_method == "slope":
        values_for_partitioning = flow_slopes
    else:
        raise ValueError("Keyword argument to partition_method invalid.")

    # Calculate proportions by normalizing by rowsums.
    denom = np.tile(values_for_partitioning.sum(1), (max_number_of_neighbors, 1)).T
    denom[denom <= 0] = 1  # to prevent runtime errors
    proportions = values_for_partitioning / denom
    proportions[drains_to_self, 0] = 1
    proportions[drains_to_self, 1:] = 0

    # Might need to sort by proportions and rearrange to follow expectations
    # of no BAD_INDEX_VALUE value in first column. KRB NOT SURE

    # mask the receiver_links by where flow doesn't occur to return
    receiver_links[flow_does_not_occur] = BAD_INDEX_VALUE
    receiver_links[inactive_link_to_neighbor] = BAD_INDEX_VALUE

    # identify the steepest link so that the steepest receiver, link, and slope
    # can be returned.
    slope_sort = np.argsort(
        np.argsort(flow_slopes, axis=1, kind="stable"), axis=1, kind="stable"
    ) == (max_number_of_neighbors - 1)
    steepest_slope = flow_slopes[slope_sort]

    # identify the steepest link and steepest receiever.
    steepest_link = receiver_links[slope_sort]
    steepest_receiver = receivers[slope_sort]
    steepest_receiver[drains_to_self] = node_id[drains_to_self]

    # Optionally, handle baselevel nodes: they are their own receivers
    if baselevel_nodes is not None:
        receivers[baselevel_nodes, 0] = node_id[baselevel_nodes]
        receivers[baselevel_nodes, 1:] = -1
        proportions[baselevel_nodes, 0] = 1
        proportions[baselevel_nodes, 1:] = 0
        receiver_links[baselevel_nodes, :] = BAD_INDEX_VALUE
        steepest_slope[baselevel_nodes] = 0.0

    # The sink nodes are those that are their own receivers (this will normally
    # include boundary nodes as well as interior ones; "pits" would be sink
    # nodes that are also interior nodes).
    (sink,) = np.where(node_id == receivers[:, 0])
    sink = as_id_array(sink)

    slopes_to_neighbors_at_node[flow_does_not_occur] = 0
    slopes_to_neighbors_at_node[inactive_link_to_neighbor] = 0

    return (
        receivers,
        proportions,
        slopes_to_neighbors_at_node,
        steepest_slope,
        steepest_receiver,
        sink,
        receiver_links,
        steepest_link,
    )


if __name__ == "__main__":  # pragma: no cover
    import doctest

    doctest.testmod()



================================================
File: flow_director/flow_director.py
================================================
#! /usr/env/python

"""flow_director.py provides a private class to help create FlowDirectors.

Provides the _FlowDirector component which does grid type testing, adds
the surface over which flow will be routed to the component, and sets up
part of the boundary condition testing.
"""

from landlab import Component
from landlab import RasterModelGrid  # for type tests
from landlab.utils.return_array import return_array_at_node


class _FlowDirector(Component):
    """Private class for creating components to calculate flow directions.

    This class is not meant to be used directly in modeling efforts.
    Instead it has the functionality that all flow direction calculators need
    to initialize and check boundary conditions.

    It also creates the following field used by all FlowDirectors.

    -  Link array identifing if flow goes with (1) or against (-1) the link
       direction: *'flow__link_direction'*

    The primary method of this class, :func:`run_one_step` is not implemented.

    Parameters
    ----------
    grid : ModelGrid
        A grid.
    surface : field name at node or array of length node
        The surface to direct flow across.

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> from landlab.components.flow_director.flow_director import _FlowDirector
    >>> mg = RasterModelGrid((3, 3), xy_spacing=(1, 1))
    >>> mg.set_closed_boundaries_at_grid_edges(True, True, True, False)
    >>> _ = mg.add_field(
    ...     "topographic__elevation",
    ...     mg.node_x + mg.node_y,
    ...     at="node",
    ... )
    >>> fd = _FlowDirector(mg, "topographic__elevation")
    >>> fd.surface_values
    array([0., 1., 2., 1., 2., 3., 2., 3., 4.])
    >>> "topographic__elevation" in mg.at_node.keys()
    True
    >>> "flow__sink_flag" in mg.at_node.keys()
    True

    _FlowDirector also works if you pass it an array instead of a field name.

    >>> import numpy as np
    >>> mg = RasterModelGrid((3, 3), xy_spacing=(1, 1))
    >>> z = np.array([0.0, 1.0, 2.0, 1.0, 2.0, 3.0, 2.0, 3.0, 4.0])
    >>> fd = _FlowDirector(mg, z)
    >>> fd.surface_values
    array([0., 1., 2., 1., 2., 3., 2., 3., 4.])
    """

    _name = "_FlowDirector"

    _unit_agnostic = True

    _info = {
        "flow__sink_flag": {
            "dtype": bool,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Boolean array, True at local lows",
        }
    }

    def __init__(self, grid, surface):
        """Initialize the _FlowDirector class."""
        # We keep a local reference to the grid
        super().__init__(grid)

        self._bc_set_code = self._grid.bc_set_code

        # set up the grid type testing
        self._is_raster = isinstance(self._grid, RasterModelGrid)
        if not self._is_raster:
            self._method = None

        # save elevations as class properites.
        self._surface = surface
        self._surface_values = return_array_at_node(grid, surface)

        grid.add_zeros("flow__sink_flag", at="node", dtype=bool, clobber=True)

    @property
    def surface_values(self):
        """Values of the surface over which flow is directed."""
        return self._surface_values

    def _changed_surface(self):
        """Check if the surface values have changed.

        If the surface values are stored as a field, it is important to
        check if they have changed since the component was instantiated.
        """
        if isinstance(self._surface, str):
            self._surface_values = return_array_at_node(self._grid, self._surface)

    def _check_updated_bc(self):
        # step 0. Check and update BCs
        if self._bc_set_code != self._grid.bc_set_code:
            self.updated_boundary_conditions()
            self._bc_set_code = self._grid.bc_set_code

    def run_one_step(self):
        """run_one_step is not implemented for this component."""
        raise NotImplementedError("run_one_step()")

    @property
    def sink_flag(self):
        """Return the array with sink flags."""
        return self._grid["node"]["flow__sink_flag"]

    @property
    def node_steepest_slope(self):
        """Return the steepest link slope at a node."""
        return self._grid["node"]["topographic__steepest_slope"]

    @property
    def link_to_flow_receiving_node(self):
        """Return the link id along the link transporting flow."""
        return self._grid["node"]["flow__link_to_receiver_node"]

    @property
    def node_receiving_flow(self):
        """Return the node ids of the nodes receiving flow."""
        return self._grid["node"]["flow__receiver_node"]


if __name__ == "__main__":  # pragma: no cover
    import doctest

    doctest.testmod()



================================================
File: flow_director/flow_director_d8.py
================================================
#! /usr/env/python

"""
flow_director_d8.py: provides the component FlowDirectorsD8.

This components finds the steepest single-path steepest descent flow
directions and considers diagonal links between nodes on a raster grid. It is
not implemented for irregular grids. For a method that works for irregular
grids and does not consider diagonal links for rasters, use
FlowDirectorSteepest instead.
"""

import numpy

from landlab import LinkStatus
from landlab.components.flow_director import flow_direction_DN
from landlab.components.flow_director.flow_director_to_one import _FlowDirectorToOne


class FlowDirectorD8(_FlowDirectorToOne):
    """Single-path (steepest direction) flow direction with diagonals on
    rasters.

    Single-path (steepest direction) flow direction finding on raster grids
    by the D8 method. This method considers flow on all eight links such that
    flow is possible on orthogonal and on diagonal links.

    The method that considers only orthogonal links (D4 method) for raster
    grids is FlowDirectorSteepest.

    This method is not implemented for Voroni grids, use
    FlowDirectorSteepest instead.

    Stores as ModelGrid fields:

    -  Node array of receivers (nodes that receive flow), or ITS OWN ID if
       there is no receiver: *'flow__receiver_node'*
    -  Node array of steepest downhill slopes:
       *'topographic__steepest_slope'*
    -  Node array containing ID of link that leads from each node to its
       receiver, or BAD_INDEX_VALUE if no link:
       *'flow__link_to_receiver_node'*
    -  Boolean node array of all local lows: *'flow__sink_flag'*

    The primary method of this class is :func:`run_one_step`.

    Examples
    --------
    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import FlowDirectorD8
    >>> mg = RasterModelGrid((3, 3), xy_spacing=(1, 1))
    >>> mg.set_closed_boundaries_at_grid_edges(True, True, True, False)
    >>> _ = mg.add_field(
    ...     "topographic__elevation",
    ...     mg.node_x + mg.node_y,
    ...     at="node",
    ... )
    >>> fd = FlowDirectorD8(mg, "topographic__elevation")
    >>> fd.surface_values
    array([0., 1., 2., 1., 2., 3., 2., 3., 4.])
    >>> fd.run_one_step()
    >>> mg.at_node["flow__receiver_node"]
    array([0, 1, 2, 3, 0, 5, 6, 7, 8])
    >>> mg.at_node["topographic__steepest_slope"]
    array([0.        , 0.        , 0.        , 0.        , 1.41421356,
           0.        , 0.        , 0.        , 0.        ])
    >>> mg.at_node["flow__link_to_receiver_node"]
    array([-1, -1, -1, -1, 12, -1, -1, -1, -1])
    >>> mg.at_node["flow__sink_flag"].astype(int)
    array([1, 1, 1, 1, 0, 1, 1, 1, 1])
    >>> mg_2 = RasterModelGrid((5, 4), xy_spacing=(1, 1))
    >>> topographic__elevation = [
    ...     [0.0, 0.0, 0.0, 0.0],
    ...     [0.0, 21.0, 10.0, 0.0],
    ...     [0.0, 31.0, 20.0, 0.0],
    ...     [0.0, 32.0, 30.0, 0.0],
    ...     [0.0, 0.0, 0.0, 0.0],
    ... ]
    >>> _ = mg_2.add_field(
    ...     "topographic__elevation",
    ...     topographic__elevation,
    ...     at="node",
    ... )
    >>> mg_2.set_closed_boundaries_at_grid_edges(True, True, True, False)
    >>> fd_2 = FlowDirectorD8(mg_2)
    >>> fd_2.run_one_step()
    >>> mg_2.at_node["flow__receiver_node"].reshape(mg_2.shape)
    array([[ 0,  1,  2,  3],
           [ 4,  1,  2,  7],
           [ 8,  6,  6, 11],
           [12, 10, 10, 15],
           [16, 17, 18, 19]])

    The flow directors also have the ability to return the flow receiver nodes

    >>> receiver = fd.direct_flow()
    >>> receiver
    array([0, 1, 2,
           3, 0, 5,
           6, 7, 8])

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    O'Callaghan, J., Mark, D. (1984). The extraction of drainage networks from
    digital elevation data. Computer Vision, Graphics, and Image Processing
    28(3), 323 - 344. https://dx.doi.org/10.1016/s0734-189x(84)80011-0

    """

    _name = "FlowDirectorD8"

    _info = {
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__sink_flag": {
            "dtype": bool,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Boolean array, True at local lows",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__steepest_slope": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "The steepest *downhill* slope",
        },
    }

    def __init__(self, grid, surface="topographic__elevation"):
        """
        Parameters
        ----------
        grid : ModelGrid
            A grid of type RasterModelGrid.
        surface : field name at node or array of length node, optional
            The surface to direct flow across, default is field at node:
            topographic__elevation,.
        """
        self._method = "D8"
        super().__init__(grid, surface)
        try:
            self._grid.nodes_at_d8
        except AttributeError:
            self._is_Voroni = True
        else:
            self._is_Voroni = False
        if self._is_Voroni:
            raise NotImplementedError(
                "FlowDirectorD8 not implemented for"
                "irregular grids, use"
                "FlowDirectorSteepest"
            )

        self.updated_boundary_conditions()

    def updated_boundary_conditions(self):
        """Method to update FlowDirectorD8 when boundary conditions change.

        Call this if boundary conditions on the grid are updated after
        the component is instantiated.
        """
        self._active_links = numpy.arange(self._grid.number_of_d8)
        nodes_at_d8 = self._grid.nodes_at_d8[self._active_links]
        self._activelink_tail = nodes_at_d8[:, 0]
        self._activelink_head = nodes_at_d8[:, 1]

    def run_one_step(self):
        """Find flow directions and save to the model grid.

        run_one_step() checks for updated boundary conditions, calculates
        slopes on links, finds baselevel nodes based on the status at node,
        calculates flow directions, and saves results to the grid.

        an alternative to direct_flow() is direct_flow() which does the same
        things but also returns the receiver nodes not return values.
        """
        self.direct_flow()

    def direct_flow(self):
        """Find flow directions, save to the model grid, and return receivers.

        direct_flow() checks for updated boundary conditions, calculates
        slopes on links, finds baselevel nodes based on the status at node,
        calculates flow directions, saves results to the grid, and returns a
        at-node array  of receiver nodes. This array is stored in the grid at:
        grid['node']['flow__receiver_node']

        an alternative to direct_flow() is run_one_step() which does the same
        things but also returns a at-node array  of receiver nodes. This array
        is stored in the grid at:
        grid['node']['flow__receiver_node']
        """
        self._check_updated_bc()

        # update the surface, if it was provided as a model grid field.
        self._changed_surface()

        # step 1. Calculate link slopes.
        link_slope = -self._grid.calc_grad_at_d8(self._surface_values)
        link_slope[self._grid.status_at_d8 != LinkStatus.ACTIVE] = 0

        # Step 2. Find and save base level nodes.
        (baselevel_nodes,) = numpy.where(
            numpy.logical_or(
                self._grid.status_at_node == self._grid.BC_NODE_IS_FIXED_VALUE,
                self._grid.status_at_node == self._grid.BC_NODE_IS_FIXED_GRADIENT,
            )
        )

        # Calculate flow directions by D8 method
        receiver, steepest_slope, sink, recvr_link = flow_direction_DN.flow_directions(
            self._surface_values,
            self._active_links,
            self._activelink_tail,
            self._activelink_head,
            link_slope,
            grid=self._grid,
            baselevel_nodes=baselevel_nodes,
        )
        # Save the four ouputs of this component.
        self._grid["node"]["flow__receiver_node"][:] = receiver
        self._grid["node"]["topographic__steepest_slope"][:] = steepest_slope
        self._grid["node"]["flow__link_to_receiver_node"][:] = recvr_link
        self._grid["node"]["flow__sink_flag"][:] = numpy.zeros_like(
            receiver, dtype=bool
        )
        self._grid["node"]["flow__sink_flag"][sink] = True

        return receiver


if __name__ == "__main__":  # pragma: no cover
    import doctest

    doctest.testmod()



================================================
File: flow_director/flow_director_dinf.py
================================================
#! /usr/env/python

"""
flow_director_dinf.py: provides the component FlowDirectorDINF.

Directs flow on raster grids only using the Dinfinity algorithm of
Tarboton 1997.
"""

import numpy

from landlab import NodeStatus
from landlab.components.flow_director import flow_direction_dinf
from landlab.components.flow_director.flow_director_to_many import _FlowDirectorToMany


class FlowDirectorDINF(_FlowDirectorToMany):
    """Flow direction on a raster grid by the D infinity method.

    Directs flow by the D infinity method (Tarboton, 1997). Each node is
    assigned two flow directions, toward the two neighboring nodes that are on
    the steepest subtriangle. Partitioning of flow is done based on the aspect
    of the subtriangle.

    Specifically, it stores as ModelGrid fields:

    -  Node array of receivers (nodes that receive flow), or ITS OWN ID if
       there is no receiver: *'flow__receiver_node'*. This array is 2D, and is
       of dimension (number of nodes x max number of receivers).
    -  Node array of flow proportions: *'flow__receiver_proportions'*. This
       array is 2D, and is of dimension (number of nodes x max number of
       receivers).
    -  Node array of links carrying flow:  *'flow__link_to_receiver_node'*.
       This array is 2D, and is of dimension (number of nodes x max number of
       receivers).
    -  Node array of downhill slopes corresponding to each receiver.:
       *'topographic__steepest_slope'* This array is 2D, and is
       of dimension (number of nodes x max number of receivers). If the slope is
       uphill or flat, the value is assigned zero.
    -  Boolean node array of all local lows: *'flow__sink_flag'*
    -  Link array identifing if flow goes with (1) or against (-1) the link
       direction: *'flow__link_direction'*

    The primary method of this class is :func:`run_one_step`.

    Examples
    --------

    This method works for both raster and irregular grids. First we will look
    at a raster example, and then an irregular example.

    >>> import numpy as numpy
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import FlowDirectorDINF
    >>> mg = RasterModelGrid((4, 4), xy_spacing=(1, 1))
    >>> mg.set_closed_boundaries_at_grid_edges(True, True, True, False)
    >>> _ = mg.add_field(
    ...     "topographic__elevation",
    ...     mg.node_x**2 + mg.node_y**2,
    ...     at="node",
    ... )

    The DINF flow director can be uses for raster grids only.

    >>> fd = FlowDirectorDINF(mg, "topographic__elevation")
    >>> fd.surface_values.reshape(mg.shape)
    array([[ 0.,  1.,   4.,  9.],
           [ 1.,  2.,   5., 10.],
           [ 4.,  5.,   8., 13.],
           [ 9., 10., 13., 18.]])
    >>> fd.run_one_step()

    Unlike flow directors that only direct flow to one node or to all
    downstream nodes, FlowDirectorDINF directs flow two nodes only. It stores
    the receiver information is a (number of nodes x 2) shape field at nodes.

    >>> mg.at_node["flow__receiver_node"]
    array([[ 0, -1],
           [ 1, -1],
           [ 2, -1],
           [ 3, -1],
           [ 4, -1],
           [ 0,  -1],
           [ 5,  1],
           [ 7, -1],
           [ 8, -1],
           [ 5, -1],
           [ 5, -1],
           [11, -1],
           [12, -1],
           [13, -1],
           [14, -1],
           [15, -1]])

    It also stores the proportions of flow going to each receiver, the link on
    which the flow moves in at node arrays, and the slope of each link.

    >>> mg.at_node["flow__receiver_proportions"]
    array([[1.        , 0.        ],
           [1.        , 0.        ],
           [1.        , 0.        ],
           [1.        , 0.        ],
           [1.        , 0.        ],
           [1.        , 0.        ],
           [0.59033447, 0.40966553],
           [1.        , 0.        ],
           [1.        , 0.        ],
           [1.        , 0.        ],
           [1.        , 0.        ],
           [1.        , 0.        ],
           [1.        , 0.        ],
           [1.        , 0.        ],
           [1.        , 0.        ],
           [1.        , 0.        ]])
    >>> mg.at_node["flow__link_to_receiver_node"]
    array([[-1, -1],
           [-1, -1],
           [-1, -1],
           [-1, -1],
           [ 3, 25],
           [24,  4],
           [ 8, 26],
           [ 9, 28],
           [14, 31],
           [11, 33],
           [32, 12],
           [16, 34],
           [21, 37],
           [18, 39],
           [19, 38],
           [20, 40]])
    >>> mg.at_node["topographic__steepest_slope"]
    array([[-1.00000000e+00, -1.41421356e+00],
           [ 1.00000000e+00, -7.12763635e+02],
           [ 3.00000000e+00,  1.41421356e+00],
           [ 5.00000000e+00,  2.82842712e+00],
           [ 1.00900000e+03,  7.12763635e+02],
           [ 1.41421356e+00,  1.00000000e+00],
           [ 3.00000000e+00,  2.82842712e+00],
           [ 1.00400000e+03,  7.10642315e+02],
           [ 1.00400000e+03,  7.12056529e+02],
           [ 3.00000000e+00,  0.00000000e+00],
           [ 4.24264069e+00,  3.00000000e+00],
           [ 1.00100000e+03,  7.09935208e+02],
           [-0.00000000e+00,  7.09935208e+02],
           [ 1.00400000e+03,  7.07813888e+02],
           [ 1.00100000e+03,  7.09935208e+02],
           [ 0.00000000e+00,  7.07813888e+02]])

    Finally, FlowDirectorDINF identifies sinks, or local lows.

    >>> mg.at_node["flow__sink_flag"].astype(int)
    array([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1])

    The flow directors also have the ability to return the flow receiver nodes
    through a function called direct_flow()

    >>> fd = FlowDirectorDINF(mg, "topographic__elevation")
    >>> fd.run_one_step()
    >>> receivers, proportions = fd.direct_flow()
    >>> receivers
    array([[ 0, -1],
           [ 1, -1],
           [ 2, -1],
           [ 3, -1],
           [ 4, -1],
           [ 0, -1],
           [ 5,  1],
           [ 7, -1],
           [ 8, -1],
           [ 5, -1],
           [ 5, -1],
           [11, -1],
           [12, -1],
           [13, -1],
           [14, -1],
           [15, -1]])
    >>> proportions
    array([[1.        , 0.        ],
           [1.        , 0.        ],
           [1.        , 0.        ],
           [1.        , 0.        ],
           [1.        , 0.        ],
           [1.        , 0.        ],
           [0.59033447, 0.40966553],
           [1.        , 0.        ],
           [1.        , 0.        ],
           [1.        , 0.        ],
           [1.        , 0.        ],
           [1.        , 0.        ],
           [1.        , 0.        ],
           [1.        , 0.        ],
           [1.        , 0.        ],
           [1.        , 0.        ]])

    For each donor node (represented by each row) the proportions should sum to
    one.

    >>> proportions.sum(axis=1)
    array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    Tarboton, D. (1997). A new method for the determination of flow directions
    and upslope areas in grid digital elevation models. Water Resources
    Research  33(2), 309-319. https://dx.doi.org/10.1029/96wr03137

    """

    _name = "FlowDirectorDINF"

    _info = {
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__receiver_proportions": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of proportion of flow sent to each receiver.",
        },
        "flow__sink_flag": {
            "dtype": bool,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Boolean array, True at local lows",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__steepest_slope": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "The steepest *downhill* slope",
        },
    }

    def __init__(self, grid, surface="topographic__elevation"):
        """
        Parameters
        ----------
        grid : ModelGrid
            A grid.
        surface : field name at node or array of length node, optional
            The surface to direct flow across, default is field at node:
            topographic__elevation.
        partition_method: string, optional
            Method for partitioning flow. Options include 'slope' (default) and
            'square_root_of_slope'.
        """

        self._method = "DINF"
        self._max_receivers = 2
        super().__init__(grid, surface)
        try:
            self._grid.nodes_at_d8
        except AttributeError:
            self._is_Voroni = True
        else:
            self._is_Voroni = False
        if self._is_Voroni:
            raise NotImplementedError(
                "FlowDirectorDINF is not implemented" " for irregular grids."
            )

        self.updated_boundary_conditions()

    def updated_boundary_conditions(self):
        """Method to update FlowDirectorDINF when boundary conditions change.

        Call this if boundary conditions on the grid are updated after
        the component is instantiated.
        """
        self._active_links = self._grid.active_links
        self._activelink_tail = self._grid.node_at_link_tail[self._grid.active_links]
        self._activelink_head = self._grid.node_at_link_head[self._grid.active_links]

    def run_one_step(self):
        """Find flow directions and save to the model grid.

        run_one_step() checks for updated boundary conditions, calculates
        slopes on links, finds basself._surface_valuesel nodes based on the status at node,
        calculates flow directions, and saves results to the grid.

        An alternative to direct_flow() is direct_flow() which does the same
        things but also returns the receiver nodes not return values.
        """
        self.direct_flow()

    def direct_flow(self):
        """Find flow directions, save to the model grid, and return receivers.

        direct_flow() checks for updated boundary conditions, calculates
        slopes on links, finds basself._surface_valuesel nodes based on the status at node,
        calculates flow directions, saves results to the grid, and returns a
        at-node array  of receiver nodes. This array is stored in the grid at:
        grid['node']['flow__receiver_nodes']

        An alternative to direct_flow() is run_one_step() which does the same
        things but also returns a at-node array  of receiver nodes. This array
        is stored in the grid at:
        grid['node']['flow__receiver_nodes']
        """
        self._check_updated_bc()

        # Step 1. Find and save base level nodes.
        (baselevel_nodes,) = numpy.where(
            numpy.logical_or(
                self._grid.status_at_node == NodeStatus.FIXED_VALUE,
                self._grid.status_at_node == NodeStatus.FIXED_GRADIENT,
            )
        )

        # Calculate flow directions
        (
            self._receivers,
            self._proportions,
            slopes_to_receivers,
            steepest_slope,
            steepest_receiver,
            sink,
            receiver_links,
            steepest_link,
        ) = flow_direction_dinf.flow_directions_dinf(
            self._grid, self._surface_values, baselevel_nodes=baselevel_nodes
        )

        # Save the four ouputs of this component.
        self._grid["node"]["flow__receiver_node"][:] = self._receivers
        self._grid["node"]["flow__receiver_proportions"][:] = self._proportions
        self._grid["node"]["topographic__steepest_slope"][:] = slopes_to_receivers
        self._grid["node"]["flow__link_to_receiver_node"][:] = receiver_links
        self._grid["node"]["flow__sink_flag"][:] = False
        self._grid["node"]["flow__sink_flag"][sink] = True

        return (self._receivers, self._proportions)


if __name__ == "__main__":  # pragma: no cover
    import doctest

    doctest.testmod()



================================================
File: flow_director/flow_director_mfd.py
================================================
#! /usr/env/python

"""
flow_director_mfd.py: provides the component FlowDirectorMFD.

This components finds the steepest single-path steepest descent flow
directions. It is equivalent to D4 method in the special case of a raster grid
in that it does not consider diagonal links between nodes. For that capability,
use FlowDirectorD8.
"""

import numpy

from landlab import NodeStatus
from landlab import VoronoiDelaunayGrid
from landlab.components.flow_director import flow_direction_mfd
from landlab.components.flow_director.flow_director_to_many import _FlowDirectorToMany


class FlowDirectorMFD(_FlowDirectorToMany):
    """Multiple-path flow direction with or without out diagonals.

    Directs flow by the multiple flow direction method. Each node is assigned
    multiple flow directions, toward all of the N neighboring nodes that are
    lower than it. If none of the neighboring nodes are lower, the location is
    identified as a pit. Flow proportions can be calculated as proportional to
    slope or proportional to the square root of slope, which is the solution to
    a steady kinematic wave.

    Specifically, it stores as ModelGrid fields:

    -  Node array of receivers (nodes that receive flow), or ITS OWN ID if
       there is no receiver: *'flow__receiver_node'*. This array is 2D, and is
       of dimension (number of nodes x max number of receivers).
    -  Node array of flow proportions: *'flow__receiver_proportions'*. This
       array is 2D, and is of dimension (number of nodes x max number of
       receivers).
    -  Node array of links carrying flow:  *'flow__link_to_receiver_node'*.
       This array is 2D, and is of dimension (number of nodes x max number of
       receivers).
    -  Node array of downhill slopes corresponding to each receiver.:
       *'topographic__steepest_slope'* This array is 2D, and is
       of dimension (number of nodes x max number of receivers). If the slope is
       uphill or flat, the value is assigned zero.
    -  Boolean node array of all local lows: *'flow__sink_flag'*
    -  Link array identifing if flow goes with (1) or against (-1) the link
       direction: *'flow__link_direction'*

    The primary method of this class is :func:`run_one_step`.

    Examples
    --------

    This method works for both raster and irregular grids. First we will look
    at a raster example, and then an irregular example.

    >>> import numpy as numpy
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import FlowDirectorMFD
    >>> mg = RasterModelGrid((3, 3), xy_spacing=(1, 1))
    >>> mg.set_closed_boundaries_at_grid_edges(True, True, True, False)
    >>> _ = mg.add_field(
    ...     "topographic__elevation",
    ...     mg.node_x + mg.node_y,
    ...     at="node",
    ... )

    The MFD flow director can be uses for raster and irregular grids. For
    raster grids, use of diagonal links is specified with the keyword
    *diagonals* (default is False).

    >>> fd = FlowDirectorMFD(mg, "topographic__elevation", diagonals=True)
    >>> fd.surface_values
    array([0., 1., 2., 1., 2., 3., 2., 3., 4.])
    >>> fd.run_one_step()

    Unlike flow directors that only direct flow to one node, FlowDirectorMFD
    directs flow to all downstream nodes. It stores the receiver information
    is a (number of nodes x maximum number or receivers) shape field at nodes.

    >>> mg.at_node["flow__receiver_node"]
    array([[ 0, -1, -1, -1, -1, -1, -1, -1],
           [ 1, -1, -1, -1, -1, -1, -1, -1],
           [ 2, -1, -1, -1, -1, -1, -1, -1],
           [ 3, -1, -1, -1, -1, -1, -1, -1],
           [-1, -1, -1,  1, -1, -1,  0, -1],
           [ 5, -1, -1, -1, -1, -1, -1, -1],
           [ 6, -1, -1, -1, -1, -1, -1, -1],
           [ 7, -1, -1, -1, -1, -1, -1, -1],
           [ 8, -1, -1, -1, -1, -1, -1, -1]])

    It also stores the proportions of flow going to each receiver, the link on
    which the flow moves in at node arrays, and the slope of each link.

    >>> mg.at_node["flow__receiver_proportions"]
    array([[1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],
           [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],
           [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],
           [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],
           [0. , 0. , 0. , 0.41421356, 0. , 0. ,  0.58578644, 0. ],
           [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],
           [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],
           [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],
           [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]])
    >>> mg.at_node["flow__link_to_receiver_node"]
    array([[-1, -1, -1, -1, -1, -1, -1, -1],
           [-1, -1, -1, -1, -1, -1, -1, -1],
           [-1, -1, -1, -1, -1, -1, -1, -1],
           [-1, -1, -1, -1, -1, -1, -1, -1],
           [-1, -1, -1,  3, -1, -1, 12, -1],
           [-1, -1, -1, -1, -1, -1, -1, -1],
           [-1, -1, -1, -1, -1, -1, -1, -1],
           [-1, -1, -1, -1, -1, -1, -1, -1],
           [-1, -1, -1, -1, -1, -1, -1, -1]])
    >>> mg.at_node["topographic__steepest_slope"]
    array([[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],
           [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],
           [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],
           [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],
           [0. , 0. , 0. , 1. , 0. , 0. , 1.41421356, 0. ],
           [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],
           [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],
           [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],
           [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]])

    Finally, FlowDirectorMFD identifies sinks, or local lows.

    >>> mg.at_node["flow__sink_flag"].astype(int)
    array([1, 1, 1, 1, 0, 1, 1, 1, 1])

    The flow directors also have the ability to return the flow receiver nodes.
    For this example, we will turn the diagonals off. This is the default
    value.

    >>> mg = RasterModelGrid((3, 3), xy_spacing=(1, 1))
    >>> mg.set_closed_boundaries_at_grid_edges(True, True, True, False)
    >>> _ = mg.add_field(
    ...     "topographic__elevation",
    ...     mg.node_x + mg.node_y,
    ...     at="node",
    ... )
    >>> fd = FlowDirectorMFD(mg, "topographic__elevation")
    >>> fd.run_one_step()
    >>> receivers, proportions = fd.direct_flow()
    >>> receivers
    array([[ 0, -1, -1, -1],
           [ 1, -1, -1, -1],
           [ 2, -1, -1, -1],
           [ 3, -1, -1, -1],
           [-1, -1, -1,  1],
           [ 5, -1, -1, -1],
           [ 6, -1, -1, -1],
           [ 7, -1, -1, -1],
           [ 8, -1, -1, -1]])
    >>> proportions
    array([[1., 0., 0., 0.],
           [1., 0., 0., 0.],
           [1., 0., 0., 0.],
           [1., 0., 0., 0.],
           [0., 0., 0., 1.],
           [1., 0., 0., 0.],
           [1., 0., 0., 0.],
           [1., 0., 0., 0.],
           [1., 0., 0., 0.]])

    For each donor node (represented by each row) the proportions should sum to
    one.

    >>> proportions.sum(axis=1)
    array([1., 1., 1., 1., 1., 1., 1., 1., 1.])

    For the second example we will use a Hexagonal Model Grid, a special type
    of Voroni Grid that has regularly spaced hexagonal cells. FlowDirectorMFD
    has multiple ways to partition flow based on slope. The default method is
    based on the slope angle. A secondary methods is to partion based on the
    square root of slope. This represents the solution to a steady kinematic
    wave.

    >>> from landlab import HexModelGrid
    >>> mg = HexModelGrid((5, 3))
    >>> _ = mg.add_field(
    ...     "topographic__elevation",
    ...     mg.node_x + numpy.round(mg.node_y),
    ...     at="node",
    ... )
    >>> fd = FlowDirectorMFD(
    ...     mg, "topographic__elevation", partition_method="square_root_of_slope"
    ... )
    >>> fd.surface_values
    array([1. , 2. , 3. ,
           1.5, 2.5, 3.5, 4.5,
           2. , 3. , 4. , 5. , 6. ,
           3.5, 4.5, 5.5, 6.5,
           4. , 5. , 6. ])
    >>> fd.run_one_step()
    >>> mg.at_node["flow__receiver_node"]
    array([[ 0, -1, -1, -1, -1, -1],
           [ 1, -1, -1, -1, -1, -1],
           [ 2, -1, -1, -1, -1, -1],
           [ 3, -1, -1, -1, -1, -1],
           [-1, -1, -1,  3,  0,  1],
           [-1, -1, -1,  4,  1,  2],
           [ 6, -1, -1, -1, -1, -1],
           [ 7, -1, -1, -1, -1, -1],
           [-1, -1, -1,  7,  3,  4],
           [-1, -1, -1,  8,  4,  5],
           [-1, -1, -1,  9,  5,  6],
           [11, -1, -1, -1, -1, -1],
           [12, -1, -1, -1, -1, -1],
           [-1, -1, 16, 12,  8,  9],
           [-1, -1, 17, 13,  9, 10],
           [15, -1, -1, -1, -1, -1],
           [16, -1, -1, -1, -1, -1],
           [17, -1, -1, -1, -1, -1],
           [18, -1, -1, -1, -1, -1]])
    >>> mg.at_node["flow__receiver_proportions"]
    array([[1. , 0. , 0. , 0. , 0. , 0. ],
           [1. , 0. , 0. , 0. , 0. , 0. ],
           [1. , 0. , 0. , 0. , 0. , 0. ],
           [1. , 0. , 0. , 0. , 0. , 0. ],
           [0. , 0. , 0. , 0.34108138,  0.41773767, 0.24118095],
           [0. , 0. , 0. , 0.34108138,  0.41773767, 0.24118095],
           [1. , 0. , 0. , 0. , 0. , 0. ],
           [1. , 0. , 0. , 0. , 0. , 0. ],
           [0. , 0. , 0. , 0.34108138,  0.41773767, 0.24118095],
           [0. , 0. , 0. , 0.34108138,  0.41773767, 0.24118095],
           [0. , 0. , 0. , 0.34108138,  0.41773767, 0.24118095],
           [1. , 0. , 0. , 0. , 0. , 0. ],
           [1. , 0. , 0. , 0. , 0. , 0. ],
           [0. , 0. , 0.19431571,  0.27480391,  0.33656468, 0.19431571],
           [0. , 0. , 0.19431571,  0.27480391,  0.33656468, 0.19431571],
           [1. , 0. , 0. , 0. , 0. , 0. ],
           [1. , 0. , 0. , 0. , 0. , 0. ],
           [1. , 0. , 0. , 0. , 0. , 0. ],
           [1. , 0. , 0. , 0. , 0. , 0. ]])
    >>> mg.at_node["flow__link_to_receiver_node"]
    array([[-1, -1, -1, -1, -1, -1],
           [-1, -1, -1, -1, -1, -1],
           [-1, -1, -1, -1, -1, -1],
           [-1, -1, -1, -1, -1, -1],
           [-1, -1, -1,  8,  3,  4],
           [-1, -1, -1,  9,  5,  6],
           [-1, -1, -1, -1, -1, -1],
           [-1, -1, -1, -1, -1, -1],
           [-1, -1, -1, 19, 12, 13],
           [-1, -1, -1, 20, 14, 15],
           [-1, -1, -1, 21, 16, 17],
           [-1, -1, -1, -1, -1, -1],
           [-1, -1, -1, -1, -1, -1],
           [-1, -1, 35, 31, 25, 26],
           [-1, -1, 37, 32, 27, 28],
           [-1, -1, -1, -1, -1, -1],
           [-1, -1, -1, -1, -1, -1],
           [-1, -1, -1, -1, -1, -1],
           [-1, -1, -1, -1, -1, -1]])
    >>> mg.at_node["topographic__steepest_slope"]
    array([[0. , 0. , 0. , 0. , 0. , 0. ],
           [0. , 0. , 0. , 0. , 0. , 0. ],
           [0. , 0. , 0. , 0. , 0. , 0. ],
           [0. , 0. , 0. , 0. , 0. , 0. ],
           [0. , 0. , 0. , 1. , 1.5, 0.5],
           [0. , 0. , 0. , 1. , 1.5, 0.5],
           [0. , 0. , 1. , 0. , 0. , 0. ],
           [0. , 0. , 0. , 0. , 0. , 0. ],
           [0. , 0. , 0. , 1. , 1.5, 0.5],
           [0. , 0. , 0. , 1. , 1.5, 0.5],
           [0. , 0. , 0. , 1. , 1.5, 0.5],
           [0. , 1. , 0. , 0. , 0. , 0. ],
           [0. , 0. , 0. , 0.5, 0. , 0. ],
           [0. , 0. , 0.5, 1. , 1.5, 0.5],
           [0. , 0. , 0.5, 1. , 1.5, 0.5],
           [0. , 1. , 1.5, 0. , 0. , 0. ],
           [0. , 0. , 0. , 0. , 0. , 0. ],
           [0. , 0. , 0.5, 0. , 0. , 0. ],
           [0. , 0.5, 0. , 0. , 0. , 0. ]])
    >>> mg.at_node["flow__sink_flag"].astype(int)
    array([1, 1, 1,
           1, 0, 0, 1,
           1, 0, 0, 0, 1,
           1, 0, 0, 1,
           1, 1, 1])

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    Freeman, T. (1991). Calculating catchment area with divergent flow based on
    a regular grid. Computers and Geosciences  17(3), 413 - 422.
    https://dx.doi.org/10.1016/0098-3004(91)90048-i

    Quinn, P., Beven, K., Chevallier, P., Planchon, O. (1991). The prediction
    of hillslope flow paths for distributed hydrological modelling using
    digital terrain models Hydrological Processes  5(1), 59-79.
    https://dx.doi.org/10.1002/hyp.3360050106

    """

    _name = "FlowDirectorMFD"

    _unit_agnostic = True

    _info = {
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__receiver_proportions": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of proportion of flow sent to each receiver.",
        },
        "flow__sink_flag": {
            "dtype": bool,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Boolean array, True at local lows",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__steepest_slope": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "The steepest *downhill* slope",
        },
    }

    def __init__(self, grid, surface="topographic__elevation", **kwargs):
        """
        Parameters
        ----------
        grid : ModelGrid
            A grid.
        surface : field name at node or array of length node, optional
            The surface to direct flow across, default is field at node:
            topographic__elevation.
        partition_method: string, optional
            Method for partitioning flow. Options include 'slope' (default) and
            'square_root_of_slope'.
        """
        # unpack kwargs:
        partition_method = kwargs.get("partition_method", "slope")
        diagonals = kwargs.get("diagonals", False)

        self._method = "MFD"

        self._is_Voroni = isinstance(grid, VoronoiDelaunayGrid)
        if self._is_Voroni:
            diagonals = False
        self._partition_method = partition_method
        self._diagonals = diagonals

        if self._is_Voroni is False and diagonals is False:
            self._max_receivers = 4
        if self._is_Voroni is False and diagonals is True:
            self._max_receivers = 8
        else:
            self._max_receivers = grid.adjacent_nodes_at_node.shape[1]

        super().__init__(grid, surface)
        self.updated_boundary_conditions()

    def updated_boundary_conditions(self):
        """Method to update FlowDirectorMFD when boundary conditions change.

        Call this if boundary conditions on the grid are updated after
        the component is instantiated.
        """
        self._active_links = self._grid.active_links
        self._activelink_tail = self._grid.node_at_link_tail[self._grid.active_links]
        self._activelink_head = self._grid.node_at_link_head[self._grid.active_links]

    def run_one_step(self):
        """Find flow directions and save to the model grid.

        run_one_step() checks for updated boundary conditions, calculates
        slopes on links, finds basself._surface_valuesel nodes based on the
        status at node, calculates flow directions, and saves results to the
        grid.

        An alternative to run_one_step() is direct_flow() which does the same
        things but also returns the receiver nodes not return values.
        """
        self.direct_flow()

    def direct_flow(self):
        """Find flow directions, save to the model grid, and return receivers.

        direct_flow() checks for updated boundary conditions, calculates
        slopes on links, finds basself._surface_valuesel nodes based on the status at node,
        calculates flow directions, saves results to the grid, and returns a
        at-node array  of receiver nodes. This array is stored in the grid at:
        grid['node']['flow__receiver_nodes']

        An alternative to direct_flow() is run_one_step() which does the same
        things but also returns a at-node array  of receiver nodes. This array
        is stored in the grid at:
        grid['node']['flow__receiver_nodes']
        """
        self._check_updated_bc()

        # step 1. Required inumpyuts for flow_directions_MFD
        # this is where diagonals are or are not included in
        # flow direction calculations

        # Option for no diagonals (default)
        if self._diagonals is False:
            neighbors_at_node = self._grid.adjacent_nodes_at_node
            links_at_node = self._grid.links_at_node
            active_link_dir_at_node = self._grid.active_link_dirs_at_node

            # this needs to be the gradient
            link_slope = self._grid.calc_grad_at_link(self._surface_values)

        # Option with diagonals.
        else:
            # concatenate the diagonal and orthogonal grid elements
            neighbors_at_node = numpy.hstack(
                (
                    self._grid.adjacent_nodes_at_node,
                    self._grid.diagonal_adjacent_nodes_at_node,
                )
            )

            active_link_dir_at_node = numpy.hstack(
                (
                    self._grid.active_link_dirs_at_node,
                    self._grid.active_diagonal_dirs_at_node,
                )
            )
            link_slope = self._grid.calc_grad_at_d8(self._surface_values)
            links_at_node = self._grid.d8s_at_node

        # Step 2. Find and save base level nodes.
        (baselevel_nodes,) = numpy.where(
            numpy.logical_or(
                self._grid.status_at_node == NodeStatus.FIXED_VALUE,
                self._grid.status_at_node == NodeStatus.FIXED_GRADIENT,
            )
        )

        # Calculate flow directions
        (
            self._receivers,
            self._proportions,
            slopes_to_receivers,
            steepest_slope,
            steepest_receiver,
            sink,
            receiver_links,
            steepest_link,
        ) = flow_direction_mfd.flow_directions_mfd(
            self._surface_values,
            neighbors_at_node,
            links_at_node,
            active_link_dir_at_node,
            link_slope,
            baselevel_nodes=baselevel_nodes,
            partition_method=self._partition_method,
        )

        # Save the four ouputs of this component.
        self._grid["node"]["flow__receiver_node"][:] = self._receivers
        self._grid["node"]["flow__receiver_proportions"][:] = self._proportions
        self._grid["node"]["topographic__steepest_slope"][:] = slopes_to_receivers
        self._grid["node"]["flow__link_to_receiver_node"][:] = receiver_links
        self._grid["node"]["flow__sink_flag"][:] = False
        self._grid["node"]["flow__sink_flag"][sink] = True

        return (self._receivers, self._proportions)


if __name__ == "__main__":  # pragma: no cover
    import doctest

    doctest.testmod()



================================================
File: flow_director/flow_director_steepest.py
================================================
#! /usr/env/python

"""
flow_director_steepest.py: provides the component FlowDirectorSteepest.

This components finds the steepest single-path steepest descent flow
directions. It is equivalent to D4 method in the special case of a raster grid
in that it does not consider diagonal links between nodes. For that capability,
use FlowDirectorD8.
"""

import numpy as np

from landlab import NodeStatus
from landlab import VoronoiDelaunayGrid
from landlab.components.flow_director import flow_direction_DN
from landlab.components.flow_director.flow_director_to_one import _FlowDirectorToOne


class FlowDirectorSteepest(_FlowDirectorToOne):
    """Single-path (steepest direction) flow direction without diagonals.

    This components finds the steepest single-path steepest descent flow
    directions. It is equivalent to D4 method in the special case of a raster
    grid in that it does not consider diagonal links between nodes. For that
    capability, use FlowDirectorD8.

    Stores as ModelGrid fields:

    -  Node array of receivers (nodes that receive flow), or ITS OWN ID if
       there is no receiver: *'flow__receiver_node'*
    -  Node array of steepest downhill slopes:
       *'topographic__steepest_slope'*
    -  Node array containing ID of link that leads from each node to its
       receiver, or grid.BAD_INDEX if no link:
       *'flow__link_to_receiver_node'*
    -  Boolean node array of all local lows: *'flow__sink_flag'*
    -  Link array identifing if flow goes with (1) or against (-1) the link
       direction: *'flow__link_direction'*

    The primary method of this class is :func:`run_one_step`.

    Examples
    --------

    This method works for both raster and irregular grids. First we will look
    at a raster example, and then an irregular example.

    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import FlowDirectorSteepest
    >>> mg = RasterModelGrid((3, 3), xy_spacing=(1, 1))
    >>> mg.set_closed_boundaries_at_grid_edges(True, True, True, False)
    >>> _ = mg.add_field(
    ...     "topographic__elevation",
    ...     mg.node_x + mg.node_y,
    ...     at="node",
    ... )
    >>> fd = FlowDirectorSteepest(mg, "topographic__elevation")
    >>> fd.surface_values
    array([0., 1., 2., 1., 2., 3., 2., 3., 4.])
    >>> fd.run_one_step()
    >>> mg.at_node["flow__receiver_node"]
    array([0, 1, 2, 3, 1, 5, 6, 7, 8])
    >>> mg.at_node["topographic__steepest_slope"]
    array([0., 0., 0., 0., 1., 0., 0., 0., 0.])
    >>> mg.at_node["flow__link_to_receiver_node"]
    array([-1, -1, -1, -1,  3, -1, -1, -1, -1])
    >>> mg.at_node["flow__sink_flag"].astype(int)
    array([1, 1, 1, 1, 0, 1, 1, 1, 1])
    >>> mg_2 = RasterModelGrid((5, 4), xy_spacing=(1, 1))
    >>> topographic__elevation = [
    ...     [0.0, 0.0, 0.0, 0.0],
    ...     [0.0, 21.0, 10.0, 0.0],
    ...     [0.0, 31.0, 20.0, 0.0],
    ...     [0.0, 32.0, 30.0, 0.0],
    ...     [0.0, 0.0, 0.0, 0.0],
    ... ]
    >>> _ = mg_2.add_field(
    ...     "topographic__elevation",
    ...     topographic__elevation,
    ...     at="node",
    ... )
    >>> mg_2.set_closed_boundaries_at_grid_edges(True, True, True, False)
    >>> fd_2 = FlowDirectorSteepest(mg_2)
    >>> fd_2.run_one_step()
    >>> mg_2.at_node["flow__receiver_node"].reshape(mg_2.shape)
    array([[ 0,  1,  2,  3],
           [ 4,  1,  2,  7],
           [ 8, 10,  6, 11],
           [12, 14, 10, 15],
           [16, 17, 18, 19]])

    And the at-link field ``'flow__link_direction'`` indicates if the flow along
    the link is with or against the direction indicated by ``'link_dirs_at_node'``
    (from tail node to head node).

    >>> mg_2.at_link["flow__link_direction"]
    array([ 0,  0,  0,  0, -1, -1,  0,  0,  0,  0,  0,  0, -1,  0,  0,  1,  0,
            0,  0, -1,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int8)

    This indicates that flow on links 4, 5, 12, and 19 goes against the
    topologic ordering -- that is that flow goes from head node to tail node --
    and that flow goes with the topologic ordering on links 15 and 22. All other
    links have no flow on them.

    The FlowDirectorSteepest attribute ``flow_link_direction_at_node`` indicates
    the link flow direction (with or against topology directions) for all links
    at node. The ordering of links at node mirrors the grid attribute
    ``links_at_node``.

    >>> fd_2.flow_link_direction_at_node()
    array([[ 0,  0,  0,  0],
           [ 0, -1,  0,  0],
           [ 0, -1,  0,  0],
           [ 0,  0,  0,  0],
           [ 0,  0,  0,  0],
           [ 0,  0,  0, -1],
           [ 0, -1,  0, -1],
           [ 0,  0,  0,  0],
           [ 0,  0,  0,  0],
           [ 1,  0,  0,  0],
           [ 0, -1,  1, -1],
           [ 0,  0,  0,  0],
           [ 0,  0,  0,  0],
           [ 1,  0,  0,  0],
           [ 0,  0,  1, -1],
           [ 0,  0,  0,  0],
           [ 0,  0,  0,  0],
           [ 0,  0,  0,  0],
           [ 0,  0,  0,  0],
           [ 0,  0,  0,  0]], dtype=int8)

    For example, this indicates that node 10 has flow going along three links
    that are attached to it. The link to the East has no flow, the link to the
    North has flow going against the topologic direction, the link to the West
    has flow going with the topologic direction, and the link to the South has
    flow going against the topologic direction.

    In many use cases, one might want to know which links are bringing flow into
    or out of the node. The flow director attribute ``flow_link_incoming_at_node``
    provides this information. Here -1 means that flow is outgoing from the node
    and 1 means it is incoming.

    >>> fd_2.flow_link_incoming_at_node()
    array([[ 0,  0,  0,  0],
           [ 0,  1,  0,  0],
           [ 0,  1,  0,  0],
           [ 0,  0,  0,  0],
           [ 0,  0,  0,  0],
           [ 0,  0,  0, -1],
           [ 0,  1,  0, -1],
           [ 0,  0,  0,  0],
           [ 0,  0,  0,  0],
           [-1,  0,  0,  0],
           [ 0,  1,  1, -1],
           [ 0,  0,  0,  0],
           [ 0,  0,  0,  0],
           [-1,  0,  0,  0],
           [ 0,  0,  1, -1],
           [ 0,  0,  0,  0],
           [ 0,  0,  0,  0],
           [ 0,  0,  0,  0],
           [ 0,  0,  0,  0],
           [ 0,  0,  0,  0]], dtype=int8)

    So if one wanted to identify the source nodes at node, you would do the
    following:

    >>> np.where(
    ...     fd_2.flow_link_incoming_at_node() == 1, mg_2.adjacent_nodes_at_node, -1
    ... )
    array([[-1, -1, -1, -1],
           [-1,  5, -1, -1],
           [-1,  6, -1, -1],
           [-1, -1, -1, -1],
           [-1, -1, -1, -1],
           [-1, -1, -1, -1],
           [-1, 10, -1, -1],
           [-1, -1, -1, -1],
           [-1, -1, -1, -1],
           [-1, -1, -1, -1],
           [-1, 14,  9, -1],
           [-1, -1, -1, -1],
           [-1, -1, -1, -1],
           [-1, -1, -1, -1],
           [-1, -1, 13, -1],
           [-1, -1, -1, -1],
           [-1, -1, -1, -1],
           [-1, -1, -1, -1],
           [-1, -1, -1, -1],
           [-1, -1, -1, -1]])

    The flow directors also have the ability to return the flow receiver nodes

    >>> receiver = fd.direct_flow()
    >>> receiver
    array([0, 1, 2,
           3, 1, 5,
           6, 7, 8])

    For the second example we will use a Hexagonal Model Grid, a special type
    of Voroni Grid that has regularly spaced hexagonal cells.

    >>> from landlab import HexModelGrid
    >>> mg = HexModelGrid((5, 3))
    >>> _ = mg.add_field(
    ...     "topographic__elevation",
    ...     mg.node_x + np.round(mg.node_y),
    ...     at="node",
    ... )
    >>> fd = FlowDirectorSteepest(mg, "topographic__elevation")
    >>> fd.surface_values
    array([1. ,  2. ,  3. ,
       1.5,  2.5,  3.5,  4.5,
     2. ,  3. ,  4. ,  5. ,  6. ,
       3.5,  4.5,  5.5,  6.5,
           4. ,  5. ,  6. ])
    >>> fd.run_one_step()
    >>> mg.at_node["flow__receiver_node"]
    array([ 0,  1,  2,
          3,  0,  1,  6,
        7,  3,  4,  5,  11,
          12,  8,  9, 15,
            16, 17, 18])
    >>> mg.at_node["topographic__steepest_slope"]
    array([0. ,  0. ,  0. ,
       0. ,  1.5,  1.5,   0. ,
     0. ,  1.5,  1.5,  1.5,  0. ,
       0. ,  1.5,  1.5,  0. ,
           0. ,  0. ,  0. ])
    >>> mg.at_node["flow__link_to_receiver_node"]
    array([-1, -1, -1,
         -1,  3,  5, -1,
       -1, 12, 14, 16, -1,
         -1, 25, 27, -1,
           -1, -1, -1])
    >>> mg.at_node["flow__sink_flag"].astype(int)
    array([1, 1, 1,
          1, 0, 0, 1,
         1, 0, 0, 0, 1,
          1, 0, 0, 1,
            1, 1, 1])
    >>> receiver = fd.direct_flow()
    >>> receiver
    array([ 0,  1,  2,
          3,  0,  1,  6,
        7,  3,  4,  5, 11,
         12,  8,  9, 15,
          16, 17, 18])

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    None Listed

    """

    _name = "FlowDirectorSteepest"

    _unit_agnostic = True

    _info = {
        "flow__link_direction": {
            "dtype": np.int8,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "link",
            "doc": (
                "Direction of flow on link. A value of -1 indicates that "
                "water flow goes from head node to tail node, while a value "
                "of 1 indicates that water flow goes from tail node to head node."
            ),
        },
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__sink_flag": {
            "dtype": bool,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Boolean array, True at local lows",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__steepest_slope": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "The steepest *downhill* slope",
        },
    }

    def __init__(self, grid, surface="topographic__elevation"):
        """
        Parameters
        ----------
        grid : ModelGrid
            A grid.
        surface : field name at node or array of length node, optional
            The surface to direct flow across, default is field at node:
            topographic__elevation,.
        """
        self._method = "D4"
        super().__init__(grid, surface)
        self._is_Voroni = isinstance(self._grid, VoronoiDelaunayGrid)

        # get 'flow__link_direction' field
        self._flow_link_direction = grid.at_link["flow__link_direction"]

        self.updated_boundary_conditions()

    def updated_boundary_conditions(self):
        """Method to update FlowDirectorSteepest when boundary conditions
        change.

        Call this if boundary conditions on the grid are updated after
        the component is instantiated.
        """
        self._active_links = self._grid.active_links
        self._activelink_tail = self._grid.node_at_link_tail[self._grid.active_links]
        self._activelink_head = self._grid.node_at_link_head[self._grid.active_links]

    def run_one_step(self):
        """Find flow directions and save to the model grid.

        run_one_step() checks for updated boundary conditions, calculates
        slopes on links, finds baselevel nodes based on the status at node,
        calculates flow directions, and saves results to the grid.

        An alternative to direct_flow() is run_one_step() which does the same
        things but also returns the receiver nodes not return values.
        """
        self.direct_flow()

    def direct_flow(self):
        """Find flow directions, save to the model grid, and return receivers.

        direct_flow() checks for updated boundary conditions, calculates
        slopes on links, finds baselevel nodes based on the status at node,
        calculates flow directions, saves results to the grid, and returns a
        at-node array  of receiver nodes. This array is stored in the grid at:
        grid['node']['flow__receiver_node']

        An alternative to direct_flow() is run_one_step() which does the same
        things but also returns a at-node array  of receiver nodes. This array
        is stored in the grid at:
        grid['node']['flow__receiver_node']
        """
        self._check_updated_bc()

        # update the surface, if it was provided as a model grid field.
        self._changed_surface()

        # step 1. Calculate link slopes at active links only.
        all_grads = -self._grid.calc_grad_at_link(self._surface_values)
        link_slope = all_grads[self._grid.active_links]

        # Step 2. Find and save base level nodes.
        (baselevel_nodes,) = np.where(
            np.logical_or(
                self._grid.status_at_node == NodeStatus.FIXED_VALUE,
                self._grid.status_at_node == NodeStatus.FIXED_GRADIENT,
            )
        )

        # Calculate flow directions
        receiver, steepest_slope, sink, recvr_link = flow_direction_DN.flow_directions(
            self._surface_values,
            self._active_links,
            self._activelink_tail,
            self._activelink_head,
            link_slope,
            grid=self._grid,
            baselevel_nodes=baselevel_nodes,
        )

        # Save the four ouputs of this component.
        self._grid["node"]["flow__receiver_node"][:] = receiver
        self._grid["node"]["topographic__steepest_slope"][:] = steepest_slope
        self._grid["node"]["flow__link_to_receiver_node"][:] = recvr_link
        self._grid["node"]["flow__sink_flag"][:] = np.zeros_like(receiver, dtype=bool)
        self._grid["node"]["flow__sink_flag"][sink] = True

        # determine link directions
        self._determine_link_directions()

        return receiver

    def _determine_link_directions(self):
        """Determine link directions and set flow_link_direction field.

        This routine is slightly different between the route-to-one and
        route-to-many methods.

        It works when DepressionFinderAndRouter is run.
        """
        # start by re-setting all links to zero.
        self._flow_link_direction[:] = 0

        # identify where flow is active on links
        is_active_flow_link = self._links_to_receiver != self._grid.BAD_INDEX

        # make an array that says which link ID is active
        active_flow_links = self._links_to_receiver[is_active_flow_link]

        # for each of those links, the position is the upstream node
        upstream_node_of_active_flow_link = np.where(is_active_flow_link)[0]

        # get the head node
        head_node_at_active_flow_link = self._grid.node_at_link_head[active_flow_links]

        # if head node is upstream node = -1, else 1
        self._flow_link_direction[
            active_flow_links[
                head_node_at_active_flow_link == upstream_node_of_active_flow_link
            ]
        ] = -1
        self._flow_link_direction[
            active_flow_links[
                head_node_at_active_flow_link != upstream_node_of_active_flow_link
            ]
        ] = 1

    def flow_link_direction_at_node(self):
        """Return array of flow link direction at node.

        This property mirrors links_at_node and indicates the relationship
        between the flow direction (determined based on the elevation of nodes)
        and the topologic link direction (in which the head and tail nodes are
        defined based on relative position in x-y space).

        It has the shape (number of nodes, maximum number of links at node).

        Recall that the standard landlab link direction goes from the tail node
        to the head node.

        A value of zero indicates that the link does not exist or is not
        active.

        A value of -1 indicates that water flow based on
        ``flow__link_to_receiver_node`` goes from head node to tail node, while
        a value of 1 indicates that water flow goes from tail node to head
        node.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowDirectorSteepest
        >>> mg = RasterModelGrid((3, 3))
        >>> mg.set_closed_boundaries_at_grid_edges(True, True, True, False)
        >>> _ = mg.add_field(
        ...     "topographic__elevation",
        ...     mg.node_x + mg.node_y,
        ...     at="node",
        ... )
        >>> fd = FlowDirectorSteepest(mg, "topographic__elevation")
        >>> fd.run_one_step()
        >>> fd.flow_link_direction_at_node()
        array([[ 0,  0,  0,  0],
               [ 0, -1,  0,  0],
               [ 0,  0,  0,  0],
               [ 0,  0,  0,  0],
               [ 0,  0,  0, -1],
               [ 0,  0,  0,  0],
               [ 0,  0,  0,  0],
               [ 0,  0,  0,  0],
               [ 0,  0,  0,  0]], dtype=int8)

        This method will be updated when the DepressionFinderAndRouter is run.

        First, without DepressionFinderAndRouter:

        >>> from landlab.components import FlowAccumulator
        >>> mg1 = RasterModelGrid((5, 5))
        >>> z1 = mg1.add_field(
        ...     "topographic__elevation",
        ...     mg1.x_of_node + 2 * mg1.y_of_node,
        ...     at="node",
        ... )
        >>> z1[12] -= 5
        >>> mg1.set_closed_boundaries_at_grid_edges(True, True, True, False)
        >>> fa1 = FlowAccumulator(mg1, flow_director="Steepest")
        >>> fa1.run_one_step()
        >>> fa1.flow_director.links_to_receiver
        array([-1, -1, -1, -1, -1,
               -1,  5, 15,  7, -1,
               -1, 19, -1, 20, -1,
               -1, 23, 24, 25, -1,
               -1, -1, -1, -1, -1])
        >>> fa1.flow_director.flow_link_direction_at_node()
        array([[ 0,  0,  0,  0],
               [ 0, -1,  0,  0],
               [ 0,  0,  0,  0],
               [ 0, -1,  0,  0],
               [ 0,  0,  0,  0],
               [ 0,  0,  0,  0],
               [ 0,  0,  0, -1],
               [ 0,  1,  0,  0],
               [ 0,  0,  0, -1],
               [ 0,  0,  0,  0],
               [ 0,  0,  0,  0],
               [ 1, -1,  0,  0],
               [-1, -1,  1,  1],
               [ 0, -1, -1,  0],
               [ 0,  0,  0,  0],
               [ 0,  0,  0,  0],
               [ 0,  0,  0, -1],
               [ 0,  0,  0, -1],
               [ 0,  0,  0, -1],
               [ 0,  0,  0,  0],
               [ 0,  0,  0,  0],
               [ 0,  0,  0,  0],
               [ 0,  0,  0,  0],
               [ 0,  0,  0,  0],
               [ 0,  0,  0,  0]], dtype=int8)

        Next with DepressionFinderAndRouter:

        >>> mg2 = RasterModelGrid((5, 5))
        >>> z2 = mg2.add_field(
        ...     "topographic__elevation",
        ...     mg2.x_of_node + 2 * mg2.y_of_node,
        ...     at="node",
        ... )
        >>> z2[12] -= 5
        >>> mg2.set_closed_boundaries_at_grid_edges(True, True, True, False)
        >>> fa2 = FlowAccumulator(
        ...     mg2,
        ...     flow_director="Steepest",
        ...     depression_finder="DepressionFinderAndRouter",
        ...     routing="D4",
        ... )
        >>> fa2.run_one_step()
        >>> fa2.flow_director.links_to_receiver
        array([-1, -1, -1, -1, -1,
               -1,  5,  6,  7, -1,
               -1, 19, 15, 20, -1,
               -1, 23, 24, 25, -1,
               -1, -1, -1, -1, -1])
        >>> fa2.flow_director.flow_link_direction_at_node()
        array([[ 0,  0,  0,  0],
               [ 0, -1,  0,  0],
               [ 0, -1,  0,  0],
               [ 0, -1,  0,  0],
               [ 0,  0,  0,  0],
               [ 0,  0,  0,  0],
               [ 0,  0,  0, -1],
               [ 0, -1,  0, -1],
               [ 0,  0,  0, -1],
               [ 0,  0,  0,  0],
               [ 0,  0,  0,  0],
               [ 1, -1,  0,  0],
               [-1, -1,  1, -1],
               [ 0, -1, -1,  0],
               [ 0,  0,  0,  0],
               [ 0,  0,  0,  0],
               [ 0,  0,  0, -1],
               [ 0,  0,  0, -1],
               [ 0,  0,  0, -1],
               [ 0,  0,  0,  0],
               [ 0,  0,  0,  0],
               [ 0,  0,  0,  0],
               [ 0,  0,  0,  0],
               [ 0,  0,  0,  0],
               [ 0,  0,  0,  0]], dtype=int8)
        """
        flow_link_direction_at_node = self._flow_link_direction[
            self._grid.links_at_node
        ]
        flow_to_bad = self._grid.links_at_node == self._grid.BAD_INDEX
        flow_link_direction_at_node[flow_to_bad] = 0

        return flow_link_direction_at_node

    def flow_link_incoming_at_node(self):
        """Return array that mirrors links at node and indicates incoming flow.

        This array has the shape
        (number of nodes, maximum number of links at node).

        Incoming flow is indicated as 1 and outgoing as -1. 0 indicates
        that no flow moves along the link or that the link does not exist.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowDirectorSteepest
        >>> mg = RasterModelGrid((3, 3))
        >>> mg.set_closed_boundaries_at_grid_edges(True, True, True, False)
        >>> _ = mg.add_field(
        ...     "topographic__elevation",
        ...     mg.node_x + mg.node_y,
        ...     at="node",
        ... )
        >>> fd = FlowDirectorSteepest(mg, "topographic__elevation")
        >>> fd.run_one_step()
        >>> fd.flow_link_incoming_at_node()
        array([[ 0,  0,  0,  0],
               [ 0,  1,  0,  0],
               [ 0,  0,  0,  0],
               [ 0,  0,  0,  0],
               [ 0,  0,  0, -1],
               [ 0,  0,  0,  0],
               [ 0,  0,  0,  0],
               [ 0,  0,  0,  0],
               [ 0,  0,  0,  0]], dtype=int8)
        """

        incoming_at_node = (
            self.flow_link_direction_at_node() * self._grid.link_dirs_at_node
        )
        return incoming_at_node

    @property
    def flow_link_direction(self):
        """Return array of flow link direction.

        This property indicates the relationship between the flow direction
        (determined based on the elevation of nodes) and the topologic link
        direction (in which the head and tail nodes are defined based on
        relative position in x-y space).

        It has the shape (number_of_links,).

        Recall that the standard landlab link direction goes from the tail node
        to the head node.

        A value of zero indicates that the link does not exist or is not
        active.

        A value of -1 indicates that water flow based on
        ``flow__link_to_receiver_node`` goes from head node to tail node, while
        a value of 1 indicates that water flow goes from tail node to head
        node.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowDirectorSteepest
        >>> mg = RasterModelGrid((3, 3))
        >>> mg.set_closed_boundaries_at_grid_edges(True, True, True, False)
        >>> _ = mg.add_field(
        ...     "topographic__elevation",
        ...     mg.node_x + mg.node_y,
        ...     at="node",
        ... )
        >>> fd = FlowDirectorSteepest(mg, "topographic__elevation")
        >>> fd.run_one_step()
        >>> fd.flow_link_direction
        array([ 0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int8)
        """
        return self._flow_link_direction

    def upstream_node_at_link(self):
        """At-link array of the upstream node based on flow direction.

        BAD_INDEX_VALUE is given if no upstream node is defined.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowDirectorSteepest
        >>> mg = RasterModelGrid((3, 3))
        >>> mg.set_closed_boundaries_at_grid_edges(True, True, True, False)
        >>> _ = mg.add_field(
        ...     "topographic__elevation",
        ...     mg.node_x + mg.node_y,
        ...     at="node",
        ... )
        >>> fd = FlowDirectorSteepest(mg, "topographic__elevation")
        >>> fd.run_one_step()
        >>> fd.upstream_node_at_link()
        array([-1, -1, -1,  4, -1, -1, -1, -1, -1, -1, -1, -1])
        """
        out = -1 * self._grid.ones(at="link", dtype=int)
        out[self._flow_link_direction == 1] = self._grid.node_at_link_tail[
            self._flow_link_direction == 1
        ]
        out[self._flow_link_direction == -1] = self._grid.node_at_link_head[
            self._flow_link_direction == -1
        ]
        return out

    def downstream_node_at_link(self):
        """At-link array of the downstream node based on flow direction.

        BAD_INDEX_VALUE is given if no downstream node is defined.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowDirectorSteepest
        >>> mg = RasterModelGrid((3, 3))
        >>> mg.set_closed_boundaries_at_grid_edges(True, True, True, False)
        >>> _ = mg.add_field(
        ...     "topographic__elevation",
        ...     mg.node_x + mg.node_y,
        ...     at="node",
        ... )
        >>> fd = FlowDirectorSteepest(mg, "topographic__elevation")
        >>> fd.run_one_step()
        >>> fd.downstream_node_at_link()
        array([-1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1])
        """
        out = -1 * self._grid.ones(at="link", dtype=int)
        out[self._flow_link_direction == 1] = self._grid.node_at_link_head[
            self._flow_link_direction == 1
        ]
        out[self._flow_link_direction == -1] = self._grid.node_at_link_tail[
            self._flow_link_direction == -1
        ]
        return out


if __name__ == "__main__":  # pragma: no cover
    import doctest

    doctest.testmod()



================================================
File: flow_director/flow_director_to_many.py
================================================
#! /usr/env/python

"""flow_director_to_many.py provides a private class to help create
FlowDirectors.

Provides the _FlowDirectorToMany component which makes sure all model
grid fields are set up correctly.
"""
import numpy as np

from landlab.components.flow_director.flow_director import _FlowDirector


class _FlowDirectorToMany(_FlowDirector):
    """Private class for creating components to calculate flow directions.

    This class is not meant to be used directly in modeling efforts. It
    inherits from the _FlowDirector class and builds on it to provide the
    functionality that all flow direction calculators need if they direct flow
    only to multiple nodes, as in  D infinity or MFD direction finding. It
    exists in contrast to the other intermediate flow director class
    _FlowDirectorToOne which provides equivalent functionality for flow
    direction algorithms such as D8 or steepest descent which directs flow only
    to one other node. As the primary difference between these two methods is
    the names of the fields they create and use, the primary function of this
    class is to create model grid fields.

    The primary method of this class, :func:`run_one_step` is not implemented.

    Parameters
    ----------
    grid : ModelGrid
        A grid.
    surface : field name at node or array of length node
        The surface to direct flow across.

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> from landlab.components.flow_director.flow_director_to_many import (
    ...     _FlowDirectorToMany,
    ... )
    >>> mg = RasterModelGrid((3, 3), xy_spacing=(1, 1))
    >>> mg.set_closed_boundaries_at_grid_edges(True, True, True, False)
    >>> _ = mg.add_field(
    ...     "topographic__elevation",
    ...     mg.node_x + mg.node_y,
    ...     at="node",
    ... )
    >>> fd = _FlowDirectorToMany(mg, "topographic__elevation")
    >>> fd.surface_values
    array([0., 1., 2., 1., 2., 3., 2., 3., 4.])
    >>> sorted(list(mg.at_node.keys()))
    ['flow__link_to_receiver_node', 'flow__receiver_node',
     'flow__receiver_proportions', 'flow__sink_flag', 'topographic__elevation',
     'topographic__steepest_slope']
    """

    _name = "FlowDirectorToMany"

    _unit_agnostic = True

    _info = {
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__receiver_proportions": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of proportion of flow sent to each receiver.",
        },
        "flow__sink_flag": {
            "dtype": bool,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Boolean array, True at local lows",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__steepest_slope": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "The steepest *downhill* slope",
        },
    }

    _max_receivers = 2

    def __init__(self, grid, surface):
        """Initialize the _FlowDirectorToMany class."""
        # run init for the inherited class
        super().__init__(grid, surface)
        self._to_n_receivers = "many"

        # set the number of recievers, proportions, and receiver links with the
        # right size.
        self.initialize_output_fields(values_per_element=self._max_receivers)
        self._receivers = grid.at_node["flow__receiver_node"]
        if np.all(self._receivers == 0):
            self._receivers.fill(self._grid.BAD_INDEX)

        self._receiver_links = grid.at_node["flow__link_to_receiver_node"]
        if np.all(self._receiver_links == 0):
            self._receiver_links.fill(self._grid.BAD_INDEX)

        self._proportions = grid.at_node["flow__receiver_proportions"]
        self._steepest_slope = grid.at_node["topographic__steepest_slope"]

    def run_one_step(self):
        """run_one_step is not implemented for this component."""
        raise NotImplementedError("run_one_step()")

    @property
    def proportions_of_flow(self):
        """Return the proportions of flow going to recievers."""
        return self._grid["node"]["flow__receiver_proportions"]


if __name__ == "__main__":  # pragma: no cover
    import doctest

    doctest.testmod()



================================================
File: flow_director/flow_director_to_one.py
================================================
#! /usr/env/python

"""flow_director_to_one.py provides a private class to help create
FlowDirectors.

Provides the _FlowDirectorToOne component which makes sure all model
grid fields are set up correctly.
"""
import numpy as np

from landlab.components.flow_director.flow_director import _FlowDirector


class _FlowDirectorToOne(_FlowDirector):
    """Private class for creating components to calculate flow directions.

    This class is not meant to be used directly in modeling efforts. It
    inherits from the _FlowDirector class and builds on it to provide the
    functionality that all flow direction calculators need if they direct flow
    only to one nodes, as in steepest descent or D8 direction finding. It
    exists in contrast to the other intermediate flow director class
    _FlowDirectorToMany which provides equivalent functionality for flow
    direction algorithms such as D infinity or D trig that route flow from one
    cell to multiple nodes. As the primary difference between these two methods
    is the names of the fields they create and use, the primary function of
    this class is to create model grid fields.

    Specifically, it stores as ModelGrid fields:

    -  Node array of receivers (nodes that receive flow), or ITS OWN ID if
       there is no receiver: *'flow__receiver_node'*
    -  Node array of steepest downhill slopes:
       *'topographic__steepest_slope'*
    -  Node array containing ID of link that leads from each node to its
       receiver, or BAD_INDEX_VALUE if no link:
       *'flow__link_to_receiver_node'*
    -  Boolean node array of all local lows: *'flow__sink_flag'*

    The primary method of this class, :func:`run_one_step` is not implemented.

    Parameters
    ----------
    grid : ModelGrid
        A grid.
    surface : field name at node or array of length node
        The surface to direct flow across.

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> from landlab.components.flow_director.flow_director_to_one import (
    ...     _FlowDirectorToOne,
    ... )
    >>> mg = RasterModelGrid((3, 3), xy_spacing=(1, 1))
    >>> mg.set_closed_boundaries_at_grid_edges(True, True, True, False)
    >>> _ = mg.add_field(
    ...     "topographic__elevation",
    ...     mg.node_x + mg.node_y,
    ...     at="node",
    ... )
    >>> fd = _FlowDirectorToOne(mg, "topographic__elevation")
    >>> fd.surface_values
    array([0., 1., 2., 1., 2., 3., 2., 3., 4.])
    >>> sorted(list(mg.at_node.keys()))
    ['flow__link_to_receiver_node',
     'flow__receiver_node',
     'flow__sink_flag',
     'topographic__elevation',
     'topographic__steepest_slope']
    """

    _name = "FlowDirectorToOne"

    _unit_agnostic = True

    _info = {
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__sink_flag": {
            "dtype": bool,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Boolean array, True at local lows",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__steepest_slope": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "The steepest *downhill* slope",
        },
    }

    def __init__(self, grid, surface):
        """Initialize the _FlowDirectorTo_One class."""
        # run init for the inherited class

        super().__init__(grid, surface)
        self.initialize_output_fields()

        self._to_n_receivers = "one"
        # initialize new fields
        self._steepest_slope = grid.at_node["topographic__steepest_slope"]

        self._links_to_receiver = grid.at_node["flow__link_to_receiver_node"]
        if np.all(self._links_to_receiver == 0):
            self._links_to_receiver.fill(self._grid.BAD_INDEX)

        self._receiver = grid.at_node["flow__receiver_node"]
        if np.all(self._receiver == 0):
            self._receiver.fill(self._grid.BAD_INDEX)

    def run_one_step(self):
        """run_one_step is not implemented for this component."""
        raise NotImplementedError("run_one_step()")

    # set properties. These are the same for all DirectToOne Directors
    # Number of Node

    @property
    def links_to_receiver(self):
        """ID of link downstream of each node, which carries the discharge."""
        return self._links_to_receiver

    @property
    def node_receiving_flow(self):
        """Return the node id of the node receiving flow.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowDirectorSteepest
        >>> mg = RasterModelGrid((3, 3))
        >>> mg.set_closed_boundaries_at_grid_edges(True, True, True, False)
        >>> _ = mg.add_field(
        ...     "topographic__elevation",
        ...     mg.node_x + mg.node_y,
        ...     at="node",
        ... )
        >>> fd = FlowDirectorSteepest(mg, "topographic__elevation")
        >>> fd.run_one_step()
        >>> fd.node_receiving_flow
        array([0, 1, 2,
               3, 1, 5,
               6, 7, 8])
        """
        return self._grid["node"]["flow__receiver_node"]


if __name__ == "__main__":  # pragma: no cover
    import doctest

    doctest.testmod()



================================================
File: flow_router/__init__.py
================================================



================================================
File: flow_router/ext/__init__.py
================================================



================================================
File: flow_router/ext/single_flow/__init__.py
================================================



================================================
File: flow_router/ext/single_flow/priority_routing/__init__.py
================================================



================================================
File: flow_router/ext/single_flow/priority_routing/_priority_queue.hpp
================================================
// C++ Header that defines the priority queue used in the algorithms of priority routing

// priority_queue
#include <queue>
// pair
#include <utility>
// function
#include <functional>
using namespace std;

using _priority_queue = std::priority_queue<std::pair<long,double>,std::vector<std::pair<long,double> >,std::function<bool(std::pair<long,double>,std::pair<long,double>)> >;



================================================
File: flow_router/ext/single_flow/priority_routing/_test_breach_c.pxd
================================================
# distutils: language = c++
import numpy as np

cimport cython
cimport numpy as cnp
from libcpp cimport bool
from libcpp.pair cimport pair


cdef extern from "_priority_queue.hpp" nogil:
    cdef cppclass _priority_queue:
        _priority_queue(...) except +
        void push(pair[cnp.int64_t, cnp.float_t])
        pair[cnp.int64_t, cnp.float_t] top() except +
        void pop()
        bool empty()
        cnp.int64_t size()



================================================
File: flow_router/ext/single_flow/priority_routing/_test_breach_c.pyx
================================================
# distutils: define_macros=NPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION
# distutils: extra_compile_args = -std=c++11
# distutils: extra_link_args = -std=c++11

# NB: apparently not possible to add language: C++ in this file
# because of the extracompile -std=c++11 (necessary to understand the
# priorityqueue template. To be compiled in C++,
# must add a .pxd file with the instruction # distutils: language = c++

import numpy as np

cimport numpy as cnp
from libcpp cimport bool
from libcpp.pair cimport pair

from numpy.testing import assert_array_almost_equal
from numpy.testing import assert_array_equal

from landlab.components.flow_router.ext.single_flow.priority_routing cimport (
    breach as breach_c,
)

import landlab.components.flow_router.ext.single_flow.priority_routing.breach as breach


cdef extern from "_priority_queue.hpp" nogil:
    cdef cppclass _priority_queue:
        _priority_queue(...) except +
        void push(pair[cnp.int64_t, cnp.float_t])
        pair[cnp.int64_t, cnp.float_t] top() except +
        void pop()
        bool empty()
        cnp.int64_t size()


def test_priority_queue():
    cdef:
        pair[cnp.int64_t, cnp.float_t] a = pair[cnp.int64_t, cnp.float_t](0, 1045.3)
        pair[cnp.int64_t, cnp.float_t] b = pair[cnp.int64_t, cnp.float_t](1, 536.3)
        pair[cnp.int64_t, cnp.float_t] c = pair[cnp.int64_t, cnp.float_t](2, 2034.12)
        _priority_queue to_do = _priority_queue(breach_c._compare_second)

    assert to_do.empty() is True
    to_do.push(a)
    to_do.push(b)
    assert to_do.empty() is False
    assert to_do.top() == b
    to_do.push(c)
    to_do.pop()
    assert to_do.top() == a


def test_init_flow_direction_queues():
    # on a grid of 7 nodes
    cdef:
        cnp.int64_t nodes_n = 7
        cnp.int64_t [:] base_level_nodes = np.array([0, 2])
        cnp.int64_t [:] closed_nodes = np.array([5])
        cnp.float_t [:] z = np.array([4.5, 3.2, 6.7, 13.2, 5.6, 100.3, 45.32])
        _priority_queue to_do = _priority_queue(breach_c._compare_second)
        cnp.int64_t [:] receivers = -1 * np.ones(nodes_n, dtype=int)
        cnp.int64_t [:] outlet_nodes = -1 * np.ones(nodes_n, dtype=int)
        cnp.int64_t [:] done = np.zeros(nodes_n, dtype=int)
        cnp.int64_t done_n = 0
    breach_c._init_flow_direction_queues(
        base_level_nodes, closed_nodes, z, to_do, receivers, outlet_nodes, done, &done_n
    )
    assert nodes_n == 7
    assert_array_equal(base_level_nodes, np.array([0, 2]))
    assert_array_equal(closed_nodes, np.array([5]))
    assert_array_almost_equal(z, np.array([4.5, 3.2, 6.7, 13.2, 5.6, 100.3, 45.32]))
    assert_array_equal(receivers, np.array([0, -1, 2, -1, -1, 5, -1]))
    assert_array_equal(done, np.array([1, 0, 1, 0, 0, 1, 0]))
    assert done_n == 3


def test_set_flooded_and_outlet():
    # on a grid of 5 nodes
    cdef:
        cnp.int64_t donor_id = 2, bad_index = -1, flooded_status = 3
        cnp.float_t min_elevation_relative_diff = 1e-2
        cnp.float_t [:] z = np.array([0.1, 67.1, 42.1, 70.3, 34.5])
        cnp.int64_t [:] receivers = np.array([0, 0, 1, -1, -1])
        cnp.int64_t [:] outlet_nodes = np.array([0, 0, -1, -1, -1])
        cnp.int64_t [:] depression_outlet_nodes = np.array([-1, -1, -1, -1, -1])
        cnp.int64_t [:] flooded_nodes = np.zeros(5, dtype=int)
        cnp.float_t [:] depression_depths = np.zeros(5, dtype=float)
        cnp.float_t [:] depression_free_elevations = (
            np.array([0.1, 67.1, 42.1, 70.3, 34.5])
        )

    breach_c._set_flooded_and_outlet(
        donor_id,
        z,
        receivers,
        outlet_nodes,
        depression_outlet_nodes,
        flooded_nodes,
        depression_depths,
        depression_free_elevations,
        flooded_status,
        bad_index,
        min_elevation_relative_diff,
    )

    assert donor_id == 2
    assert_array_almost_equal(z, np.array([0.1, 67.1, 42.1, 70.3, 34.5]))
    assert_array_equal(receivers, np.array([0, 0, 1, -1, -1]))
    assert_array_equal(outlet_nodes, np.array([0, 0, 0, -1, -1]))
    assert_array_equal(depression_outlet_nodes, np.array([-1, -1, 1, -1, -1]))
    assert_array_equal(flooded_nodes, np.array([0, 0, 3, 0, 0]))
    assert_array_almost_equal(depression_depths, np.array([0, 0, 67.1 - 42.1, 0, 0]))
    assert_array_almost_equal(
        depression_free_elevations, np.array([0.1, 67.1, 67.771, 70.3, 34.5])
    )
    assert flooded_status == 3
    assert bad_index == -1

    donor_id = 3
    receivers = np.array([0, 0, 1, 2, -1])

    breach_c._set_flooded_and_outlet(
        donor_id,
        z,
        receivers,
        outlet_nodes,
        depression_outlet_nodes,
        flooded_nodes,
        depression_depths,
        depression_free_elevations,
        flooded_status,
        bad_index,
        min_elevation_relative_diff,
    )
    assert donor_id == 3
    assert_array_almost_equal(z, np.array([0.1, 67.1, 42.1, 70.3, 34.5]))
    assert_array_equal(receivers, np.array([0, 0, 1, 2, -1]))
    assert_array_equal(outlet_nodes, np.array([0, 0, 0, 0, -1]))
    assert_array_equal(depression_outlet_nodes, np.array([-1, -1, 1, -1, -1]))
    assert_array_equal(flooded_nodes, np.array([0, 0, 3, 0, 0]))
    assert_array_almost_equal(depression_depths, np.array([0, 0, 67.1 - 42.1, 0, 0]))
    assert_array_almost_equal(
        depression_free_elevations, np.array([0.1, 67.1, 67.771, 70.3, 34.5])
    )
    assert flooded_status == 3
    assert bad_index == -1


def test_set_receiver():
    # on a grid of 6 nodes
    cdef:
        cnp.int64_t donor_id = 3, receiver_id = 4, done_n = 1
        cnp.int64_t [:] receivers = np.array([0, -1, -1, -1, -1, -1])
        cnp.int64_t [:] done = np.array([1, 0, 0, 0, 0, 0])
    breach_c._set_receiver(donor_id, receiver_id, receivers, done, &done_n)
    assert donor_id == 3
    assert receiver_id == 4
    assert done_n == 2
    assert_array_equal(receivers, np.array([0, -1, -1, 4, -1, -1]))
    assert_array_equal(done, np.array([1, 0, 0, 1, 0, 0]))


def test_set_donor_properties():
    # on a grid of 9 nodes
    cdef:
        cnp.int64_t donor_id = 5, receiver_id = 7
        cnp.int64_t [:] _sorted_pseudo_heads = np.array(
            [
                0, 0, 1, 1, 1, 2, 2, 3, 3,
                3, 4, 4, 4, 4, 5, 5, 5, 6, 6, 7, 7, 7, 8, 8
            ]
        )
        cnp.int64_t [:] sorted_pseudo_tails = np.array(
            [
                3, 1, 0, 4, 2, 1, 5, 0, 6,
                4, 7, 3, 1, 5, 4, 2, 8, 7, 3, 8, 4, 6, 5, 7
            ]
        )
        cnp.int64_t [:, :] head_start_end_indexes = np.array(
            [
                [0, 2, 5, 7, 10, 14, 17, 19, 22],
                [1, 4, 6, 9, 13, 16, 18, 21, 23]
            ]
        )
        cnp.int64_t [:] sorted_dupli_links = np.array(
            [
                2, 0, 0, 3, 1, 1, 4,
                2, 7, 5, 8, 5, 3, 6, 6, 4, 9,
                10, 7, 11, 8, 10, 9, 11
            ]
        )
        cnp.float_t [:] sorted_dupli_gradients = np.array(
            [
                0.03879335, 0.04387396,
                0.04387396, 0.08236696, 0.12232775,
                0.12232775, 0.02936549, 0.03879335, 0.28953386, 0.16503428,
                0.18134194, 0.16503428, 0.08236696, 0.06932627, 0.06932627,
                0.02936549, 0.00526324, 0.30584151, 0.28953386, 0.24540497,
                0.18134194, 0.30584151, 0.00526324, 0.24540497
            ]
        )
        cnp.float_t [:] z = np.array(
            [
                2.29047865, 3.60669759, 7.27652998, 1.12667805,
                6.0777065, 8.15749462, 9.81269383, 0.63744841, 7.99959748
            ]
        )
        cnp.float_t [:] steepest_slopes = np.zeros(9, dtype=float)
        cnp.int64_t [:] links_to_receivers = -1 * np.ones(9, dtype=int)

    breach_c._set_donor_properties(
        donor_id,
        receiver_id,
        sorted_pseudo_tails,
        head_start_end_indexes,
        sorted_dupli_links,
        sorted_dupli_gradients,
        z,
        steepest_slopes,
        links_to_receivers,
    )
    assert donor_id == 5
    assert receiver_id == 7
    assert_array_equal(
        sorted_pseudo_tails,
        np.array(
            [
                3, 1, 0, 4, 2, 1, 5, 0, 6,
                4, 7, 3, 1, 5, 4, 2, 8, 7, 3, 8, 4, 6, 5, 7
            ]
        )
    )
    assert_array_equal(
        head_start_end_indexes,
        np.array(
            [
                [0, 2, 5, 7, 10, 14, 17, 19, 22],
                [1, 4, 6, 9, 13, 16, 18, 21, 23]
            ]
        )
    )
    assert_array_equal(
        sorted_dupli_links,
        np.array(
            [
                2, 0, 0, 3, 1, 1, 4,
                2, 7, 5, 8, 5, 3, 6, 6, 4, 9,
                10, 7, 11, 8, 10, 9, 11
            ]
        )
    )
    assert_array_almost_equal(
        sorted_dupli_gradients,
        np.array(
            [
                0.03879335,
                0.04387396,
                0.04387396, 0.08236696, 0.12232775,
                0.12232775, 0.02936549, 0.03879335, 0.28953386, 0.16503428,
                0.18134194, 0.16503428, 0.08236696, 0.06932627, 0.06932627,
                0.02936549, 0.00526324, 0.30584151, 0.28953386, 0.24540497,
                0.18134194, 0.30584151, 0.00526324, 0.24540497
            ]
        )
    )
    assert_array_almost_equal(
        z,
        np.array(
            [
                2.29047865, 3.60669759, 7.27652998, 1.12667805,
                6.0777065, 8.15749462, 9.81269383, 0.63744841, 7.99959748
            ]
        )
    )
    assert_array_almost_equal(
        steepest_slopes,
        np.array(
            [
                0., 0.,
                0., 0., 0., 0.245405,
                0., 0., 0.
            ]
        )
    )
    assert_array_equal(
        links_to_receivers, np.array([-1, -1, -1, -1, -1, 11, -1, -1, -1])
    )

#######################################################################################


def test_direct_flow_c():
    # Grid of 25 nodes
    cdef:
        cnp.int64_t nodes_n = 25, flooded_status = 3, bad_index = -1
        cnp.float_t min_elevation_relative_diff = 1e-2
        cnp.int64_t neighbors_max_number = 50
        cnp.int64_t[:] base_level_nodes = np.array(
            [0, 1, 2, 4, 5, 9, 10, 14, 15, 19, 20, 21, 22, 23, 24]
        )
        cnp.int64_t[:] base_level_nodes_0 = np.copy(base_level_nodes)
        cnp.int64_t[:] closed_nodes = np.array([3])
        cnp.int64_t[:] closed_nodes_0 = np.copy(closed_nodes)
        cnp.int64_t[:] _sorted_pseudo_heads = np.array(
            [
                0, 0, 1, 1, 1,
                2, 2, 2, 3, 3, 3, 4, 4, 5, 5, 5, 6,
                6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 10, 10, 10,
                11, 11, 11, 11, 12, 12, 12, 12, 13, 13, 13, 13, 14, 14, 14, 15, 15,
                15, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18, 19, 19, 19, 20,
                20, 21, 21, 21, 22, 22, 22, 23, 23, 23, 24, 24
            ]
        )  # unused, for info
        cnp.int64_t[:] sorted_pseudo_tails = np.array(
            [
                5, 1, 6, 2, 0, 3,
                7, 1, 8, 2, 4, 9, 3, 6, 10, 0, 7,
                11, 1, 5, 12, 8, 6, 2, 7, 9, 3, 13, 8, 4, 14, 15, 11, 5,
                16, 6, 12, 10, 13, 17, 7, 11, 14, 18, 8, 12, 19, 9, 13, 16, 20,
                10, 21, 15, 17, 11, 12, 22, 16, 18, 19, 23, 17, 13, 24, 18, 14, 21,
                15, 20, 22, 16, 17, 23, 21, 24, 18, 22, 19, 23
            ]
        )
        cnp.int64_t[:] sorted_pseudo_tails_0 = np.copy(sorted_pseudo_tails)
        cnp.float_t[:] sorted_dupli_gradients = np.array(
            [
                0.1955672,
                0.04387396, 0.20686654, 0.12232775, 0.04387396,
                0.20499506, 0.22130272, 0.12232775, 0.22909731, 0.20499506,
                0.16503428, 0.17027403, 0.16503428, 0.05517331, 0.1382939,
                0.1955672,  0.30584151, 0.10637057, 0.20686654, 0.05517331,
                0.30061369, 0.24540497, 0.30584151, 0.22130272, 0.24540497,
                0.23433706, 0.22909731, 0.10635323, 0.23433706, 0.17027403,
                0.05572124, 0.06218312, 0.08709664, 0.1382939,  0.20119303,
                0.10637057, 0.10114275, 0.08709664, 0.16156195, 0.12329631,
                0.30061369, 0.10114275, 0.07226259, 0.01279225, 0.10635323,
                0.16156195, 0.08976513, 0.05572124, 0.07226259, 0.17627952,
                0.00161023, 0.06218312, 0.1169648,  0.17627952, 0.17903947,
                0.20119303, 0.12329631, 0.0359346,  0.17903947, 0.05105789,
                0.0302948,  0.13452944, 0.05105789, 0.01279225, 0.14332401,
                0.0302948,  0.08976513, 0.06092495, 0.00161023, 0.06092495,
                0.09800927, 0.1169648,  0.0359346,  0.22152193, 0.09800927,
                0.02150023, 0.13452944, 0.22152193, 0.14332401, 0.02150023
            ]
        )
        cnp.float_t[:] sorted_dupli_gradients_0 = np.copy(sorted_dupli_gradients)
        cnp.int64_t[:] sorted_dupli_links = np.array(
            [
                4, 0, 5, 1, 0,
                2, 6, 1, 7, 2, 3, 8, 3, 9, 13, 4, 10,
                14, 5, 9, 15, 11, 10, 6, 11, 12, 7, 16, 12, 8, 17, 22, 18, 13,
                23, 14, 19, 18, 20, 24, 15, 19, 21, 25, 16, 20, 26, 17, 21, 27, 31,
                22, 32, 27, 28, 23, 24, 33, 28, 29, 30, 34, 29, 25, 35, 30, 26, 36,
                31, 36, 37, 32, 33, 38, 37, 39, 34, 38, 35, 39
            ]
        )
        cnp.int64_t[:] sorted_dupli_links_0 = np.copy(sorted_dupli_links)
        cnp.int64_t[:, :] head_start_end_indexes = np.array(
            [
                [
                    0, 2, 5,
                    8, 11, 13, 16, 20, 24, 28, 31, 34, 38, 42, 46, 49, 52,
                    56, 60, 64, 67, 69, 72, 75, 78
                ],
                [
                    1, 4, 7, 10, 12, 15, 19, 23, 27, 30, 33, 37, 41, 45, 48, 51,
                    55, 59, 63, 66, 68, 71, 74, 77, 79
                ]
            ]
        )
        cnp.int64_t[:, :] head_start_end_indexes_0 = np.copy(head_start_end_indexes)
        cnp.float_t[:] depression_depths = np.zeros(25, dtype=float)
        cnp.int64_t[:] outlet_nodes = -1 * np.ones(25, dtype=int)
        cnp.int64_t[:] depression_outlet_nodes = -1 * np.ones(25, dtype=int)
        cnp.int64_t[:] flooded_nodes = np.zeros(25, dtype=int)
        cnp.int64_t[:] links_to_receivers = -1 * np.ones(25, dtype=int)
        cnp.int64_t[:] receivers = -1 * np.ones(25, dtype=int)
        cnp.float_t[:] steepest_slopes = np.zeros(25, dtype=float)
        cnp.float_t[:] z = np.array(
            [
                2.29047865, 3.60669759, 7.27652998,
                1.12667805, 6.0777065,
                8.15749462, 9.81269383, 0.63744841, 7.99959748, 0.96948555,
                4.00867748, 6.62157669, 9.65585909, 4.80900059, 2.64112287,
                5.8741712,  0.5857857,  5.95696968, 4.42523296, 5.33407687,
                5.92247815, 4.09472964, 7.03500768, 0.38934984, 1.03435662
            ]
        )
        cnp.float_t[:] depression_free_elevations = z.copy()
        cnp.float_t[:] z_0 = np.copy(z)

    breach_c._direct_flow_c(
        nodes_n,
        base_level_nodes,
        closed_nodes,
        sorted_pseudo_tails,
        sorted_dupli_gradients,
        sorted_dupli_links,
        head_start_end_indexes,
        outlet_nodes,
        depression_outlet_nodes,
        flooded_nodes,
        depression_depths,
        depression_free_elevations,
        links_to_receivers,
        receivers,
        steepest_slopes,
        z,
        flooded_status,
        bad_index,
        neighbors_max_number,
        min_elevation_relative_diff,
    )
    assert nodes_n == 25
    assert flooded_status == 3
    assert bad_index == -1
    assert neighbors_max_number == 50
    assert_array_equal(base_level_nodes, base_level_nodes_0)
    assert_array_equal(closed_nodes, closed_nodes_0)
    assert_array_equal(sorted_pseudo_tails, sorted_pseudo_tails_0)
    assert_array_almost_equal(sorted_dupli_gradients, sorted_dupli_gradients_0)
    assert_array_equal(sorted_dupli_links, sorted_dupli_links_0)
    assert_array_equal(head_start_end_indexes, head_start_end_indexes_0)
    assert_array_almost_equal(
        depression_depths,
        np.array(
            [
                0., 0., 0., 0., 0.,
                0., 0., 6.63908157, 0., 0.,
                0., 0., 0., 0., 0.,
                0., 3.50894394, 0., 0., 0.,
                0., 0., 0., 0., 0.
            ]
        )
    )
    assert_array_equal(
        outlet_nodes,
        np.array(
            [
                0, 1, 2, 3, 4, 5, 1, 2, 9, 9, 10, 10, 14,
                14, 14, 15, 21, 21, 23, 19, 20, 21, 22, 23, 24
            ]
        )
    )
    assert_array_equal(
        depression_outlet_nodes,
        np.array(
            [
                -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1,
                -1, -1, -1, 21, -1, -1, -1, -1, -1, -1, -1, -1
            ]
        )
    )
    assert_array_equal(
        flooded_nodes,
        np.array(
            [
                0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3,
                0, 0, 0, 0, 0, 0, 0, 0
            ]
        )
    )
    assert_array_equal(
        links_to_receivers,
        np.array(
            [
                -1, -1, -1, -1, -1, -1, 5, 6, 12, -1, -1, 18, 20,
                21, -1, -1, 32, 28, 34, -1, -1, -1, -1, -1, -1
            ]
        )
    )
    assert_array_equal(
        receivers,
        np.array(
            [
                0, 1, 2, 3, 4, 5, 1, 2, 9, 9, 10, 10,
                13, 14, 14, 15, 21, 16, 23, 19, 20, 21, 22, 23, 24
            ]
        )
    )
    assert_array_almost_equal(
        steepest_slopes,
        np.array(
            [
                0., 0., 0., 0., 0.,
                0., 0.20686654, 0., 0.23433706, 0.,
                0., 0.08709664, 0.16156195, 0.07226259, 0.,
                0., 0., 0.17903947, 0.13452944, 0.,
                0., 0., 0., 0., 0.
            ]
        )
    )
    assert_array_almost_equal(z, z_0)


def test_direct_flow():
    # Grid of 25 nodes
    cdef:
        cnp.int64_t nodes_n = 25, flooded_status = 3, bad_index = -1
        cnp.int64_t neighbors_max_number = 50
        cnp.float_t min_elevation_relative_diff = 1e-2
        cnp.int64_t[:] base_level_nodes = np.array(
            [0, 1, 2, 4, 5, 9, 10, 14, 15, 19, 20, 21, 22, 23, 24]
        )
        cnp.int64_t[:] base_level_nodes_0 = np.copy(base_level_nodes)
        cnp.int64_t[:] closed_nodes = np.array([3])
        cnp.int64_t[:] closed_nodes_0 = np.copy(closed_nodes)
        cnp.int64_t[:] _sorted_pseudo_heads = np.array(
            [
                0, 0, 1, 1, 1,
                2, 2, 2, 3, 3, 3, 4, 4, 5, 5, 5, 6,
                6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 10, 10, 10,
                11, 11, 11, 11, 12, 12, 12, 12, 13, 13, 13, 13, 14, 14, 14, 15, 15,
                15, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18, 19, 19, 19, 20,
                20, 21, 21, 21, 22, 22, 22, 23, 23, 23, 24, 24
            ]
        )  # unused, for info
        cnp.int64_t[:] sorted_pseudo_tails = np.array(
            [
                5, 1, 6, 2, 0, 3,
                7, 1, 8, 2, 4, 9, 3, 6, 10, 0, 7,
                11, 1, 5, 12, 8, 6, 2, 7, 9, 3, 13, 8, 4, 14, 15, 11, 5,
                16, 6, 12, 10, 13, 17, 7, 11, 14, 18, 8, 12, 19, 9, 13, 16, 20,
                10, 21, 15, 17, 11, 12, 22, 16, 18, 19, 23, 17, 13, 24, 18, 14, 21,
                15, 20, 22, 16, 17, 23, 21, 24, 18, 22, 19, 23
            ]
        )
        cnp.int64_t[:] sorted_pseudo_tails_0 = np.copy(sorted_pseudo_tails)
        cnp.float_t[:] sorted_dupli_gradients = np.array(
            [
                0.1955672,
                0.04387396, 0.20686654, 0.12232775, 0.04387396,
                0.20499506, 0.22130272, 0.12232775, 0.22909731, 0.20499506,
                0.16503428, 0.17027403, 0.16503428, 0.05517331, 0.1382939,
                0.1955672,  0.30584151, 0.10637057, 0.20686654, 0.05517331,
                0.30061369, 0.24540497, 0.30584151, 0.22130272, 0.24540497,
                0.23433706, 0.22909731, 0.10635323, 0.23433706, 0.17027403,
                0.05572124, 0.06218312, 0.08709664, 0.1382939,  0.20119303,
                0.10637057, 0.10114275, 0.08709664, 0.16156195, 0.12329631,
                0.30061369, 0.10114275, 0.07226259, 0.01279225, 0.10635323,
                0.16156195, 0.08976513, 0.05572124, 0.07226259, 0.17627952,
                0.00161023, 0.06218312, 0.1169648,  0.17627952, 0.17903947,
                0.20119303, 0.12329631, 0.0359346,  0.17903947, 0.05105789,
                0.0302948,  0.13452944, 0.05105789, 0.01279225, 0.14332401,
                0.0302948,  0.08976513, 0.06092495, 0.00161023, 0.06092495,
                0.09800927, 0.1169648,  0.0359346,  0.22152193, 0.09800927,
                0.02150023, 0.13452944, 0.22152193, 0.14332401, 0.02150023
            ]
        )
        cnp.float_t[:] sorted_dupli_gradients_0 = np.copy(sorted_dupli_gradients)
        cnp.int64_t[:] sorted_dupli_links = np.array(
            [
                4, 0, 5, 1, 0,
                2, 6, 1, 7, 2, 3, 8, 3, 9, 13, 4, 10,
                14, 5, 9, 15, 11, 10, 6, 11, 12, 7, 16, 12, 8, 17, 22, 18, 13,
                23, 14, 19, 18, 20, 24, 15, 19, 21, 25, 16, 20, 26, 17, 21, 27, 31,
                22, 32, 27, 28, 23, 24, 33, 28, 29, 30, 34, 29, 25, 35, 30, 26, 36,
                31, 36, 37, 32, 33, 38, 37, 39, 34, 38, 35, 39
            ]
        )
        cnp.int64_t[:] sorted_dupli_links_0 = np.copy(sorted_dupli_links)
        cnp.int64_t[:, :] head_start_end_indexes = np.array(
            [
                [
                    0, 2, 5,
                    8, 11, 13, 16, 20, 24, 28, 31, 34, 38, 42, 46, 49, 52,
                    56, 60, 64, 67, 69, 72, 75, 78
                ],
                [
                    1, 4, 7, 10, 12, 15, 19, 23, 27, 30, 33, 37, 41, 45, 48, 51,
                    55, 59, 63, 66, 68, 71, 74, 77, 79
                ]
            ]
        )
        cnp.int64_t[:, :] head_start_end_indexes_0 = np.copy(head_start_end_indexes)
        cnp.float_t[:] depression_depths = np.zeros(25, dtype=float)
        cnp.int64_t[:] outlet_nodes = -1 * np.ones(25, dtype=int)
        cnp.int64_t[:] depression_outlet_nodes = -1 * np.ones(25, dtype=int)
        cnp.int64_t[:] flooded_nodes = np.zeros(25, dtype=int)
        cnp.int64_t[:] links_to_receivers = -1 * np.ones(25, dtype=int)
        cnp.int64_t[:] receivers = -1 * np.ones(25, dtype=int)
        cnp.float_t[:] steepest_slopes = np.zeros(25, dtype=float)
        cnp.float_t[:] z = np.array(
            [
                2.29047865, 3.60669759, 7.27652998,
                1.12667805, 6.0777065,
                8.15749462, 9.81269383, 0.63744841, 7.99959748, 0.96948555,
                4.00867748, 6.62157669, 9.65585909, 4.80900059, 2.64112287,
                5.8741712,  0.5857857,  5.95696968, 4.42523296, 5.33407687,
                5.92247815, 4.09472964, 7.03500768, 0.38934984, 1.03435662
            ]
        )
        cnp.float_t[:] depression_free_elevations = z.copy()
        cnp.float_t[:] z_0 = np.copy(z)

    breach._direct_flow(
        nodes_n,
        base_level_nodes,
        closed_nodes,
        sorted_pseudo_tails,
        sorted_dupli_gradients,
        sorted_dupli_links,
        head_start_end_indexes,
        outlet_nodes,
        depression_outlet_nodes,
        flooded_nodes,
        depression_depths,
        depression_free_elevations,
        links_to_receivers,
        receivers,
        steepest_slopes,
        z,
        flooded_status,
        bad_index,
        neighbors_max_number,
        min_elevation_relative_diff,
    )
    assert nodes_n == 25
    assert flooded_status == 3
    assert bad_index == -1
    assert neighbors_max_number == 50
    assert_array_equal(base_level_nodes, base_level_nodes_0)
    assert_array_equal(closed_nodes, closed_nodes_0)
    assert_array_equal(sorted_pseudo_tails, sorted_pseudo_tails_0)
    assert_array_almost_equal(sorted_dupli_gradients, sorted_dupli_gradients_0)
    assert_array_equal(sorted_dupli_links, sorted_dupli_links_0)
    assert_array_equal(head_start_end_indexes, head_start_end_indexes_0)
    assert_array_almost_equal(
        depression_depths,
        np.array(
            [
                0., 0., 0., 0., 0.,
                0., 0., 6.63908157, 0., 0.,
                0., 0., 0., 0., 0.,
                0., 3.50894394, 0., 0., 0.,
                0., 0., 0., 0., 0.
            ]
        )
    )
    assert_array_equal(
        outlet_nodes,
        np.array(
            [
                0, 1, 2, 3, 4, 5, 1, 2, 9, 9, 10, 10, 14,
                14, 14, 15, 21, 21, 23, 19, 20, 21, 22, 23, 24
            ]
        )
    )
    assert_array_equal(
        depression_outlet_nodes,
        np.array(
            [
                -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1,
                -1, -1, -1, 21, -1, -1, -1, -1, -1, -1, -1, -1
            ]
        )
    )
    assert_array_equal(
        flooded_nodes,
        np.array(
            [
                0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3,
                0, 0, 0, 0, 0, 0, 0, 0
            ]
        )
    )
    assert_array_equal(
        links_to_receivers,
        np.array(
            [
                -1, -1, -1, -1, -1, -1, 5, 6, 12, -1, -1, 18, 20,
                21, -1, -1, 32, 28, 34, -1, -1, -1, -1, -1, -1
            ]
        )
    )
    assert_array_equal(
        receivers,
        np.array(
            [
                0, 1, 2, 3, 4, 5, 1, 2, 9, 9, 10, 10,
                13, 14, 14, 15, 21, 16, 23, 19, 20, 21, 22, 23, 24
            ]
        )
    )
    assert_array_almost_equal(
        steepest_slopes,
        np.array(
            [
                0., 0., 0., 0., 0.,
                0., 0.20686654, 0., 0.23433706, 0.,
                0., 0.08709664, 0.16156195, 0.07226259, 0.,
                0., 0., 0.17903947, 0.13452944, 0.,
                0., 0., 0., 0., 0.
            ]
        )
    )
    assert_array_almost_equal(z, z_0)



================================================
File: flow_router/ext/single_flow/priority_routing/breach.pxd
================================================
# distutils: language = c++
import numpy as np

cimport cython
cimport numpy as cnp
from libcpp cimport bool
from libcpp.pair cimport pair


cdef extern from "_priority_queue.hpp" nogil:
    cdef cppclass _priority_queue:
        _priority_queue(...) except +
        void push(pair[cnp.int64_t, cnp.float_t])
        pair[cnp.int64_t, cnp.float_t] top() except +
        void pop()
        bool empty()
        cnp.int64_t size()


cdef bool _compare_second(pair[int, double] a, pair[int, double] b) nogil


cdef void _init_flow_direction_queues(
    const cnp.int64_t [:] base_level_nodes,
    const cnp.int64_t [:] closed_nodes,
    cnp.float_t [:] z,
    _priority_queue& to_do,
    cnp.int64_t [:] receivers,
    cnp.int64_t [:] outlet_nodes,
    cnp.int64_t [:] done, cnp.int64_t* done_n_ptr
) nogil


cdef void _set_flooded_and_outlet(
    cnp.int64_t donor_id,
    cnp.float_t [:] z,
    cnp.int64_t [:] receivers,
    cnp.int64_t [:] outlet_nodes,
    cnp.int64_t [:] depression_outlet_nodes,
    cnp.int64_t [:] flooded_nodes,
    cnp.float_t [:] depression_depths,
    cnp.float_t [:] depression_free_elevations,
    cnp.int64_t flooded_status,
    cnp.int64_t bad_index,
    cnp.float_t min_elevation_relative_diff
) nogil


cdef void _set_receiver(
    cnp.int64_t donor_id,
    cnp.int64_t receiver_id,
    cnp.int64_t [:] receivers,
    cnp.int64_t [:] done,
    cnp.int64_t* done_n_ptr,
) nogil


cdef void _set_donor_properties(
    cnp.int64_t donor_id,
    cnp.int64_t receiver_id,
    cnp.int64_t [:] sorted_pseudo_tails,
    const cnp.int64_t [:, :] head_start_end_indexes,
    const cnp.int64_t [:] sorted_dupli_links,
    cnp.float_t [:] sorted_dupli_gradients,
    cnp.float_t [:] z,
    cnp.float_t [:] steepest_slopes,
    cnp.int64_t [:] links_to_receivers,
) nogil


cdef void _direct_flow_c(
    cnp.int64_t nodes_n,
    const cnp.int64_t[:] base_level_nodes,
    const cnp.int64_t[:] closed_nodes,
    cnp.int64_t[:] sorted_pseudo_tails,
    cnp.float_t[:] sorted_dupli_gradients,
    const cnp.int64_t[:] sorted_dupli_links,
    const cnp.int64_t[:, :] head_start_end_indexes,
    cnp.int64_t [:] outlet_nodes,
    cnp.int64_t [:] depression_outlet_nodes,
    cnp.int64_t[:] flooded_nodes,
    cnp.float_t[:] depression_depths,
    cnp.float_t [:] depression_free_elevations,
    cnp.int64_t[:] links_to_receivers,
    cnp.int64_t[:] receivers,
    cnp.float_t[:] steepest_slopes,
    cnp.float_t[:] z,
    cnp.int64_t flooded_status,
    cnp.int64_t bad_index,
    cnp.int64_t neighbors_max_number,
    cnp.float_t min_elevation_relative_diff,
)



================================================
File: flow_router/ext/single_flow/priority_routing/breach.pyx
================================================
# distutils: define_macros=NPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION
# distutils: extra_compile_args = -std=c++11
# distutils: extra_link_args = -std=c++11
""" Contains the cython functions for the component method
Flow_router.run_directions(). Flow directions and depression handling are done
adapting Barnes et al., 2014 algorithm #4."""

import numpy as np

cimport cython
cimport numpy as cnp
from libcpp cimport bool
from libcpp.pair cimport pair


# 1-3. Instantiate the Queues (Steps noted #1-3, #5-6 in Barnes, 2014's algorithm #4
####################################################################################
cdef extern from "_priority_queue.hpp" nogil:
    cdef cppclass _priority_queue:
        _priority_queue(...) except +
        void push(pair[cnp.int64_t, cnp.float_t])
        pair[cnp.int64_t, cnp.float_t] top() except +
        void pop()
        bool empty()
        cnp.int64_t size()


cdef bool _compare_second(pair[int, double] a, pair[int, double] b) nogil:
    return a.second > b.second


@cython.boundscheck(False)  # turn off bounds-checking for entire function
cdef void _init_flow_direction_queues(
    const cnp.int64_t [:] base_level_nodes,
    const cnp.int64_t [:] closed_nodes,
    cnp.float_t [:] z,
    _priority_queue& to_do,
    cnp.int64_t [:] receivers,
    cnp.int64_t [:] outlet_nodes,
    cnp.int64_t [:] done,
    cnp.int64_t* done_n_ptr,
) nogil:
    """
    Add the base-level nodes to the queue and update receivers for base-level and
    closed nodes. Updates to_do, receivers, outlet_nodes, done and the value pointed
    by done_n_ptr.

    Implementation remarks: It's important to pass the priority_queue to_do as a
    reference, otherwise it won't be modified. I didn't manage to use the bint type
    for the variable done, so here 0 is for False and 1 for True.

    Params
    ------
    base_level_nodes: memoryview(long).
        Base-level nodes.
    closed_nodes: memoryview(long).
        Closed nodes.
    z: memoryview(double).
        Elevation of eachn
    to_do: _priority_queue&
        Queue which handles the nodes where to direct flow
    receivers: memoryview(long).
        Receiver of each node (ordered by node id).
    outlet_nodes: memoryview(long).
        Base-level outlet of each node (ordered by node id).
    done: memoryview(long).
        Flag for each node (ordered by node id). If 1, the node has been done,
        otherwise 0.
    done_n_ptr: long*
        Pointer to the number of done nodes. Necessary for multithreading
        (future evolution).
    """

    cdef:
        cnp.int64_t node_id, i, n = len(base_level_nodes), m = len(closed_nodes)
        pair[cnp.int64_t, cnp.float_t] node_pair

    for i in range(n):
        # NB: for node_i in open_boundary raises a compiling error with nogil.
        node_id = base_level_nodes[i]
        node_pair = pair[cnp.int64_t, cnp.float_t](node_id, z[node_id])
        to_do.push(node_pair)
        receivers[node_id] = node_id
        outlet_nodes[node_id] = node_id
        done[node_id] = 1
        done_n_ptr[0] += 1

    for i in range(m):
        node_id = closed_nodes[i]
        receivers[node_id] = node_id
        outlet_nodes[node_id] = node_id
        done[node_id] = 1
        done_n_ptr[0] += 1
        # NB: done_n_ptr[0]: method to dereference the pointer. The
        # cython.operator.dereference doesn't seem to work on this pointer to a
        # long-type variable.

# 4. Functions necessary for the flow direction processing (Steps #11 - 20)
###########################################################################


@cython.boundscheck(False)
cdef void _set_flooded_and_outlet(
    cnp.int64_t donor_id,
    cnp.float_t [:] z,
    cnp.int64_t [:] receivers,
    cnp.int64_t [:] outlet_nodes,
    cnp.int64_t [:] depression_outlet_nodes,
    cnp.int64_t [:] flooded_nodes,
    cnp.float_t [:] depression_depths,
    cnp.float_t [:] depression_free_elevations,
    cnp.int64_t flooded_status,
    cnp.int64_t bad_index,
    cnp.float_t min_elevation_relative_diff,
) nogil:
    """ Updates the base-level outlet nodes (outlet_nodes), the depression outlet
    nodes (depression_outlet_nodes), the flooded status (flooded_nodes), and the
    depths of the depressions (depression_depths) for the node donor_id depending
    on its surface z value and the one of its receiver outlet and neighbors.

    Parameters
    ----------
    donor_id: long.
        Id of the node giving the flow.
    z: memoryview(double).
        Values of the surface where the flow is directed, for each node.
    receivers: memoryview(long).
        Ids of the receiver nodes for each node considered as a donor, ordered by
        donor id.
    outlet_nodes: memoryview(long).
        Ids of the base-level outlet node, for each donor node, ordered by the donor
        node ids.
    depression_outlet_nodes: memoryview(long).
        Outlet nodes of the depression.
    flooded_nodes: memoryview(bool).
        Flooded status for each node.
    depression_depths: memoryview(double).
        Depths of the depression (if existing) below the level of the depression outlet
        for each node, ordered by node id.
    depression_free_elevations: memoryview(double).
        Elevation of the surface corrected from depressions.
    flooded_status: long.
        Constant for flooded status.
    bad_index: long.
        Constant for bad index.
    min_elevation_relative_diff: double
        Minimum relative difference in elevation for the depression_free_elevations
        surface.
    """
    cdef:
        cnp.int64_t receiver_id = receivers[donor_id]
        cnp.int64_t receiver_depression_outlet = (
            receiver_id if (
                depression_outlet_nodes[receiver_id] == bad_index
            ) else depression_outlet_nodes[receiver_id]
        )
        cnp.int64_t receiver_outlet = (
            receiver_id if (
                outlet_nodes[receiver_id] == bad_index
            ) else outlet_nodes[receiver_id]
        )
    outlet_nodes[donor_id] = receiver_outlet
    if z[donor_id] < z[receiver_depression_outlet]:
        depression_outlet_nodes[donor_id] = receiver_depression_outlet
        flooded_nodes[donor_id] = flooded_status
        depression_depths[donor_id] = z[receiver_depression_outlet] - z[donor_id]
        depression_free_elevations[donor_id] = (
            (1 + min_elevation_relative_diff) * depression_free_elevations[receiver_id]
        )


@cython.boundscheck(False)
cdef void _set_receiver(
    cnp.int64_t donor_id,
    cnp.int64_t receiver_id,
    cnp.int64_t [:] receivers,
    cnp.int64_t [:] done,
    cnp.int64_t* done_n_ptr,
) nogil:
    """ Updates the receiver (receivers) and the process statuses (done) of the donor
    node donor_id.

    Parameters
    ----------
    donor_id: long
        Id of the donor node.
    receiver_id: long
        Id of the receiver node.
    receivers: memoryview(long)
        Ids of the receivers for all donor nodes, ordered by the id of the donor nodes.
    done: memoryview(bool)
        Process statuses for all nodes. 1 for done. 0 otherwise.
    """
    receivers[donor_id] = receiver_id
    done[donor_id] = 1  # 1 For True
    done_n_ptr[0] += 1


@cython.boundscheck(False)
cdef void _set_donor_properties(
    cnp.int64_t donor_id,
    cnp.int64_t receiver_id,
    cnp.int64_t [:] sorted_pseudo_tails,
    const cnp.int64_t [:, :] head_start_end_indexes,
    const cnp.int64_t [:] sorted_dupli_links,
    cnp.float_t [:] sorted_dupli_gradients,
    cnp.float_t [:] z,
    cnp.float_t [:] steepest_slopes,
    cnp.int64_t [:] links_to_receivers,
) nogil:
    """ Updates the steepest_slopes and the links_to_receivers of a donor in function
    of the slopes with its neighbors and the head-tail links of the grid. Steepest
    slope is set to 0. if the donor is in a depression.

    Parameters
    ----------
    donor_id: long
        Id of the donor node.
    receiver_id: long.
        Id of the receiver node.
    sorted_pseudo_tails: memoryview(long).
        All tails and heads for all existing links in the grid, ordered by
        increasing head and tail ids in sorted_pseudo_heads (see FlowRouter.py file).
    head_start_end_indexes: memoryview(longxlong).
        Start (array[0, :] and end (array[1, :]) index of all possible value nodes in
        the sorted_pseudo_heads (ordered by increasing head and tail ids, see
        FlowRouter.py file).
    sorted_dupli_links: memoryview(long).
        All existing links in the grid, duplicated, ordered by
        increasing head and tail ids in sorted_pseudo_heads (see FlowRouter.py file).
    sorted_dupli_gradients: memoryview(double).
        Gradients of the surface for all links duplicated, ordered by  increasing
        head and tail ids in sorted_pseudo_heads.
    z: memoryview(double).
        Values of the surface where the flow is directed, for each node.
    steepest_slopes: memoryview(double).
        Steepest slope for each node.
    links_to_receivers: memoryview(long).
        Ids of the links between a donor and a receiver, ordered by the ids of the
        donor nodes.
    """

    # range of indexes where donor_id is founded in sorted_pseudo_heads
    cdef:
        cnp.int64_t idx1 = head_start_end_indexes[0, donor_id]
        cnp.int64_t idx2 = head_start_end_indexes[1, donor_id] + 1

        cnp.int64_t [:] s = sorted_pseudo_tails[idx1:idx2]
        cnp.int64_t n = len(s), c = -1, i

    # loop to bypass the impossibility to use
    # sorted_pseudo_tails[idx1:idx2] == receiver_id with memoryviews
    for i in range(n):
        if s[i] == receiver_id:
            c = idx1 + i
            break
    steepest_slopes[donor_id] = (
        sorted_dupli_gradients[c] if (
            z[receiver_id] <= z[donor_id]
        ) else 0
    )
    links_to_receivers[donor_id] = sorted_dupli_links[c]

# ######################################################################################
# Main functions to direct flow


@cython.boundscheck(False)
cdef void _direct_flow_c(
    cnp.int64_t nodes_n,
    const cnp.int64_t[:] base_level_nodes,
    const cnp.int64_t[:] closed_nodes,
    cnp.int64_t[:] sorted_pseudo_tails,
    cnp.float_t[:] sorted_dupli_gradients,
    const cnp.int64_t[:] sorted_dupli_links,
    const cnp.int64_t[:, :] head_start_end_indexes,
    cnp.int64_t [:] outlet_nodes,
    cnp.int64_t [:] depression_outlet_nodes,
    cnp.int64_t[:] flooded_nodes,
    cnp.float_t[:] depression_depths,
    cnp.float_t[:] depression_free_elevations,
    cnp.int64_t[:] links_to_receivers,
    cnp.int64_t[:] receivers,
    cnp.float_t[:] steepest_slopes,
    cnp.float_t[:] z,
    cnp.int64_t flooded_status,
    cnp.int64_t bad_index,
    cnp.int64_t neighbors_max_number,
    cnp.float_t min_elevation_relative_diff,
):
    """
    Main function implementing the flow directing through breaching depressions.
    Updates outlet_nodes, depression_outlet_nodes, flooded_nodes, links_to_receivers,
    receivers, steepest_slopes.

    Params
    ------
    nodes_n: long.
        Number of nodes to handle.
    base_level_nodes: memoryview(long).
        Base-level nodes.
    closed_nodes: memoryview(long).
        Closed nodes.
    sorted_pseudo_tails: memoryview(long).
        All tails and heads for all existing links in the grid, ordered by
        increasing head and tail ids in sorted_pseudo_heads (see FlowRouter.py file).
    sorted_dupli_gradients: memoryview(double).
        Gradients of the surface for all links duplicated, ordered by increasing
        head and tail ids in sorted_pseudo_heads.
    sorted_dupli_links: memoryview(long).
        All existing links in the grid, duplicated, ordered by
        increasing head and tail ids in sorted_pseudo_heads (see FlowRouter.py file).
    head_start_end_indexes: memoryview(longxlong).
        Start (array[0, :] and end (array[1, :]) index of all possible value nodes in
        the sorted_pseudo_heads (ordered by increasing head and tail ids, see
        FlowRouter.py file).
    outlet_nodes: memoryview(long).
        Ids of the base-level outlet node, for each donor node, ordered by the donor
        node ids.
    depression_outlet_nodes: memoryview(long).
        Outlet nodes of the depression, ordered by the donor node ids.
    flooded_nodes: memoryview(bool).
        Flooded status for each node, ordered by node ids.
    depression_depths: memoryview(double).
        Depths of the depression (if existing) below the level of the depression outlet
        for each node, ordered by node ids.
    depression_free_elevations: memoryview(double).
        Elevation of the surface corrected from depressions.
    links_to_receivers: memoryview(long).
        Ids of the links between a donor and a receiver, ordered by the ids of the
        donor nodes.
    receivers: memoryview(long)
        Ids of the receivers for all donor nodes, ordered by the id of the donor nodes.
    steepest_slopes: memoryview(double).
        Steepest slope for each node, ordered by node id.
    z: memoryview(double).
        Values of the surface where the flow is directed, for each node, ordered by
        node id.
    flooded_status: long.
        Constant for flooded status.
    bad_index: long.
        Constant for bad index.
    neighbors_max_number: long
        Maximum number of neighbors in the grid.
    min_elevation_relative_diff: double
        Minimum relative difference in elevation for the depression_free_elevations
        surface.
    """
    cdef:
        cnp.int64_t [:] done
        # cnp.int64_t [:] tmp_neighbors
        # cnp.int64_t [:] neighbors_to_do
        _priority_queue to_do = _priority_queue(_compare_second)
        cnp.int64_t receiver_id, donor_id, i, j, done_n
        # cnp.int64_t [:] neighbors
        pair[cnp.int64_t, cnp.float_t] node_pair

    done = np.full(nodes_n, 0, dtype=int)
    # tmp_neighbors = np.full(neighbors_max_number, 0, dtype=int)
    # neighbors_to_do = np.array([], dtype=int)

    # done_n is input only for MULTITHREADING, a future evolution, and
    # is not checked in this function.
    done_n = 0

    _init_flow_direction_queues(
        base_level_nodes, closed_nodes, z, to_do, receivers, outlet_nodes, done, &done_n
    )

    for j in range(nodes_n):
        # a while loop is possible here, but prefer for loop, with future
        # multithreading evolution.
        if to_do.empty():
            break
        receiver_id = to_do.top().first
        to_do.pop()
        done[0] = 1

        # Get the neighbors to handle.
        idx1 = head_start_end_indexes[0, receiver_id]
        idx2 = head_start_end_indexes[1, receiver_id] + 1

        # Handle each neighbor.
        for i in range(idx1, idx2):
            donor_id = sorted_pseudo_tails[i]
            if done[donor_id] == 1:
                continue  # 0 for False.
            _set_receiver(donor_id, receiver_id, receivers, done, &done_n)
            _set_flooded_and_outlet(
                donor_id,
                z,
                receivers,
                outlet_nodes,
                depression_outlet_nodes,
                flooded_nodes,
                depression_depths,
                depression_free_elevations,
                flooded_status,
                bad_index,
                min_elevation_relative_diff,
            )
            _set_donor_properties(
                donor_id,
                receiver_id,
                sorted_pseudo_tails,
                head_start_end_indexes,
                sorted_dupli_links,
                sorted_dupli_gradients,
                z,
                steepest_slopes,
                links_to_receivers,
            )
            node_pair = pair[cnp.int64_t, cnp.float_t](donor_id, z[donor_id])
            to_do.push(node_pair)


def _direct_flow(
    cnp.int64_t nodes_n,
    const cnp.int64_t[:] base_level_nodes,
    const cnp.int64_t[:] closed_nodes,
    cnp.int64_t[:] sorted_pseudo_tails,
    cnp.float_t[:] sorted_dupli_gradients,
    const cnp.int64_t[:] sorted_dupli_links,
    const cnp.int64_t[:, :] head_start_end_indexes,
    cnp.int64_t [:] outlet_nodes,
    cnp.int64_t [:] depression_outlet_nodes,
    cnp.int64_t[:] flooded_nodes,
    cnp.float_t[:] depression_depths,
    cnp.float_t[:] depression_free_elevations,
    cnp.int64_t[:] links_to_receivers,
    cnp.int64_t[:] receivers,
    cnp.float_t[:] steepest_slopes,
    cnp.float_t[:] z,
    cnp.int64_t flooded_status,
    cnp.int64_t bad_index,
    cnp.int64_t neighbors_max_number,
    cnp.float_t min_elevation_relative_diff,
):
    """
    Main function calling the function that implements flow directing through
    breaching depressions. Updates outlet_nodes, depression_outlet_nodes,
    flooded_nodes, links_to_receivers, receivers, steepest_slopes.

    Params
    ------
    nodes_n: long.
        Number of nodes to handle.
    base_level_nodes: memoryview(long).
        Base-level nodes.
    closed_nodes: memoryview(long).
        Closed nodes.
    sorted_pseudo_tails: memoryview(long).
        All tails and heads for all existing links in the grid, ordered by
        increasing head and tail ids in sorted_pseudo_heads (see FlowRouter.py file).
    sorted_dupli_gradients: memoryview(double).
        Gradients of the surface for all links duplicated, ordered by increasing
        head and tail ids in sorted_pseudo_heads.
    sorted_dupli_links: memoryview(long).
        All existing links in the grid, duplicated, ordered by
        increasing head and tail ids in sorted_pseudo_heads (see FlowRouter.py file).
    head_start_end_indexes: memoryview(longxlong).
        Start (array[0, :] and end (array[1, :]) index of all possible value nodes in
        the sorted_pseudo_heads (ordered by increasing head and tail ids, see
        FlowRouter.py file).
    outlet_nodes: memoryview(long).
        Ids of the base-level outlet node, for each donor node, ordered by the donor
        node ids.
    depression_outlet_nodes: memoryview(long).
        Outlet nodes of the depression, ordered by the donor node ids.
    flooded_nodes: memoryview(bool).
        Flooded status for each node, ordered by node ids.
    depression_depths: memoryview(double).
        Depths of the depression (if existing) below the level of the depression outlet
        for each node, ordered by node ids.
    depression_free_elevations: memoryview(double).
        Elevation of the surface corrected from depressions.
    links_to_receivers: memoryview(long).
        Ids of the links between a donor and a receiver, ordered by the ids of the
        donor nodes.
    receivers: memoryview(long)
        Ids of the receivers for all donor nodes, ordered by the id of the donor nodes.
    steepest_slopes: memoryview(double).
        Steepest slope for each node, ordered by node id.
    z: memoryview(double).
        Values of the surface where the flow is directed, for each node, ordered by
        node id.
    flooded_status: long.
        Constant for flooded status.
    bad_index: long.
        Constant for bad index.
    neighbors_max_number: long
        Maximum number of neighbors in the grid.
    min_elevation_relative_diff: double
        Minimum relative difference in elevation for the depression_free_elevations
        surface.
    """
    _direct_flow_c(
        nodes_n,
        base_level_nodes,
        closed_nodes,
        sorted_pseudo_tails,
        sorted_dupli_gradients,
        sorted_dupli_links,
        head_start_end_indexes,
        outlet_nodes,
        depression_outlet_nodes,
        flooded_nodes,
        depression_depths,
        depression_free_elevations,
        links_to_receivers,
        receivers,
        steepest_slopes,
        z,
        flooded_status,
        bad_index,
        neighbors_max_number,
        min_elevation_relative_diff,
    )



================================================
File: fracture_grid/__init__.py
================================================
from .fracture_grid import FractureGridGenerator

__all__ = ["FractureGridGenerator"]



================================================
File: fracture_grid/fracture_grid.py
================================================
#! /usr/env/python

"""Create 2D grid with randomly generated fractures.

Created: September 2013 by Greg Tucker
Last significant modification: conversion to proper component 7/2019 GT
"""

import numpy as np

from landlab import Component
from landlab import HexModelGrid
from landlab import RasterModelGrid


def _calc_fracture_starting_position_raster(shape):
    """Choose a random starting position along one of the sides of the grid.

    Parameters
    ----------
    shape : tuple of int
        Number of rows and columns in the grid

    Returns
    -------
    (c, r) : tuple of int
        Fracture starting coordinates (column and row IDs)
    """
    grid_side = np.random.randint(0, 3)  # east, north, west, south

    if (grid_side % 2) == 0:  # east or west
        c = (1 - grid_side // 2) * (shape[1] - 1)
        r = np.random.randint(0, shape[0] - 1)
    else:
        c = np.random.randint(0, shape[1] - 1)
        r = (1 - grid_side // 2) * (shape[0] - 1)

    return (c, r)


def _calc_fracture_starting_position_and_angle_hex(shape, is_horiz, spacing):
    """Choose a random starting position along one of the sides of the grid.

    Parameters
    ----------
    shape : tuple of int
        Number of rows and columns in the grid

    Returns
    -------
    (x, y, ang) : tuple of float
        Fracture starting coordinates and angle (radians)
    """
    grid_side = np.random.randint(0, 3)  # east, north, west, south
    ang = np.pi * np.random.rand()
    if is_horiz:
        row_spacing = 0.5 * 3.0**0.5 * spacing
        col_spacing = spacing
    else:
        row_spacing = spacing
        col_spacing = 0.5 * 3.0**0.5 * spacing

    if (grid_side % 2) == 0:  # east or west
        c = (1 - grid_side // 2) * (shape[1] - 1)
        r = np.random.randint(0, shape[0] - 1)
        if grid_side == 0:  # east
            ang += np.pi / 2
        else:  # west
            ang += 1.5 * np.pi
    else:
        c = np.random.randint(0, shape[1] - 1)
        r = (1 - grid_side // 2) * (shape[0] - 1)
        if grid_side == 1:  # north
            ang += np.pi

    x = c * col_spacing
    y = r * row_spacing

    epsilon = 0.001 * spacing  # tiny offset to ensure points start inside grid
    if x > epsilon:
        x -= epsilon
    if y > epsilon:
        y -= epsilon

    return (x, y, ang)


def _calc_fracture_orientation(coords, shape):
    """Choose a random orientation for the fracture.

    Parameters
    ----------
    coords : tuple of int
        Starting coordinates (one of which should be zero) as *x*, *y*.
    shape : tuple of int
        Number of rows and columns

    Returns
    -------
    ang : float
        Fracture angle relative to horizontal

    Notes
    -----
    The angle depends on which side of the grid the fracture starts on:
        - east: 90-270
        - north: 180-360
        - west: 270-450 (mod 360)
        - south: 0-180
    """
    x, y = coords
    ang = np.pi * np.random.rand()

    if x == shape[1] - 1:  # east
        ang += np.pi / 2
    elif y == shape[0] - 1:  # north
        ang += np.pi
    elif x == 0:  # west
        ang += 1.5 * np.pi

    return ang


def _calc_fracture_step_sizes(ang):
    """Calculate the sizes of steps dx and dy to be used when "drawing" the
    fracture onto the grid.

    Parameters
    ----------
    start_yx : tuple of int
        Starting grid coordinates
    ang : float
        Fracture angle relative to horizontal (radians)

    Returns
    -------
    (dy, dx) : tuple of float
        Step sizes in y and x directions. One will always be unity, and the
        other will always be <1.

    Examples
    --------
    >>> np.round(_calc_fracture_step_sizes(0 * np.pi / 6), 3)
    array([1., 0.])
    >>> np.round(_calc_fracture_step_sizes(1 * np.pi / 6), 3)
    array([1.   , 0.577])
    >>> np.round(_calc_fracture_step_sizes(2 * np.pi / 6), 3)
    array([0.577, 1.   ])
    >>> np.round(_calc_fracture_step_sizes(3 * np.pi / 6), 3)
    array([0., 1.])
    >>> np.round(_calc_fracture_step_sizes(4 * np.pi / 6), 3)
    array([-0.577,  1.   ])
    >>> np.round(_calc_fracture_step_sizes(5 * np.pi / 6), 3)
    array([-1.   ,  0.577])
    >>> np.round(_calc_fracture_step_sizes(6 * np.pi / 6), 3)
    array([-1.,  0.])
    >>> np.round(_calc_fracture_step_sizes(7 * np.pi / 6), 3)
    array([-1.   , -0.577])
    >>> np.round(_calc_fracture_step_sizes(8 * np.pi / 6), 3)
    array([-0.577, -1.   ])
    >>> np.round(_calc_fracture_step_sizes(9 * np.pi / 6), 3)
    array([-0., -1.])
    >>> np.round(_calc_fracture_step_sizes(10 * np.pi / 6), 3)
    array([ 0.577, -1.   ])
    >>> np.round(_calc_fracture_step_sizes(11 * np.pi / 6), 3)
    array([ 1.   , -0.577])
    >>> np.round(_calc_fracture_step_sizes(12 * np.pi / 6), 3)
    array([ 1., -0.])
    """
    dx = np.cos(ang)
    dy = np.sin(ang)
    multiplier = 1.0 / max(np.abs(dx), np.abs(dy))
    dx *= multiplier
    dy *= multiplier

    return dx, dy


def _trace_fracture_through_grid_raster(m, start_xy, spacing):
    """Create a 2D fracture in a grid.

    Creates a "fracture" in a 2D grid, m, by setting cell values to unity
    along the trace of the fracture (i.e., "drawing" a line throuh the
    grid).

    Parameters
    ----------
    m : 2D Numpy array
        Array that represents the grid
    start_xy : tuple of int
        Starting grid coordinates (col, row) for fracture
    spacing : tuple of float
        Step sizes in x and y directions

    Returns
    -------
    None, but changes contents of m
    """
    x0, y0 = start_xy
    dx, dy = spacing

    x = x0
    y = y0

    while (
        round(x) < np.size(m, 1)
        and round(y) < np.size(m, 0)
        and round(x) >= 0
        and round(y) >= 0
    ):
        m[int(y + 0.5)][int(x + 0.5)] = 1
        x += dx
        y += dy


class FractureGridGenerator(Component):
    """Create a 2D grid with randomly generated fractures.

    The grid contains the value 1 where fractures (one cell wide) exist, and
    0 elsewhere. The idea is to use this for simulations based on weathering
    and erosion of, and/or flow within, fracture networks.

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> grid = RasterModelGrid((5, 5))
    >>> fg = FractureGridGenerator(grid=grid, frac_spacing=3)
    >>> fg.run_one_step()
    >>> grid.at_node["fracture_at_node"].reshape((5, 5))
    array([[1, 0, 0, 1, 0],
           [0, 1, 1, 1, 1],
           [0, 0, 0, 1, 1],
           [0, 0, 0, 1, 1],
           [0, 0, 0, 1, 0]], dtype=int8)

    Notes
    -----
    Potential improvements:

    - Fractures could be defined by links rather than nodes (i.e., return a
        link array with a code indicating whether the link crosses a fracture
        or not)
    - Fractures could have a finite length rather than extending all the way
        across the grid
    - Use of starting position along either x or y axis makes fracture net
        somewhat asymmetric. One would need a different algorithm to make it
        fully (statistically) symmetric.

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    None Listed

    """

    _name = "FractureGridGenerator"

    _unit_agnostic = True

    _info = {
        "fracture_at_node": {
            "dtype": np.int8,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "presence (1) or absence (0) of fracture",
        }
    }

    def __init__(self, grid, frac_spacing=10.0, seed=0):
        """Initialize the FractureGridGenerator.

        Parameters
        ----------
        frac_spacing : int, optional
            Average spacing of fractures (in grid cells) (default = 10)
        seed : int, optional
            Seed used for random number generator (default = 0)

        """

        self._frac_spacing = frac_spacing
        self._seed = seed
        super().__init__(grid)

        if isinstance(grid, RasterModelGrid):
            self._make_frac_grid = self.make_frac_grid_raster
        elif isinstance(grid, HexModelGrid):
            self._make_frac_grid = self.make_frac_grid_hex
        else:
            raise TypeError("grid must be RasterModelGrid or HexModelGrid")

        if "fracture_at_node" not in grid.at_node:
            grid.add_zeros("fracture_at_node", at="node", dtype=np.int8)

        np.random.seed(seed)

    def run_one_step(self):
        """Run FractureGridGenerator and create a random fracture grid."""
        self._make_frac_grid(self._frac_spacing)

    def make_frac_grid_raster(self, frac_spacing):
        """Create a raster grid that contains a network of random fractures.

        Creates a grid containing a network of random fractures, which are
        represented as 1's embedded in a grid of 0's. The grid is stored in
        the "fracture_at_node" field.

        Parameters
        ----------
        frac_spacing : int
            Average spacing of fractures (in grid cells)
        """
        # Make an initial grid of all zeros. If user specified a model grid,
        # use that. Otherwise, use the given dimensions.
        nr = self._grid.number_of_node_rows
        nc = self._grid.number_of_node_columns
        m = self._grid.at_node["fracture_at_node"].reshape((nr, nc))

        # Add fractures to grid
        nfracs = (nr + nc) // frac_spacing
        for _ in range(nfracs):
            (c, r) = _calc_fracture_starting_position_raster((nr, nc))
            ang = _calc_fracture_orientation((c, r), (nr, nc))
            (dx, dy) = _calc_fracture_step_sizes(ang)

            _trace_fracture_through_grid_raster(m, (c, r), (dx, dy))

    def make_frac_grid_hex(self, frac_spacing):
        """Create a hex grid that contains a network of random fractures.

        Creates a grid containing a network of random fractures, which are
        represented as 1's embedded in a grid of 0's. The grid is stored in
        the "fracture_at_node" field.

        Parameters
        ----------
        frac_spacing : int
            Average spacing of fractures (in # of grid-cell widths)
        """
        # Make an initial grid of all zeros
        nr = self._grid.number_of_node_rows
        nc = self._grid.number_of_node_columns
        m = self._grid.at_node["fracture_at_node"]  # .reshape((nr, nc))

        # Add fractures to grid
        nfracs = (nr + nc) // frac_spacing
        for _ in range(nfracs):
            (x, y, ang) = _calc_fracture_starting_position_and_angle_hex(
                (nr, nc),
                is_horiz=(self._grid.orientation[0] == "h"),
                spacing=self._grid.spacing,
            )

            dx = 0.5 * np.cos(ang)
            dy = 0.5 * np.sin(ang)

            # the following is a TERRIBLE brute-force, algorithm, with its only
            # redeeming feature being that it was quick to code
            xmax = np.amax(self._grid.x_of_node)
            ymax = np.amax(self._grid.y_of_node)
            while x >= 0 and x <= xmax and y >= 0 and y <= ymax:
                distx2 = (self._grid.x_of_node - x) ** 2
                disty2 = (self._grid.y_of_node - y) ** 2
                dist = np.sqrt(distx2 + disty2)
                closest_node = np.argmin(dist)
                x += dx
                y += dy
                m[closest_node] = 1



================================================
File: gflex/__init__.py
================================================
from .flexure import gFlex

__all__ = ["gFlex"]



================================================
File: gflex/flexure.py
================================================
"""This is a Landlab wrapper for A Wickert's gFlex flexure model (Wickert et
al., submitted to Geoscientific Model Development). The most up-to-date version
of his code can be found at github.com/awickert/gFlex.

This Landlab wrapper will use a snapshot of that code, which YOU need to
install on your own machine.
A stable snapshot of gFlex is hosted on PyPI, which is the recommended version
to install.
If you have pip (the Python package install tool), simply run
'pip install gFlex' from a command prompt.
Alternatively, you can download and unpack the code (from github, or with PyPI,
pypi.python.org/pypi/gFlex/), then run 'python setup.py install'.

Created on Thu Feb 19 18:47:11 2015

@author: daniel.hobley (SiccarPoint @Github)

...following AW's run_in_script_2D.py.
"""

import numpy as np
import scipy.constants

from landlab import Component
from landlab import FieldError
from landlab import RasterModelGrid

try:
    import gflex
except ImportError:
    NO_GFLEX = True
else:
    NO_GFLEX = False


class gFlex(Component):
    """This is a Landlab wrapper for A Wickert's gFlex flexure model (Wickert
    et al., 2016, Geoscientific Model Development). The most up-to-date version
    of his code can be found at github.com/awickert/gFlex.

    This Landlab wrapper will use a snapshot of that code, which YOU need to
    install on your own machine.
    A stable snapshot of gFlex is hosted on PyPI, which is the recommended
    version to install.
    If you have pip (the Python package install tool), simply run
    'pip install gFlex' from a command prompt.
    Alternatively, you can download and unpack the code (from github, or with
    PyPI, pypi.python.org/pypi/gFlex/), then run 'python setup.py install'.

    Note that gFlex maintains its own internal version if the grid, but this
    should not affect performance.

    This component will modify the topographic__elevation field only if one
    already exists. Note that the gFlex component **demands lengths in
    meters**, including the grid dimensions.
    The component also recognises the gFlex specific parameters 'Method',
    'PlateSolutionType', 'Solver', and 'Quiet'. See the gFlex software
    documentation for more details.

    Examples
    --------

    NB: these tests are not actually run as our automated testing becomes
    confused if gFlex is not installed on the testing machine!

    >>> from landlab import RasterModelGrid
    >>> from landlab.components import gFlex
    >>> mg = RasterModelGrid((10, 10), xy_spacing=25000.0)
    >>> z = mg.add_zeros("topographic__elevation", at="node", dtype=float)
    >>> stress = mg.add_zeros("surface_load__stress", at="node", dtype=float)
    >>> stress.view().reshape(mg.shape)[3:7, 3:7] += 1.0e6
    >>> gf = gFlex(
    ...     mg, BC_E="0Moment0Shear", BC_N="Periodic", BC_S="Periodic"
    ... )  # doctest: +SKIP
    >>> gf.run_one_step()  # doctest: +SKIP

    N-S profile across flexed plate:

    >>> z.reshape(mg.shape)[:, 5]  # doctest: +SKIP
    array([-4.54872677, -4.6484927 , -4.82638669, -5.03001546, -5.15351385,
           -5.15351385, -5.03001546, -4.82638669, -4.6484927 , -4.54872677])

    W-E profile, noting the free BC to the east side:

    >>> z.reshape(mg.shape)[5, :]  # doctest: +SKIP
    array([-0.43536739, -1.19197738, -2.164915  , -3.2388464 , -4.2607558 ,
           -5.15351385, -5.89373366, -6.50676947, -7.07880156, -7.63302576])

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Wickert, A. (2016). Open-source modular solutions for flexural isostasy:
    gFlex v1.0. Geoscientific Model Development  9(3), 997-1017.
    https://dx.doi.org/10.5194/gmd-9-997-2016

    **Additional References**

    None Listed

    """

    _name = "gFlex"

    _unit_agnostic = True

    _cite_as = """
    @article{wickert2016open,
      author = {Wickert, A. D.},
      title = {{Open-source modular solutions for flexural isostasy: gFlex v1.0}},
      issn = {1991-959X},
      doi = {10.5194/gmd-9-997-2016},
      pages = {997--1017},
      number = {3},
      volume = {9},
      journal = {Geoscientific Model Development},
      year = {2016}
    }
    """
    _info = {
        "lithosphere_surface__elevation_increment": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": (
                "The change in elevation of the top of the lithosphere (the "
                "land surface) in one timestep"
            ),
        },
        "surface_load__stress": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "Pa",
            "mapping": "node",
            "doc": "Magnitude of stress exerted by surface load",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
    }

    def __init__(
        self,
        grid,
        Youngs_modulus=6.5e11,
        Poissons_ratio=0.25,
        rho_mantle=3300.0,
        rho_fill=0.0,
        elastic_thickness=35000.0,
        Method="FD",
        Solver="direct",
        PlateSolutionType="vWC1994",
        quiet=True,
        BC_W="0Displacement0Slope",
        BC_E="0Displacement0Slope",
        BC_N="0Displacement0Slope",
        BC_S="0Displacement0Slope",
        g=scipy.constants.g,
    ):
        """Constructor for Wickert's gFlex in Landlab.

        Parameters
        ----------
        Youngs_modulus : float
            Young's modulus for the lithosphere.
        Poissons_ratio : float
            Poisson's ratio for the lithosphere.
        rho_mantle : float (kg*m**-3)
            The density of the mantle.
        rho_fill : float (kg*m**-3)
            The density of the infilling material (air, water...)
        elastic_thickness : float (m)
            The elastic thickness of the lithosphere.
        BC_W, BC_E, BC_N, BC_S : {'0Displacement0Slope', '0Moment0Shear',
                                  'Periodic'}
            The boundary condition status of each grid edge, following gFlex's
            definitions. Periodic boundaries must be paired (obviously).
        g : float (m*s**-2)
            The acceleration due to gravity.
        """
        super().__init__(grid)

        assert isinstance(grid, RasterModelGrid)

        if NO_GFLEX:
            raise ImportError(
                "gFlex not installed! For installation instructions see "
                + "gFlex on GitHub: https://github.com/awickert/gFlex"
            )
        BC_options = (
            "0Displacement0Slope",
            "0Moment0Shear",
            "0Slope0Shear",
            "Periodic",
        )

        # instantiate the module:
        self._flex = gflex.F2D()
        flex = self._flex

        # set up the grid variables:

        flex.dx = grid.dx
        flex.dy = grid.dy

        # we assume these properties are fixed in this relatively
        # straightforward implementation, but they can still be set if you
        # want:
        flex.Method = Method
        flex.PlateSolutionType = PlateSolutionType
        flex.Solver = Solver
        flex.Quiet = quiet

        flex.E = float(Youngs_modulus)
        flex.nu = float(Poissons_ratio)
        flex.rho_m = float(rho_mantle)
        flex.rho_fill = float(rho_fill)
        flex.g = float(g)
        flex.BC_W = BC_W
        flex.BC_E = BC_E
        flex.BC_S = BC_S
        flex.BC_N = BC_N
        for i in (flex.BC_E, flex.BC_W, flex.BC_N, flex.BC_S):
            assert i in BC_options

        if BC_W == "Periodic":
            assert BC_E == "Periodic"
        if BC_E == "Periodic":
            assert BC_W == "Periodic"
        if BC_N == "Periodic":
            assert BC_S == "Periodic"
        if BC_S == "Periodic":
            assert BC_N == "Periodic"

        Te_in = elastic_thickness
        try:
            flex.Te = float(Te_in)
        except ValueError:
            try:
                flex.Te = grid.at_node[Te_in].view().reshape(grid.shape)
            except TypeError:
                flex.Te = Te_in.view().reshape(grid.shape)
            self._input_var_names.add(Te_in)
            self._output_var_names.add(Te_in)

        # set up the link between surface load stresses in the gFlex component
        # and the LL grid field:
        flex.qs = grid.at_node["surface_load__stress"].view().reshape(grid.shape)

        # create a holder for the "pre-flexure" state of the grid, to allow
        # updating of elevs:
        self._pre_flex = np.zeros(grid.number_of_nodes, dtype=float)

        # create the primary output field:
        self._grid.add_zeros(
            "lithosphere_surface__elevation_increment",
            at="node",
            dtype=float,
            clobber=True,
        )

    def flex_lithosphere(self):
        """Executes (& finalizes, from the perspective of gFlex) the core
        method of gFlex.

        Note that flexure of the lithosphere proceeds to steady state in
        a single timestep.
        """
        self._flex.qs = (
            self._grid.at_node["surface_load__stress"].view().reshape(self._grid.shape)
        )
        self._flex.initialize()
        self._flex.run()
        self._flex.finalize()

        self._grid.at_node["lithosphere_surface__elevation_increment"][
            :
        ] = self._flex.w.view().ravel()

        try:
            self._grid.at_node["topographic__elevation"]
            # a topo exists...
        except FieldError:
            pass
        else:
            topo_diff = (
                self._grid.at_node["lithosphere_surface__elevation_increment"]
                - self._pre_flex
            )
            self._grid.at_node["topographic__elevation"] += topo_diff
            self._pre_flex += topo_diff

    def run_one_step(self):
        """Flex the lithosphere to find its steady state form.

        The standardized run method for this component.

        Parameters
        ----------
        None
        """
        self._flex_lithosphere()



================================================
File: gravel_bedrock_eroder/__init__.py
================================================
from .gravel_bedrock_eroder import GravelBedrockEroder

__all__ = ["GravelBedrockEroder"]



================================================
File: gravel_bedrock_eroder/gravel_bedrock_eroder.py
================================================
#!/usr/bin/env python3
"""
Model bedrock incision and gravel transport and abrasion in a network of rivers.

@author: gtucker
"""

import numpy as np

from landlab import Component
from landlab import HexModelGrid
from landlab.grid.diagonals import DiagonalsMixIn

_DT_MAX = 1.0e-2
_ONE_SIXTH = 1.0 / 6.0
_SEVEN_SIXTHS = 7.0 / 6.0


class GravelBedrockEroder(Component):
    """Drainage network evolution of rivers with gravel alluvium overlying bedrock.

    Model drainage network evolution for a network of rivers that have
    a layer of gravel alluvium overlying bedrock.

    :class:`~.GravelBedrockEroder` is designed to operate together with a flow-routing
    component such as :class:`~.FlowAccumulator`, so that each grid node has
    a defined flow direction toward one of its neighbor nodes. Each core node
    is assumed to contain one outgoing fluvial channel, and (depending on
    the drainage structure) zero, one, or more incoming channels. These channels are
    treated as effectively sub-grid-scale features that are embedded in valleys
    that have a width of one grid cell.

    As with the :class:`~.GravelRiverTransporter` component, the rate of gravel
    transport out of a given node is calculated as the product of bankfull discharge,
    channel gradient (to the 7/6 power), a dimensionless transport coefficient, and
    an intermittency factor that represents the fraction of time that bankfull
    flow occurs. The derivation of the transport law is given by Wickert &
    Schildgen (2019), and it derives from the assumption that channels are
    gravel-bedded and that they "instantaneously" adjust their width such that
    bankfull bed shear stress is just slightly higher than the threshold for
    grain motion. The substrate is assumed to consist entirely of gravel-size
    material with a given bulk porosity. The component calculates the loss of
    gravel-sized material to abrasion (i.e., conversion to finer sediment, which
    is not explicitly tracked) as a function of the volumetric transport rate,
    an abrasion coefficient with units of inverse length, and the local transport
    distance (for example, if a grid node is carrying a gravel load ``Qs`` to a
    neighboring node ``dx`` meters downstream, the rate of gravel loss in volume per
    time per area at the node will be ``beta * Qs * dx``, where ``beta`` is the abrasion
    coefficient).

    Sediment mass conservation is calculated across each entire
    grid cell. For example, if a cell has surface area ``A``, a total volume influx
    ``Qin``, and downstream transport rate ``Qs``, the resulting rate of change of
    alluvium thickness will be ``(Qin - Qs / (A * (1 - phi))``, plus gravel produced by
    plucking erosion of bedrock (``phi`` is porosity).

    Bedrock is eroded by a combination of abrasion and plucking. Abrasion per unit
    channel length is calculated as the product of volumetric sediment discharge
    and an abrasion coefficient. Sediment produced by abrasion is assumed to
    go into wash load that is removed from the model domain. Plucking is calculated
    using a discharge-slope expression, and a user-defined fraction of plucked
    material is added to the coarse alluvium.

    Parameters
    ----------
    grid : ModelGrid
        A Landlab model grid object
    intermittency_factor : float (default 0.01)
        Fraction of time that bankfull flow occurs
    transport_coefficient : float (default 0.041)
        Dimensionless transport efficiency factor (see Wickert & Schildgen 2019)
    abrasion_coefficient : float (default 0.0 1/m)
        Abrasion coefficient with units of inverse length
    sediment_porosity : float (default 0.35)
        Bulk porosity of bed sediment
    depth_decay_scale : float (default 1.0)
        Scale for depth decay in bedrock exposure function
    plucking_coefficient : float or (n_core_nodes,) array of float (default 1.0e-4 1/m)
        Rate coefficient for bedrock erosion by plucking
    coarse_fraction_from_plucking : float or (n_core_nodes,) array of float (default 1.0)
        Fraction of plucked material that becomes part of gravel sediment load

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import FlowAccumulator
    >>> grid = RasterModelGrid((3, 3), xy_spacing=1000.0)
    >>> elev = grid.add_zeros("topographic__elevation", at="node")
    >>> sed = grid.add_zeros("soil__depth", at="node")
    >>> sed[4] = 300.0
    >>> grid.status_at_node[grid.perimeter_nodes] = grid.BC_NODE_IS_CLOSED
    >>> grid.status_at_node[5] = grid.BC_NODE_IS_FIXED_VALUE
    >>> fa = FlowAccumulator(grid, runoff_rate=10.0)
    >>> fa.run_one_step()
    >>> eroder = GravelBedrockEroder(grid, abrasion_coefficient=0.0005)
    >>> rock_elev = grid.at_node["bedrock__elevation"]
    >>> for _ in range(200):
    ...     rock_elev[grid.core_nodes] += 1.0
    ...     elev[grid.core_nodes] += 1.0
    ...     fa.run_one_step()
    ...     eroder.run_one_step(10000.0)
    ...
    >>> int(elev[4] * 100)
    2266
    """

    _name = "GravelBedrockEroder"

    _unit_agnostic = True

    _info = {
        "bedload_sediment__rate_of_loss_to_abrasion": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/y",
            "mapping": "node",
            "doc": "Rate of bedload sediment volume loss to abrasion per unit area",
        },
        "bedload_sediment__volume_influx": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m**3/y",
            "mapping": "node",
            "doc": "Volumetric incoming streamwise bedload sediment transport rate",
        },
        "bedload_sediment__volume_outflux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m**3/y",
            "mapping": "node",
            "doc": "Volumetric outgoing streamwise bedload sediment transport rate",
        },
        "bedrock__abrasion_rate": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/y",
            "mapping": "node",
            "doc": "rate of bedrock lowering by abrasion",
        },
        "bedrock__elevation": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "elevation of the bedrock surface",
        },
        "bedrock__exposure_fraction": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "fractional exposure of bedrock",
        },
        "bedrock__plucking_rate": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/y",
            "mapping": "node",
            "doc": "rate of bedrock lowering by plucking",
        },
        "bedrock__lowering_rate": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/y",
            "mapping": "node",
            "doc": "Rate of lowering of bedrock surface",
        },
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__upstream_node_order": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array containing downstream-to-upstream ordered list of node IDs",
        },
        "sediment__rate_of_change": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/y",
            "mapping": "node",
            "doc": "Time rate of change of sediment thickness",
        },
        "soil__depth": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of soil or weathered bedrock",
        },
        "surface_water__discharge": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m**3/y",
            "mapping": "node",
            "doc": "Volumetric discharge of surface water",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__steepest_slope": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "The steepest *downhill* slope",
        },
    }

    def __init__(
        self,
        grid,
        intermittency_factor=0.01,
        transport_coefficient=0.041,
        abrasion_coefficient=0.0,
        sediment_porosity=0.35,
        depth_decay_scale=1.0,
        plucking_coefficient=1.0e-4,
        coarse_fraction_from_plucking=1.0,
    ):
        """Initialize GravelBedrockEroder."""

        super().__init__(grid)

        # Parameters
        self._trans_coef = transport_coefficient
        self._intermittency_factor = intermittency_factor
        self._abrasion_coef = abrasion_coefficient
        self._porosity_factor = 1.0 / (1.0 - sediment_porosity)
        self._depth_decay_scale = depth_decay_scale
        if (
            isinstance(plucking_coefficient, np.ndarray)
            and len(plucking_coefficient) == self.grid.number_of_nodes
        ):
            plucking_coefficient = plucking_coefficient[self.grid.core_nodes]
        self._plucking_coef = plucking_coefficient
        if (
            isinstance(coarse_fraction_from_plucking, np.ndarray)
            and len(coarse_fraction_from_plucking) == self.grid.number_of_nodes
        ):
            coarse_fraction_from_plucking = coarse_fraction_from_plucking[
                self.grid.core_nodes
            ]
        self._pluck_coarse_frac = coarse_fraction_from_plucking

        # Fields and arrays
        self._elev = grid.at_node["topographic__elevation"]
        self._sed = grid.at_node["soil__depth"]
        if "bedrock__elevation" in grid.at_node:
            self._bedrock__elevation = grid.at_node["bedrock__elevation"]
        else:
            self._bedrock__elevation = grid.add_zeros(
                "bedrock__elevation", at="node", dtype=float
            )
            self._bedrock__elevation[:] = self._elev - self._sed
        self._discharge = grid.at_node["surface_water__discharge"]
        self._slope = grid.at_node["topographic__steepest_slope"]
        self._receiver_node = grid.at_node["flow__receiver_node"]
        self._receiver_link = grid.at_node["flow__link_to_receiver_node"]
        super().initialize_output_fields()
        self._sediment_influx = grid.at_node["bedload_sediment__volume_influx"]
        self._sediment_outflux = grid.at_node["bedload_sediment__volume_outflux"]
        self._dHdt = grid.at_node["sediment__rate_of_change"]
        self._rock_lowering_rate = grid.at_node["bedrock__lowering_rate"]
        self._abrasion = grid.at_node["bedload_sediment__rate_of_loss_to_abrasion"]
        self._rock_exposure_fraction = grid.at_node["bedrock__exposure_fraction"]
        self._rock_abrasion_rate = grid.at_node["bedrock__abrasion_rate"]
        self._pluck_rate = grid.at_node["bedrock__plucking_rate"]

        self._setup_length_of_flow_link()

    def _setup_length_of_flow_link(self):
        """Set up a float or array containing length of the flow link from
        each node, which is needed for the abrasion rate calculations.
        """
        if isinstance(self.grid, HexModelGrid):
            self._flow_link_length_over_cell_area = (
                self.grid.spacing / self.grid.area_of_cell[0]
            )
            self._flow_length_is_variable = False
        elif isinstance(self.grid, DiagonalsMixIn):
            self._flow_length_is_variable = True
            self._grid_has_diagonals = True
            self._update_flow_link_length_over_cell_area()
        else:
            self._flow_length_is_variable = True
            self._grid_has_diagonals = False
            self._update_flow_link_length_over_cell_area()

    def _update_flow_link_length_over_cell_area(self):
        """Update the ratio of the length of link along which water flows out of
        each node to the area of the node's cell."""
        if self._grid_has_diagonals:
            flow_link_len = self.grid.length_of_d8
        else:
            flow_link_len = self.grid.length_of_link
        self._flow_link_length_over_cell_area = (
            flow_link_len[self._receiver_link[self.grid.core_nodes]]
            / self.grid.area_of_cell[self.grid.cell_at_node[self.grid.core_nodes]]
        )

    def calc_implied_depth(self, grain_diameter=0.01):
        """Utility function that calculates and returns water depth implied by
        slope and grain diameter, using Wickert & Schildgen (2019) equation 8.

        The equation is::

            h = ((rho_s - rho / rho)) * (1 + epsilon) * tau_c * (D / S)

        where the factors on the right are sediment and water density, excess
        shear-stress factor, critical Shields stress, grain diameter, and slope
        gradient. Here the prefactor on ``D/S`` assumes sediment density of 2650 kg/m3,
        water density of 1000 kg/m3, shear-stress factor of 0.2, and critical
        Shields stress of 0.0495, giving a value of 0.09801.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> grid = RasterModelGrid((3, 3), xy_spacing=1000.0)
        >>> elev = grid.add_zeros("topographic__elevation", at="node")
        >>> elev[3:] = 10.0
        >>> sed = grid.add_zeros("soil__depth", at="node")
        >>> sed[3:] = 100.0
        >>> fa = FlowAccumulator(grid)
        >>> fa.run_one_step()
        >>> eroder = GravelBedrockEroder(grid)
        >>> water_depth = eroder.calc_implied_depth()
        >>> int(water_depth[4] * 1000)
        98
        """
        depth_factor = 0.09801
        depth = np.zeros(self._grid.number_of_nodes)
        nonzero_slope = self._slope > 0.0
        depth[nonzero_slope] = (
            depth_factor * grain_diameter / self._slope[nonzero_slope]
        )
        return depth

    def calc_implied_width(self, grain_diameter=0.01, time_unit="y"):
        """Utility function that calculates and returns channel width implied by
        discharge, slope, and grain diameter, using Wickert & Schildgen (2019)
        equation 16.

        The equation is::

            b = kb * Q * S**(7/6) / D**(3/2)

        where the dimensional prefactor, which includes sediment and water
        density, gravitational acceleration, critical Shields stress, and the
        transport factor epsilon, is::

            kb = 0.17 g**(-1/2) (((rho_s - rho) / rho) (1 + eps) tau_c*)**(-5/3)

        Using ``g = 9.8 m/s2``, ``rho_s = 2650`` (quartz), ``rho = 1000 kg/m3``, ``eps = 0.2``,
        and ``tau_c* = 0.0495``, ``kb ~ 2.61 s/m**(1/2)``. Converting to years,
        ``kb = 8.26e-8``.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> grid = RasterModelGrid((3, 3), xy_spacing=10000.0)
        >>> elev = grid.add_zeros("topographic__elevation", at="node")
        >>> elev[3:] = 100.0
        >>> sed = grid.add_zeros("soil__depth", at="node")
        >>> sed[3:] = 100.0
        >>> fa = FlowAccumulator(grid)
        >>> fa.run_one_step()
        >>> eroder = GravelBedrockEroder(grid)
        >>> chan_width = eroder.calc_implied_width()
        >>> int(chan_width[4] * 100)
        3833
        >>> grid.at_node["surface_water__discharge"] *= 1.0 / (3600 * 24 * 365.25)
        >>> chan_width = eroder.calc_implied_width(time_unit="s")
        >>> int(chan_width[4] * 100)
        3838
        """
        if time_unit[0] == "y":
            width_fac = 8.26e-8
        else:
            width_fac = 2.61  # assume seconds if not years
        width = (
            width_fac
            * self._discharge
            * self._slope ** (7.0 / 6.0)
            / (grain_diameter**1.5)
        )
        return width

    def calc_rock_exposure_fraction(self):
        """Update the bedrock exposure fraction.

        The result is stored in the ``bedrock__exposure_fraction`` field.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> grid = RasterModelGrid((3, 4), xy_spacing=100.0)
        >>> elev = grid.add_zeros("topographic__elevation", at="node")
        >>> sed = grid.add_zeros("soil__depth", at="node")
        >>> sed[4] = 1000.0
        >>> sed[5] = 0.0
        >>> fa = FlowAccumulator(grid)
        >>> fa.run_one_step()
        >>> eroder = GravelBedrockEroder(grid)
        >>> eroder.calc_rock_exposure_fraction()
        >>> eroder._rock_exposure_fraction[4:6]
        array([0., 1.])
        >>> sed[4] = 1.0  # exposure frac should be 1/e ~ 0.3679
        >>> sed[5] = 2.0  # exposure frac should be 1/e^2 ~ 0.1353
        >>> eroder.calc_rock_exposure_fraction()
        >>> np.round(eroder._rock_exposure_fraction[4:6], 4)
        array([0.3679, 0.1353])
        """
        self._rock_exposure_fraction[:] = np.exp(-self._sed / self._depth_decay_scale)

    def calc_transport_rate(self):
        """Calculate and return bed-load transport rate.

        Calculation uses Wickert-Schildgen approach, and provides
        volume per time rate. Transport rate is modulated by available
        sediment, using the exponential function ``(1 - exp(-H / Hs))``,
        so that transport rate approaches zero as sediment thickness
        approaches zero. Rate is a volume per time. The result is
        stored in the ``bedload_sediment__volume_outflux`` field.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> grid = RasterModelGrid((3, 3), xy_spacing=100.0)
        >>> elev = grid.add_zeros("topographic__elevation", at="node")
        >>> elev[3:] = 1.0
        >>> sed = grid.add_zeros("soil__depth", at="node")
        >>> sed[3:] = 100.0
        >>> fa = FlowAccumulator(grid)
        >>> fa.run_one_step()
        >>> eroder = GravelBedrockEroder(grid)
        >>> eroder.calc_transport_rate()
        >>> round(eroder._sediment_outflux[4], 4)
        0.019
        """
        self._sediment_outflux[:] = (
            self._trans_coef
            * self._intermittency_factor
            * self._discharge
            * self._slope**_SEVEN_SIXTHS
            * (1.0 - self._rock_exposure_fraction)
        )

    def calc_abrasion_rate(self):
        """Update the volume rate of bedload loss to abrasion, per unit area.

        Here we use the average of incoming and outgoing sediment flux to
        calculate the loss rate to abrasion. The result is stored in the
        ``bedload_sediment__rate_of_loss_to_abrasion`` field.

        The factor dx (node spacing) appears in the denominator to represent
        flow segment length (i.e., length of the link along which water is
        flowing in the cell) divided by cell area. This would need to be updated
        to handle non-raster and/or non-uniform grids.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> grid = RasterModelGrid((3, 3), xy_spacing=1000.0)
        >>> elev = grid.add_zeros("topographic__elevation", at="node")
        >>> elev[3:] = 10.0
        >>> sed = grid.add_zeros("soil__depth", at="node")
        >>> sed[3:] = 100.0
        >>> fa = FlowAccumulator(grid)
        >>> fa.run_one_step()
        >>> eroder = GravelBedrockEroder(grid, abrasion_coefficient=0.0002)
        >>> eroder.calc_transport_rate()
        >>> eroder.calc_abrasion_rate()
        >>> int(eroder._abrasion[4] * 1e8)
        19
        """
        cores = self._grid.core_nodes
        self._abrasion[cores] = (
            self._abrasion_coef
            * 0.5
            * (self._sediment_outflux[cores] + self._sediment_influx[cores])
            * self._flow_link_length_over_cell_area
        )

    def calc_bedrock_abrasion_rate(self):
        """Update the rate of bedrock abrasion.

        Note: assumes _abrasion (of sediment) and _rock_exposure_fraction
        have already been updated. Like _abrasion, the rate is a length
        per time (equivalent to rate of lowering of the bedrock surface by
        abrasion). Result is stored in the field ``bedrock__abrasion_rate``.

        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> grid = RasterModelGrid((3, 4), xy_spacing=100.0)
        >>> elev = grid.add_zeros("topographic__elevation", at="node")
        >>> elev[:] = 0.01 * grid.x_of_node
        >>> sed = grid.add_zeros("soil__depth", at="node")
        >>> sed[:] = 1.0
        >>> fa = FlowAccumulator(grid)
        >>> fa.run_one_step()
        >>> eroder = GravelBedrockEroder(grid, abrasion_coefficient=1.0e-4)
        >>> eroder.calc_rock_exposure_fraction()
        >>> round(eroder._rock_exposure_fraction[6], 4)
        0.3679
        >>> eroder.calc_transport_rate()
        >>> np.round(eroder._sediment_outflux[5:7], 3)
        array([0.024, 0.012])
        >>> eroder.calc_abrasion_rate()
        >>> np.round(eroder._abrasion[5:7], 9)
        array([1.2e-08, 6.0e-09])
        >>> eroder.calc_bedrock_abrasion_rate()
        >>> np.round(eroder._rock_abrasion_rate[5:7], 10)
        array([4.4e-09, 2.2e-09])
        """
        self._rock_abrasion_rate = self._abrasion * self._rock_exposure_fraction

    def calc_bedrock_plucking_rate(self):
        """Update the rate of bedrock erosion by plucking.

        The rate is a volume per area per time [L/T], equivalent to the
        rate of lowering of the bedrock surface relative to the underlying
        material as a result of plucking. Result is stored in the field
        ``bedrock__plucking_rate``.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> grid_res = 100.0
        >>> grid = RasterModelGrid((3, 3), xy_spacing=grid_res)
        >>> elev = grid.add_zeros("topographic__elevation", at="node")
        >>> elev[4] = 1.0
        >>> sed = grid.add_zeros("soil__depth", at="node")
        >>> fa = FlowAccumulator(grid)
        >>> fa.run_one_step()
        >>> eroder = GravelBedrockEroder(grid)
        >>> eroder.calc_rock_exposure_fraction()
        >>> eroder.calc_bedrock_plucking_rate()
        >>> predicted_plucking_rate = 1.0e-6 * 1.0e4 * 0.01 ** (7.0 / 6.0) / grid_res
        >>> round(predicted_plucking_rate, 9)  # Kp Q S^(7/6)
        4.64e-07
        >>> int(round(eroder._pluck_rate[4] * 1e9))
        464
        """
        cores = self._grid.core_nodes
        self._pluck_rate[cores] = (
            self._plucking_coef
            * self._intermittency_factor
            * self._discharge[cores]
            * self._slope[cores] ** _SEVEN_SIXTHS
            * self._rock_exposure_fraction[cores]
        ) * self._flow_link_length_over_cell_area

    def calc_sediment_influx(self):
        """Update the volume influx at each node.

        Result is stored in the field ``bedload_sediment__volume_influx``.
        """
        self._sediment_influx[:] = 0.0
        for c in self.grid.core_nodes:  # send sediment downstream
            r = self._receiver_node[c]
            self._sediment_influx[r] += self._sediment_outflux[c]

    def calc_sediment_rate_of_change(self):
        """Update the rate of thickness change of coarse sediment at each core node.

        Result is stored in the field ``sediment__rate_of_change``.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> grid = RasterModelGrid((3, 4), xy_spacing=100.0)
        >>> elev = grid.add_zeros("topographic__elevation", at="node")
        >>> elev[:] = 0.01 * grid.x_of_node
        >>> sed = grid.add_zeros("soil__depth", at="node")
        >>> sed[:] = 100.0
        >>> grid.status_at_node[grid.perimeter_nodes] = grid.BC_NODE_IS_CLOSED
        >>> grid.status_at_node[4] = grid.BC_NODE_IS_FIXED_VALUE
        >>> fa = FlowAccumulator(grid)
        >>> fa.run_one_step()
        >>> eroder = GravelBedrockEroder(grid)
        >>> eroder.calc_transport_rate()
        >>> eroder.calc_sediment_influx()
        >>> eroder.calc_sediment_rate_of_change()
        >>> np.round(eroder._sediment_outflux[4:7], 3)
        array([0.   , 0.038, 0.019])
        >>> np.round(eroder._sediment_influx[4:7], 3)
        array([0.038, 0.019, 0.   ])
        >>> np.round(eroder._dHdt[5:7], 8)
        array([-2.93e-06, -2.93e-06])
        """
        cores = self.grid.core_nodes
        self._dHdt[cores] = self._porosity_factor * (
            (self._sediment_influx[cores] - self._sediment_outflux[cores])
            / self.grid.area_of_cell[self.grid.cell_at_node[cores]]
            + (self._pluck_rate[cores] * self._pluck_coarse_frac)
            - self._abrasion[cores]
        )

    def _update_slopes(self):
        """Update self._slope.

        Result is stored in field ``topographic__steepest_slope``.
        """
        dz = np.maximum(self._elev - self._elev[self._receiver_node], 0.0)
        if self._flow_length_is_variable:
            if self._grid_has_diagonals:
                link_len = self.grid.length_of_d8
            else:
                link_len = self.grid.length_of_link
            self._slope[self.grid.core_nodes] = (
                dz[self.grid.core_nodes] / link_len[self.grid.core_nodes]
            )
        else:
            self._slope[self.grid.core_nodes] = (
                dz[self.grid.core_nodes] / self.grid.spacing
            )

    def update_rates(self):
        """Update rate of sediment thickness change, and rate of bedrock lowering by abrasion
        and plucking.

        Combined rate of rock lowering relative to underlying material is stored in the field
        ``bedrock__lowering_rate``.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> grid = RasterModelGrid((3, 4), xy_spacing=100.0)
        >>> elev = grid.add_zeros("topographic__elevation", at="node")
        >>> elev[:] = 0.01 * grid.x_of_node
        >>> sed = grid.add_zeros("soil__depth", at="node")
        >>> sed[:] = 1000.0
        >>> grid.status_at_node[grid.perimeter_nodes] = grid.BC_NODE_IS_CLOSED
        >>> grid.status_at_node[4] = grid.BC_NODE_IS_FIXED_VALUE
        >>> fa = FlowAccumulator(grid)
        >>> fa.run_one_step()
        >>> eroder = GravelBedrockEroder(grid)
        >>> eroder.run_one_step(1000.0)
        >>> np.round(elev[4:7], 4)
        array([0.    , 0.9971, 1.9971])
        """
        self._update_slopes()
        self.calc_rock_exposure_fraction()
        self.calc_transport_rate()
        self.calc_sediment_influx()

        if self._flow_length_is_variable:
            self._update_flow_link_length_over_cell_area()
        self.calc_bedrock_plucking_rate()
        if self._abrasion_coef > 0.0:
            self.calc_abrasion_rate()
            self.calc_bedrock_abrasion_rate()
        self.calc_sediment_rate_of_change()
        self._rock_lowering_rate = self._pluck_rate + self._rock_abrasion_rate

    def _update_rock_sed_and_elev(self, dt):
        """Update rock elevation, sediment thickness, and elevation
        using current rates of change extrapolated forward by time dt.
        """
        self._sed += self._dHdt * dt
        self._bedrock__elevation -= self._rock_lowering_rate * dt
        self._elev[:] = self._bedrock__elevation + self._sed

    def _estimate_max_time_step_size(self, upper_limit_dt=1.0e6):
        """
        Estimate the maximum possible time-step size that avoids
        flattening any streamwise slope or exhausting sediment.

        The ``upper_limit_dt`` parameter handles the special case of
        a nonexistent upper limit, which only occurs when there are
        no nodes at which either sediment or slope gradient is
        declining. Value is arbitrary as long as it is >= the user-provided
        global time-step size (in :meth:`~.run_one_step`).

        Parameters
        ----------
        dt : float (default 1.0e6)
            Maximum time step size
        """
        sed_is_declining = np.logical_and(self._dHdt < 0.0, self._sed > 0.0)
        if np.any(sed_is_declining):
            min_time_to_exhaust_sed = np.amin(
                -self._sed[sed_is_declining] / self._dHdt[sed_is_declining]
            )
        else:
            min_time_to_exhaust_sed = upper_limit_dt
        dzdt = self._dHdt - self._rock_lowering_rate
        rate_diff = dzdt[self._receiver_node] - dzdt
        height_above_rcvr = self._elev - self._elev[self._receiver_node]
        slope_is_declining = np.logical_and(rate_diff > 0.0, height_above_rcvr > 0.0)
        if np.any(slope_is_declining):
            min_time_to_flatten_slope = np.amin(
                height_above_rcvr[slope_is_declining] / rate_diff[slope_is_declining]
            )
        else:
            min_time_to_flatten_slope = upper_limit_dt
        return 0.5 * min(min_time_to_exhaust_sed, min_time_to_flatten_slope)

    def run_one_step(self, global_dt):
        """Advance solution by time interval global_dt, subdividing
        into sub-steps as needed."""
        time_remaining = global_dt
        while time_remaining > 0.0:
            self.update_rates()
            max_dt = self._estimate_max_time_step_size()
            this_dt = min(max_dt, time_remaining)
            this_dt = max(this_dt, _DT_MAX)
            self._update_rock_sed_and_elev(this_dt)
            time_remaining -= this_dt



================================================
File: gravel_river_transporter/__init__.py
================================================
from .gravel_river_transporter import GravelRiverTransporter

__all__ = ["GravelRiverTransporter"]



================================================
File: gravel_river_transporter/gravel_river_transporter.py
================================================
import numpy as np
from scipy.sparse.linalg import spsolve

from landlab import Component
from landlab import HexModelGrid
from landlab.grid.diagonals import DiagonalsMixIn


def make_empty_matrix_and_rhs(grid):
    from scipy.sparse import csc_matrix

    mat = csc_matrix((grid.number_of_core_nodes, grid.number_of_core_nodes))
    rhs = np.zeros(grid.number_of_core_nodes)
    return mat, rhs


def zero_out_matrix(grid, mat, rcvr, mat_id):
    for i in grid.core_nodes:
        j = mat_id[i]
        mat[j, j] = 0.0
        r = rcvr[i]
        if grid.status_at_node[r] == grid.BC_NODE_IS_CORE:
            k = mat_id[r]
            mat[j, k] = 0.0
            mat[k, k] = 0.0
            mat[k, j] = 0.0


class GravelRiverTransporter(Component):
    """Model drainage network evolution for a network of transport-limited
    gravel-bed rivers with downstream abrasion.

    GravelRiverTransporter is designed to operate together with a flow-routing
    component such as PriorityFloodFlowRouter, so that each grid node has
    a defined flow direction toward one of its neighbor nodes. Each core node
    is assumed to contain one outgoing fluvial channel, and (depending on
    the drainage structure) zero, one, or more incoming channels. These channels are
    treated as effectively sub-grid-scale features that are embedded in valleys
    that have a width of one grid cell. The rate of gravel transport out of
    a given node is calculated as the product of bankfull discharge, channel
    gradient (to the 7/6 power), a dimensionless transport coefficient, and
    an intermittency factor that represents the fraction of time that bankfull
    flow occurs. The derivation of the transport law is given by Wickert &
    Schildgen (2019), and it derives from the assumption that channels are
    gravel-bedded and that they "instantaneously" adjust their width such that
    bankfull bed shear stress is just slightly higher than the threshold for
    grain motion. The substrate is assumed to consist entirely of gravel-size
    material with a given bulk porosity. The component calculates the loss of
    gravel-sized material to abrasion (i.e., conversion to finer sediment, which
    is not explicitly tracked) as a function of the volumetric transport rate,
    an abrasion coefficient with units of inverse length, and the local transport
    distance (for example, if a grid node is carrying a gravel load Qs to a
    neighboring node dx meters downstream, the rate of gravel loss in volume per
    time per area at the node will be beta Qs dx, where beta is the abrasion
    coefficient). Sediment mass conservation is calculated across each entire
    grid cell. For example, if a cell has surface area A, a total volume influx
    Qin, and downstream transport rate Qs, the resulting rate of change of
    elevation will be (Qin - Qs / (A (1 - phi)), where phi is porosity.

    Parameters
    ----------
    grid : ModelGrid
        A Landlab model grid object
    intermittency_factor : float (default 0.01)
        Fraction of time that bankfull flow occurs
    transport_coefficient : float (default 0.041)
        Dimensionless transport efficiency factor (see Wickert & Schildgen 2019)
    abrasion_coefficient : float (default 0.0 1/m)
        Abrasion coefficient with units of inverse length
    sediment_porosity : float (default 0.35)
        Bulk porosity of bed sediment
    solver : string (default "explicit")
        Solver type (currently only "explicit" is tested and operational)

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import FlowAccumulator
    >>> grid = RasterModelGrid((3, 3), xy_spacing=1000.0)
    >>> elev = grid.add_zeros("topographic__elevation", at="node")
    >>> grid.status_at_node[grid.perimeter_nodes] = grid.BC_NODE_IS_CLOSED
    >>> grid.status_at_node[5] = grid.BC_NODE_IS_FIXED_VALUE
    >>> fa = FlowAccumulator(grid, runoff_rate=10.0)
    >>> fa.run_one_step()
    >>> transporter = GravelRiverTransporter(grid, abrasion_coefficient=0.0005)
    >>> for _ in range(200):
    ...     fa.run_one_step()
    ...     elev[grid.core_nodes] += 1.0
    ...     transporter.run_one_step(10000.0)
    ...
    >>> int(elev[4] * 100)
    2366
    """

    _ONE_SIXTH = 1.0 / 6.0

    _name = "GravelRiverTransporter"

    _unit_agnostic = True

    _info = {
        "bedload_sediment__rate_of_loss_to_abrasion": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/y",
            "mapping": "node",
            "doc": "Rate of bedload sediment volume loss to abrasion per unit area",
        },
        "bedload_sediment__volume_influx": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m**2/y",
            "mapping": "node",
            "doc": "Volumetric incoming streamwise bedload sediment transport rate",
        },
        "bedload_sediment__volume_outflux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m**2/y",
            "mapping": "node",
            "doc": "Volumetric outgoing streamwise bedload sediment transport rate",
        },
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__upstream_node_order": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array containing downstream-to-upstream ordered list of node IDs",
        },
        "sediment__rate_of_change": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/y",
            "mapping": "node",
            "doc": "Time rate of change of sediment thickness",
        },
        "surface_water__discharge": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m**3/y",
            "mapping": "node",
            "doc": "Volumetric discharge of surface water",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__steepest_slope": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "The steepest *downhill* slope",
        },
    }

    def __init__(
        self,
        grid,
        intermittency_factor=0.01,
        transport_coefficient=0.041,
        abrasion_coefficient=0.0,
        sediment_porosity=0.35,
        solver="explicit",
    ):
        """Initialize GravelRiverTransporter."""

        super().__init__(grid)

        # Parameters
        self._trans_coef = transport_coefficient
        self._intermittency_factor = intermittency_factor
        self._abrasion_coef = abrasion_coefficient
        self._porosity_factor = 1.0 / (1.0 - sediment_porosity)

        # Fields and arrays
        self._elev = grid.at_node["topographic__elevation"]
        self._discharge = grid.at_node["surface_water__discharge"]
        self._slope = grid.at_node["topographic__steepest_slope"]
        self._receiver_node = grid.at_node["flow__receiver_node"]
        self._receiver_link = grid.at_node["flow__link_to_receiver_node"]
        super().initialize_output_fields()
        self._sediment_influx = grid.at_node["bedload_sediment__volume_influx"]
        self._sediment_outflux = grid.at_node["bedload_sediment__volume_outflux"]
        self._dzdt = grid.at_node["sediment__rate_of_change"]
        self._abrasion = grid.at_node["bedload_sediment__rate_of_loss_to_abrasion"]

        # Constants
        self._SEVEN_SIXTHS = 7.0 / 6.0

        # Solver type
        if solver == "explicit":
            self.run_one_step = self.run_one_step_simple_explicit
        elif solver == "matrix":
            import warnings

            from landlab.utils.matrix import get_core_node_at_node

            warnings.warn(
                "Matrix-based solver is experimental & not fully tested", stacklevel=2
            )
            self.run_one_step = self.run_one_step_matrix_inversion
            self._mat, self._rhs = make_empty_matrix_and_rhs(grid)
            self._mat_id = np.zeros(grid.number_of_nodes, dtype=int)
            self._mat_id = get_core_node_at_node(grid)
        else:
            raise ValueError("Solver type not recognized")

        self._setup_length_of_flow_link()

    def _setup_length_of_flow_link(self):
        """Set up a float or array containing length of the flow link from each node,
        which is needed for the abrasion rate calculations.
        """
        if isinstance(self.grid, HexModelGrid):
            self._flow_link_length_over_cell_area = (
                self.grid.spacing / self.grid.area_of_cell[0]
            )
            self._flow_length_is_variable = False
        elif isinstance(self.grid, DiagonalsMixIn):
            self._flow_length_is_variable = True
            self._grid_has_diagonals = True
        else:
            self._flow_length_is_variable = True
            self._grid_has_diagonals = False

    def _update_flow_link_length_over_cell_area(self):
        """Update the ratio of the length of link along which water flows out of
        each node to the area of the node's cell."""
        if self._grid_has_diagonals:
            self._flow_link_length_over_cell_area = (
                self.grid.length_of_d8[self._receiver_link[self.grid.core_nodes]]
                / self.grid.area_of_cell[self.grid.cell_at_node[self.grid.core_nodes]]
            )
        else:
            self._flow_link_length_over_cell_area = (
                self.grid.length_of_link[self._receiver_link[self.grid.core_nodes]]
                / self.grid.area_of_cell[self.grid.cell_at_node[self.grid.core_nodes]]
            )

    def calc_implied_depth(self, grain_diameter=0.01):
        """Utility function that calculates and returns water depth implied by
        slope and grain diameter, using Wickert & Schildgen (2019) equation 8.

        The equation is

            h = ((rho_s - rho / rho)) (1 + epsilon) tau_c* (D / S)

        where the factors on the right are sediment and water density, excess
        shear-stress factor, critical Shields stress, grain diameter, and slope
        gradient. Here the prefactor on D/S assumes sediment density of 2650 kg/m3,
        water density of 1000 kg/m3, shear-stress factor of 0.2, and critical
        Shields stress of 0.0495, giving a value of 0.09801.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> grid = RasterModelGrid((3, 3), xy_spacing=1000.0)
        >>> elev = grid.add_zeros("topographic__elevation", at="node")
        >>> elev[3:] = 10.0
        >>> fa = FlowAccumulator(grid)
        >>> fa.run_one_step()
        >>> transporter = GravelRiverTransporter(grid)
        >>> depth = transporter.calc_implied_depth()
        >>> int(depth[4] * 1000)
        98
        """
        DEPTH_FACTOR = 0.09801
        depth = np.zeros(self._grid.number_of_nodes)
        nonzero_slope = self._slope > 0.0
        depth[nonzero_slope] = (
            DEPTH_FACTOR * grain_diameter / self._slope[nonzero_slope]
        )
        return depth

    def calc_implied_width(self, grain_diameter=0.01, time_unit="y"):
        """Utility function that calculates and returns channel width implied by
        discharge, slope, and grain diameter, using Wickert & Schildgen (2019)
        equation 16.

        The equation is

            b = kb Q S**(7/6) / D**(3/2)

        where the dimensional prefactor, which includes sediment and water
        density, gravitational acceleration, critical Shields stress, and the
        transport factor epsilon, is

            kb = 0.17 g**(-1/2) (((rho_s - rho) / rho) (1 + eps) tau_c*)**(-5/3)

        Using g = 9.8 m/s2, rho_s = 2650 (quartz), rho = 1000 kg/m3, eps = 0.2,
        and tau_c* = 0.0495, kb ~ 2.61 s/m**(1/2). Converting to years,
        kb = 8.26e-8.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> grid = RasterModelGrid((3, 3), xy_spacing=10000.0)
        >>> elev = grid.add_zeros("topographic__elevation", at="node")
        >>> elev[3:] = 100.0
        >>> fa = FlowAccumulator(grid)
        >>> fa.run_one_step()
        >>> transporter = GravelRiverTransporter(grid)
        >>> width = transporter.calc_implied_width()
        >>> int(width[4] * 100)
        3833
        >>> grid.at_node["surface_water__discharge"] *= 1.0 / (3600 * 24 * 365.25)
        >>> width = transporter.calc_implied_width(time_unit="s")
        >>> int(width[4] * 100)
        3838
        """
        if time_unit[0] == "y":
            width_fac = 8.26e-8
        else:
            width_fac = 2.61  # assume seconds if not years
        width = np.zeros(self._grid.number_of_nodes)
        width = (
            width_fac
            * self._discharge
            * self._slope ** (7.0 / 6.0)
            / (grain_diameter**1.5)
        )
        return width

    def calc_transport_capacity(self):
        """Calculate and return bed-load transport capacity.

        Calculation uses Wickert-Schildgen approach, and provides
        volume per time rate.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> grid = RasterModelGrid((3, 3), xy_spacing=100.0)
        >>> elev = grid.add_zeros("topographic__elevation", at="node")
        >>> elev[3:] = 1.0
        >>> fa = FlowAccumulator(grid)
        >>> fa.run_one_step()
        >>> transporter = GravelRiverTransporter(grid)
        >>> transporter.calc_transport_capacity()
        >>> round(transporter._sediment_outflux[4], 4)
        0.019
        """
        self._sediment_outflux[:] = (
            self._trans_coef
            * self._intermittency_factor
            * self._discharge
            * self._slope**self._SEVEN_SIXTHS
        )

    def calc_abrasion_rate(self):
        """Update the rate of bedload loss to abrasion, per unit area.

        Here we use the average of incoming and outgoing sediment flux to
        calculate the loss rate to abrasion.

        The factor dx (node spacing) appears in the denominator to represent
        flow segment length (i.e., length of the link along which water is
        flowing in the cell) divided by cell area. This would need to be updated
        to handle non-raster and/or non-uniform grids.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> grid = RasterModelGrid((3, 3), xy_spacing=1000.0)
        >>> elev = grid.add_zeros("topographic__elevation", at="node")
        >>> elev[3:] = 10.0
        >>> fa = FlowAccumulator(grid)
        >>> fa.run_one_step()
        >>> transporter = GravelRiverTransporter(grid, abrasion_coefficient=0.0002)
        >>> transporter.calc_transport_capacity()
        >>> transporter.calc_abrasion_rate()
        >>> int(transporter._abrasion[4] * 1e8)
        19
        """
        cores = self._grid.core_nodes
        if self._flow_length_is_variable:
            self._update_flow_link_length_over_cell_area()
        self._abrasion[cores] = (
            self._abrasion_coef
            * 0.5
            * (self._sediment_outflux[cores] + self._sediment_influx[cores])
            * self._flow_link_length_over_cell_area
        )

    def calc_sediment_rate_of_change(self):
        """Update the rate of thickness change of coarse sediment at each core node.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> grid = RasterModelGrid((3, 4), xy_spacing=100.0)
        >>> elev = grid.add_zeros("topographic__elevation", at="node")
        >>> elev[:] = 0.01 * grid.x_of_node
        >>> grid.status_at_node[grid.perimeter_nodes] = grid.BC_NODE_IS_CLOSED
        >>> grid.status_at_node[4] = grid.BC_NODE_IS_FIXED_VALUE
        >>> fa = FlowAccumulator(grid)
        >>> fa.run_one_step()
        >>> transporter = GravelRiverTransporter(grid)
        >>> transporter.calc_sediment_rate_of_change()
        >>> np.round(transporter._sediment_outflux[4:7], 3)
        array([0.   , 0.038, 0.019])
        >>> np.round(transporter._sediment_influx[4:7], 3)
        array([0.038, 0.019, 0.   ])
        >>> np.round(transporter._dzdt[5:7], 8)
        array([-2.93e-06, -2.93e-06])
        """
        self.calc_transport_capacity()
        if self._abrasion_coef > 0.0:
            self.calc_abrasion_rate()
        cores = self.grid.core_nodes
        self._sediment_influx[:] = 0.0
        for c in cores:  # send sediment downstream
            r = self._receiver_node[c]
            self._sediment_influx[r] += self._sediment_outflux[c]
        self._dzdt[cores] = self._porosity_factor * (
            (self._sediment_influx[cores] - self._sediment_outflux[cores])
            / self.grid.area_of_cell[self.grid.cell_at_node[cores]]
            - self._abrasion[cores]
        )

    def run_one_step_simple_explicit(self, dt):
        """Advance solution by time interval dt.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> grid = RasterModelGrid((3, 4), xy_spacing=100.0)
        >>> elev = grid.add_zeros("topographic__elevation", at="node")
        >>> elev[:] = 0.01 * grid.x_of_node
        >>> grid.status_at_node[grid.perimeter_nodes] = grid.BC_NODE_IS_CLOSED
        >>> grid.status_at_node[4] = grid.BC_NODE_IS_FIXED_VALUE
        >>> fa = FlowAccumulator(grid)
        >>> fa.run_one_step()
        >>> transporter = GravelRiverTransporter(grid, solver="explicit")
        >>> transporter.run_one_step(1000.0)
        >>> np.round(elev[4:7], 4)
        array([0.    , 0.9971, 1.9971])
        """
        self.calc_sediment_rate_of_change()
        self._elev += self._dzdt * dt

    def _fill_matrix_and_rhs(self, dt):
        """Fill out entries in a sparse matrix and corresponding right-hand side
        vector.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> grid = RasterModelGrid((3, 4), xy_spacing=100.0)
        >>> elev = grid.add_zeros("topographic__elevation", at="node")
        >>> elev[:] = 0.01 * grid.x_of_node
        >>> grid.status_at_node[grid.perimeter_nodes] = grid.BC_NODE_IS_CLOSED
        >>> grid.status_at_node[4] = grid.BC_NODE_IS_FIXED_VALUE
        >>> fa = FlowAccumulator(grid)
        >>> transporter = GravelRiverTransporter(grid, solver="matrix")
        >>> transporter._mat.toarray()
        array([[0., 0.],
               [0., 0.]])
        >>> fa.run_one_step()
        >>> transporter._receiver_node[5:7]
        array([4, 5])
        >>> transporter._fill_matrix_and_rhs(1000.0)
        >>> transporter._rhs
        array([1., 2.])
        """
        prefac = (
            self._trans_coef * self._intermittency_factor * self._porosity_factor * dt
        ) / self.grid.dx**2
        a = prefac * (1.0 / self.grid.dx + self._abrasion_coef / 2)
        b = prefac * (1.0 / self.grid.dx - self._abrasion_coef / 2)
        f = self._discharge * (self._slope ** (self._ONE_SIXTH))

        zero_out_matrix(self.grid, self._mat, self._receiver_node, self._mat_id)
        for i in self.grid.core_nodes:
            j = self._mat_id[i]
            self._rhs[j] = self._elev[i]
            self._mat[j, j] += 1 + a * f[i]
            r = self._receiver_node[i]
            if self.grid.status_at_node[r] == self.grid.BC_NODE_IS_CORE:
                k = self._mat_id[r]
                self._mat[j, k] -= a * f[i]
                self._mat[k, k] += b * f[i]
                self._mat[k, j] -= b * f[i]
            else:
                self._rhs[j] += a * f[i] * self._elev[r]

    def run_one_step_matrix_inversion(self, dt):
        """Advance solution by time interval dt.

        WARNING: EXPERIMENTAL AND NOT FULLY TESTED - USE AT OWN RISK!

        Notes
        -----
        Does not update abrasion rate or sediment outflux fields.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> grid = RasterModelGrid((3, 4), xy_spacing=100.0)
        >>> elev = grid.add_zeros("topographic__elevation", at="node")
        >>> elev[:] = 0.01 * grid.x_of_node
        >>> grid.status_at_node[grid.perimeter_nodes] = grid.BC_NODE_IS_CLOSED
        >>> grid.status_at_node[4] = grid.BC_NODE_IS_FIXED_VALUE
        >>> fa = FlowAccumulator(grid)
        >>> fa.run_one_step()
        >>> transporter = GravelRiverTransporter(grid, solver="matrix")
        >>> transporter.run_one_step == transporter.run_one_step_matrix_inversion
        True
        >>> transporter.run_one_step(1000.0)
        """
        self._fill_matrix_and_rhs(dt)
        self._elev[self.grid.core_nodes] = spsolve(self._mat, self._rhs)



================================================
File: groundwater/README.md
================================================
## Welcome to the README for the GroundwaterDupuitPercolator component submodule.

[![status](https://joss.theoj.org/papers/6936ca6851c622de48b2c5f6cf45a7bd/status.svg)](https://joss.theoj.org/papers/6936ca6851c622de48b2c5f6cf45a7bd)

The GroundwaterDupuitPercolator is a component in Landlab for simulating shallow
subsurface flow. A [paper describing it](https://joss.theoj.org/papers/6936ca6851c622de48b2c5f6cf45a7bd)
was published in February 2020 in the Journal of Open Source Software. Here we
summarize installation, documentation, tutorials, tests, and getting help with
this component.

As this component lives within the larger Landlab package ecosystem, most of the
information below provides links into the [main Landlab documentation](https://landlab.csdms.io/).

### Installation
To use this component, you will need to install Landlab. Two options for
installation are available:
[a pre-packaged binary](https://landlab.csdms.io/installation.html)
distributed through PyPI or conda-forge and a
[source code installation](https://landlab.csdms.io/install/).

The dependencies of the Landlab package are described [here](https://landlab.csdms.io/development/practices/dependencies.html).

### Documentation
The documentation specific to this component is housed within the Landlab
documentation. There are two pages in the documentation that are most relevant
to this component:
- [The component API](https://landlab.csdms.io/generated/api/landlab.components.groundwater.dupuit_percolator.html).
- [A page](https://landlab.csdms.io/user_guide/dupuit_theory.html)
describing the theory and numerical implementation of this component.

If you are new to Landlab and components, we recommend that you also look at the
[User Guide](https://landlab.csdms.io/user_guide/),
in particular, the page on the [model grid](https://landlab.csdms.io/user_guide/grid.html), and [components](https://landlab.csdms.io/user_guide/components.html).

### Tutorials
There is a [Jupyter notebook in the Landlab Tutorials repository](https://mybinder.org/v2/gh/landlab/landlab/release?filepath=notebooks/tutorials/groundwater/groundwater_flow.ipynb)
that describes the use of the `GroundwaterDupuitPercolator`.
The link takes you to a binder instance of this notebook. Its filepath within
the repository is `notebooks/tutorials/groundwater/groundwater_flow.ipynb`

A directory of all Landlab notebooks can be found (as a binder instance) [here](https://mybinder.org/v2/gh/landlab/landlab/release?filepath=notebooks/welcome.ipynb)

### Tests of this Component
Along with the rest of the Landlab package, this component uses
[`pytest`](https://docs.pytest.org/en/latest/)
to  discover and run its tests. General information about running the Landlab
tests can be found [here](https://landlab.csdms.io/development/practices/writing_tests.html).

If you want to run the tests locally, you will need to use a
[source code installation](https://landlab.csdms.io/install/).

### Getting Help
If you have any questions, comments, issues, or bugs related to this submodule,
please [open an Issue](https://github.com/landlab/landlab/issues/new) so we can
respond.



================================================
File: groundwater/__init__.py
================================================
#!/usr/bin/env python
"""
.. codeauthor:: D Litwin, G Tucker

.. sectionauthor:: D Litwin, G Tucker
"""

from .dupuit_percolator import GroundwaterDupuitPercolator

__all__ = ["GroundwaterDupuitPercolator"]



================================================
File: groundwater/dupuit_percolator.py
================================================
"""GroundwaterDupuitPercolator Component.

@author: G Tucker, D Litwin, K Barnhart
"""

from warnings import warn

import numpy as np

from landlab import Component
from landlab.grid.mappers import map_mean_of_link_nodes_to_link
from landlab.grid.mappers import map_value_at_max_node_to_link
from landlab.utils import return_array_at_link
from landlab.utils import return_array_at_node


# regularization functions used to deal with numerical demons of seepage
def _regularize_G(u, reg_factor):
    """Smooths transition of step function with an exponential.

    0<=u<=1.
    """
    return np.exp(-(1 - u) / reg_factor)


def _regularize_R(u):
    """ramp function on u."""
    return u * np.greater_equal(u, 0)


def _update_thickness(dt, h0, b, f, dqdx, n, r):
    """analytical solution for the linearized governing equation."""
    out = b * (
        1
        - r
        * np.log(
            1
            + np.exp((1 - (h0 + ((f - dqdx) * dt) / n) / b) / r)
            * (1 - np.exp(-(b - h0) / (b * r)))
        )
    )
    out[f <= dqdx] = (h0 + (1 / n * (f - dqdx)) * dt)[f <= dqdx]
    return out


def get_link_hydraulic_conductivity(grid, K):
    """Returns array of hydraulic conductivity on links, allowing for aquifers
    with laterally anisotropic hydraulic conductivity.

    Parameters
    ----------
    K: (2x2) array of floats (m/s)
        The hydraulic conductivity tensor:
        [[Kxx, Kxy],[Kyx,Kyy]]
    """

    u = grid.unit_vector_at_link
    K_link = np.zeros(len(u))
    for i in range(len(u)):
        K_link[i] = np.dot(np.dot(u[i, :], K), u[i, :])
    return K_link


class GroundwaterDupuitPercolator(Component):
    r"""
    Simulate groundwater flow in a shallow unconfined aquifer.

    The GroundwaterDupuitPercolator solves the Boussinesq equation for
    flow in an unconfined aquifer over an impermeable aquifer base and
    calculates groundwater return flow to the surface. This method uses the
    Dupuit-Forcheimer approximation. This means that the model assumes the
    aquifer is laterally extensive in comparison to its thickness, such that
    the vertical component of flow is negligible. It also assumes that the
    capillary fringe is small, such that the water table can be modeled as a
    free surface. Please consider the applicability of these assumptions when
    using this model. For more details, see component documentation
    :ref:`here <dupuit-theory>`.

    Examples
    --------
    Import the grid class and component

    >>> from landlab import RasterModelGrid
    >>> from landlab.components import GroundwaterDupuitPercolator

    Initialize the grid and component

    >>> grid = RasterModelGrid((10, 10), xy_spacing=10.0)
    >>> elev = grid.add_zeros("topographic__elevation", at="node")
    >>> abe = grid.add_zeros("aquifer_base__elevation", at="node")
    >>> elev[:] = 5.0
    >>> gdp = GroundwaterDupuitPercolator(grid)

    Run component forward.

    >>> dt = 1e4
    >>> for i in range(100):
    ...     gdp.run_one_step(dt)
    ...

    When the model generates groundwater return flow, the surface water flux
    out of the domain can be calculated only after a FlowAccumulator is run.
    Below is a more complex example that demonstrates this case.

    >>> from landlab.components import FlowAccumulator

    Set boundary conditions and initialize grid

    >>> grid = RasterModelGrid((5, 41), xy_spacing=10.0)
    >>> grid.set_closed_boundaries_at_grid_edges(True, True, False, True)

    Make a sloping, 3 m thick aquifer, initially fully saturated

    >>> elev = grid.add_zeros("topographic__elevation", at="node")
    >>> elev[:] = grid.x_of_node / 100 + 3
    >>> base = grid.add_zeros("aquifer_base__elevation", at="node")
    >>> base[:] = grid.x_of_node / 100
    >>> wt = grid.add_zeros("water_table__elevation", at="node")
    >>> wt[:] = grid.x_of_node / 100 + 3

    Initialize components

    >>> gdp = GroundwaterDupuitPercolator(grid, recharge_rate=1e-7)
    >>> fa = FlowAccumulator(grid, runoff_rate="surface_water__specific_discharge")

    Advance timestep. Default units are meters and seconds, though the component
    is unit agnostic.

    >>> dt = 1e3
    >>> for i in range(1000):
    ...     gdp.run_one_step(dt)
    ...

    Calculate surface water flux out of domain

    >>> fa.run_one_step()
    >>> np.testing.assert_almost_equal(gdp.calc_sw_flux_out(), 0.0005077)


    Notes
    -----
    Below is a summary of the theory and numerical implementation of
    the ``GroundwaterDupuitPercolator``. A complete description can be found
    :ref:`here <dupuit-theory>`.

    Groundwater discharge per unit length, :math:`q`, is calculated as:

    .. math::
        q = -K_{sat} h \big( \nabla z \big) \cos^2 (\alpha)

    where :math:`K_{sat}` is the saturated hydraulic conductivity, :math:`h` is
    the aquifer thickness, and :math:`\alpha` is the slope angle of the aquifer base.

    Surface water discharge per unit area, :math:`q_s`, is calculated as:

    .. math::
        q_s = \mathcal{G}_r \bigg( \frac{h}{d} \bigg) \mathcal{R} \big(-\nabla \cdot q + f \big)

    where :math:`\mathcal{G}_r` is a smoothed step function, :math:`\mathcal{R}`
    is the ramp function, :math:`d` is the regolith thickness, and :math:`f` is
    the recharge rate.

    The evolution of aquifer thickness is then given by:

    .. math::
        n \frac{\partial h}{\partial t} = f - q_s - \nabla \cdot q

    where :math:`n` is the drainable porosity.

    A semi-analytical approach is used to update aquifer thickness :math:`h`, in which
    the differential equation above is linearized by assuming :math:`\nabla \cdot q` varies
    minimally over the duration of a timestep. In this approach, :math:`q` is
    reported at the beginning of the timestep, while :math:`q_s` is reported at the
    end of the timestep.

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Litwin, D. G., Tucker, G.E., Barnhart, K. R., Harman, C. J. (2020).
    GroundwaterDupuitPercolator: A Landlab component for groundwater flow.
    Journal of Open Source Software, 5(46), 1935, https://doi.org/10.21105/joss.01935.

    **Additional References**

    Marçais, J., de Dreuzy, J. R. & Erhel, J. Dynamic coupling of subsurface
    and seepage flows solved within a regularized partition formulation.
    Advances in Water Resources 109, 94–105 (2017).

    Childs, E. C. Drainage of Groundwater Resting on a Sloping Bed. Water
    Resources Research 7, 1256–1263 (1971).
    """

    _name = "GroundwaterDupuitPercolator"

    _cite_as = """@article{litwin2020groundwater,
      doi = {10.21105/joss.01935},
      url = {https://doi.org/10.21105/joss.01935},
      year = {2020},
      publisher = {The Open Journal},
      volume = {5},
      number = {46},
      pages = {1935},
      author = {David Litwin and Gregory Tucker and Katherine Barnhart and Ciaran Harman},
      title = {GroundwaterDupuitPercolator: A Landlab component for groundwater flow},
      journal = {Journal of Open Source Software}
    }"""

    _unit_agnostic = True

    _info = {
        "aquifer__thickness": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "thickness of saturated zone",
        },
        "aquifer_base__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "elevation of impervious layer",
        },
        "aquifer_base__gradient": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/m",
            "mapping": "link",
            "doc": "gradient of the aquifer base in the link direction",
        },
        "average_surface_water__specific_discharge": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/s",
            "mapping": "node",
            "doc": "average surface water specific discharge over variable timesteps",
        },
        "groundwater__specific_discharge": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m2/s",
            "mapping": "link",
            "doc": "discharge per width in link dir",
        },
        "groundwater__velocity": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/s",
            "mapping": "link",
            "doc": "velocity of groundwater in link direction",
        },
        "hydraulic__gradient": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/m",
            "mapping": "link",
            "doc": "gradient of water table in link direction",
        },
        "surface_water__specific_discharge": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/s",
            "mapping": "node",
            "doc": "rate of seepage to surface",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "water_table__elevation": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "elevation of water table",
        },
    }

    def __init__(
        self,
        grid,
        hydraulic_conductivity=0.001,
        porosity=0.2,
        recharge_rate=1.0e-8,
        regularization_f=1e-2,
        courant_coefficient=0.5,
        vn_coefficient=0.8,
        callback_fun=lambda *args, **kwargs: None,
        **callback_kwds,
    ):
        r"""
        Parameters
        ----------
        grid: ModelGrid
            Landlab ModelGrid object
        hydraulic_conductivity: float, field name, array of float or function.
            the aquifer saturated hydraulic conductivity, m/s.
            If function is given, it should take a landlab ModelGrid and return
            an array of floats at link. This may be used if the lateral hydraulic
            conductivity is not vertically homogenous and the effective hydraulic
            conductivity needs to be modified based upon on the position of the
            water table. See component tests for example.
            Default = 0.001 m/s
        porosity: float, field name or array of float
            the drainable porosity of the aquifer [-]
            Default = 0.2
        recharge_rate: float, field name, or array of float
            Rate of recharge, m/s
            Default = 1.0e-8 m/s
        regularization_f: float
            factor controlling the smoothness of the transition between
            surface and subsurface flow
            Default = 0.01
        courant_coefficient: float (-)
            The muliplying factor on the condition that the timestep is
            smaller than the minimum link length over groundwater flow
            velocity. This parameter is only used with
            ``run_with_adaptive_time_step_solver`` and must be greater than
            zero.
            Default = 0.5
        vn_coefficient: float (-)
            The multiplying factor C for the condition :math:`dt >= C*dx^2/(4D)`,
            where :math:`D = Kh/n` is the diffusivity of the Boussinesq
            equation. This arises from a von Neumann stability analysis of
            the Boussinesq equation when the hydraulic gradient is small.
            This parameter is only used with ``run_with_adaptive_time_step_solver``
            and must be greater than zero.
            Default = 0.8
        callback_fun: function(grid, recharge_rate, substep_dt, \*\*kwargs)
            Optional function that will be executed at the end of each sub-timestep
            in the run_with_adaptive_time_step_solver method. Intended purpose
            is to write output not otherwise visible outside of the method call.
            The function should have three required arguments:
            grid: the ModelGrid instance used by GroundwaterDupuitPercolator
            recharge_rate: an array at node that is the specified recharge rate
            substep_dt: the length of the current substep determined internally
            by run_with_adaptive_time_step_solver to meet stability criteria.
        callback_kwds: any additional keyword arguments for the provided callback_fun.
        """
        super().__init__(grid)

        # Shorthand
        self._cores = grid.core_nodes

        # Create fields:

        self._elev = self._grid.at_node["topographic__elevation"]
        self._base = self._grid.at_node["aquifer_base__elevation"]

        self.initialize_output_fields()

        self._wtable = self._grid.at_node["water_table__elevation"]
        self._wtable[grid.closed_boundary_nodes] = 0

        self._thickness = self._grid.at_node["aquifer__thickness"]
        self._thickness[:] = self._wtable - self._base
        self._thickness[grid.closed_boundary_nodes] = 0

        self._hydr_grad = self._grid.at_link["hydraulic__gradient"]
        self._base_grad = self._grid.at_link["aquifer_base__gradient"]

        self._q = self._grid.at_link["groundwater__specific_discharge"]
        self._qs = self._grid.at_node["surface_water__specific_discharge"]
        self._qsavg = self.grid.at_node["average_surface_water__specific_discharge"]

        self._vel = self._grid.at_link["groundwater__velocity"]

        # Convert parameters to fields if needed, and store a reference
        self.K = hydraulic_conductivity
        self.recharge = recharge_rate
        self.n = porosity
        self._r = regularization_f

        # save courant_coefficient (and test)
        self.courant_coefficient = courant_coefficient
        self.vn_coefficient = vn_coefficient

        # set callback function
        self._callback_kwds = callback_kwds
        self.callback_fun = callback_fun

    @property
    def callback_fun(self):
        r"""callback function for adaptive timestep solver

        Parameters
        ----------
        callback_fun: function(grid, recharge_rate, substep_dt, \*\*callback_kwds)
            Optional function that will be executed at the end of each sub-timestep
            in the run_with_adaptive_time_step_solver method. Intended purpose
            is to write output not otherwise visible outside of the method call.
            The function should have three required arguments:
            grid: the ModelGrid instance used by GroundwaterDupuitPercolator
            recharge_rate: an array at node that is the specified recharge rate
            substep_dt: the length of the current substep determined internally
            by run_with_adaptive_time_step_solver to meet stability criteria.
        """
        return self._callback_fun

    @callback_fun.setter
    def callback_fun(self, new_val):
        try:  # New style callback function.
            new_val(self._grid, self.recharge, 0.0, **self._callback_kwds)
            self._callback_fun = new_val
        except TypeError as exc:  # Nonfunctional callback function.
            raise ValueError(
                f"{str(exc)}: Please supply a callback function with the form "
                "function(grid, recharge_rate, substep_dt, **kwargs)"
            ) from exc

    @property
    def courant_coefficient(self):
        """Courant coefficient for adaptive time step.

        Parameters
        ----------
        courant_coefficient: float (-)
            The muliplying factor on the condition that the timestep is
            smaller than the minimum link length over groundwater flow
            velocity. This parameter is only used with
            ``run_with_adaptive_time_step_solver`` and must be greater than
            zero.
        """
        return self._courant_coefficient

    @courant_coefficient.setter
    def courant_coefficient(self, new_val):
        if new_val <= 0:
            raise ValueError("courant_coefficient must be > 0.")
        self._courant_coefficient = new_val

    @property
    def vn_coefficient(self):
        """Coefficient for the diffusive timestep condition in
        the adaptive timestep solver.

        Parameters
        ----------
        vn_coefficient: float (-)
            The multiplying factor C for the condition dt >= C*dx^2/(4D),
            where D = Kh/n is the diffusivity of the Boussinesq
            equation. This arises from a von Neumann stability analysis of
            the Boussinesq equation when the hydraulic gradient is small.
            This parameter is only used with ``run_with_adaptive_time_step_solver``
            and must be greater than zero.
        """
        return self._vn_coefficient

    @vn_coefficient.setter
    def vn_coefficient(self, new_val):
        """set coefficient for the diffusive timestep condition in
        the adaptive timestep solver."""
        if new_val <= 0:
            raise ValueError("vn_coefficient must be > 0.")
        self._vn_coefficient = new_val

    @property
    def K(self):
        """hydraulic conductivity at link (m/s)"""
        if self._kfunc:
            self._K = return_array_at_link(self._grid, self._func(self._grid))
        return self._K

    @K.setter
    def K(self, new_val):
        """set hydraulic conductivity at link (m/s)"""
        if callable(new_val):
            self._kfunc = True

            if (
                not isinstance(new_val(self._grid), np.ndarray)
                and len(new_val(self._grid)) == self._grid.number_of_links
            ):
                raise TypeError(
                    "If a function is provided it must take a ModelGrid and "
                    "return an array of length number_of_links."
                )
            else:
                self._func = new_val
                self._K = return_array_at_link(self._grid, self._func(self._grid))
        else:
            self._kfunc = False
            self._K = return_array_at_link(self._grid, new_val)

    @property
    def recharge(self):
        """recharge rate (m/s)"""
        return self._recharge

    @recharge.setter
    def recharge(self, new_val):
        """set recharge rate (m/s)"""
        self._recharge = return_array_at_node(self._grid, new_val)

    @property
    def n(self):
        """drainable porosity of the aquifer (-)"""
        return self._n

    @n.setter
    def n(self, new_val):
        """set aquifer drainable porosity (-)"""
        self._n = return_array_at_node(self._grid, new_val)
        self._n_link = map_mean_of_link_nodes_to_link(self._grid, self._n)

    @property
    def number_of_substeps(self):
        """
        The number of substeps used by the run_with_adaptive_time_step_solver
        method in the latest method call.
        """
        if self._num_substeps:
            return self._num_substeps
        else:
            warn("The method run_with_adaptive_time_step_solver has not been used")

        return self._num_substeps

    def calc_recharge_flux_in(self):
        """Calculate flux into the domain from recharge.

        Includes recharge that may immediately become saturation excess
        overland flow. (m3/s)
        """
        return np.sum(
            self._grid.cell_area_at_node[self._cores] * self._recharge[self._cores]
        )

    def calc_gw_flux_out(self):
        """Groundwater flux through open boundaries may be positive (out of the
        domain) or negative (into the domain).

        This function determines the correct sign for specific discharge
        based upon this convention, and sums the flux across the
        boundary faces. (m3/s)
        """
        # get links at open boundary nodes
        open_nodes = self._grid.open_boundary_nodes
        links_at_open = self._grid.links_at_node[open_nodes]
        link_dirs_at_open = self._grid.active_link_dirs_at_node[open_nodes]

        # find active links at open boundary nodes
        active_links_at_open = links_at_open[link_dirs_at_open != 0]
        active_link_dirs_at_open = link_dirs_at_open[link_dirs_at_open != 0]

        # get groundwater specific discharge at these locations
        q_at_open_links = self._grid.at_link["groundwater__specific_discharge"][
            active_links_at_open
        ]

        # get cell widths at these locations
        faces = self._grid.face_at_link[active_links_at_open]
        face_widths = self._grid.length_of_face[faces]

        # get volume flux out at these locations
        gw_volume_flux_rate_out = (
            q_at_open_links * active_link_dirs_at_open * face_widths
        )

        return np.sum(gw_volume_flux_rate_out)

    def calc_sw_flux_out(self):
        """Surface water flux out of the domain through seepage and saturation
        excess.

        Note that model does not allow for reinfiltration.  (m3/s)
        """
        return np.sum(
            self._grid.at_node["surface_water__discharge"][
                self._grid.open_boundary_nodes
            ]
        )

    def calc_gw_flux_at_node(self):
        """Calculate the sum of the groundwater flux leaving a node.

        (m2/s)
        """
        gw = (
            self._grid.at_link["groundwater__specific_discharge"][
                self._grid.links_at_node
            ]
            * self._grid.link_dirs_at_node
        )
        gw_out = -np.sum(gw * (gw < 0), axis=1)
        return gw_out

    def calc_total_storage(self):
        """calculate the current water storage in the aquifer (m3)"""
        return np.sum(
            self._n[self._cores]
            * self._grid.cell_area_at_node[self._cores]
            * self._grid.at_node["aquifer__thickness"][self._cores]
        )

    def run_one_step(self, dt):
        """Advance component by one time step of size dt.

        Parameters
        ----------
        dt: float
            The imposed timestep.
        """

        # check water table above surface
        if (self._wtable > self._elev).any():
            warn(
                "water table above elevation surface. "
                "Setting water table elevation here to "
                "elevation surface"
            )
            self._wtable[self._wtable > self._elev] = self._elev[
                self._wtable > self._elev
            ]
            self._thickness[self._cores] = (self._wtable - self._base)[self._cores]

        # Calculate base gradient
        self._base_grad[self._grid.active_links] = self._grid.calc_grad_at_link(
            self._base
        )[self._grid.active_links]
        cosa = np.cos(np.arctan(self._base_grad))

        # Calculate hydraulic gradient
        self._hydr_grad[self._grid.active_links] = (
            self._grid.calc_grad_at_link(self._wtable) * cosa
        )[self._grid.active_links]

        # Calculate groundwater velocity
        self._vel[:] = -self._K * self._hydr_grad

        # Aquifer thickness at links (upwind)
        hlink = (
            map_value_at_max_node_to_link(
                self._grid, "water_table__elevation", "aquifer__thickness"
            )
            * cosa
        )

        # Calculate specific discharge
        self._q[:] = hlink * self._vel

        # Groundwater flux divergence
        dqdx = self._grid.calc_flux_div_at_node(self._q)

        # Regolith thickness
        reg_thickness = self._elev - self._base

        # update thickness from analytical
        self._thickness[self._cores] = _update_thickness(
            dt, self._thickness, reg_thickness, self._recharge, dqdx, self._n, self._r
        )[self._cores]
        self._thickness[self._thickness < 0] = 0.0

        # Recalculate water surface height
        self._wtable[:] = self._base + self._thickness

        # Calculate surface discharge at nodes
        self._qs[:] = _regularize_G(
            self._thickness / reg_thickness, self._r
        ) * _regularize_R(self._recharge - dqdx)

    def run_with_adaptive_time_step_solver(self, dt):
        """
        Advance component by one time step of size dt, subdividing the timestep
        into substeps as necessary to meet stability conditions.
        Note this method returns the fluxes at the last substep, but also
        returns a new field, average_surface_water__specific_discharge, that is
        averaged over all subtimesteps. To return state during substeps,
        provide a callback_fun.

        Parameters
        ----------
        dt: float
            The imposed timestep.
        """

        # check water table above surface
        if (self._wtable > self._elev).any():
            warn(
                "water table above elevation surface. "
                "Setting water table elevation here to "
                "elevation surface"
            )
            self._wtable[self._wtable > self._elev] = self._elev[
                self._wtable > self._elev
            ]
            self._thickness[self._cores] = (self._wtable - self._base)[self._cores]

        # Calculate base gradient
        self._base_grad[self._grid.active_links] = self._grid.calc_grad_at_link(
            self._base
        )[self._grid.active_links]
        cosa = np.cos(np.arctan(self._base_grad))

        # Initialize reg_thickness
        reg_thickness = self._elev - self._base

        # Initialize for average surface discharge
        qs_cumulative = np.zeros_like(self._elev)

        # Initialize variable timestep
        remaining_time = dt
        self._num_substeps = 0

        while remaining_time > 0.0:
            # Calculate hydraulic gradient
            self._hydr_grad[self._grid.active_links] = (
                self._grid.calc_grad_at_link(self._wtable) * cosa
            )[self._grid.active_links]

            # Calculate groundwater velocity
            self._vel[:] = -self._K * self._hydr_grad

            # Aquifer thickness at links (upwind)
            hlink = (
                map_value_at_max_node_to_link(
                    self._grid, "water_table__elevation", "aquifer__thickness"
                )
                * cosa
            )

            # Calculate specific discharge
            self._q[:] = hlink * self._vel

            # Groundwater flux divergence
            dqdx = self._grid.calc_flux_div_at_node(self._q)

            # calculate criteria for timestep
            dt_vn = self._vn_coefficient * np.min(
                np.divide(
                    (self._n_link * self._grid.length_of_link**2),
                    (4 * self._K * hlink),
                    where=hlink > 0,
                    out=np.ones_like(self._q) * 1e15,
                )
            )

            dt_courant = self._courant_coefficient * np.min(
                np.divide(
                    self._grid.length_of_link,
                    abs(self._vel / self._n_link),
                    where=abs(self._vel) > 0,
                    out=np.ones_like(self._q) * 1e15,
                )
            )
            substep_dt = min([dt_courant, dt_vn, remaining_time])
            # 0 = courant limited, 1 = vn limited, 2 = not limited
            # print(np.argmin(np.array([self._dt_courant, self._dt_vn, remaining_time])))

            # update thickness from analytical
            self._thickness[self._cores] = _update_thickness(
                substep_dt,
                self._thickness,
                reg_thickness,
                self._recharge,
                dqdx,
                self._n,
                self._r,
            )[self._cores]
            self._thickness[self._thickness < 0] = 0.0

            # Recalculate water surface height
            self._wtable[:] = self._base + self._thickness

            # Calculate surface discharge at nodes
            self._qs[:] = _regularize_G(
                self._thickness / reg_thickness, self._r
            ) * _regularize_R(self._recharge - dqdx)

            # add cumulative sw discharge in substeps
            qs_cumulative += self._qs * substep_dt

            # calculate the time remaining and advance count of substeps
            remaining_time -= substep_dt
            self._num_substeps += 1

            # run callback function if supplied
            self._callback_fun(
                self._grid, self._recharge, substep_dt, **self._callback_kwds
            )

        self._qsavg[:] = qs_cumulative / dt



================================================
File: hack_calculator/__init__.py
================================================
from .hack_calculator import HackCalculator

__all__ = ["HackCalculator"]



================================================
File: hack_calculator/hack_calculator.py
================================================
"""Calculate Hack parameters."""

import collections
from itertools import chain

import numpy as np
import pandas as pd
from scipy.optimize import curve_fit

from landlab import Component
from landlab.components.profiler.channel_profiler import ChannelProfiler
from landlab.utils.distance_to_divide import calculate_distance_to_divide


def _hacks_law(A, C, h):
    """Given A, C, and h calculate L.

    Where L = C * A**h

    Examples
    --------
    >>> import numpy as np
    >>> from landlab.components.hack_calculator.hack_calculator import _hacks_law
    >>> _hacks_law(1, 1, 1)
    1
    >>> np.testing.assert_array_equal(_hacks_law([1, 2], 1, 1), np.array([1, 2]))
    >>> np.testing.assert_array_equal(_hacks_law([1, 2], 3, 2), np.array([3, 12]))
    """
    assert isinstance(C, (np.number, int, float))
    assert isinstance(h, (np.number, int, float))
    L = C * np.power(np.asarray(A), h)
    return L


def _estimate_hack_coeff(A, L):
    """Estimate Hack parameters.

    Given A and L, estimate C and h Where

    L = C * A**h

    Examples
    --------
    >>> import numpy as np
    >>> np.random.seed(42)
    >>> from landlab.components.hack_calculator.hack_calculator import (
    ...     _estimate_hack_coeff,
    ...     _hacks_law,
    ... )
    >>> C = 0.5
    >>> h = 0.75
    >>> A = np.arange(1, 1000)
    >>> L = _hacks_law(A, C, h) + np.random.randn(A.size)
    >>> C_hat, h_hat = _estimate_hack_coeff(A, L)
    >>> np.round(C_hat, decimals=3)
    0.497
    >>> np.round(h_hat, decimals=3)
    0.751
    """
    popt, pcov = curve_fit(_hacks_law, A, L, (0.5, 0.7))
    return popt


def _flatten(list_):
    """
    Examples
    --------
    >>> from landlab.components.hack_calculator.hack_calculator import _flatten
    >>> struct = [[1, 2, 3, 4], [[5, 6, 7, 9], [9, 10, 11, 12]], [13, 14, 15, 16]]
    >>> out = _flatten(struct)
    >>> np.testing.assert_array_equal(
    ...     out, np.array([1, 2, 3, 4, 5, 6, 7, 9, 9, 10, 11, 12, 13, 14, 15, 16])
    ... )
    >>> assert _flatten(None) is None
    """
    if list_ is None:
        return None
    if not hasattr(list_, "__iter__"):
        return [list_]
    else:
        return list(chain(*map(_flatten, list_)))


class HackCalculator(Component):
    """This component calculates Hack's law parameters for drainage basins.

    Hacks law is given as

    ..:math:
        L = C * A**h

    Where :math:`L` is the distance to the drainage divide along the channel,
    :math:`A` is the drainage area, and :math:`C`and :math:`h` are
    parameters.

    The HackCalculator uses a ChannelProfiler to determine the nodes on which
    to calculate the parameter fit.

    Examples
    --------
    >>> import pandas as pd
    >>> pd.set_option("display.max_columns", None)
    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import FlowAccumulator, FastscapeEroder, HackCalculator
    >>> np.random.seed(42)
    >>> mg = RasterModelGrid((50, 100), xy_spacing=100)
    >>> z = mg.add_zeros("node", "topographic__elevation")
    >>> z[mg.core_nodes] += np.random.randn(mg.core_nodes.size)
    >>> fa = FlowAccumulator(mg)
    >>> fs = FastscapeEroder(mg, K_sp=0.001)
    >>> for i in range(100):
    ...     fa.run_one_step()
    ...     fs.run_one_step(1000)
    ...     z[mg.core_nodes] += 0.01 * 1000
    ...
    >>> hc = HackCalculator(mg)
    >>> hc.calculate_hack_parameters()
    >>> largest_outlet = mg.boundary_nodes[
    ...     np.argsort(mg.at_node["drainage_area"][mg.boundary_nodes])[-1:]
    ... ][0]
    >>> largest_outlet
    4978
    >>> hc.hack_coefficient_dataframe.loc[largest_outlet, "A_max"]
    2830000.0
    >>> hc.hack_coefficient_dataframe.round(2)
    A_max     C          h     basin_outlet_id
    4978      2830000.0  0.31  0.62

    >>> hc = HackCalculator(
    ...     mg, number_of_watersheds=3, main_channel_only=False, save_full_df=True
    ... )
    >>> hc.calculate_hack_parameters()
    >>> hc.hack_coefficient_dataframe.round(2)
    A_max     C          h     basin_outlet_id
    39        2170000.0  0.13  0.69
    4929      2350000.0  0.13  0.68
    4978      2830000.0  0.23  0.64
    >>> hc.full_hack_dataframe.head().round(2)
    basin_outlet_id    A     L_obs      L_est   node_id
    39                 39.0  2170000.0  3200.0  2903.43
    139                39.0  2170000.0  3100.0  2903.43
    238                39.0    10000.0     0.0    71.61
    239                39.0  2160000.0  3000.0  2894.22
    240                39.0    10000.0     0.0    71.61

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    Hack, J. T. Studies of longitudinal stream profiles in Virginia and
    Maryland (Vol. 294). U.S. Geological Survey Professional Paper 294-B (1957).
    https://doi.org/10.3133/pp294B

    """

    _name = "HackCalculator"

    _unit_agnostic = True

    _info = {
        "distance_to_divide": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Distance from drainage divide.",
        },
        "drainage_area": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m**2",
            "mapping": "node",
            "doc": "Upstream accumulated surface area contributing to the node's discharge",
        },
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__upstream_node_order": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array containing downstream-to-upstream ordered list of node IDs",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
    }

    def __init__(self, grid, save_full_df=False, **kwds):
        """
        Parameters
        ----------
        grid : Landlab Model Grid instance, required
        save_full_df: bool
            Flag indicating whether to create the ``full_hack_dataframe``.
        **kwds :
            Values to pass to the ChannelProfiler.
        """
        super().__init__(grid)
        super().initialize_output_fields()
        self._dist = grid.at_node["distance_to_divide"]

        self.profiler = ChannelProfiler(grid, **kwds)
        self._save_full_df = save_full_df

    @property
    def hack_coefficient_dataframe(self):
        """Hack coefficient dataframe.

        This dataframe is created and stored on the component.

        It is a pandas dataframe with one row for each basin for which Hack
        parameters are calculated. Thus, there are as many rows as the
        number of watersheds identified by the ChannelProfiler.

        The dataframe has the following index and columns.

            * Index
                * **basin_outlet_id**: The node ID of the watershed outlet
                  where each set of Hack parameters was estimated.

            * Columns
                * **A_max**: The drainage area of the watershed outlet.
                * **C**: The Hack coefficient as defined in the equations above.
                * **h**: The Hack exponent as defined in the equations above.
        """
        if hasattr(self, "_df"):
            return self._df
        else:
            raise RuntimeError(
                "The hack_coefficient_dataframe does not yet exist. "
                "Try running calculate_hack_parameters"
            )

    @property
    def full_hack_dataframe(self):
        """Full Hack calculation dataframe.

        This dataframe is optionally created and stored on the component when
        the keyword argument ``full_hack_dataframe=True`` is passed to the
        component init.

        It is pandas dataframe with a row for every model grid cell used to
        estimate the Hack parameters. It has the following index and columns.

            * Index
                * *node_id**: The node ID of the model grid cell.

            * Columns
                * **basin_outlet_id**: The node IDs of watershed outlet
                * **A**: The drainage are of the model grid cell.
                * **L_obs**: The observed distance to the divide.
                * **L_est**: The predicted distance to divide based on the
                  Hack coefficient fit.
        """
        if not self._save_full_df:
            raise NotImplementedError(
                "This instance of a HackCalculator was not set up to save "
                "the full_hack_dataframe. Try recreating it with "
                "save_full_df=True."
            )
        else:
            if hasattr(self, "_full_df"):
                return self._full_df
            else:
                raise RuntimeError(
                    "The full_hack_dataframe does not yet exist. "
                    "Try running calculate_hack_parameters"
                )

    def calculate_hack_parameters(self):
        """Calculate Hack parameters for desired watersheds."""
        out = collections.OrderedDict()
        self.profiler.run_one_step()

        self._dist[:] = calculate_distance_to_divide(self._grid, longest_path=True)

        if self._save_full_df:
            internal_df = []

        # for watershed in watersheds (in profile structure)
        for outlet_node in self.profiler._data_struct:
            seg_tuples = self.profiler._data_struct[outlet_node].keys()

            watershed = [
                self.profiler._data_struct[outlet_node][seg]["ids"]
                for seg in seg_tuples
            ]

            A_max = self._grid.at_node["drainage_area"][outlet_node]

            nodes = np.unique(_flatten(watershed))

            A = self._grid.at_node["drainage_area"][nodes]
            L = self._dist[nodes]
            C, h = _estimate_hack_coeff(A, L)

            out[outlet_node] = {"A_max": A_max, "C": C, "h": h}

            if self._save_full_df:
                internal_df.append(
                    pd.DataFrame.from_dict(
                        {
                            "basin_outlet_id": outlet_node * np.ones(A.shape),
                            "A": A,
                            "L_obs": L,
                            "L_est": C * A**h,
                            "node_id": nodes,
                        }
                    )
                )

        self._df = pd.DataFrame.from_dict(
            out, orient="index", columns=["A_max", "C", "h"]
        )
        self._df.index.name = "basin_outlet_id"

        if self._save_full_df:
            hdf = (
                pd.concat(internal_df, ignore_index=True)
                .set_index("node_id")
                .sort_index()
            )
            self._full_df = hdf



================================================
File: hand_calculator/__init__.py
================================================
#!/usr/bin/env python
"""
.. codeauthor:: D Litwin

.. sectionauthor:: D Litwin
"""

from .hand_calculator import HeightAboveDrainageCalculator

__all__ = ["HeightAboveDrainageCalculator"]



================================================
File: hand_calculator/hand_calculator.py
================================================
"""Landlab component to calculate height above nearest drainage.

@author: D Litwin
"""

from warnings import warn

import numpy as np

from landlab import Component
from landlab.utils import return_array_at_node


class HeightAboveDrainageCalculator(Component):
    """
    Calculate the elevation difference between each node and its nearest
    drainage node in a DEM.

    This component implements the method described by Nobre et al (2011). A
    single direction flow director (D8 or steepest descent) must be run prior
    to HeightAboveDrainageCalculator to supply the flow directions. This component does
    not fill depressions in a DEM, but rather it treats them as drainage nodes.
    For best results, please run one of the available pit filling components
    prior to HeightAboveDrainageCalculator.

    Examples
    --------

    >>> import numpy as np
    >>> from numpy.testing import assert_equal

    >>> from landlab import RasterModelGrid
    >>> from landlab.components import HeightAboveDrainageCalculator, FlowAccumulator

    >>> mg = RasterModelGrid((4, 5))
    >>> z = mg.add_zeros("topographic__elevation", at="node")
    >>> mg.set_status_at_node_on_edges(
    ...     right=mg.BC_NODE_IS_CLOSED,
    ...     bottom=mg.BC_NODE_IS_FIXED_VALUE,
    ...     left=mg.BC_NODE_IS_CLOSED,
    ...     top=mg.BC_NODE_IS_CLOSED,
    ... )
    >>> elev = np.array(
    ...     [[2, 1, 0, 1, 2], [3, 2, 1, 2, 3], [4, 3, 2, 3, 4], [5, 4, 4, 4, 5]]
    ... )
    >>> z[:] = elev.reshape(len(z))
    >>> elev
    array([[2, 1, 0, 1, 2],
           [3, 2, 1, 2, 3],
           [4, 3, 2, 3, 4],
           [5, 4, 4, 4, 5]])

    >>> fa = FlowAccumulator(mg, flow_director="D8")
    >>> fa.run_one_step()

    >>> channel__mask = mg.zeros(at="node")
    >>> channel__mask[[2, 7]] = 1
    >>> channel__mask.reshape(elev.shape)
    array([[0., 0., 1., 0., 0.],
           [0., 0., 1., 0., 0.],
           [0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0.]])

    >>> hd = HeightAboveDrainageCalculator(mg, channel_mask=channel__mask)
    >>> hd.run_one_step()

    >>> mg.at_node["height_above_drainage__elevation"].reshape(elev.shape)
    array([[2., 0., 0., 0., 0.],
           [3., 2., 0., 2., 3.],
           [4., 2., 1., 2., 4.],
           [5., 4., 4., 4., 5.]])

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    Nobre, A. D., Cuartas, L. A., Hodnett, M., Rennó, C. D., Rodrigues, G.,
    Silveira, A., et al. (2011). Height Above the Nearest Drainage – a
    hydrologically relevant new terrain model. Journal of Hydrology, 404(1),
    13–29. https://doi.org/10.1016/j.jhydrol.2011.03.051

    """

    _name = "HeightAboveDrainageCalculator"

    _unit_agnostic = True

    _info = {
        "channel__mask": {
            "dtype": np.uint8,
            "intent": "in",
            "optional": True,
            "units": "-",
            "mapping": "node",
            "doc": "Logical map of at which grid nodes channels are present",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__upstream_node_order": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array containing downstream-to-upstream ordered list of node IDs",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "height_above_drainage__elevation": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Elevation above the nearest channel node",
        },
    }

    def __init__(self, grid, channel_mask="channel__mask"):
        """
        Parameters
        ----------
        grid : ModelGrid
            Landlab ModelGrid object
        channel_mask : field name, array of uint8
            Logical map of nodes where drainage is present
        """
        super().__init__(grid)

        if grid.at_node["flow__receiver_node"].size != grid.size("node"):
            raise NotImplementedError(
                "A route-to-multiple flow director has been "
                "run on this grid. The landlab development team has not "
                "verified that HeightAboveDrainageCalculator is compatible with "
                "route-to-multiple methods. Please open a GitHub Issue "
                "to start this process."
            )

        self._grid = grid
        self._channel_mask = return_array_at_node(self._grid, channel_mask)
        self._elev = grid.at_node["topographic__elevation"]
        self._receivers = grid.at_node["flow__receiver_node"]
        self._node_order = grid.at_node["flow__upstream_node_order"]

        # height above nearest drainage
        if "height_above_drainage__elevation" not in grid.at_node:
            grid.add_zeros("height_above_drainage__elevation", at="node", dtype=float)
        self._hand = grid.at_node["height_above_drainage__elevation"]

    @property
    def channel_mask(self):
        return self._channel_mask

    @channel_mask.setter
    def channel_mask(self, new_val):
        self._channel_mask = return_array_at_node(self._grid, new_val)

    def run_one_step(self):
        is_drainage_node = self._channel_mask
        is_drainage_node[self._grid.open_boundary_nodes] = 1

        # check for pits
        self_draining_nodes = np.where(
            self._receivers == np.arange(self._grid.number_of_nodes)
        )
        pits = np.setxor1d(self_draining_nodes, self._grid.boundary_nodes)
        if pits.any():
            warn(
                "Pits detected in the flow directions supplied. "
                "Pits will be treated as drainage nodes."
            )
            is_drainage_node[pits] = 1

        # iterate downstream through stack to find nearest drainage elevation
        nearest_drainage_elev = np.empty(self._elev.shape)
        for n in self._node_order:
            r = self._receivers[n]
            # if not drainage node set drainage elevation to downstream.
            if not is_drainage_node[n]:
                nearest_drainage_elev[n] = nearest_drainage_elev[r]
            else:  # set elevation of drainage to self.
                nearest_drainage_elev[n] = self._elev[n]

        self._hand[:] = self._elev - nearest_drainage_elev



================================================
File: lake_fill/__init__.py
================================================
from .lake_fill_barnes import LakeMapperBarnes

__all__ = ["LakeMapperBarnes"]



================================================
File: lake_fill/lake_fill_barnes.py
================================================
#!/usr/env/python

"""lake_fill_barnes.py.

Fill sinks in a landscape to the brim, following the Barnes et al.
(2014) algorithms.
"""


import heapq
import itertools
from collections import deque

import numpy as np

from landlab import Component
from landlab import NodeStatus
from landlab import RasterModelGrid
from landlab.components import FlowAccumulator
from landlab.utils import StablePriorityQueue
from landlab.utils.return_array import return_array_at_node

LARGE_ELEV = 9999999999.0

# TODO: Needs to have rerouting functionality...


def _fill_one_node_to_flat(fill_surface, all_neighbors, pitq, openq, closedq, dummy):
    """Implements the Barnes et al. algorithm for a simple fill. Assumes the
    _open and _closed lists have already been updated per Barnes algos 2&3, lns
    1-7.

    Parameters
    ----------
    fill_surface : 1-D array of length nnodes
        The surface to fill in LL node order. Modified in place.
    all_neighbors : (nnodes, max_nneighbours) array
        Adjacent nodes at each node.
    pitq : heap queue (i.e., a structured list)
        Current nodes known to be in a lake, if already identified.
    openq : StablePriorityQueue object
        Ordered queue of nodes remaining to be checked out by the algorithm
        that are known not to be in a lake.
    closedq : 1-D boolean array of length nnodes
        Nodes already or not to be explored by the algorithm.
    dummy : any Python object
        Necessary for direct comparison with _fill_one_node_to_slant.

    Examples
    --------
    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> mg = RasterModelGrid((5, 6))
    >>> for edge in ("left", "top", "bottom"):
    ...     mg.status_at_node[mg.nodes_at_edge(edge)] = mg.BC_NODE_IS_CLOSED
    ...
    >>> z = np.array(
    ...     [
    ...         [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    ...         [0.0, 2.1, 1.1, 0.6, 1.6, 0.0],
    ...         [0.0, 2.0, 1.0, 0.5, 1.5, 0.0],
    ...         [0.0, 2.2, 1.2, 0.7, 1.7, 0.0],
    ...         [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    ...     ]
    ... ).flatten()
    >>> zw = z.copy()
    >>> openq = StablePriorityQueue()
    >>> pitq = []
    >>> closedq = mg.zeros("node", dtype=bool)
    >>> closedq[mg.status_at_node == mg.BC_NODE_IS_CLOSED] = True
    >>> edges = np.array([11, 17, 23])
    >>> for edgenode in edges:
    ...     openq.add_task(edgenode, priority=z[edgenode])
    ...
    >>> closedq[edges] = True
    >>> while True:
    ...     try:
    ...         _fill_one_node_to_flat(
    ...             zw, mg.adjacent_nodes_at_node, pitq, openq, closedq, None
    ...         )
    ...     except KeyError:
    ...         break
    ...

    Now check the values make sense.

    >>> lake = np.array(
    ...     [
    ...         [0, 0, 0, 0, 0, 0],
    ...         [0, 0, 1, 1, 0, 0],
    ...         [0, 0, 1, 1, 0, 0],
    ...         [0, 0, 1, 1, 0, 0],
    ...         [0, 0, 0, 0, 0, 0],
    ...     ],
    ...     dtype=bool,
    ... ).flatten()

    >>> np.allclose(zw[lake], z[16])
    True
    >>> np.all(np.greater(zw[lake], z[lake]))
    True
    >>> np.allclose(zw[~lake], z[~lake])
    True
    """
    try:
        c = heapq.heappop(pitq)
    except IndexError:
        c = openq.pop_task()
        # this will raise a KeyError once it's exhausted both queues
    cneighbors = all_neighbors[c]
    openneighbors = cneighbors[np.logical_not(closedq[cneighbors])]  # for efficiency
    closedq[openneighbors] = True
    for n in openneighbors:
        if fill_surface[n] <= fill_surface[c]:
            fill_surface[n] = fill_surface[c]
            heapq.heappush(pitq, n)
        else:
            openq.add_task(n, priority=fill_surface[n])


class LakeMapperBarnes(Component):
    """A Landlab implementation of the Barnes et al. (2014) lake filling & lake
    routing algorithms, lightly modified and adapted for Landlab by DEJH. This
    component is designed as a direct replacement for the LakeMapper as
    existing pre-Aug 2018, and provides a suite of properties to access
    information about the lakes created each time it is run. Only significant
    difference is the way the lakes are coded: this component uses the (unique)
    ID of the outlet node, whereas DepressionFinderAndRouter uses one of the
    pit node IDs. Note also this component does not offer the `lake_codes` or
    `display_depression_map` options, for essentially this reason. Use
    `lake_map` instead for both. It also uses a much more Landlabbian
    `run_one_step()` method as its driver, superceding
    DepressionFinderAndRouter's `map_depressions()`.

    A variety of options is provided. Flow routing is route-to-one in this
    implementation, but can be either D4 ("steepest") or D8 on a raster.
    The surface can be filled to either flat or a very slight downward
    incline, such that subsequent flow routing will run over the lake surface.
    This incline is applied at machine precision to minimise the chances of
    creating false outlets by overfill, and note that the gradient as
    calculated on such surfaces may still appear to be zero.
    The filling can either be performed in place, or on a new (water) surface
    distinct from the original (rock) surface. For efficiency, data structures
    describing the lakes and their properties are only created, and existing
    flow direction and flow accumulation fields modified, if those flags are
    set at instantiation.

    With care, this component can be used to create a dynamically flooding
    surface in a fluvial landscape (interacting with, e.g., the
    StreamPowerEroder). See the run_one_step docstring for an example.

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Barnes, R., Lehman, C., Mulla, D. (2014). Priority-flood: An optimal
    depression-filling and watershed-labeling algorithm for digital elevation
    models. Computers and Geosciences  62(C), 117 - 127.
    https://dx.doi.org/10.1016/j.cageo.2013.04.024

    **Additional References**

    None Listed
    """

    _name = "LakeMapperBarnes"

    _cite_as = """
    @article{BARNES2014117,
        title = {Priority-flood: An optimal depression-filling and
                 watershed-labeling algorithm for digital elevation models},
        journal = "Computers & Geosciences",
        volume = "62",
        pages = "117 - 127",
        year = "2014",
        issn = "0098-3004",
        doi = "https://doi.org/10.1016/j.cageo.2013.04.024",
        url = "http://www.sciencedirect.com/science/article/pii/S0098300413001337",
        author = "Richard Barnes and Clarence Lehman and David Mulla",
        keywords = "Pit filling, Terrain analysis, Hydrology, Drainage network, Modeling, GIS"
        }"""

    _unit_agnostic = True

    _info = {
        "drainage_area": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m**2",
            "mapping": "node",
            "doc": "Upstream accumulated surface area contributing to the node's discharge",
        },
        "flow__data_structure_delta": {
            "dtype": int,
            "intent": "inout",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": (
                "Node array containing the elements delta[1:] of the data "
                "structure 'delta' used for construction of the "
                "downstream-to-upstream node array"
            ),
        },
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "inout",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "inout",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__sink_flag": {
            "dtype": bool,
            "intent": "inout",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Boolean array, True at local lows",
        },
        "flow__upstream_node_order": {
            "dtype": int,
            "intent": "inout",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array containing downstream-to-upstream ordered list of node IDs",
        },
        "surface_water__discharge": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m**3/s",
            "mapping": "node",
            "doc": "Volumetric discharge of surface water",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
    }

    def __init__(
        self,
        grid,
        surface="topographic__elevation",
        method="Steepest",
        fill_flat=True,
        fill_surface="topographic__elevation",
        redirect_flow_steepest_descent=False,
        reaccumulate_flow=False,
        ignore_overfill=False,
        track_lakes=True,
    ):
        """Initialize the component.

        Parameters
        ----------
        grid : ModelGrid
            A grid.
        surface : field name at node or array of length node
            The surface to direct flow across.
        method : {'Steepest', 'D8'}
            Whether or not to recognise diagonals as valid flow paths, if a raster.
            Otherwise, no effect.
        fill_flat : bool
            If True, pits will be filled to perfectly horizontal. If False, the new
            surface will be slightly inclined to give steepest descent flow paths
            to the outlet.
        fill_surface : bool
            Sets the field or array to fill. If fill_surface is surface, this
            operation occurs in place, and is faster.
            Note that the component will overwrite fill_surface if it exists; to
            supply an existing water level to it, supply that water level field as
            surface, not fill_surface.
        redirect_flow_steepest_descent : bool
            If True, the component outputs modified versions of the
            'flow__receiver_node', 'flow__link_to_receiver_node',
            'flow__sink_flag', and 'topographic__steepest_slope' fields. These
            are the fields output by the FlowDirector components, so set to
            True if you wish to pass this LakeFiller to the FlowAccumulator,
            or if you wish to work directly with the new, correct flow directions
            and slopes without rerunning these components on your new surface.
            Ensure the necessary fields already exist, and have already been
            calculated by a FlowDirector! This also means you need to instantiate
            your FlowDirector **before** you instantiate the LakeMapperBarnes.
            Note that the new topographic__steepest_slope will always be set to
            zero, even if fill_flat=False (i.e., there is actually a miniscule
            gradient on the new topography, which gets ignored).
        reaccumulate_flow : bool
            If True, and redirect_flow_steepest_descent is True, the run method
            will (re-)accumulate the flow after redirecting the flow. This means
            the 'drainage_area', 'surface_water__discharge',
            'flow__upstream_node_order', and the other various flow accumulation
            fields (see output field names) will now reflect the new drainage
            patterns without having to manually reaccumulate the discharge. If
            True but redirect_flow_steepest_descent is False, raises an
            ValueError.
        ignore_overfill : bool
            If True, suppresses the Error that would normally be raised during
            creation of a gentle incline on a fill surface (i.e., if not
            fill_flat). Typically this would happen on a synthetic DEM where more
            than one outlet is possible at the same elevation. If True, the
            was_there_overfill property can still be used to see if this has
            occurred.
        track_lakes : bool
            If True, the component permits a slight hit to performance in order to
            explicitly track which nodes have been filled, and to enable queries
            on that data in retrospect. Set to False to simply fill the surface
            and be done with it.

        """
        super().__init__(grid)

        if "flow__receiver_node" in grid.at_node and grid.at_node[
            "flow__receiver_node"
        ].size != grid.size("node"):
            raise NotImplementedError(
                "A route-to-multiple flow director has been "
                "run on this grid. The landlab development team has not "
                "verified that LakeMapperBarnes is compatible with "
                "route-to-multiple methods. Please open a GitHub Issue "
                "to start this process."
            )

        self._pit = []
        self._closed = self._grid.zeros(at="node", dtype=bool)
        self._gridclosednodes = (
            self._grid.status_at_node == self._grid.BC_NODE_IS_CLOSED
        )
        # close up the BC_NODE_IS_CLOSED permanently:
        self._closed[self._gridclosednodes] = True

        # this component maintains its own internal count of how often it has
        # been called. This is to enable "cheap" data access of the various
        # available data structures without needless recalculation
        self._runcounter = itertools.count()
        self._runcount = -1  # not yet run
        self._lastcountforlakemap = -1  # lake_map has not yet been called
        self._PitTop = LARGE_ELEV  # variable to not overfill slanted surfaces
        self._ignore_overfill = ignore_overfill
        self._overfill_flag = False
        self._track_lakes = track_lakes

        # get the neighbour call set up:
        if method not in {"Steepest", "D8"}:
            raise ValueError(f"{method}: method must be 'Steepest' or 'D8'")
        if method == "D8":
            if isinstance(grid, RasterModelGrid):
                self._allneighbors = np.concatenate(
                    (
                        self._grid.adjacent_nodes_at_node,
                        self._grid.diagonal_adjacent_nodes_at_node,
                    ),
                    axis=1,
                )
            else:  # not a raster
                raise ValueError(
                    (
                        "D8 is not a valid value for method if grid type is "
                        + "{gridtype}!"
                    ).format(gridtype=type(grid))
                )
        else:
            self._allneighbors = self._grid.adjacent_nodes_at_node

        # A key difference from the "pure" Barnes algorithm for LL is that
        # we must'n flood from all the edges. Instead, we can only flood from
        # a true grid edge, i.e., only the FIXED boundary types. (Both
        # CLOSED and LOOPED assume flow either can't get out there, or at
        # least, there's more land in that direction that will be handled
        # otherwise.) Note we add a test that there is at least some kind
        # of outlet!!
        self._edges = np.where(
            np.logical_or(
                self._grid.status_at_node == NodeStatus.FIXED_VALUE,
                self._grid.status_at_node == NodeStatus.FIXED_GRADIENT,
            )
        )[0]
        if self._edges.size == 0:
            raise ValueError(
                "No valid outlets found on the grid! You cannot run the "
                + "filling algorithms!"
            )
        # and finally, close these up permanently as well (edges will always
        # be edges...)
        self._closed[self._edges] = True
        # ...note there's a slight of hand here, Because of the ordering of LL
        # grids, the last node will always be a boundary node, even for very
        # odd Voronois. This enables us to treat out -1s in the neighbour
        # arrays as always True. But, just in case...
        assert self._closed[-1]

        # check if we are modifying in place or not. This gets used to check
        # it makes sense to calculate various properties.
        self._inplace = surface is fill_surface
        # then
        self._surface = return_array_at_node(grid, surface)
        self._fill_surface = return_array_at_node(grid, fill_surface)

        # NOTE: buggy functionality of return_array_at_node here means
        # component can't yet handle arrays as opposed to fields...
        # This will be resolved by a modification to return_array_at_node

        # now, work out what constitutes a "surface" under various input opts:
        self._dontredirect = not redirect_flow_steepest_descent

        if redirect_flow_steepest_descent:
            # this routine only permitted if we store the lake info, so
            if not track_lakes:
                raise ValueError("You must track_lakes to redirect the flow!")
            # Check we have the necessary fields already existing.
            # These will raise FieldErrors if they don't.
            # This will cause a bunch of our tests to break, so users will
            # never see this.
            self._receivers = self._grid.at_node["flow__receiver_node"]
            self._receiverlinks = self._grid.at_node["flow__link_to_receiver_node"]
            self._steepestslopes = self._grid.at_node["topographic__steepest_slope"]
            # if raster, do the neighbors & diagonals separate when rerouting
            # so we'll need to pull these separately:
            if method == "D8":  # Raster test unnecessary given tests above
                self._neighbor_arrays = (
                    self._grid.adjacent_nodes_at_node,
                    self._grid.diagonal_adjacent_nodes_at_node,
                )
                self._link_arrays = (
                    self._grid.links_at_node,
                    self._grid.d8s_at_node[:, 4:],
                )
                self._neighbor_lengths = self._grid.length_of_d8
            else:
                self._neighbor_arrays = (self._grid.adjacent_nodes_at_node,)
                self._link_arrays = (self._grid.links_at_node,)
                self._neighbor_lengths = self._grid.length_of_link

        if reaccumulate_flow:
            if not redirect_flow_steepest_descent:
                raise ValueError(
                    "You must also redirect_flow_steepest_descent if you "
                    + "want to reaccumulate_flow!"
                )
            self._reaccumulate = True
            self._fa = FlowAccumulator(self._grid, flow_director=method)
        else:
            self._reaccumulate = False

        self._fill_flat = fill_flat
        if fill_flat:
            self._fill_one_node = _fill_one_node_to_flat
        else:
            self._fill_one_node = self._fill_one_node_to_slant

    def _fill_one_node_to_slant(
        self, fill_surface, all_neighbors, pitq, openq, closedq, ignore_overfill
    ):
        """Implements the Barnes et al. algorithm to obtain a naturally
        draining surface, updating a single node. Assumes the _open and _closed
        lists have already been updated per Barnes algos 2&3, lns 1-7.

        Parameters
        ----------
        fill_surface : 1-D array of length nnodes
            The surface to fill in LL node order. Modified in place.
        all_neighbors : (nnodes, max_nneighbours) array
            Adjacent nodes at each node.
        pitq : heap queue (i.e., a structured list)
            Current nodes known to be in a lake, if already identified.
        openq : StablePriorityQueue object
            Ordered queue of nodes remaining to be checked out by the algorithm
            that are known not to be in a lake.
        closedq : 1-D boolean array of length nnodes
            Nodes already or not to be explored by the algorithm.
        ignore_overfill : bool
            If False, method will raise a ValueError if adding an increment
            to the node's elevation would fundamentally alter the resulting
            drainage pattern (e.g., it would create a new outlet somewhere).
            If True, the elevation of the node will be changed regardless.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import LakeMapperBarnes, FlowAccumulator
        >>> from landlab.utils import StablePriorityQueue
        >>> mg = RasterModelGrid((5, 6))
        >>> for edge in ("left", "top", "bottom"):
        ...     mg.status_at_node[mg.nodes_at_edge(edge)] = mg.BC_NODE_IS_CLOSED
        ...
        >>> z = mg.add_zeros("topographic__elevation", at="node", dtype=float)
        >>> z.reshape(mg.shape)[2, 1:-1] = [2.0, 1.0, 0.5, 1.5]
        >>> z.reshape(mg.shape)[1, 1:-1] = [2.1, 1.1, 0.6, 1.6]
        >>> z.reshape(mg.shape)[3, 1:-1] = [2.2, 1.2, 0.7, 1.7]
        >>> z_init = z.copy()
        >>> fa = FlowAccumulator(mg)
        >>> lmb = LakeMapperBarnes(mg, method="Steepest")
        >>> lmb._closed = mg.zeros("node", dtype=bool)
        >>> lmb._closed[mg.status_at_node == mg.BC_NODE_IS_CLOSED] = True
        >>> edges = np.array([11, 17, 23])
        >>> open = StablePriorityQueue()
        >>> for edgenode in edges:
        ...     open.add_task(edgenode, priority=z[edgenode])
        ...
        >>> lmb._closed[edges] = True
        >>> first_nodes_checked = []

        >>> for i in range(3):  # run a couple of steps
        ...     lmb._fill_one_node_to_slant(
        ...         z, mg.adjacent_nodes_at_node, lmb._pit, open, lmb._closed, False
        ...     )
        ...     print(open.peek_at_task())
        ...     assert lmb._pit == []  # these steps don't find pits
        ...
        17
        23
        16

        >>> lmb._fill_one_node_to_slant(
        ...     z, mg.adjacent_nodes_at_node, lmb._pit, open, lmb._closed, False
        ... )
        >>> lmb._pit == [15]  # Popping 16 off "open" puts 15 in "pit"
        True
        >>> np.isclose(z[15], z[16])  # 15 has also been filled in this step
        True
        >>> z[15] > z[16]  # ...but 15 is incrementally greater than 16
        True

        >>> lmb._fill_one_node_to_slant(
        ...     z, mg.adjacent_nodes_at_node, lmb._pit, open, lmb._closed, False
        ... )
        >>> lmb._pit == [9, 21, 14]  # 15 pops of pit, these neighbors go in
        True
        >>> np.allclose(z[15], [z[9], z[21], z[14]])  # now filled
        True
        >>> np.all([z[9] == z[21], z[21] == z[14]])  # these perfectly level
        True
        >>> z[9] > z[15]  # ...but incrementally above 15
        True

        >>> lmb._fill_one_node_to_slant(
        ...     z, mg.adjacent_nodes_at_node, lmb._pit, open, lmb._closed, False
        ... )
        >>> lmb._pit == [8, 21, 14]  # 9 popped off pit, 8 went in. And so on.
        True

        Test a failing example. This behaviour exists to prevent the
        application of the gradient from fundamentally altering the drainage
        pattern that "should" result.

        >>> mg = RasterModelGrid((3, 7))
        >>> for edge in ("top", "right", "bottom"):
        ...     mg.status_at_node[mg.nodes_at_edge(edge)] = mg.BC_NODE_IS_CLOSED
        ...
        >>> z = mg.add_zeros("topographic__elevation", at="node", dtype=float)
        >>> z.reshape(mg.shape)[1, 1:-1] = [1.0, 0.2, 0.1, 1.0000000000000004, 1.5]
        >>> z_init = z.copy()
        >>> fa = FlowAccumulator(mg)
        >>> lmb = LakeMapperBarnes(mg, method="Steepest")
        >>> lmb._closed = mg.zeros("node", dtype=bool)
        >>> lmb._closed[mg.status_at_node == mg.BC_NODE_IS_CLOSED] = True
        >>> open = StablePriorityQueue()
        >>> edges = np.array([7])
        >>> for edgenode in edges:
        ...     open.add_task(edgenode, priority=z[edgenode])
        ...
        >>> lmb._closed[edges] = True
        >>> while True:
        ...     try:
        ...         lmb._fill_one_node_to_slant(
        ...             z, mg.adjacent_nodes_at_node, lmb._pit, open, lmb._closed, False
        ...         )
        ...     except KeyError:
        ...         break
        ...     except ValueError:
        ...         print("ValueError was raised: here we overfilled")
        ...
        ValueError was raised: here we overfilled
        """
        try:
            topopen = openq.peek_at_task()
        except KeyError:
            noopen = True
        else:
            noopen = False
        try:
            toppit = pitq[0]
        except IndexError:
            nopit = True
        else:
            nopit = False
        if (
            not (nopit or noopen) and topopen == toppit
        ):  # intentionally tight comparison
            # not clear how this occurs, but present in Barnes ->
            # DEJH suspects this should be an elevation comparison given the
            # text description. Regardless, this is only to ensure
            # repeatability, so it's not vital even if these cases don't
            # trigger
            c = openq.pop_task()
            self._PitTop = LARGE_ELEV
        if not nopit:
            c = heapq.heappop(pitq)
            if np.isclose(self._PitTop, LARGE_ELEV):
                self._PitTop = fill_surface[c]
        else:
            c = openq.pop_task()  # again, returns KeyError if empty
            self._PitTop = LARGE_ELEV

        for n in all_neighbors[c]:
            if closedq[n]:
                continue
            else:
                closedq[n] = True
            nextval = np.nextafter(fill_surface[c], LARGE_ELEV)
            # DEJH believes that in the LL use cases this is impossible,
            # but retained as comments since present in Barnes algorithm
            # if self._gridclosednodes[n]:
            #     heapq.heappush(pitq, n)
            if fill_surface[n] <= nextval:  # former elif
                if self._PitTop < fill_surface[n] and nextval >= fill_surface[n]:
                    if ignore_overfill:
                        self._overfill_flag = True
                    else:
                        raise ValueError(
                            "Pit is overfilled due to creation of two "
                            + "outlets as the minimum gradient gets applied. "
                            + "Suppress this Error with the ignore_overfill "
                            + "flag at component instantiation."
                        )
                fill_surface[n] = nextval
                heapq.heappush(pitq, n)
            else:
                openq.add_task(n, priority=fill_surface[n])

    def _fill_to_flat_with_tracking(
        self, fill_surface, all_neighbors, pitq, openq, closedq
    ):
        """Implements the Barnes et al. algorithm for a simple fill over the
        grid. Assumes the _open and _closed lists have already been updated per
        Barnes algos 2&3, lns 1-7.

        This version runs a little more slowly to enable tracking of which
        nodes are linked to which outlets.

        Parameters
        ----------
        fill_surface : 1-D array
            The surface to fill in LL node order. Modified in place.
        all_neighbors : (nnodes, max_nneighbours) array
            Adjacent nodes at each node.
        pitq : heap queue (i.e., a structured list)
            Current nodes known to be in a lake, if already identified.
        openq : StablePriorityQueue object
            Ordered queue of nodes remaining to be checked out by the algorithm
            that are known not to be in a lake.
        closedq : 1-D boolean array of length nnodes
            Nodes already or not to be explored by the algorithm.

        Returns
        -------
        lakemappings : {outlet_ID : [nodes draining to outlet]}
            Dict with outlet nodes of individual lakes as keys, and lists of
            each node inundated (i.e., depth > 0.) by that lake. Note
            len(keys) is the number of individually mapped lakes.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import LakeMapperBarnes, FlowAccumulator
        >>> from landlab.utils import StablePriorityQueue
        >>> mg = RasterModelGrid((5, 6))
        >>> for edge in ("left", "top", "bottom"):
        ...     mg.status_at_node[mg.nodes_at_edge(edge)] = mg.BC_NODE_IS_CLOSED
        ...
        >>> z = mg.add_zeros("topographic__elevation", at="node", dtype=float)
        >>> z[:] = mg.node_x.max() - mg.node_x
        >>> z[[10, 23]] = 1.1  # raise "guard" exit nodes
        >>> z[7] = 2.0  # is a lake on its own
        >>> z[9] = 0.5
        >>> z[15] = 0.3
        >>> z[14] = 0.6  # [9, 14, 15] is a lake
        >>> z[22] = 0.9  # a non-contiguous lake node also draining to 16
        >>> z_init = z.copy()
        >>> fa = FlowAccumulator(mg)
        >>> lmb = LakeMapperBarnes(mg, method="Steepest")
        >>> lmb._closed = mg.zeros("node", dtype=bool)
        >>> lmb._closed[mg.status_at_node == mg.BC_NODE_IS_CLOSED] = True
        >>> open = StablePriorityQueue()
        >>> edges = np.array([11, 17, 23])
        >>> for edgenode in edges:
        ...     open.add_task(edgenode, priority=z[edgenode])
        ...
        >>> lmb._closed[edges] = True
        >>> out = lmb._fill_to_flat_with_tracking(
        ...     z, mg.adjacent_nodes_at_node, lmb._pit, open, lmb._closed
        ... )
        >>> out == {8: deque([7]), 16: deque([15, 9, 14, 22])}
        True
        """
        lakemappings = {}
        outlet_ID = self._grid.BAD_INDEX
        while True:
            try:
                c = heapq.heappop(pitq)
            except IndexError:
                try:
                    c = openq.pop_task()
                    outlet_ID = c
                except KeyError:
                    break
            else:
                try:
                    lakemappings[outlet_ID].append(c)  # add this node to lake
                except KeyError:  # this is the first node of a new lake
                    lakemappings[outlet_ID] = deque([c])

            cneighbors = all_neighbors[c]
            openneighbors = cneighbors[
                np.logical_not(closedq[cneighbors])
            ]  # for efficiency
            closedq[openneighbors] = True
            for n in openneighbors:
                if fill_surface[n] <= fill_surface[c]:
                    fill_surface[n] = fill_surface[c]
                    heapq.heappush(pitq, n)
                else:
                    openq.add_task(n, priority=fill_surface[n])
            # print(np.sort(openq.tasks_currently_in_queue()), pitq)
        return lakemappings

    def _fill_to_slant_with_optional_tracking(
        self,
        fill_surface,
        all_neighbors,
        pitq,
        openq,
        closedq,
        ignore_overfill,
        track_lakes,
    ):
        """Implements the Barnes et al. algorithm to obtain a naturally
        draining surface over the grid. Assumes the _open and _closed lists
        have already been updated per Barnes algos 2&3, lns 1-7.

        This version runs a little more slowly to enable tracking of which
        nodes are linked to which outlets.

        Parameters
        ----------
        fill_surface : 1-D array of length nnodes
            The surface to fill in LL node order. Modified in place.
        all_neighbors : (nnodes, max_nneighbours) array
            Adjacent nodes at each node.
        pitq : heap queue (i.e., a structured list)
            Current nodes known to be in a lake, if already identified.
        openq : StablePriorityQueue object
            Ordered queue of nodes remaining to be checked out by the algorithm
            that are known not to be in a lake.
        closedq : 1-D boolean array of length nnodes
            Nodes already or not to be explored by the algorithm.
        ignore_overfill : bool
            If False, method will raise a ValueError if adding an increment
            to the node's elevation would fundamentally alter the resulting
            drainage pattern (e.g., it would create a new outlet somewhere).
            If True, the elevation of the node will be changed regardless.
        track_lakes : bool
            If True, returns a dict with data on the lakes created. If false,
            returns an empty dict.

        Returns
        -------
        lakemappings : dict
            If track_lakes, {outlet_ID : [nodes draining to outlet]}. This is
            a dict with outlet nodes of individual lakes as keys, and lists
            (strictly, deques) of each node inundated (i.e., depth > 0.) by
            that lake. Note len(keys) is the number of individually mapped
            lakes.
            If not track_lakes, an empty dict.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import LakeMapperBarnes, FlowAccumulator
        >>> from landlab.utils import StablePriorityQueue
        >>> mg = RasterModelGrid((5, 6))
        >>> for edge in ("left", "top", "bottom"):
        ...     mg.status_at_node[mg.nodes_at_edge(edge)] = mg.BC_NODE_IS_CLOSED
        ...
        >>> z = mg.add_zeros("topographic__elevation", at="node", dtype=float)
        >>> z.reshape(mg.shape)[2, 1:-1] = [2.0, 1.0, 0.5, 1.5]
        >>> z.reshape(mg.shape)[1, 1:-1] = [2.1, 1.1, 0.6, 1.6]
        >>> z.reshape(mg.shape)[3, 1:-1] = [2.2, 1.2, 0.7, 1.7]
        >>> z_init = z.copy()
        >>> fa = FlowAccumulator(mg)
        >>> lmb = LakeMapperBarnes(mg, method="Steepest")
        >>> lmb._closed = mg.zeros("node", dtype=bool)
        >>> lmb._closed[mg.status_at_node == mg.BC_NODE_IS_CLOSED] = True
        >>> open = StablePriorityQueue()
        >>> edges = np.array([11, 17, 23])
        >>> for edgenode in edges:
        ...     open.add_task(edgenode, priority=z[edgenode])
        ...
        >>> lmb._closed[edges] = True
        >>> out = lmb._fill_to_slant_with_optional_tracking(
        ...     z, mg.adjacent_nodes_at_node, lmb._pit, open, lmb._closed, False, True
        ... )
        >>> out == {16: deque([15, 9, 8, 14, 20, 21])}
        True

        Test two pits:

        >>> z[:] = mg.node_x.max() - mg.node_x
        >>> z[[10, 23]] = 1.1  # raise "guard" exit nodes
        >>> z[7] = 2.0  # is a lake on its own
        >>> z[9] = 0.5
        >>> z[15] = 0.3
        >>> z[14] = 0.6  # [9, 14, 15] is a lake
        >>> z[22] = 0.9  # a non-contiguous lake node also draining to 16
        >>> z_init = z.copy()
        >>> fa = FlowAccumulator(mg)
        >>> lmb = LakeMapperBarnes(mg, method="Steepest")
        >>> lmb._closed = mg.zeros("node", dtype=bool)
        >>> lmb._closed[mg.status_at_node == mg.BC_NODE_IS_CLOSED] = True
        >>> open = StablePriorityQueue()
        >>> edges = np.array([11, 17, 23])
        >>> for edgenode in edges:
        ...     open.add_task(edgenode, priority=z[edgenode])
        ...
        >>> lmb._closed[edges] = True
        >>> out = lmb._fill_to_slant_with_optional_tracking(
        ...     z, mg.adjacent_nodes_at_node, lmb._pit, open, lmb._closed, False, True
        ... )
        >>> out == {8: deque([7]), 16: deque([15, 9, 14, 22])}
        True
        >>> fr = FlowAccumulator(mg, flow_director="D4")
        >>> fr.run_one_step()
        >>> np.all(mg.at_node["flow__sink_flag"][mg.core_nodes] == 0)
        True
        >>> drainage_area = [
        ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ...     [0.0, 1.0, 2.0, 3.0, 1.0, 1.0],
        ...     [0.0, 1.0, 4.0, 9.0, 11.0, 11.0],
        ...     [0.0, 1.0, 2.0, 1.0, 1.0, 0.0],
        ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ... ]
        >>> np.allclose(mg.at_node["drainage_area"].reshape(mg.shape), drainage_area)
        True

        With track_lakes == False, fill still works just fine, but the dict
        returned is empty:

        >>> z[:] = mg.node_x.max() - mg.node_x  # all this as above
        >>> z[[10, 23]] = 1.1  # raise "guard" exit nodes
        >>> z[7] = 2.0  # is a lake on its own
        >>> z[9] = 0.5
        >>> z[15] = 0.3
        >>> z[14] = 0.6  # [9, 14, 15] is a lake
        >>> z[22] = 0.9  # a non-contiguous lake node also draining to 16
        >>> z_init = z.copy()
        >>> fa = FlowAccumulator(mg)
        >>> lmb = LakeMapperBarnes(mg, method="Steepest")
        >>> lmb._closed = mg.zeros("node", dtype=bool)
        >>> lmb._closed[mg.status_at_node == mg.BC_NODE_IS_CLOSED] = True
        >>> open = StablePriorityQueue()
        >>> edges = np.array([11, 17, 23])
        >>> for edgenode in edges:
        ...     open.add_task(edgenode, priority=z[edgenode])
        ...
        >>> lmb._closed[edges] = True
        >>> lmb._fill_to_slant_with_optional_tracking(
        ...     z, mg.adjacent_nodes_at_node, lmb._pit, open, lmb._closed, False, False
        ... )  # empty dict now
        {}

        >>> fr.run_one_step()  # drains fine still, as above
        >>> np.allclose(mg.at_node["drainage_area"].reshape(mg.shape), drainage_area)
        True

        Test a failing example:

        >>> mg = RasterModelGrid((3, 7))
        >>> for edge in ("top", "right", "bottom"):
        ...     mg.status_at_node[mg.nodes_at_edge(edge)] = mg.BC_NODE_IS_CLOSED
        ...
        >>> z = mg.add_zeros("topographic__elevation", at="node", dtype=float)
        >>> z.reshape(mg.shape)[1, 1:-1] = [1.0, 0.2, 0.1, 1.0000000000000004, 1.5]
        >>> z_init = z.copy()
        >>> fa = FlowAccumulator(mg)
        >>> lmb = LakeMapperBarnes(mg, method="Steepest")
        >>> lmb._closed = mg.zeros("node", dtype=bool)
        >>> lmb._closed[mg.status_at_node == mg.BC_NODE_IS_CLOSED] = True
        >>> open = StablePriorityQueue()
        >>> edges = np.array([7])
        >>> for edgenode in edges:
        ...     open.add_task(edgenode, priority=z[edgenode])
        ...
        >>> lmb._closed[edges] = True
        >>> lmb._fill_to_slant_with_optional_tracking(
        ...     z, mg.adjacent_nodes_at_node, lmb._pit, open, lmb._closed, False, True
        ... )
        Traceback (most recent call last):
        ...
        ValueError: Pit is overfilled due to creation of two outlets as the
        minimum gradient gets applied. Suppress this Error with the
        ignore_overfill flag at component instantiation.

        ValueError was raised because Pit is overfilled due to creation of
        two outlets as the minimum gradient gets applied. Suppress this Error
        with the ignore_overfill flag at component instantiation.
        """
        lakemappings = {}
        outlet_ID = self._grid.BAD_INDEX
        while True:
            try:
                topopen = openq.peek_at_task()
            except KeyError:
                noopen = True
                topopen = None
            else:
                noopen = False
            try:
                toppit = pitq[0]
            except IndexError:
                nopit = True
                toppit = None
            else:
                nopit = False
            # as above, DEJH is unclear how this clause triggers, so
            # retained but untested ->
            if (not (nopit or noopen)) and (topopen == toppit):
                # intentionally tight comparison
                c = openq.pop_task()
                outlet_ID = c
                self._PitTop = LARGE_ELEV
            elif not nopit:
                c = heapq.heappop(pitq)
                if np.isclose(self._PitTop, LARGE_ELEV):
                    self._PitTop = fill_surface[c]
                if track_lakes:
                    try:
                        lakemappings[outlet_ID].append(c)
                        # ^add this node to lake
                    except KeyError:
                        # ^this is the first node of a new lake
                        lakemappings[outlet_ID] = deque([c])
            else:
                try:
                    c = openq.pop_task()
                    # ^again, returns KeyError if empty
                except KeyError:
                    break
                outlet_ID = c
                self._PitTop = LARGE_ELEV

            for n in all_neighbors[c]:
                if closedq[n]:
                    continue
                else:
                    closedq[n] = True
                nextval = np.nextafter(fill_surface[c], LARGE_ELEV)
                # as in non-tracker, DEJH believes this is redundant in LL
                # if self._gridclosednodes[n]:
                #     heapq.heappush(pitq, n)
                if fill_surface[n] <= nextval:  # formerly elif
                    if self._PitTop < fill_surface[n] and nextval >= fill_surface[n]:
                        if ignore_overfill:
                            self._overfill_flag = True
                        else:
                            raise ValueError(
                                "Pit is overfilled due to creation of two"
                                " outlets as the minimum gradient gets"
                                " applied. Suppress this Error with the"
                                " ignore_overfill flag at component"
                                " instantiation."
                            )
                    fill_surface[n] = nextval
                    heapq.heappush(pitq, n)
                else:
                    openq.add_task(n, priority=fill_surface[n])
        return lakemappings

    def _track_original_surface(self):
        """This helper method ensures that if flow is to be redircted, the
        _redirect_flowdirs() method can still get access to this information
        when it needs it. The idea here is that the operation is essentially
        free when surface and fill_surface were different to start with, which
        should make us faster.

        Examples
        --------

        >>> from landlab import RasterModelGrid
        >>> from landlab.components import LakeMapperBarnes, FlowAccumulator
        >>> mg = RasterModelGrid((5, 6), xy_spacing=2.0)
        >>> z = mg.add_zeros("topographic__elevation", at="node", dtype=float)
        >>> z_new = mg.add_zeros("topographic__fill", at="node", dtype=float)
        >>> fa = FlowAccumulator(mg)
        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="D8",
        ...     surface="topographic__elevation",
        ...     fill_surface="topographic__fill",
        ...     redirect_flow_steepest_descent=False,
        ...     track_lakes=False,
        ... )
        >>> orig_surf = lmb._track_original_surface()
        >>> z is orig_surf
        True
        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="D8",
        ...     surface="topographic__elevation",
        ...     fill_surface="topographic__elevation",
        ...     redirect_flow_steepest_descent=False,
        ...     track_lakes=False,
        ... )
        >>> orig_surf = lmb._track_original_surface()
        >>> z is orig_surf
        False
        """
        if self._inplace:
            orig_surf = self._surface.copy()
        else:
            orig_surf = self._surface
        return orig_surf

    def _redirect_flowdirs(self, surface, lake_dict, openq):
        """For nodes within lakes that have already been defined, modifies
        existing FlowDirector fields to account for the lake filling, viz.
        'flow__receiver_node', 'flow__link_to_receiver_node',
        'flow__sink_flag', and 'topographic__steepest_slope'.

        Note that the topographic__steepest_slope of a lake node will always
        be exactly 0., even if fill_flat is False. This is because we are
        adding an increment to elevation at machine precision.

        Examples
        --------

        >>> import numpy as np
        >>> from collections import deque
        >>> from landlab import NodeStatus, RasterModelGrid
        >>> from landlab.components import (
        ...     LakeMapperBarnes,
        ...     FlowDirectorSteepest,
        ...     FlowAccumulator,
        ... )
        >>> from landlab.utils import StablePriorityQueue
        >>> mg = RasterModelGrid((5, 6), xy_spacing=2.0)
        >>> for edge in ("left", "top", "bottom"):
        ...     mg.status_at_node[mg.nodes_at_edge(edge)] = mg.BC_NODE_IS_CLOSED
        ...
        >>> z = mg.add_zeros("topographic__elevation", at="node", dtype=float)
        >>> z[:] = mg.node_x.max() - mg.node_x
        >>> z[23] = 1.3
        >>> z[15] = -2.0  # this deep pit causes the outlet to first drain *in*
        >>> z[10] = 1.3  # raise "guard" exit nodes
        >>> z[7] = 2.0  # is a lake on its own, if D8
        >>> z[9] = -1.0
        >>> z[14] = 0.6  # [9, 14, 15] is a lake in both methods
        >>> z[16] = 1.2
        >>> z[22] = 0.9  # a non-contiguous lake node also draining to 16 if D8
        >>> z_init = z.copy()
        >>> fd = FlowDirectorSteepest(mg)
        >>> fa = FlowAccumulator(mg)
        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     fill_flat=True,
        ...     redirect_flow_steepest_descent=True,
        ...     track_lakes=True,
        ... )

        In this test, we won't run the lmb. Instead, directly specify the
        correct answer:

        >>> lake_dict = {8: deque([7]), 16: deque([15, 9, 14, 22])}
        >>> fd.run_one_step()  # fill the director fields
        >>> fa.run_one_step()  # get a drainage_area
        >>> np.all(mg.at_node["flow__sink_flag"][[7, 15, 22]])  # sinks
        True
        >>> nodes_in_lakes = np.array([7, 8, 9, 14, 15, 16, 22])
        >>> nodes_not_in_lakes = np.setdiff1d(mg.nodes.flat, nodes_in_lakes)
        >>> openq = StablePriorityQueue()  # empty dummy

        Note we're here defining the outlets as inside the lakes, which isn't
        actually the behaviour of the component, but helps us demonstrate
        what changes, below.

        Now save the info we already have on the Flow fields:

        >>> receivers_init = mg.at_node["flow__receiver_node"].copy()
        >>> rec_links_init = mg.at_node["flow__link_to_receiver_node"].copy()
        >>> steepest_init = mg.at_node["topographic__steepest_slope"].copy()
        >>> drainage_area = mg.at_node["drainage_area"].copy()
        >>> orig_surf = lmb._track_original_surface()

        Note flow doesn't make it to the outlets:

        >>> outlets = np.where(mg.status_at_node == NodeStatus.FIXED_VALUE)
        >>> drainage_area[outlets].sum() == mg.cell_area_at_node[mg.core_nodes].sum()
        False

        Now, run the method:

        >>> lmb._redirect_flowdirs(orig_surf, lake_dict, openq)

        Now the flow directions all ignore the pits:

        >>> np.all(
        ...     mg.at_node["flow__receiver_node"].reshape(mg.shape)
        ...     == [
        ...         [0, 1, 2, 3, 4, 5],
        ...         [6, 8, 9, 15, 9, 11],
        ...         [12, 14, 15, 16, 17, 17],
        ...         [18, 20, 14, 15, 16, 23],
        ...         [24, 25, 26, 27, 28, 29],
        ...     ]
        ... )
        True

        (Note the filling of the pits might redirect the occasional node
        not in a lake, but on its perimeter - if the node used to drain
        into the lake, but now has a steeper descent path elsewhere.)

        There are now no pits:

        >>> np.any(mg.at_node["flow__sink_flag"][[7, 15, 22]])
        False

        The lake nodes now flow out:

        >>> mg.at_node["flow__receiver_node"][lake_dict[16]]
        array([16, 15, 15, 16])
        >>> mg.at_node["flow__receiver_node"][lake_dict[8]]
        array([8])

        ...and any outlet nodes that used to drain into the lake now drain
        out.

        >>> receivers_init[16]
        15
        >>> mg.at_node["flow__receiver_node"][16]
        17
        >>> np.isclose(mg.at_node["topographic__steepest_slope"][16], 0.6)
        True

        If we reaccumulate the flow, we'll now see that the boundary nodes do
        now accumulate the total available discharge:

        >>> area, discharge = fa.accumulate_flow(update_flow_director=False)
        >>> mg.at_node["drainage_area"][outlets].sum() == (
        ...     mg.cell_area_at_node[mg.core_nodes].sum()
        ... )
        True
        """
        closedq = self._grid.ones(at="node", dtype=int)
        # Using a slightly different approach. We recognise three types: lake
        # (0), lake margin (1), and closed (2). This lets us work the
        # perimeter too. Open each lake as needed.
        # close the known boundary nodes:
        closedq[self._grid.status_at_node != NodeStatus.CORE] = 2

        # now the actual loop. Work forward lake by lake to avoid unnecessary
        # processing (nodes outside lakes are already correct, by definition).
        for outlet, lakenodes in lake_dict.items():
            # open the lake:
            closedq[lakenodes] = 0
            # make a deque for liminal nodes:
            liminal_nodes = deque([])
            openq.add_task(outlet, priority=surface[outlet])

            # it's possible the outlet used to drain *into* the lake,
            # so it needs separate consideration. Likewise, the gradients
            # of the perimeter nodes are likely to be wrong.
            if self._grid.status_at_node[outlet] != NodeStatus.CORE:
                # don't do anything if the outlet happens to be a boundary
                pass
            else:
                out_elev = LARGE_ELEV
                for neighbor_set, link_set in zip(
                    self._neighbor_arrays, self._link_arrays
                ):
                    not_lake_neighbors = np.not_equal(closedq[neighbor_set[outlet]], 0)
                    minusones = np.equal(neighbor_set[outlet], -1)
                    not_lake_neighbors[minusones] = False
                    closednodes = np.equal(
                        self._grid.status_at_node[neighbor_set[outlet]],
                        self._grid.BC_NODE_IS_CLOSED,
                    )  # closed BCs can't count
                    not_lake_neighbors[closednodes] = False
                    try:
                        min_val = np.amin(
                            surface[neighbor_set[outlet][not_lake_neighbors]]
                        )
                    except ValueError:
                        continue
                    if min_val < out_elev:
                        viable_nodes = neighbor_set[outlet][not_lake_neighbors]
                        min_neighbor_byTrue = np.argmin(surface[viable_nodes])
                        min_neighbor = viable_nodes[min_neighbor_byTrue]
                        min_link = link_set[outlet][not_lake_neighbors][
                            min_neighbor_byTrue
                        ]
                        out_elev = min_val
                self._receivers[outlet] = min_neighbor
                self._receiverlinks[outlet] = min_link
                self._steepestslopes[outlet] = (
                    surface[outlet] - surface[min_neighbor]
                ) / self._neighbor_lengths[min_link]

            while True:
                try:
                    c = openq.pop_task()
                except KeyError:
                    break
                else:
                    closedq[c] = 2  # close it
                    # if raster, do the neighbors & diagonals separate...
                    for neighbor_set, link_set in zip(
                        self._neighbor_arrays, self._link_arrays
                    ):
                        for n, l in zip(neighbor_set[c, :], link_set[c, :]):
                            # fully closed
                            if (closedq[n] == 2) or (n == -1):
                                continue
                            elif self._grid.status_at_node[n] != NodeStatus.CORE:
                                closedq[n] = 2
                                continue
                            else:
                                if closedq[n] == 0:
                                    self._receivers[n] = c
                                    self._receiverlinks[n] = l
                                    self._steepestslopes[n] = 0.0
                                    closedq[n] = 2  # close it
                                    openq.add_task(n, priority=surface[n])
                                else:  # it's liminal (1); grads likely wrong
                                    # ...but it's not if set by the outlet...
                                    if c == outlet:
                                        # still need these nodes to be explored
                                        # by other lake nodes as needed, so
                                        # don't close either
                                        pass
                                    else:  # liminal to actual lake
                                        closedq[n] = 2
                                        liminal_nodes.append(n)
                                        # ...& don't add to the queue

            # TODO: obvious case for Cython accel here
            # Now know which nodes we need to reassess. So:
            for liminal in liminal_nodes:
                min_elev = LARGE_ELEV
                min_link = -1
                for neighbor_set, link_set in zip(
                    self._neighbor_arrays, self._link_arrays
                ):
                    neighbors = neighbor_set[liminal]
                    neighbors_valid = np.not_equal(neighbors, -1)
                    closednodes = np.equal(
                        self._grid.status_at_node[neighbors],
                        self._grid.BC_NODE_IS_CLOSED,
                    )  # closed BCs can't count
                    neighbors_valid[closednodes] = False
                    neighbors_to_check = neighbors[neighbors_valid]
                    if len(neighbors_to_check) == 0:
                        continue
                    else:
                        min_neighbor_now = np.amin(
                            self._fill_surface[neighbors_to_check]
                        )
                        if min_neighbor_now < min_elev:
                            min_elev = min_neighbor_now
                            links_available = link_set[liminal][neighbors_valid]
                            min_link_of_valid = np.argmin(
                                self._fill_surface[neighbors_to_check]
                            )
                            min_receiver = neighbors_to_check[min_link_of_valid]
                            min_link = links_available[min_link_of_valid]
                            max_grad = (
                                self._fill_surface[liminal] - min_elev
                            ) / self._neighbor_lengths[min_link]
                        else:
                            pass
                assert min_link != -1, neighbors_valid
                # ^link successfully found
                self._receivers[liminal] = min_receiver
                self._receiverlinks[liminal] = min_link
                self._steepestslopes[liminal] = max_grad

            # by the time we get here, we've removed all the pits! So...
            self._grid.at_node["flow__sink_flag"][lakenodes] = 0
            # reclose the lake:
            closedq[outlet] = 1
            closedq[lakenodes] = 1
            closedq[liminal_nodes] = 1

    def update(self):
        """Alias for running one step."""
        self.run_one_step()

    def run_one_step(self):
        """Fills the surface to fill all pits. Note that a second run on a
        surface that has already been filled will *not* "see" any existing.

        lakes correctly - it will see lakes, but with zero depths. In
        particular, if fill_flat is False, an attempt to fill a
        surface a second time will raise a ValueError unless ignore_overfill.
        (In this case, setting ignore_overfill is True will give the expected
        behaviour.)

        If reaccumulate_flow was True at instantiation, run_one_step also
        updates all the various flow fields produced by the FlowDirector and
        FlowAccumulator components.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import LakeMapperBarnes, FlowAccumulator
        >>> from landlab.components import FlowDirectorSteepest
        >>> mg = RasterModelGrid((5, 6), xy_spacing=2.0)
        >>> for edge in ("left", "top", "bottom"):
        ...     mg.status_at_node[mg.nodes_at_edge(edge)] = mg.BC_NODE_IS_CLOSED
        ...
        >>> mg.at_node["topographic__elevation"] = [
        ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ...     [0.0, 2.1, 1.1, 0.6, 1.6, 0.0],
        ...     [0.0, 2.0, 1.0, 0.5, 1.5, 0.0],
        ...     [0.0, 2.2, 1.2, 0.7, 1.7, 0.0],
        ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ... ]
        >>> z = mg.at_node["topographic__elevation"]
        >>> z_init = mg.at_node["topographic__elevation"].copy()
        >>> fa = FlowAccumulator(mg)
        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     surface=z_init,
        ...     fill_flat=False,
        ...     redirect_flow_steepest_descent=False,
        ...     track_lakes=False,
        ... )

        TODO: once return_array_at_node is fixed, this example should also
        take fill_surface...

        >>> lmb.run_one_step()
        >>> z_out = [
        ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ...     [0.0, 2.1, 1.5, 1.5, 1.6, 0.0],
        ...     [0.0, 2.0, 1.5, 1.5, 1.5, 0.0],
        ...     [0.0, 2.2, 1.5, 1.5, 1.7, 0.0],
        ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ... ]
        >>> np.allclose(z.reshape(mg.shape), z_out)
        True
        >>> try:
        ...     lmb.lake_map  # not created, as we aren't tracking
        ... except ValueError:
        ...     print(
        ...         "ValueError was raised:"
        ...         " Enable tracking to access information about lakes"
        ...     )
        ...
        ValueError was raised: Enable tracking to access information about lakes
        >>> lmb.was_there_overfill  # everything fine with slope adding
        False

        >>> fd = FlowDirectorSteepest(mg)
        >>> fa = FlowAccumulator(mg)  # routing will work fine now
        >>> fd.run_one_step()
        >>> fa.run_one_step()
        >>> np.all(mg.at_node["flow__sink_flag"][mg.core_nodes] == 0)
        True
        >>> drainage_area = [
        ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ...     [0.0, 4.0, 8.0, 12.0, 4.0, 4.0],
        ...     [0.0, 4.0, 16.0, 36.0, 40.0, 40.0],
        ...     [0.0, 4.0, 8.0, 4.0, 4.0, 4.0],
        ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ... ]
        >>> np.allclose(mg.at_node["drainage_area"].reshape(mg.shape), drainage_area)
        True

        Test two pits:

        >>> z[:] = mg.node_x.max() - mg.node_x
        >>> z[23] = 1.3
        >>> z[15] = 0.3
        >>> z[10] = 1.3  # raise "guard" exit nodes
        >>> z[7] = 2.0  # is a lake on its own, if D8
        >>> z[9] = 0.5
        >>> z[14] = 0.6  # [9, 14, 15] is a lake in both methods
        >>> z[16] = 1.2
        >>> z[22] = 0.9  # a non-contiguous lake node also draining to 16 if D8
        >>> z_init = z.copy()
        >>> fa = FlowAccumulator(mg)
        >>> lmb = LakeMapperBarnes(mg, method="D8", fill_flat=True, track_lakes=True)
        >>> lmb.run_one_step()  # note the D8 routing now
        >>> lmb.lake_dict == {22: deque([15, 9, 14])}
        True
        >>> lmb.number_of_lakes
        1
        >>> lmb.lake_depths  # z was both surface and "fill_surface"
        Traceback (most recent call last):
        ...
        ValueError: surface and fill_surface must be different fields or arrays
        to enable the property lake_depths!

        ValueError was raised because surface and fill_surface must be
        different fields or arrays to enable the property fill_depth!

        >>> z[:] = z_init
        >>> lmb = LakeMapperBarnes(
        ...     mg, method="Steepest", fill_flat=False, track_lakes=True
        ... )
        >>> lmb.run_one_step()  # compare to the method='D8' lakes, above...
        >>> lmb.lake_dict == {8: deque([7]), 16: deque([15, 9, 14, 22])}
        True
        >>> lmb.number_of_lakes
        2
        >>> np.allclose(lmb.lake_areas, np.array([16.0, 4.0]))
        True
        >>> lmb.run_one_step()
        Traceback (most recent call last):
        ...
        ValueError: Pit is overfilled due to creation of two outlets as
        the minimum gradient gets applied. Suppress this Error with the
        ignore_overfill flag at component instantiation.

        ValueError was raised because Pit is overfilled due to creation
        of two outlets as the minimum gradient gets applied. Suppress this
        Error with the ignore_overfill flag at component instantiation.

        Suppress this behavior with ignore_overfill:

        >>> z[:] = z_init
        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     fill_flat=False,
        ...     track_lakes=True,
        ...     ignore_overfill=True,
        ... )
        >>> lmb.run_one_step()
        >>> lmb.lake_dict == {8: deque([7]), 16: deque([15, 9, 14, 22])}
        True
        >>> lmb.run_one_step()
        >>> np.allclose(lmb.lake_areas, np.array([16.0, 4.0]))  # found them!
        True

        The component can redirect flow to account for the fills that have
        been carried out (all necessary fields get updated):

        >>> z[:] = z_init
        >>> fd.run_one_step()
        >>> init_flowdirs = mg.at_node["flow__receiver_node"].copy()
        >>> fa.run_one_step()
        >>> init_areas = mg.at_node["drainage_area"].copy()
        >>> init_qw = mg.at_node["surface_water__discharge"].copy()
        >>> fa = FlowAccumulator(mg)
        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     fill_flat=False,
        ...     track_lakes=True,
        ...     redirect_flow_steepest_descent=False,
        ...     ignore_overfill=True,
        ... )
        >>> lmb.run_one_step()
        >>> np.all(mg.at_node["flow__receiver_node"] == init_flowdirs)
        True

        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     fill_flat=False,
        ...     track_lakes=True,
        ...     redirect_flow_steepest_descent=True,
        ...     ignore_overfill=True,
        ... )
        >>> lmb.run_one_step()
        >>> np.all(mg.at_node["flow__receiver_node"] == init_flowdirs)
        False

        However, note that unless the reaccumulate_flow argument is also
        set, the 'drainage_area' and 'surface_water__discharge' fields
        *won't* also get updated:

        >>> np.all(mg.at_node["drainage_area"] == init_areas)
        True
        >>> np.all(mg.at_node["surface_water__discharge"] == init_qw)
        True

        >>> fa = FlowAccumulator(mg)
        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     fill_flat=False,
        ...     track_lakes=True,
        ...     redirect_flow_steepest_descent=True,
        ...     reaccumulate_flow=True,
        ...     ignore_overfill=True,
        ... )
        >>> lmb.run_one_step()
        >>> np.all(mg.at_node["drainage_area"] == init_areas)
        False
        >>> np.all(mg.at_node["surface_water__discharge"] == init_qw)
        False

        Be sure to set both redirect_flow_steepest_descent and
        reaccumulate_flow to True if you want to reaccumulate flow...

        >>> try:
        ...     lmb = LakeMapperBarnes(
        ...         mg,
        ...         method="Steepest",
        ...         fill_flat=False,
        ...         track_lakes=True,
        ...         redirect_flow_steepest_descent=False,
        ...         reaccumulate_flow=True,
        ...         ignore_overfill=True,
        ...     )
        ... except ValueError:
        ...     print("Oops!")
        ...
        Oops!

        The component is completely happy with irregular grids:

        >>> from landlab import HexModelGrid, FieldError
        >>> hmg = HexModelGrid((5, 4), spacing=2.0)
        >>> z_hex = hmg.add_zeros("topographic__elevation", at="node")
        >>> z_hex[:] = hmg.node_x
        >>> z_hex[11] = -3.0
        >>> z_hex[12] = -1.0
        >>> z_hex_init = z_hex.copy()
        >>> z_hex
        array([   2.,   4.,   6.,   8.,
               1.,   3.,   5.,   7.,   9.,
            0.,   2.,   -3.,  -1.,   8.,  10.,
               1.,   3.,   5.,   7.,   9.,
                  2.,   4.,  6.,   8.])

        As you can see, nodes 11 and 12 are now a pit. If they were to fill
        they would fill to the level of 2, the lowest downstream value.

        >>> fa = FlowAccumulator(hmg)
        >>> lmb = LakeMapperBarnes(
        ...     hmg, method="Steepest", fill_flat=True, track_lakes=False
        ... )
        >>> lmb.run_one_step()
        >>> np.allclose(z_hex[10:13], 2.0)
        True

        >>> hmg = HexModelGrid((5, 4), spacing=2.0)
        >>> z_hex = hmg.add_zeros("topographic__elevation", at="node")
        >>> z_hex[:] = z_hex_init
        >>> try:
        ...     lmb = LakeMapperBarnes(
        ...         hmg,
        ...         method="Steepest",
        ...         fill_flat=False,
        ...         surface=z_hex_init,
        ...         redirect_flow_steepest_descent=True,
        ...         track_lakes=True,
        ...     )
        ... except FieldError:
        ...     print("Oops!")  # flowdir field must already exist!
        ...
        Oops!
        >>> fd = FlowDirectorSteepest(hmg)
        >>> fa = FlowAccumulator(hmg)
        >>> lmb = LakeMapperBarnes(
        ...     hmg,
        ...     method="Steepest",
        ...     fill_flat=False,
        ...     surface=z_hex_init,
        ...     redirect_flow_steepest_descent=True,
        ...     track_lakes=True,
        ... )
        >>> fd.run_one_step()
        >>> lmb.run_one_step()
        >>> np.allclose(z_hex[10:13], 2.0)
        True
        >>> z_hex[11] > z_hex[10]
        True
        >>> z_hex[12] > z_hex[11]
        True
        >>> np.allclose(lmb.lake_depths[10:14], np.array([0.0, 5.0, 3.0, 0.0]))
        True
        >>> np.testing.assert_array_almost_equal(lmb.lake_volumes, 27.712, decimal=3)

        Together, all this means that we can now run a topographic growth
        model that permits flooding as it runs:

        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import LakeMapperBarnes, FlowAccumulator
        >>> from landlab.components import FlowDirectorSteepest
        >>> from landlab.components import FastscapeEroder
        >>> mg = RasterModelGrid((6, 8))
        >>> for edge in ("right", "top", "bottom"):
        ...     mg.status_at_node[mg.nodes_at_edge(edge)] = mg.BC_NODE_IS_CLOSED
        ...

        Because it is what we want the FastscapeEroder to see and work on,
        it's actually the water surface that needs to go in as
        'topographic__elevation'. We'll also need to keep track of the bed
        elevation though, since the LakeMapper will need it. We start them
        equal (i.e., topo starts dry).

        >>> z_water = mg.add_zeros("topographic__elevation", at="node", dtype=float)
        >>> z_water[:] = mg.node_x
        >>> z_water[11] = 1.5
        >>> z_water[19] = 0.5
        >>> z_water[34] = 1.1
        >>> z_bed = mg.add_zeros("bedrock__elevation", at="node", dtype=float)
        >>> z_bed[:] = z_water  # topo starts dry

        Let's just take a look:

        >>> np.all(
        ...     np.equal(
        ...         np.round(z_water, 2).reshape(mg.shape),
        ...         [
        ...             [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0],
        ...             [0.0, 1.0, 2.0, 1.5, 4.0, 5.0, 6.0, 7.0],
        ...             [0.0, 1.0, 2.0, 0.5, 4.0, 5.0, 6.0, 7.0],
        ...             [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0],
        ...             [0.0, 1.0, 1.1, 3.0, 4.0, 5.0, 6.0, 7.0],
        ...             [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0],
        ...         ],
        ...     )
        ... )
        True

        >>> fd = FlowDirectorSteepest(mg)
        >>> fa = FlowAccumulator(mg)
        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="D8",
        ...     fill_flat=True,
        ...     surface="bedrock__elevation",
        ...     fill_surface="topographic__elevation",
        ...     redirect_flow_steepest_descent=True,
        ...     reaccumulate_flow=True,
        ...     track_lakes=True,
        ... )
        >>> sp = FastscapeEroder(mg, K_sp=1.0, m_sp=0.0, n_sp=1.0)
        >>> fd.run_one_step()
        >>> fa.run_one_step()  # node 18 is draining into the pit...
        >>> np.isclose(mg.at_node["topographic__steepest_slope"][18], 1.5)
        True
        >>> np.allclose(
        ...     mg.at_node["drainage_area"].reshape(mg.shape),
        ...     [
        ...         [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ...         [2.0, 2.0, 1.0, 4.0, 3.0, 2.0, 1.0, 0.0],
        ...         [1.0, 1.0, 1.0, 13.0, 3.0, 2.0, 1.0, 0.0],
        ...         [2.0, 2.0, 1.0, 4.0, 3.0, 2.0, 1.0, 0.0],
        ...         [6.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.0],
        ...         [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ...     ],
        ... )
        True
        >>> lmb.run_one_step()  # now node 18 drains correctly, outward ->
        >>> np.isclose(mg.at_node["topographic__steepest_slope"][18], 1.0)
        True
        >>> np.allclose(
        ...     mg.at_node["drainage_area"].reshape(mg.shape),
        ...     [
        ...         [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ...         [13.0, 13.0, 12.0, 4.0, 3.0, 2.0, 1.0, 0.0],
        ...         [2.0, 2.0, 1.0, 7.0, 3.0, 2.0, 1.0, 0.0],
        ...         [2.0, 2.0, 1.0, 1.0, 3.0, 2.0, 1.0, 0.0],
        ...         [7.0, 7.0, 6.0, 4.0, 3.0, 2.0, 1.0, 0.0],
        ...         [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ...     ],
        ... )
        True
        >>> np.all(
        ...     np.equal(
        ...         np.round(z_water, 2).reshape(mg.shape),
        ...         [
        ...             [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0],
        ...             [0.0, 1.0, 2.0, 2.0, 4.0, 5.0, 6.0, 7.0],
        ...             [0.0, 1.0, 2.0, 2.0, 4.0, 5.0, 6.0, 7.0],
        ...             [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0],
        ...             [0.0, 1.0, 1.1, 3.0, 4.0, 5.0, 6.0, 7.0],
        ...             [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0],
        ...         ],
        ...     )
        ... )
        True

        >>> sp.run_one_step(0.05)  # note m=0 to illustrate effect of slopes
        >>> np.all(
        ...     np.equal(
        ...         np.round(z_water, 2).reshape(mg.shape),
        ...         [
        ...             [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0],
        ...             [0.0, 0.95, 1.95, 2.0, 3.9, 4.95, 5.95, 7.0],
        ...             [0.0, 0.95, 1.95, 2.0, 3.9, 4.95, 5.95, 7.0],
        ...             [0.0, 0.95, 1.95, 2.93, 3.93, 4.95, 5.95, 7.0],
        ...             [0.0, 0.95, 1.09, 2.91, 3.95, 4.95, 5.95, 7.0],
        ...             [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0],
        ...         ],
        ...     )
        ... )
        True

        If we want to keep this going honouring the depths of the lakes try
        this next in your loop:

        >>> z_bed[:] = np.minimum(z_water, z_bed)
        >>> np.all(
        ...     np.equal(
        ...         np.round(z_bed, 2).reshape(mg.shape),
        ...         [
        ...             [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0],
        ...             [0.0, 0.95, 1.95, 1.5, 3.9, 4.95, 5.95, 7.0],
        ...             [0.0, 0.95, 1.95, 0.5, 3.9, 4.95, 5.95, 7.0],
        ...             [0.0, 0.95, 1.95, 2.93, 3.93, 4.95, 5.95, 7.0],
        ...             [0.0, 0.95, 1.09, 2.91, 3.95, 4.95, 5.95, 7.0],
        ...             [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0],
        ...         ],
        ...     )
        ... )
        True
        >>> fd.run_one_step()
        >>> fa.run_one_step()
        >>> lmb.run_one_step()

        Lake node depths are now updated in lmb:

        >>> np.round([lmb.lake_depths[lake] for lake in lmb.lake_dict.values()], 2)
        array([[0.45, 1.45]])

        ...and the "topography" (i.e., water surface) at the flooded nodes
        has lowered itself as the lip of the outlet was eroded in the last
        step:

        >>> np.all(
        ...     np.equal(
        ...         np.round(z_water, 2).reshape(mg.shape),
        ...         [
        ...             [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0],
        ...             [0.0, 0.95, 1.95, 1.95, 3.9, 4.95, 5.95, 7.0],
        ...             [0.0, 0.95, 1.95, 1.95, 3.9, 4.95, 5.95, 7.0],
        ...             [0.0, 0.95, 1.95, 2.93, 3.93, 4.95, 5.95, 7.0],
        ...             [0.0, 0.95, 1.09, 2.91, 3.95, 4.95, 5.95, 7.0],
        ...             [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0],
        ...         ],
        ...     )
        ... )
        True

        >>> sp.run_one_step(0.05)

        ...and so on.

        Note that this approach, without passing `flooded_nodes` to the
        FastscapeEroder run method, is both more "Landlabbic" and also
        ensures the information about the lake and the water surface
        topography are all updated cleanly and correctly.
        """
        if "flow__receiver_node" in self._grid.at_node and self._grid.at_node[
            "flow__receiver_node"
        ].size != self._grid.size("node"):
            raise NotImplementedError(
                "A route-to-multiple flow director has been "
                "run on this grid. The landlab development team has not "
                "verified that LakeMapperBarnes is compatible with "
                "route-to-multiple methods. Please open a GitHub Issue "
                "to start this process."
            )
        # do the prep:
        # create the StasblePriorityQueue locaslly to permit garbage collection
        _open = StablePriorityQueue()
        # increment the run counter
        self._runcount = next(self._runcounter)
        # First get _fill_surface in order.
        self._fill_surface[:] = self._surface  # surfaces begin identical
        # note this is nice & efficent if _fill_surface is _surface
        # if we're doing a redirect, we're going to need to preserve this
        # initial topo, so let's do that:
        if not self._dontredirect:
            orig_topo = self._track_original_surface()
        # now, return _closed to its initial cond, w only the BC_NODE_IS_CLOSED
        # and grid draining nodes pre-closed:
        closedq = self._closed.copy()
        if self._track_lakes:
            for edgenode in self._edges:
                _open.add_task(edgenode, priority=self._surface[edgenode])
            closedq[self._edges] = True
            if self._fill_flat:
                self._lakemappings = self._fill_to_flat_with_tracking(
                    self._fill_surface,
                    self._allneighbors,
                    self._pit,
                    _open,
                    closedq,
                )
            else:
                self._lakemappings = self._fill_to_slant_with_optional_tracking(
                    self._fill_surface,
                    self._allneighbors,
                    self._pit,
                    _open,
                    closedq,
                    ignore_overfill=self._ignore_overfill,
                    track_lakes=True,
                )
            if not self._dontredirect:
                self._redirect_flowdirs(orig_topo, self._lakemappings, _open)
                if self._reaccumulate:
                    _, _ = self._fa.accumulate_flow(update_flow_director=False)

        else:  # not tracked
            # note we've already checked _dontredirect is True in setup,
            # so we don't need to worry about these cases.
            for edgenode in self._edges:
                _open.add_task(edgenode, priority=self._surface[edgenode])
            closedq[self._edges] = True
            while True:
                try:
                    self._fill_one_node(
                        self._fill_surface,
                        self._allneighbors,
                        self._pit,
                        _open,
                        closedq,
                        self._ignore_overfill,
                    )
                except KeyError:  # run out of nodes to fill...
                    break

    @property
    def lake_dict(self):
        """Return a dictionary where the keys are the outlet nodes of each
        lake, and the values are deques of nodes within each lake. Items are
        not returned in ID order. The outlet nodes are NOT part of the lake.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import LakeMapperBarnes, FlowAccumulator
        >>> mg = RasterModelGrid((5, 6))
        >>> for edge in ("left", "top", "bottom"):
        ...     mg.status_at_node[mg.nodes_at_edge(edge)] = mg.BC_NODE_IS_CLOSED
        ...
        >>> # z = mg.add_zeros("topographic__elevation", at="node", dtype=float)
        >>> # z.reshape(mg.shape)[2, 1:-1] = [2.0, 1.0, 0.5, 1.5]
        >>> # z.reshape(mg.shape)[1, 1:-1] = [2.1, 1.1, 0.6, 1.6]
        >>> # z.reshape(mg.shape)[3, 1:-1] = [2.2, 1.2, 0.7, 1.7]
        >>> mg.at_node["topographic__elevation"] = [
        ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ...     [0.0, 2.1, 1.1, 0.6, 1.6, 0.0],
        ...     [0.0, 2.0, 1.0, 0.5, 1.5, 0.0],
        ...     [0.0, 2.2, 1.2, 0.7, 1.7, 0.0],
        ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ... ]
        >>> z = mg.at_node["topographic__elevation"]
        >>> z_init = mg.at_node["topographic__elevation"].copy()
        >>> fa = FlowAccumulator(mg)
        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     surface=z_init,
        ...     fill_surface=z,
        ...     fill_flat=False,
        ...     redirect_flow_steepest_descent=False,
        ...     track_lakes=False,
        ... )
        >>> lmb.run_one_step()
        >>> try:
        ...     lmb.lake_dict
        ... except ValueError:
        ...     print(
        ...         "ValueError was raised: "
        ...         + "Enable tracking to access information about lakes"
        ...     )
        ...
        ValueError was raised: Enable tracking to access information about lakes

        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     surface=z_init,
        ...     fill_surface=z,
        ...     fill_flat=False,
        ...     redirect_flow_steepest_descent=False,
        ...     track_lakes=True,
        ... )
        >>> lmb.run_one_step()
        >>> lmb.lake_dict == {16: deque([15, 9, 8, 14, 20, 21])}
        True
        """
        if not self._track_lakes:
            raise ValueError("Enable tracking to access information about lakes")
        return self._lakemappings

    @property
    def lake_outlets(self):
        """Returns the outlet for each lake, not necessarily in ID order.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import LakeMapperBarnes, FlowAccumulator
        >>> mg = RasterModelGrid((5, 6))
        >>> for edge in ("left", "top", "bottom"):
        ...     mg.status_at_node[mg.nodes_at_edge(edge)] = mg.BC_NODE_IS_CLOSED
        ...
        >>> mg.at_node["topographic__elevation"] = [
        ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ...     [0.0, 2.1, 1.1, 0.6, 1.6, 0.0],
        ...     [0.0, 2.0, 1.0, 0.5, 1.5, 0.0],
        ...     [0.0, 2.2, 1.2, 0.7, 1.7, 0.0],
        ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ... ]
        >>> z = mg.at_node["topographic__elevation"]
        >>> z_init = mg.at_node["topographic__elevation"].copy()
        >>> fa = FlowAccumulator(mg)
        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     surface=z_init,
        ...     fill_surface=z,
        ...     fill_flat=False,
        ...     redirect_flow_steepest_descent=False,
        ...     track_lakes=False,
        ... )
        >>> lmb.run_one_step()
        >>> try:
        ...     lmb.lake_outlets
        ... except ValueError:
        ...     print(
        ...         "ValueError was raised:"
        ...         " Enable tracking to access information about lakes"
        ...     )
        ...
        ValueError was raised: Enable tracking to access information about lakes

        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     surface=z_init,
        ...     fill_surface=z,
        ...     fill_flat=False,
        ...     redirect_flow_steepest_descent=False,
        ...     track_lakes=True,
        ... )
        >>> lmb.run_one_step()
        >>> lmb.lake_outlets == [16]
        True
        """
        if not self._track_lakes:
            raise ValueError("Enable tracking to access information about lakes")
        return list(self._lakemappings.keys())

    @property
    def number_of_lakes(self):
        """Return the number of individual lakes. Lakes sharing outlet nodes
        are considered part of the same lake.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import LakeMapperBarnes, FlowAccumulator
        >>> mg = RasterModelGrid((5, 6))
        >>> for edge in ("left", "top", "bottom"):
        ...     mg.status_at_node[mg.nodes_at_edge(edge)] = mg.BC_NODE_IS_CLOSED
        ...
        >>> z = mg.add_zeros("topographic__elevation", at="node", dtype=float)
        >>> z[:] = mg.node_x.max() - mg.node_x
        >>> z[[10, 23]] = 1.1  # raise "guard" exit nodes
        >>> z[7] = 2.0  # is a lake on its own
        >>> z[9] = 0.5
        >>> z[15] = 0.3
        >>> z[14] = 0.6  # [9, 14, 15] is a lake
        >>> z[22] = 0.9  # a non-contiguous lake node also draining to 16
        >>> z_init = z.copy()
        >>> fa = FlowAccumulator(mg)
        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     surface=z_init,
        ...     fill_surface=z,
        ...     fill_flat=False,
        ...     redirect_flow_steepest_descent=False,
        ...     track_lakes=False,
        ... )
        >>> lmb.run_one_step()
        >>> try:
        ...     lmb.number_of_lakes
        ... except ValueError:
        ...     print(
        ...         "ValueError was raised:"
        ...         " Enable tracking to access information about lakes"
        ...     )
        ...
        ValueError was raised: Enable tracking to access information about lakes

        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     surface=z_init,
        ...     fill_surface=z,
        ...     fill_flat=False,
        ...     redirect_flow_steepest_descent=False,
        ...     track_lakes=True,
        ... )
        >>> lmb.run_one_step()
        >>> lmb.number_of_lakes
        2
        """
        if not self._track_lakes:
            raise ValueError("Enable tracking to access information about lakes")
        return len(self._lakemappings)

    @property
    def lake_map(self):
        """Return an array of ints, where each node within a lake is labelled
        with its outlet node ID. The outlet nodes are NOT part of the lakes.
        Nodes not in a lake are labelled with BAD_INDEX_VALUE (default
        -1).

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import LakeMapperBarnes, FlowAccumulator
        >>> mg = RasterModelGrid((5, 6))
        >>> for edge in ("left", "top", "bottom"):
        ...     mg.status_at_node[mg.nodes_at_edge(edge)] = mg.BC_NODE_IS_CLOSED
        ...
        >>> z = mg.add_zeros("topographic__elevation", at="node", dtype=float)
        >>> z[:] = mg.node_x.max() - mg.node_x
        >>> z[[10, 23]] = 1.1  # raise "guard" exit nodes
        >>> z[7] = 2.0  # is a lake on its own
        >>> z[9] = 0.5
        >>> z[15] = 0.3
        >>> z[14] = 0.6  # [9, 14, 15] is a lake
        >>> z[22] = 0.9  # a non-contiguous lake node also draining to 16
        >>> z_init = z.copy()
        >>> fa = FlowAccumulator(mg)
        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     fill_flat=False,
        ...     redirect_flow_steepest_descent=False,
        ...     track_lakes=False,
        ... )
        >>> lmb.run_one_step()
        >>> try:
        ...     lmb.lake_map
        ... except ValueError:
        ...     print(
        ...         "ValueError was raised:"
        ...         " Enable tracking to access information about lakes"
        ...     )
        ...
        ValueError was raised: Enable tracking to access information about lakes

        >>> z[:] = z_init
        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     fill_flat=False,
        ...     redirect_flow_steepest_descent=False,
        ...     track_lakes=True,
        ... )
        >>> lmb.run_one_step()
        >>> lake_map = [
        ...     [-1, -1, -1, -1, -1, -1],
        ...     [-1, 8, -1, 16, -1, -1],
        ...     [-1, -1, 16, 16, -1, -1],
        ...     [-1, -1, -1, -1, 16, -1],
        ...     [-1, -1, -1, -1, -1, -1],
        ... ]
        >>> np.all(lmb.lake_map.reshape(mg.shape) == lake_map)
        True

        Note that the outlet node is NOT part of the lake.

        Updating the elevations works fine:

        >>> z.reshape(mg.shape)[1:4, 1:-1] = [
        ...     [2.1, 1.1, 0.6, 1.6],
        ...     [2.0, 1.0, 0.5, 1.5],
        ...     [2.2, 1.2, 0.7, 1.7],
        ... ]

        >>> lmb.run_one_step()
        >>> new_lake_map = [
        ...     [-1, -1, -1, -1, -1, -1],
        ...     [-1, -1, 16, 16, -1, -1],
        ...     [-1, -1, 16, 16, -1, -1],
        ...     [-1, -1, 16, 16, -1, -1],
        ...     [-1, -1, -1, -1, -1, -1],
        ... ]
        >>> np.all(lmb.lake_map.reshape(mg.shape) == new_lake_map)
        True
        """
        if self._runcount > self._lastcountforlakemap:
            # things have changed since last call to lake_map
            self._lake_map = np.full(
                self._grid.number_of_nodes, self._grid.BAD_INDEX, dtype=int
            )
            for outlet, lakenodes in self.lake_dict.items():
                self._lake_map[lakenodes] = outlet
        else:
            pass  # old map is fine
        self._lastcountforlakemap = self._runcount
        return self._lake_map

    @property
    def lake_at_node(self):
        """Return a boolean array, True if the node is flooded, False
        otherwise. The outlet nodes are NOT part of the lakes.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import LakeMapperBarnes, FlowAccumulator
        >>> mg = RasterModelGrid((5, 6))
        >>> for edge in ("left", "top", "bottom"):
        ...     mg.status_at_node[mg.nodes_at_edge(edge)] = mg.BC_NODE_IS_CLOSED
        ...
        >>> z = mg.add_zeros("topographic__elevation", at="node", dtype=float)
        >>> z[:] = mg.node_x.max() - mg.node_x
        >>> z[[10, 23]] = 1.1  # raise "guard" exit nodes
        >>> z[7] = 2.0  # is a lake on its own
        >>> z[9] = 0.5
        >>> z[15] = 0.3
        >>> z[14] = 0.6  # [9, 14, 15] is a lake
        >>> z[22] = 0.9  # a non-contiguous lake node also draining to 16
        >>> z_init = z.copy()
        >>> fa = FlowAccumulator(mg)
        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     fill_flat=False,
        ...     redirect_flow_steepest_descent=False,
        ...     track_lakes=True,
        ... )
        >>> lmb.run_one_step()
        >>> lake_at_node = np.array(
        ...     [
        ...         [0, 0, 0, 0, 0, 0],
        ...         [0, 1, 0, 1, 0, 0],
        ...         [0, 0, 1, 1, 0, 0],
        ...         [0, 0, 0, 0, 1, 0],
        ...         [0, 0, 0, 0, 0, 0],
        ...     ],
        ...     dtype=bool,
        ... )
        >>> np.all(lmb.lake_at_node.reshape(mg.shape) == lake_at_node)
        True
        """
        return self.lake_map != self._grid.BAD_INDEX

    @property
    def lake_depths(self):
        """Return the change in surface elevation at each node this step.
        Requires that surface and fill_surface were not the same array at
        instantiation.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import LakeMapperBarnes, FlowAccumulator

        >>> mg = RasterModelGrid((5, 6))
        >>> for edge in ("left", "top", "bottom"):
        ...     mg.status_at_node[mg.nodes_at_edge(edge)] = mg.BC_NODE_IS_CLOSED
        ...
        >>> z = mg.add_zeros("topographic__elevation", at="node", dtype=float)
        >>> z[:] = mg.node_x.max() - mg.node_x
        >>> z[[10, 23]] = 1.1  # raise "guard" exit nodes
        >>> z[7] = 2.0  # is a lake on its own
        >>> z[9] = 0.5
        >>> z[15] = 0.3
        >>> z[14] = 0.6  # [9, 14, 15] is a lake
        >>> z[22] = 0.9  # a non-contiguous lake node also draining to 16
        >>> z_init = z.copy()
        >>> fa = FlowAccumulator(mg)
        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     fill_flat=False,
        ...     redirect_flow_steepest_descent=False,
        ...     track_lakes=True,
        ... )
        >>> lmb.run_one_step()

        This won't work as surface & fill_surface are both z

        >>> lmb.lake_depths
        Traceback (most recent call last):
        ...
        ValueError: surface and fill_surface must be different fields or arrays
        to enable the property lake_depths!

        `ValueError` was raised because surface and fill_surface must be
        different fields or arrays to enable the property lake_depths!

        >>> z[:] = z_init
        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     fill_flat=False,
        ...     surface=z_init,
        ...     redirect_flow_steepest_descent=False,
        ...     track_lakes=True,
        ... )
        >>> lmb.run_one_step()
        >>> lake_depths = [
        ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ...     [0.0, 1.0, 0.0, 0.5, 0.0, 0.0],
        ...     [0.0, 0.0, 0.4, 0.7, 0.0, 0.0],
        ...     [0.0, 0.0, 0.0, 0.0, 0.1, 0.0],
        ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ... ]
        >>> np.all(
        ...     np.equal(lmb.lake_depths.reshape(mg.shape), lake_depths)
        ... )  # slope applied, so...
        False
        >>> np.allclose(lmb.lake_depths.reshape(mg.shape), lake_depths)
        True
        """
        if self._inplace:
            raise ValueError(
                "surface and fill_surface must be different fields or "
                + "arrays to enable the property lake_depths!"
            )
        return self._fill_surface - self._surface

    @property
    def lake_areas(self):
        """A nlakes-long array of the area of each lake. The order is the same
        as that of the keys in lake_dict, and of lake_outlets. Note that outlet
        nodes are not parts of the lakes.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import LakeMapperBarnes, FlowAccumulator
        >>> mg = RasterModelGrid((5, 6))
        >>> for edge in ("left", "top", "bottom"):
        ...     mg.status_at_node[mg.nodes_at_edge(edge)] = mg.BC_NODE_IS_CLOSED
        ...
        >>> z = mg.add_zeros("topographic__elevation", at="node", dtype=float)
        >>> z[:] = mg.node_x.max() - mg.node_x
        >>> z[[10, 23]] = 1.1  # raise "guard" exit nodes
        >>> z[7] = 2.0  # is a lake on its own
        >>> z[9] = 0.5
        >>> z[15] = 0.3
        >>> z[14] = 0.6  # [9, 14, 15] is a lake
        >>> z[22] = 0.9  # a non-contiguous lake node also draining to 16
        >>> z_init = z.copy()
        >>> fa = FlowAccumulator(mg)
        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     fill_flat=False,
        ...     redirect_flow_steepest_descent=False,
        ...     track_lakes=False,
        ... )
        >>> lmb.run_one_step()
        >>> try:
        ...     lmb.lake_areas  # note track_lakes=False
        ... except ValueError:
        ...     print(
        ...         "ValueError was raised: "
        ...         + "Enable tracking to access information about lakes"
        ...     )
        ...
        ValueError was raised: Enable tracking to access information about lakes

        >>> z[:] = z_init
        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     fill_flat=False,
        ...     redirect_flow_steepest_descent=False,
        ...     track_lakes=True,
        ... )
        >>> lmb.run_one_step()
        >>> lmb.lake_outlets
        [16, 8]
        >>> np.allclose(lmb.lake_areas, np.array([4.0, 1.0]))
        True
        """
        lakeareas = np.empty(self.number_of_lakes, dtype=float)
        for i, lakenodes in enumerate(self.lake_dict.values()):
            lakeareas[i] = self._grid.cell_area_at_node[lakenodes].sum()
        return lakeareas

    @property
    def lake_volumes(self):
        """A nlakes-long array of the volume of each lake. The order is the
        same as that of the keys in lake_dict, and of lake_outlets. Note that
        this calculation is performed relative to the initial surface, so is
        only a true lake volume if the initial surface was the rock suface (not
        an earlier water level).

        Requires that surface and fill_surface were not the same array at
        instantiation.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import LakeMapperBarnes, FlowAccumulator
        >>> mg = RasterModelGrid((5, 6))
        >>> for edge in ("left", "top", "bottom"):
        ...     mg.status_at_node[mg.nodes_at_edge(edge)] = mg.BC_NODE_IS_CLOSED
        ...
        >>> z = mg.add_zeros("topographic__elevation", at="node", dtype=float)
        >>> z[:] = mg.node_x.max() - mg.node_x
        >>> z[[10, 23]] = 1.1  # raise "guard" exit nodes
        >>> z[7] = 2.0  # is a lake on its own
        >>> z[9] = 0.5
        >>> z[15] = 0.3
        >>> z[14] = 0.6  # [9, 14, 15] is a lake
        >>> z[22] = 0.9  # a non-contiguous lake node also draining to 16
        >>> z_init = z.copy()
        >>> fa = FlowAccumulator(mg)
        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     fill_flat=False,
        ...     redirect_flow_steepest_descent=False,
        ...     track_lakes=True,
        ... )
        >>> lmb.run_one_step()
        >>> lmb.lake_volumes  # won't work as surface & fill_surface are both z
        Traceback (most recent call last):
        ...
        ValueError: surface and fill_surface must be different fields or arrays
        to enable the property lake_depths!

        >>> z[:] = z_init
        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     fill_flat=False,
        ...     surface=z_init,
        ...     redirect_flow_steepest_descent=False,
        ...     track_lakes=True,
        ... )
        >>> lmb.run_one_step()
        >>> lmb.lake_outlets
        [16, 8]
        >>> np.allclose(lmb.lake_volumes, np.array([1.7, 1.0]))
        True
        """
        lake_vols = np.empty(self.number_of_lakes, dtype=float)
        col_vols = self._grid.cell_area_at_node * self.lake_depths
        for i, lakenodes in enumerate(self.lake_dict.values()):
            lake_vols[i] = col_vols[lakenodes].sum()
        return lake_vols

    @property
    def was_there_overfill(self):
        """If the ignore_overfill flag was set to True at instantiation, this
        property indicates if any depression in the grid has, at any point,
        been overfilled.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import LakeMapperBarnes, FlowAccumulator
        >>> mg = RasterModelGrid((3, 7))
        >>> for edge in ("top", "right", "bottom"):
        ...     mg.status_at_node[mg.nodes_at_edge(edge)] = mg.BC_NODE_IS_CLOSED
        ...
        >>> z = mg.add_zeros("topographic__elevation", at="node", dtype=float)
        >>> z.reshape(mg.shape)[1, 1:-1] = [1.0, 0.2, 0.1, 1.0000000000000004, 1.5]
        >>> z_init = z.copy()
        >>> fa = FlowAccumulator(mg)
        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     fill_flat=True,
        ...     redirect_flow_steepest_descent=False,
        ...     ignore_overfill=False,
        ...     track_lakes=True,
        ... )
        >>> lmb.run_one_step()
        >>> lmb.was_there_overfill
        Traceback (most recent call last):
        ...
        ValueError: was_there_overfill is only defined if filling to an inclined surface!

        `ValueError` was raised because was_there_overfill is only defined if
        filling to an inclined surface!

        >>> z_init = z.copy()
        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     fill_flat=False,
        ...     redirect_flow_steepest_descent=False,
        ...     ignore_overfill=False,
        ...     track_lakes=True,
        ... )
        >>> lmb.run_one_step()
        Traceback (most recent call last):
        ...
        ValueError: Pit is overfilled due to creation of two outlets as the
        minimum gradient gets applied. Suppress this Error with the ignore_overfill
        flag at component instantiation.

        `ValueError` was raised because Pit is overfilled due to creation of
        two outlets as the minimum gradient gets applied. Suppress this Error
        with the ignore_overfill flag at component instantiation.

        >>> z_init = z.copy()
        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     fill_flat=False,
        ...     redirect_flow_steepest_descent=False,
        ...     ignore_overfill=True,
        ...     track_lakes=True,
        ... )
        >>> lmb.run_one_step()
        >>> lmb.was_there_overfill
        True

        >>> z.reshape(mg.shape)[1, 1:-1] = [1.0, 0.2, 0.1, 1.0, 1.5]
        >>> lmb.run_one_step()
        >>> lmb.was_there_overfill  # still true as was in the previous example
        True

        >>> z.reshape(mg.shape)[1, 1:-1] = [1.0, 0.2, 0.1, 1.0, 1.5]
        >>> lmb = LakeMapperBarnes(
        ...     mg,
        ...     method="Steepest",
        ...     fill_flat=False,
        ...     redirect_flow_steepest_descent=False,
        ...     ignore_overfill=True,
        ...     track_lakes=True,
        ... )
        >>> lmb.run_one_step()
        >>> lmb.was_there_overfill  # Now reset
        False

        >>> lmb.run_one_step()  # 2nd run on same fill_surface creates overfill
        >>> lmb.was_there_overfill
        True

        Note however that in this last example, values have NOT changed.
        """
        if self._fill_flat is True:
            raise ValueError(
                "was_there_overfill is only defined if filling to an "
                + "inclined surface!"
            )
        return self._overfill_flag



================================================
File: landslides/__init__.py
================================================
from .landslide_probability import LandslideProbability

__all__ = ["LandslideProbability"]



================================================
File: landslides/landslide_probability.py
================================================
#!/usr/env/python
"""Landlab component that simulates landslide probability of failure as well as
mean relative wetness and probability of saturation.

Relative wetness and factor-of-safety are based on the infinite slope
stability model driven by topographic and soils inputs and recharge provided
by user as inputs to the component. For each node, component simulates mean
relative wetness as well as the probability of saturation based on Monte Carlo
simulation of relative wetness where the probability is the number of
iterations with relative wetness >= 1.0 divided by the number of iterations.
Probability of failure for each node is also simulated in the Monte Carlo
simulation as the number of iterations with factor-of-safety <= 1.0
divided by the number of iterations.

.. codeauthor:: R.Strauch, E.Istanbulluoglu, & S.S.Nudurupati

University of Washington

Ref 1: Strauch et. al. 2017, 'A hydro-climatological approach to predicting
regional landslide probability using Landlab, Earth Surface Dynamics, In prep.

Ref 2: 'The Landlab LandslideProbability Component User Manual' @
https://github.com/RondaStrauch/pub_strauch_etal_esurf/blob/master/LandslideComponentUsersManual.pdf

Created on Thu Aug 20, 2015
Last edit June 7, 2017
"""

import copy

import numpy as np
import scipy.constants
from scipy import interpolate
from statsmodels.distributions.empirical_distribution import ECDF

from landlab import Component


class LandslideProbability(Component):
    """Landslide probability component using the infinite slope stability
    model.

    Landlab component designed to calculate probability of failure at
    each grid node based on the infinite slope stability model
    stability index (Factor of Safety).

    The driving force for failure is provided by the user in the form of
    groundwater recharge; four options for providing recharge are supported.
    The model uses topographic and soil characteristics provided as input
    by the user.

    The main method of the LandslideProbability class is
    `calculate_landslide_probability()``, which calculates the mean soil
    relative wetness, probability of soil saturation, and probability of
    failure at each node based on a Monte Carlo simulation.

    **Usage:**

    Option 1 - Uniform recharge

    .. code-block:: python

        LandslideProbability(
            grid,
            number_of_iterations=250,
            groundwater__recharge_distribution="uniform",
            groundwater__recharge_min_value=5.0,
            groundwater__recharge_max_value=121.0,
        )

    Option 2 - Lognormal recharge

    .. code-block:: python

        LandslideProbability(
            grid,
            number_of_iterations=250,
            groundwater__recharge_distribution="lognormal",
            groundwater__recharge_mean=30.0,
            groundwater__recharge_standard_deviation=0.25,
        )

    Option 3 - Lognormal_spatial recharge

    .. code-block:: python

        LandslideProbability(
            grid,
            number_of_iterations=250,
            groundwater__recharge_distribution="lognormal_spatial",
            groundwater__recharge_mean=np.random.randint(20, 120, grid_size),
            groundwater__recharge_standard_deviation=np.random.rand(grid_size),
        )

    Option 4 - Data_driven_spatial recharge

    .. code-block:: python

        LandslideProbability(
            grid,
            number_of_iterations=250,
            groundwater__recharge_distribution="data_driven_spatial",
            groundwater__recharge_HSD_inputs=[HSD_dict, HSD_id_dict, fract_dict],
        )

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> from landlab.components.landslides import LandslideProbability
    >>> import numpy as np

    Create a grid on which to calculate landslide probability.

    >>> grid = RasterModelGrid((5, 4), xy_spacing=(0.2, 0.2))

    Check the number of core nodes.

    >>> grid.number_of_core_nodes
    6

    The grid will need some input data. To check the names of the fields
    that provide the input to this component, use the *input_var_names*
    class property.

    >>> sorted(LandslideProbability.input_var_names)
    ['soil__density',
     'soil__internal_friction_angle',
     'soil__maximum_total_cohesion',
     'soil__minimum_total_cohesion',
     'soil__mode_total_cohesion',
     'soil__saturated_hydraulic_conductivity',
     'soil__thickness',
     'soil__transmissivity',
     'topographic__slope',
     'topographic__specific_contributing_area']

    Check the units for the fields.

    >>> LandslideProbability.var_units("topographic__specific_contributing_area")
    'm'

    Create an input field.

    >>> grid.at_node["topographic__slope"] = np.random.rand(grid.number_of_nodes)

    If you are not sure about one of the input or output variables, you can
    get help for specific variables.

    >>> LandslideProbability.var_help("soil__transmissivity")
    name: soil__transmissivity
    description:
      mode rate of water transmitted through a unit width of saturated
      soil - either provided or calculated with Ksat and soil depth
    units: m2/day
    unit agnostic: False
    at: node
    intent: in

    Additional required fields for component.

    >>> scatter_dat = np.random.randint(1, 10, grid.number_of_nodes)
    >>> grid.at_node["topographic__specific_contributing_area"] = np.sort(
    ...     np.random.randint(30, 900, grid.number_of_nodes).astype(float)
    ... )
    >>> grid.at_node["soil__transmissivity"] = np.sort(
    ...     np.random.randint(5, 20, grid.number_of_nodes).astype(float), -1
    ... )
    >>> grid.at_node["soil__saturated_hydraulic_conductivity"] = np.sort(
    ...     np.random.randint(2, 10, grid.number_of_nodes).astype(float), -1
    ... )
    >>> grid.at_node["soil__mode_total_cohesion"] = np.sort(
    ...     np.random.randint(30, 900, grid.number_of_nodes).astype(float)
    ... )
    >>> grid.at_node["soil__minimum_total_cohesion"] = (
    ...     grid.at_node["soil__mode_total_cohesion"] - scatter_dat
    ... )
    >>> grid.at_node["soil__maximum_total_cohesion"] = (
    ...     grid.at_node["soil__mode_total_cohesion"] + scatter_dat
    ... )
    >>> grid.at_node["soil__internal_friction_angle"] = np.sort(
    ...     np.random.randint(26, 40, grid.number_of_nodes).astype(float)
    ... )
    >>> grid.at_node["soil__thickness"] = np.sort(
    ...     np.random.randint(1, 10, grid.number_of_nodes).astype(float)
    ... )
    >>> grid.at_node["soil__density"] = 2000.0 * np.ones(grid.number_of_nodes)

    Instantiate the 'LandslideProbability' component to work on this grid,
    and run it.

    >>> ls_prob = LandslideProbability(grid)
    >>> np.allclose(grid.at_node["landslide__probability_of_failure"], 0.0)
    True

    Run the *calculate_landslide_probability* method to update output
    variables with grid

    >>> ls_prob.calculate_landslide_probability()

    Check the output variable names.

    >>> sorted(ls_prob.output_var_names)
    ['landslide__probability_of_failure',
     'soil__mean_relative_wetness',
     'soil__probability_of_saturation']

    Check the output from the component, including array at one node.

    >>> np.allclose(grid.at_node["landslide__probability_of_failure"], 0.0)
    False
    >>> core_nodes = ls_prob.grid.core_nodes

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Strauch, R., Istanbulluoglu, E., Nudurupati, S., Bandaragoda, C.,
    Gasparini, N., Tucker, G. (2018). A hydroclimatological approach to
    predicting regional landslide probability using Landlab Earth Surface
    Dynamics  6(1), 49-75. https://dx.doi.org/10.5194/esurf-6-49-2018

    **Additional References**

    None Listed

    """

    # component name
    _name = "Landslide Probability"

    _unit_agnostic = False

    __version__ = "1.0"

    _cite_as = """
    @article{strauch2018hydroclimatological,
      author = {Strauch, Ronda and Istanbulluoglu, Erkan and Nudurupati,
      Sai Siddhartha and Bandaragoda, Christina and Gasparini, Nicole M and
      Tucker, Gregory E},
      title = {{A hydroclimatological approach to predicting regional landslide
      probability using Landlab}},
      issn = {2196-6311},
      doi = {10.5194/esurf-6-49-2018},
      pages = {49--75},
      number = {1},
      volume = {6},
      journal = {Earth Surface Dynamics},
      year = {2018}
    }
    """
    _info = {
        "landslide__probability_of_failure": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "None",
            "mapping": "node",
            "doc": "number of times FS is <=1 out of number of iterations user selected",
        },
        "soil__density": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "kg/m3",
            "mapping": "node",
            "doc": "wet bulk density of soil",
        },
        "soil__internal_friction_angle": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "degrees",
            "mapping": "node",
            "doc": "critical angle just before failure due to friction between particles",
        },
        "soil__maximum_total_cohesion": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "Pa or kg/m-s2",
            "mapping": "node",
            "doc": "maximum of combined root and soil cohesion at node",
        },
        "soil__mean_relative_wetness": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "None",
            "mapping": "node",
            "doc": (
                "Indicator of soil wetness; relative depth perched water table "
                "within the soil layer"
            ),
        },
        "soil__minimum_total_cohesion": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "Pa or kg/m-s2",
            "mapping": "node",
            "doc": "minimum of combined root and soil cohesion at node",
        },
        "soil__mode_total_cohesion": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "Pa or kg/m-s2",
            "mapping": "node",
            "doc": "mode of combined root and soil cohesion at node",
        },
        "soil__probability_of_saturation": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "None",
            "mapping": "node",
            "doc": (
                "number of times relative wetness is >=1 out of number of "
                "iterations user selected"
            ),
        },
        "soil__saturated_hydraulic_conductivity": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m/day",
            "mapping": "node",
            "doc": (
                "mode rate of water transmitted through soil - provided if "
                "transmissivity is NOT provided to calculate tranmissivity "
                "with soil depth"
            ),
        },
        "soil__thickness": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "soil depth to restrictive layer",
        },
        "soil__transmissivity": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m2/day",
            "mapping": "node",
            "doc": (
                "mode rate of water transmitted through a unit width of saturated "
                "soil - either provided or calculated with Ksat and soil depth"
            ),
        },
        "topographic__slope": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "tan theta",
            "mapping": "node",
            "doc": "gradient of the ground surface",
        },
        "topographic__specific_contributing_area": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "specific contributing (upslope area/cell face ) that drains to node",
        },
    }

    def __init__(
        self,
        grid,
        number_of_iterations=250,
        g=scipy.constants.g,
        groundwater__recharge_distribution="uniform",
        groundwater__recharge_min_value=20.0,
        groundwater__recharge_max_value=120.0,
        groundwater__recharge_mean=None,
        groundwater__recharge_standard_deviation=None,
        groundwater__recharge_HSD_inputs=(),
        seed=0,
    ):
        """
        Parameters
        ----------
        grid: RasterModelGrid
            A raster grid.
        number_of_iterations: int, optional
            Number of iterations to run Monte Carlo simulation (default=250).
        groundwater__recharge_distribution: str, optional
            single word indicating recharge distribution, either 'uniform',
            'lognormal', 'lognormal_spatial,' or 'data_driven_spatial'.
            (default='uniform')
        groundwater__recharge_min_value: float, optional (mm/d)
            minium groundwater recharge for 'uniform' (default=20.)
        groundwater__recharge_max_value: float, optional (mm/d)
            maximum groundwater recharge for 'uniform' (default=120.)
        groundwater__recharge_mean: float, optional (mm/d)
            mean grounwater recharge for 'lognormal'
            and 'lognormal_spatial' (default=None)
        groundwater__recharge_standard_deviation: float, optional (mm/d)
            standard deviation of grounwater recharge for 'lognormal'
            and 'lognormal_spatial' (default=None)
        groundwater__recharge_HSD_inputs: list, optional
            list of 3 dictionaries in order (default=[]) - HSD_dict
            {Hydrologic Source Domain (HSD) keys: recharge numpy array values},
            {node IDs keys: list of HSD_Id values}, HSD_fractions {node IDS
            keys: list of HSD fractions values} (none)
            Note: this input method is a very specific one, and to use this method,
            one has to refer Ref 1 & Ref 2 mentioned above, as this set of
            inputs require rigorous pre-processing of data.
        g: float, optional (m/sec^2)
            acceleration due to gravity.
        seed: int, optional
            seed for random number generation. if seed is assigned any value
            other than the default value of zero, it will create different
            sequence. To create a certain sequence repititively, use the same
            value as input for seed.
        """
        # Initialize seeded random number generation
        self._seed_generator(seed)

        super().__init__(grid)

        # Store parameters and do unit conversions
        self._n = int(number_of_iterations)
        self._g = g
        self._groundwater__recharge_distribution = groundwater__recharge_distribution
        # Following code will deal with the input distribution and associated
        # parameters
        # Uniform distribution
        if self._groundwater__recharge_distribution == "uniform":
            self._recharge_min = groundwater__recharge_min_value
            self._recharge_max = groundwater__recharge_max_value
            self._Re = np.random.uniform(
                self._recharge_min, self._recharge_max, size=self._n
            )
            self._Re /= 1000.0  # Convert mm to m
        # Lognormal Distribution - Uniform in space
        elif self._groundwater__recharge_distribution == "lognormal":
            assert (
                groundwater__recharge_mean is not None
            ), "Input mean of the distribution!"
            assert (
                groundwater__recharge_standard_deviation is not None
            ), "Input standard deviation of the distribution!"
            self._recharge_mean = groundwater__recharge_mean
            self._recharge_stdev = groundwater__recharge_standard_deviation
            self._mu_lognormal = np.log(
                (self._recharge_mean**2)
                / np.sqrt(self._recharge_stdev**2 + self._recharge_mean**2)
            )
            self._sigma_lognormal = np.sqrt(
                np.log((self._recharge_stdev**2) / (self._recharge_mean**2) + 1)
            )
            self._Re = np.random.lognormal(
                self._mu_lognormal, self._sigma_lognormal, self._n
            )
            self._Re /= 1000.0  # Convert mm to m
        # Lognormal Distribution - Variable in space
        elif self._groundwater__recharge_distribution == "lognormal_spatial":
            assert groundwater__recharge_mean.shape[0] == (
                self._grid.number_of_nodes
            ), "Input array should be of the length of grid.number_of_nodes!"
            assert groundwater__recharge_standard_deviation.shape[0] == (
                self._grid.number_of_nodes
            ), "Input array should be of the length of grid.number_of_nodes!"
            self._recharge_mean = groundwater__recharge_mean
            self._recharge_stdev = groundwater__recharge_standard_deviation
        # Custom HSD inputs - Hydrologic Source Domain -> Model Domain
        elif self._groundwater__recharge_distribution == "data_driven_spatial":
            self._HSD_dict = groundwater__recharge_HSD_inputs[0]
            self._HSD_id_dict = groundwater__recharge_HSD_inputs[1]
            self._fract_dict = groundwater__recharge_HSD_inputs[2]
            self._interpolate_HSD_dict()

        # Check if all output fields are initialized
        self.initialize_output_fields()

        # Create a switch to imply whether Ksat is provided.
        if np.all(self._grid.at_node["soil__saturated_hydraulic_conductivity"] == 0):
            self._Ksat_provided = 0  # False
        else:
            self._Ksat_provided = 1  # True

        self._nodal_values = self._grid.at_node

    def calculate_factor_of_safety(self, i):
        """Method to calculate factor of safety.

        Method calculates factor-of-safety stability index by using
        node specific parameters, creating distributions of these parameters,
        and calculating the index by sampling these distributions 'n' times.

        The index is calculated from the 'infinite slope stabilty
        factor-of-safety equation' in the format of Pack RT, Tarboton DG,
        and Goodwin CN (1998),The SINMAP approach to terrain stability mapping.

        Parameters
        ----------
        i: int
            index of core node ID.
        """
        # generate distributions to sample from to provide input parameters
        # currently triangle distribution using mode, min, & max
        self._a = np.float32(
            self._grid.at_node["topographic__specific_contributing_area"][i]
        )
        self._theta = np.float32(self._grid.at_node["topographic__slope"][i])
        self._Tmode = np.float32(self._grid.at_node["soil__transmissivity"][i])
        self._Ksatmode = np.float32(
            self._grid.at_node["soil__saturated_hydraulic_conductivity"][i]
        )
        self._Cmode = np.float32(self._grid.at_node["soil__mode_total_cohesion"][i])
        self._Cmin = np.float32(self._grid.at_node["soil__minimum_total_cohesion"][i])
        self._Cmax = np.float32(self._grid.at_node["soil__maximum_total_cohesion"][i])
        self._phi_mode = np.float32(
            self._grid.at_node["soil__internal_friction_angle"][i]
        )
        self._rho = np.float32(self._grid.at_node["soil__density"][i])
        self._hs_mode = np.float32(self._grid.at_node["soil__thickness"][i])

        # recharge distribution based on distribution type
        if self._groundwater__recharge_distribution == "data_driven_spatial":
            self._calculate_HSD_recharge(i)
            self._Re /= 1000.0  # mm->m
        elif self._groundwater__recharge_distribution == "lognormal_spatial":
            mu_lognormal = np.log(
                (self._recharge_mean[i] ** 2)
                / np.sqrt(self._recharge_stdev[i] ** 2 + self._recharge_mean[i] ** 2)
            )
            sigma_lognormal = np.sqrt(
                np.log(
                    (self._recharge_stdev[i] ** 2) / (self._recharge_mean[i] ** 2) + 1
                )
            )
            self._Re = np.random.lognormal(mu_lognormal, sigma_lognormal, self._n)
            self._Re /= 1000.0  # Convert mm to m

        # Cohesion
        # if don't provide fields of min and max C, uncomment 2 lines below
        #    Cmin = self._Cmode-0.3*self._Cmode
        #    Cmax = self._Cmode+0.3*self._Cmode
        self._C = np.random.triangular(
            self._Cmin, self._Cmode, self._Cmax, size=self._n
        )

        # phi - internal angle of friction provided in degrees
        phi_min = self._phi_mode - 0.18 * self._phi_mode
        phi_max = self._phi_mode + 0.32 * self._phi_mode
        self._phi = np.random.triangular(phi_min, self._phi_mode, phi_max, size=self._n)
        # soil thickness
        # hs_min = min(0.005, self._hs_mode-0.3*self._hs_mode) # Alternative
        hs_min = self._hs_mode - 0.3 * self._hs_mode
        hs_max = self._hs_mode + 0.1 * self._hs_mode
        self._hs = np.random.triangular(hs_min, self._hs_mode, hs_max, size=self._n)
        self._hs[self._hs <= 0.0] = 0.005
        if self._Ksat_provided:
            # Hydraulic conductivity (Ksat)
            Ksatmin = self._Ksatmode - (0.3 * self._Ksatmode)
            Ksatmax = self._Ksatmode + (0.1 * self._Ksatmode)
            self._Ksat = np.random.triangular(
                Ksatmin, self._Ksatmode, Ksatmax, size=self._n
            )
            self._T = self._Ksat * self._hs
        else:
            # Transmissivity (T)
            Tmin = self._Tmode - (0.3 * self._Tmode)
            Tmax = self._Tmode + (0.1 * self._Tmode)
            self._T = np.random.triangular(Tmin, self._Tmode, Tmax, size=self._n)

        # calculate Factor of Safety for n number of times
        # calculate components of FS equation
        self._C_dim = self._C / (
            self._hs * self._rho * self._g
        )  # dimensionless cohesion
        self._rel_wetness = ((self._Re) / self._T) * (
            self._a / np.sin(np.arctan(self._theta))
        )  # relative wetness
        # calculate probability of saturation
        countr = 0
        for val in self._rel_wetness:  # find how many RW values >= 1
            if val >= 1.0:
                countr = countr + 1  # number with RW values (>=1)
        # probability: No. high RW values/total No. of values (n)
        self._soil__probability_of_saturation = np.float32(countr) / self._n
        # Maximum Rel_wetness = 1.0
        np.place(self._rel_wetness, self._rel_wetness > 1, 1.0)
        self._soil__mean_relative_wetness = np.mean(self._rel_wetness)
        Y = np.tan(np.radians(self._phi)) * (1 - (self._rel_wetness * 0.5))
        # convert from degrees; 0.5 = water to soil density ratio
        # calculate Factor-of-safety
        self._FS = (self._C_dim / np.sin(np.arctan(self._theta))) + (
            np.cos(np.arctan(self._theta)) * (Y / np.sin(np.arctan(self._theta)))
        )
        count = 0
        for val in self._FS:  # find how many FS values <= 1
            if val <= 1.0:
                count = count + 1  # number with unstable FS values (<=1)
        # probability: No. unstable values/total No. of values (n)
        self._landslide__probability_of_failure = np.float32(count) / self._n

    def calculate_landslide_probability(self):
        """Main method of Landslide Probability class.

        Method creates arrays for output variables then loops through
        all the core nodes to run the method
        'calculate_factor_of_safety.' Output parameters probability of
        failure, mean relative wetness, and probability of saturation
        are assigned as fields to nodes.
        """
        # Create arrays for data with -9999 as default to store output
        self._mean_Relative_Wetness = np.full(self._grid.number_of_nodes, -9999.0)
        self._prob_fail = np.full(self._grid.number_of_nodes, -9999.0)
        self._prob_sat = np.full(self._grid.number_of_nodes, -9999.0)
        # Run factor of safety Monte Carlo for all core nodes in domain
        # i refers to each core node id
        for i in self._grid.core_nodes:
            self.calculate_factor_of_safety(i)
            # Populate storage arrays with calculated values
            self._mean_Relative_Wetness[i] = self._soil__mean_relative_wetness
            self._prob_fail[i] = self._landslide__probability_of_failure
            self._prob_sat[i] = self._soil__probability_of_saturation
        # Values can't be negative
        self._mean_Relative_Wetness[self._mean_Relative_Wetness < 0.0] = 0.0
        self._prob_fail[self._prob_fail < 0.0] = 0.0
        # assign output fields to nodes
        self._grid.at_node["soil__mean_relative_wetness"] = self._mean_Relative_Wetness
        self._grid.at_node["landslide__probability_of_failure"] = self._prob_fail
        self._grid.at_node["soil__probability_of_saturation"] = self._prob_sat

    def _seed_generator(self, seed=0):
        """Method to initiate random seed.

        Seed the random-number generator. This method will create the
        same sequence again by re-seeding with the same value (default
        value is zero). To create a sequence other than the default,
        assign non-zero value for seed.
        """
        np.random.seed(seed)

    def _interpolate_HSD_dict(self):
        """Method to extrapolate input data.

        This method uses a non-parametric approach to expand the input
        recharge array to the length of number of iterations. Output is
        a new dictionary of interpolated recharge for each HSD id.
        """
        HSD_dict = copy.deepcopy(self._HSD_dict)
        # First generate interpolated Re for each HSD grid
        Yrand = np.sort(np.random.rand(self._n))
        # n random numbers (0 to 1) in a column
        for vkey in HSD_dict.keys():
            if isinstance(HSD_dict[vkey], int):
                continue  # loop back up if value is integer (e.g. -9999)
            Re_temp = HSD_dict[vkey]  # an array of annual Re for 1 HSD grid
            Fx = ECDF(Re_temp)  # instantiate to get probabilities with Re
            Fx_ = Fx(Re_temp)  # probability array associated with Re data
            # interpolate function based on recharge data & probability
            f = interpolate.interp1d(
                Fx_, Re_temp, bounds_error=False, fill_value=min(Re_temp)
            )
            # array of Re interpolated from Yrand probabilities (n count)
            Re_interpolated = f(Yrand)
            # replace values in HSD_dict with interpolated Re
            HSD_dict[vkey] = Re_interpolated

        self._interpolated_HSD_dict = HSD_dict

    def _calculate_HSD_recharge(self, i):
        """Method to calculate recharge based on upstream fractions.

        This method calculates the resultant recharge at node i of the
        model domain, using recharge of contributing HSD ids and the
        areal fractions of upstream contributing HSD ids. Output is a
        numpy array of recharge at node i.
        """
        store_Re = np.zeros(self._n)
        HSD_id_list = self._HSD_id_dict[i]
        fract_list = self._fract_dict[i]
        for j in range(0, len(HSD_id_list)):
            Re_temp = self._interpolated_HSD_dict[HSD_id_list[j]]
            fract_temp = fract_list[j]
            Re_adj = Re_temp * fract_temp
            store_Re = np.vstack((store_Re, np.array(Re_adj)))
        self._Re = np.sum(store_Re, 0)



================================================
File: lateral_erosion/__init__.py
================================================
from .lateral_erosion import LateralEroder

__all__ = ["LateralEroder"]



================================================
File: lateral_erosion/lateral_erosion.py
================================================
"""Grid-based simulation of lateral erosion by channels in a drainage network.

ALangston
"""

import numpy as np

from landlab import Component
from landlab import RasterModelGrid
from landlab.components.flow_accum import FlowAccumulator

from .node_finder import node_finder

# Hard coded constants
cfl_cond = 0.3  # CFL timestep condition
wid_coeff = 0.4  # coefficient for calculating channel width
wid_exp = 0.35  # exponent for calculating channel width


class LateralEroder(Component):
    """Laterally erode neighbor node through fluvial erosion.

    Landlab component that finds a neighbor node to laterally erode and
    calculates lateral erosion.
    See the publication:

    Langston, A.L., Tucker, G.T.: Developing and exploring a theory for the
    lateral erosion of bedrock channels for use in landscape evolution models.
    Earth Surface Dynamics, 6, 1-27,
    `https://doi.org/10.5194/esurf-6-1-2018 <https://www.earth-surf-dynam.net/6/1/2018/>`_

    Examples
    --------
    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import FlowAccumulator, LateralEroder
    >>> np.random.seed(2010)

    Define grid and initial topography

    * 5x4 grid with baselevel in the lower left corner
    * All other boundary nodes closed
    * Initial topography is plane tilted up to the upper right with noise

    >>> mg = RasterModelGrid((5, 4), xy_spacing=10.0)
    >>> mg.set_status_at_node_on_edges(
    ...     right=mg.BC_NODE_IS_CLOSED,
    ...     top=mg.BC_NODE_IS_CLOSED,
    ...     left=mg.BC_NODE_IS_CLOSED,
    ...     bottom=mg.BC_NODE_IS_CLOSED,
    ... )
    >>> mg.status_at_node[1] = mg.BC_NODE_IS_FIXED_VALUE
    >>> rand_noise = np.array(
    ...     [
    ...         [0.00436992, 0.03225985, 0.03107455, 0.00461312],
    ...         [0.03771756, 0.02491226, 0.09613959, 0.07792969],
    ...         [0.08707156, 0.03080568, 0.01242658, 0.08827382],
    ...         [0.04475065, 0.07391732, 0.08221057, 0.02909259],
    ...         [0.03499337, 0.09423741, 0.01883171, 0.09967794],
    ...     ]
    ... ).flatten()
    >>> mg.at_node["topographic__elevation"] = (
    ...     mg.node_y / 10.0 + mg.node_x / 10.0 + rand_noise
    ... )
    >>> U = 0.001
    >>> dt = 100

    Instantiate flow accumulation and lateral eroder and run each for one step

    >>> fa = FlowAccumulator(
    ...     mg,
    ...     surface="topographic__elevation",
    ...     flow_director="FlowDirectorD8",
    ...     runoff_rate=None,
    ...     depression_finder=None,
    ... )
    >>> latero = LateralEroder(mg, latero_mech="UC", Kv=0.001, Kl_ratio=1.5)

    Run one step of flow accumulation and lateral erosion to get the dzlat array
    needed for the next part of the test.

    >>> fa.run_one_step()
    >>> mg, dzlat = latero.run_one_step(dt)

    Evolve the landscape until the first occurence of lateral erosion. Save arrays
    volume of lateral erosion and topographic elevation before and after the first
    occurence of lateral erosion

    >>> while min(dzlat) == 0.0:
    ...     oldlatvol = mg.at_node["volume__lateral_erosion"].copy()
    ...     oldelev = mg.at_node["topographic__elevation"].copy()
    ...     fa.run_one_step()
    ...     mg, dzlat = latero.run_one_step(dt)
    ...     newlatvol = mg.at_node["volume__lateral_erosion"]
    ...     newelev = mg.at_node["topographic__elevation"]
    ...     mg.at_node["topographic__elevation"][mg.core_nodes] += U * dt
    ...

    Before lateral erosion occurs, *volume__lateral_erosion* has values at
    nodes 6 and 10.

    >>> np.around(oldlatvol, decimals=0)
    array([ 0.,  0., 0., 0.,
            0.,  0., 79., 0.,
            0.,  0., 24., 0.,
            0.,  0., 0., 0.,
            0.,  0., 0., 0.])


    After lateral erosion occurs at node 6, *volume__lateral_erosion* is reset to 0

    >>> np.around(newlatvol, decimals=0)
    array([ 0.,  0.,  0.,  0.,
            0.,  0.,  0.,  0.,
            0.,  0., 24.,  0.,
            0.,  0.,  0.,  0.,
            0.,  0.,  0.,  0.])


    After lateral erosion at node 6, elevation at node 6 is reduced by -1.41
    (the elevation change stored in dzlat[6]). It is also provided as the
    at-node grid field *lateral_erosion__depth_increment*.

    >>> np.around(oldelev, decimals=2)
    array([0.  , 1.03, 2.03, 3.  ,
           1.04, 1.77, 2.45, 4.08,
           2.09, 2.65, 3.18, 5.09,
           3.04, 3.65, 4.07, 6.03,
           4.03, 5.09, 6.02, 7.1 ])

    >>> np.around(newelev, decimals=2)
    array([0.  , 1.03, 2.03, 3.  ,
           1.04, 1.77, 1.03, 4.08,
           2.09, 2.65, 3.18, 5.09,
           3.04, 3.65, 4.07, 6.03,
           4.03, 5.09, 6.02, 7.1 ])

    >>> np.around(dzlat, decimals=2)
    array([ 0.  ,  0.  ,  0.  ,  0.  ,
            0.  ,  0.  , -1.41,  0.  ,
            0.  ,  0.  ,  0.  ,  0.  ,
            0.  ,  0.  ,  0.  ,  0.  ,
            0.  ,  0.  ,  0.  ,  0. ])

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Langston, A., Tucker, G. (2018). Developing and exploring a theory for the
    lateral erosion of bedrock channels for use in landscape evolution models.
    Earth Surface Dynamics  6(1), 1--27.
    https://dx.doi.org/10.5194/esurf-6-1-2018

    **Additional References**

    None Listed

    """

    _name = "LateralEroder"

    _unit_agnostic = False

    _cite_as = """
    @article{langston2018developing,
      author = {Langston, A. L. and Tucker, G. E.},
      title = {{Developing and exploring a theory for the lateral erosion of
      bedrock channels for use in landscape evolution models}},
      doi = {10.5194/esurf-6-1-2018},
      pages = {1---27},
      number = {1},
      volume = {6},
      journal = {Earth Surface Dynamics},
      year = {2018}
    }
    """
    _info = {
        "drainage_area": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m**2",
            "mapping": "node",
            "doc": "Upstream accumulated surface area contributing to the node's discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__upstream_node_order": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array containing downstream-to-upstream ordered list of node IDs",
        },
        "lateral_erosion__depth_increment": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Change in elevation at each node from lateral erosion during time step",
        },
        "sediment__influx": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m3/y",
            "mapping": "node",
            "doc": "Sediment flux (volume per unit time of sediment entering each node)",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__steepest_slope": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "The steepest *downhill* slope",
        },
        "volume__lateral_erosion": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m3",
            "mapping": "node",
            "doc": "Array tracking volume eroded at each node from lateral erosion",
        },
    }

    def __init__(
        self,
        grid,
        latero_mech="UC",
        alph=0.8,
        Kv=0.001,
        Kl_ratio=1.0,
        solver="basic",
        inlet_on=False,
        inlet_node=None,
        inlet_area=None,
        qsinlet=0.0,
        flow_accumulator=None,
    ):
        """
        Parameters
        ----------
        grid : ModelGrid
            A Landlab square cell raster grid object
        latero_mech : string, optional (defaults to UC)
            Lateral erosion algorithm, choices are "UC" for undercutting-slump
            model and "TB" for total block erosion
        alph : float, optional (defaults to 0.8)
            Parameter describing potential for deposition, dimensionless
        Kv : float, node array, or field name
            Bedrock erodibility in vertical direction, 1/years
        Kl_ratio : float, optional (defaults to 1.0)
            Ratio of lateral to vertical bedrock erodibility, dimensionless
        solver : string
            Solver options:
                (1) 'basic' (default): explicit forward-time extrapolation.
                    Simple but will become unstable if time step is too large or
                    if bedrock erodibility is vry high.
                (2) 'adaptive': subdivides global time step as needed to
                    prevent slopes from reversing.
        inlet_node : integer, optional
            Node location of inlet (source of water and sediment)
        inlet_area : float, optional
            Drainage area at inlet node, must be specified if inlet node is "on", m^2
        qsinlet : float, optional
            Sediment flux supplied at inlet, optional. m3/year
        flow_accumulator : Instantiated Landlab FlowAccumulator, optional
            When solver is set to "adaptive", then a valid Landlab FlowAccumulator
            must be passed. It will be run within sub-timesteps in order to update
            the flow directions and drainage area.
        """
        super().__init__(grid)

        assert isinstance(
            grid, RasterModelGrid
        ), "LateralEroder requires a sqare raster grid."

        if "flow__receiver_node" in grid.at_node and grid.at_node[
            "flow__receiver_node"
        ].size != grid.size("node"):
            raise NotImplementedError(
                "A route-to-multiple flow director has been "
                "run on this grid. The LateralEroder is not currently "
                "compatible with route-to-multiple methods. Use a route-to-"
                "one flow director."
            )

        if solver not in ("basic", "adaptive"):
            raise ValueError(
                "value for solver not understood ({val} not one of {valid})".format(
                    val=solver, valid=", ".join(("basic", "adaptive"))
                )
            )

        if latero_mech not in ("UC", "TB"):
            raise ValueError(
                "value for latero_mech not understood ({val} not one of {valid})".format(
                    val=latero_mech, valid=", ".join(("UC", "TB"))
                )
            )

        if inlet_on and (inlet_node is None or inlet_area is None):
            raise ValueError(
                "inlet_on is True, but no inlet_node or inlet_area is provided."
            )

        if Kv is None:
            raise ValueError(
                "Kv must be set as a float, node array, or field name. It was None."
            )

        if solver == "adaptive":
            if not isinstance(flow_accumulator, FlowAccumulator):
                raise ValueError(
                    "When the adaptive solver is used, a valid "
                    "FlowAccumulator must be passed on "
                    "instantiation."
                )
            self._flow_accumulator = flow_accumulator

        # Create fields needed for this component if not already existing
        if "volume__lateral_erosion" not in grid.at_node:
            grid.add_zeros("volume__lateral_erosion", at="node")
        self._vol_lat = grid.at_node["volume__lateral_erosion"]

        if "sediment__influx" not in grid.at_node:
            grid.add_zeros("sediment__influx", at="node")
        self._qs_in = grid.at_node["sediment__influx"]

        if "lateral_erosion__depth_increment" not in grid.at_node:
            grid.add_zeros("lateral_erosion__depth_increment", at="node")
        self._dzlat = grid.at_node["lateral_erosion__depth_increment"]

        # for backward compatibility (remove in version 3.0.0+)
        grid.at_node["sediment__flux"] = grid.at_node["sediment__influx"]

        # you can specify the type of lateral erosion model you want to use.
        # But if you don't the default is the undercutting-slump model
        if latero_mech == "TB":
            self._TB = True
            self._UC = False
        else:
            self._UC = True
            self._TB = False
        # option use adaptive time stepping. Default is fixed dt supplied by user
        if solver == "basic":
            self.run_one_step = self.run_one_step_basic
        elif solver == "adaptive":
            self.run_one_step = self.run_one_step_adaptive
        self._alph = alph
        self._Kv = Kv  # can be overwritten with spatially variable
        self._Klr = float(Kl_ratio)  # default ratio of Kv/Kl is 1. Can be overwritten

        self._dzdt = grid.add_zeros(
            "dzdt", at="node", clobber=True
        )  # elevation change rate (M/Y)
        # optional inputs
        self._inlet_on = inlet_on
        if inlet_on:
            self._inlet_node = inlet_node
            self._inlet_area = inlet_area
            # runoff is an array with values of the area of each node (dx**2)
            runoffinlet = np.full(grid.number_of_nodes, grid.dx**2, dtype=float)
            # Change the runoff at the inlet node to node area + inlet node
            runoffinlet[inlet_node] += inlet_area
            grid.add_field("water__unit_flux_in", runoffinlet, at="node", clobber=True)
            # set qsinlet at inlet node. This doesn't have to be provided, defaults
            # to 0.
            self._qsinlet = qsinlet
            self._qs_in[self._inlet_node] = self._qsinlet

        # handling Kv for floats (inwhich case it populates an array N_nodes long) or
        # for arrays of Kv. Checks that length of Kv array is good.
        self._Kv = np.ones(self._grid.number_of_nodes, dtype=float) * Kv

    def run_one_step_basic(self, dt=1.0):
        """Calculate vertical and lateral erosion for a time period 'dt'.

        Parameters
        ----------
        dt : float
            Model timestep [T]
        """
        Klr = self._Klr
        grid = self._grid
        UC = self._UC
        TB = self._TB
        inlet_on = self._inlet_on  # this is a true/false flag
        Kv = self._Kv
        qs_in = self._qs_in
        dzdt = self._dzdt
        alph = self._alph
        vol_lat = self._grid.at_node["volume__lateral_erosion"]
        kw = 10.0
        F = 0.02

        # May 2, runoff calculated below (in m/s) is important for calculating
        # discharge and water depth correctly. renamed runoffms to prevent
        # confusion with other uses of runoff
        runoffms = (Klr * F / kw) ** 2
        # Kl is calculated from ratio of lateral to vertical K parameters
        Kl = Kv * Klr
        z = grid.at_node["topographic__elevation"]
        # clear qsin for next loop
        qs_in = grid.add_zeros("sediment__influx", at="node", clobber=True)
        qs = grid.add_zeros("qs", at="node", clobber=True)
        lat_nodes = np.zeros(grid.number_of_nodes, dtype=int)
        dzver = np.zeros(grid.number_of_nodes)
        vol_lat_dt = np.zeros(grid.number_of_nodes)

        # dz_lat needs to be reset. Otherwise, once a lateral node
        # erode's once, it will continue eroding at every subsequent
        # time setp. If you want to track all lateral erosion, create
        # another attribute, or add self.dzlat to itself after each time step.
        self._dzlat.fill(0.0)

        if inlet_on is True:
            inlet_node = self._inlet_node
            qsinlet = self._qsinlet
            qs_in[inlet_node] = qsinlet
            q = grid.at_node["surface_water__discharge"]
            da = q / grid.dx**2
        # if inlet flag is not on, proceed as normal.
        else:
            da = grid.at_node["drainage_area"]
        # flow__upstream_node_order is node array contianing downstream to
        # upstream order list of node ids
        s = grid.at_node["flow__upstream_node_order"]
        max_slopes = grid.at_node["topographic__steepest_slope"]
        flowdirs = grid.at_node["flow__receiver_node"]

        # make a list l, where node status is interior (signified by label 0) in s
        interior_s = s[np.where(grid.status_at_node[s] == 0)[0]]
        dwnst_nodes = interior_s.copy()
        # reverse list so we go from upstream to down stream
        dwnst_nodes = dwnst_nodes[::-1]
        max_slopes[:] = max_slopes.clip(0)
        for i in dwnst_nodes:
            # calc deposition and erosion
            dep = alph * qs_in[i] / da[i]
            ero = -Kv[i] * da[i] ** (0.5) * max_slopes[i]
            dzver[i] = dep + ero
            # potential lateral erosion initially set to 0
            petlat = 0.0
            # water depth in meters, needed for lateral erosion calc
            wd = wid_coeff * (da[i] * runoffms) ** wid_exp

            # Choose lateral node for node i. If node i flows downstream, continue.
            # if node i is the first cell at the top of the drainage network, don't go
            # into this loop because in this case, node i won't have a "donor" node
            if i in flowdirs:
                # node_finder picks the lateral node to erode based on angle
                # between segments between three nodes
                [lat_node, inv_rad_curv] = node_finder(grid, i, flowdirs, da)
                # node_finder returns the lateral node ID and the radius of curvature
                lat_nodes[i] = lat_node
                # if the lateral node is not 0 or -1 continue. lateral node may be
                # 0 or -1 if a boundary node was chosen as a lateral node. then
                # radius of curavature is also 0 so there is no lateral erosion
                if lat_node > 0 and z[lat_node] > z[i]:
                    # if the elevation of the lateral node is higher than primary node,
                    # calculate a new potential lateral erosion (L/T), which is negative
                    petlat = -Kl[i] * da[i] * max_slopes[i] * inv_rad_curv
                    # the calculated potential lateral erosion is mutiplied by
                    # the length of the node and the bank height, then added
                    # to an array, vol_lat_dt, for volume eroded laterally
                    # *per timestep* at each node. This vol_lat_dt is reset to zero for
                    # each timestep loop. vol_lat_dt is added to itself in case
                    # more than one primary nodes are laterally eroding this lat_node
                    # volume of lateral erosion per timestep
                    vol_lat_dt[lat_node] += abs(petlat) * grid.dx * wd

            # send sediment downstream. sediment eroded from vertical incision
            # and lateral erosion is sent downstream
            #            print("debug before 406")
            qs_in[flowdirs[i]] += (
                qs_in[i] - (dzver[i] * grid.dx**2) - (petlat * grid.dx * wd)
            )  # qsin to next node
        qs[:] = qs_in - (dzver * grid.dx**2)
        dzdt[:] = dzver * dt
        vol_lat[:] += vol_lat_dt * dt
        # this loop determines if enough lateral erosion has happened to change
        # the height of the neighbor node.
        for i in dwnst_nodes:
            lat_node = lat_nodes[i]
            wd = wid_coeff * (da[i] * runoffms) ** wid_exp
            # greater than zero now bc inactive neighbors are value -1
            if lat_node > 0 and z[lat_node] > z[i]:
                # vol_diff is the volume that must be eroded from lat_node so that its
                # elevation is the same as node downstream of primary node
                # UC model: this would represent undercutting (the water height at
                # node i), slumping, and instant removal.
                if UC == 1:
                    voldiff = (z[i] + wd - z[flowdirs[i]]) * grid.dx**2
                # TB model: entire lat node must be eroded before lateral erosion
                # occurs
                if TB == 1:
                    voldiff = (z[lat_node] - z[flowdirs[i]]) * grid.dx**2
                # if the total volume eroded from lat_node is greater than the volume
                # needed to be removed to make node equal elevation,
                # then instantaneously remove this height from lat node. already has
                # timestep in it
                if vol_lat[lat_node] >= voldiff:
                    self._dzlat[lat_node] = z[flowdirs[i]] - z[lat_node]  # -0.001
                    # after the lateral node is eroded, reset its volume eroded to
                    # zero
                    vol_lat[lat_node] = 0.0
        # combine vertical and lateral erosion.
        dz = dzdt + self._dzlat
        # change height of landscape
        z[:] += dz
        return grid, self._dzlat

    def run_one_step_adaptive(self, dt=1.0):
        """Run time step with adaptive time stepping to prevent slope
        flattening."""
        Klr = self._Klr
        grid = self._grid
        UC = self._UC
        TB = self._TB
        inlet_on = self._inlet_on  # this is a true/false flag
        Kv = self._Kv
        qs_in = self._qs_in
        dzdt = self._dzdt
        alph = self._alph
        vol_lat = self._grid.at_node["volume__lateral_erosion"]
        kw = 10.0
        F = 0.02
        runoffms = (Klr * F / kw) ** 2
        Kl = Kv * Klr
        z = grid.at_node["topographic__elevation"]
        # clear qsin for next loop
        qs_in = grid.add_zeros("sediment__influx", at="node", clobber=True)
        qs = grid.add_zeros("qs", at="node", clobber=True)
        lat_nodes = np.zeros(grid.number_of_nodes, dtype=int)
        dzver = np.zeros(grid.number_of_nodes)
        vol_lat_dt = np.zeros(grid.number_of_nodes)

        # dz_lat needs to be reset. Otherwise, once a lateral node erode's
        # once, it will continue eroding at every subsequent time setp.
        # If you want to track all lateral erosion, create another attribute,
        # or add self.dzlat to itself after each time step.
        self._dzlat.fill(0.0)

        if inlet_on is True:
            # define inlet_node
            inlet_node = self._inlet_node
            qsinlet = self._qsinlet
            qs_in[inlet_node] = qsinlet
            q = grid.at_node["surface_water__discharge"]
            da = q / grid.dx**2
        # if inlet flag is not on, proceed as normal.
        else:
            # renamed this drainage area set by flow router
            da = grid.at_node["drainage_area"]
        s = grid.at_node["flow__upstream_node_order"]
        max_slopes = grid.at_node["topographic__steepest_slope"]
        flowdirs = grid.at_node["flow__receiver_node"]
        interior_s = s[np.where(grid.status_at_node[s] == 0)[0]]
        dwnst_nodes = interior_s.copy()
        # reverse list so we go from upstream to down stream
        dwnst_nodes = dwnst_nodes[::-1]
        # local time
        time = 0.0
        globdt = dt

        while time < globdt:
            max_slopes[:] = max_slopes.clip(0)
            # here calculate dzdt for each node, with initial time step
            for i in dwnst_nodes:
                dep = alph * qs_in[i] / da[i]
                ero = -Kv[i] * da[i] ** (0.5) * max_slopes[i]
                dzver[i] = dep + ero
                petlat = 0.0
                # water depth in meters, needed for lateral erosion calc
                wd = wid_coeff * (da[i] * runoffms) ** wid_exp

                if i in flowdirs:
                    # node_finder picks the lateral node to erode based on angle
                    # between segments between three nodes
                    [lat_node, inv_rad_curv] = node_finder(grid, i, flowdirs, da)
                    # node_finder returns the lateral node ID and the radius of
                    # curvature
                    lat_nodes[i] = lat_node
                    # if the lateral node is not 0 or -1 continue.
                    if lat_node > 0 and z[lat_node] > z[i]:
                        # if the elevation of the lateral node is higher than
                        # primary node, calculate a new potential lateral
                        # erosion (L/T), which is negative
                        petlat = -Kl[i] * da[i] * max_slopes[i] * inv_rad_curv
                        # the calculated potential lateral erosion is mutiplied
                        # by the length of the node and the bank height, then
                        # added to an array, vol_lat_dt, for volume eroded
                        # laterally  *per timestep* at each node. This vol_lat_dt
                        # is reset to zero for each timestep loop. vol_lat_dt
                        # is added to itself more than one primary nodes are
                        # laterally eroding this lat_node volume of lateral
                        # erosion per timestep
                        vol_lat_dt[lat_node] += abs(petlat) * grid.dx * wd
                # send sediment downstream. sediment eroded from vertical incision
                # and lateral erosion is sent downstream
                qs_in[flowdirs[i]] += (
                    qs_in[i] - (dzver[i] * grid.dx**2) - (petlat * grid.dx * wd)
                )  # qsin to next node
            # summing qs for this entire timestep
            qs[:] += qs_in - (dzver * grid.dx**2)
            dzdt[:] = dzver
            # Do a time-step check
            # If the downstream node is eroding at a slower rate than the
            # upstream node, there is a possibility of flow direction reversal,
            # or at least a flattening of the landscape.
            # Limit dt so that this flattening or reversal doesn't happen.
            # How close you allow these two points to get to eachother is
            # determined by the cfl timestep condition, hard coded to equal 0.3
            # dtn is an arbitrarily large number to begin with, but will be
            # adapted as we step through the nodes
            dtn = dt * 50  # starting minimum timestep for this round
            for i in dwnst_nodes:
                # are points converging? ie, downstream eroding slower than upstream
                dzdtdif = dzdt[flowdirs[i]] - dzdt[i]
                # if points converging, find time to zero slope
                if dzdtdif > 1.0e-5 and max_slopes[i] > 1e-5:
                    # time to flat between points
                    dtflat = (z[i] - z[flowdirs[i]]) / dzdtdif
                    # if time to flat is smaller than dt, take the lower value
                    if dtflat < dtn:
                        dtn = dtflat
                    #                        assert dtn>0, "dtn <0 at dtflat"
                    # if dzdtdif*dtflat will make upstream lower than downstream, find
                    # time to flat
                    if dzdtdif * dtflat > (z[i] - z[flowdirs[i]]):
                        dtn = (z[i] - z[flowdirs[i]]) / dzdtdif
            dtn *= cfl_cond
            # new minimum timestep for this round of nodes
            dt = min(abs(dtn), dt)
            assert dt > 0.0, "timesteps less than 0."

            # vol_lat is the total volume eroded from the lateral nodes through
            # the entire model run. So vol_lat is itself plus vol_lat_dt (for current loop)
            # times stable timestep size
            vol_lat[:] += vol_lat_dt * dt
            # this loop determines if enough lateral erosion has happened to change
            # the height of the neighbor node.
            for i in dwnst_nodes:
                lat_node = lat_nodes[i]
                wd = wid_coeff * (da[i] * runoffms) ** wid_exp
                # greater than zero now bc inactive neighbors are value -1
                if lat_node > 0 and z[lat_node] > z[i]:
                    # vol_diff is the volume that must be eroded from lat_node so that its
                    # elevation is the same as node downstream of primary node
                    # UC model: this would represent undercutting (the water height
                    # at node i), slumping, and instant removal.
                    if UC == 1:
                        voldiff = (z[i] + wd - z[flowdirs[i]]) * grid.dx**2
                    # TB model: entire lat node must be eroded before lateral
                    # erosion occurs
                    if TB == 1:
                        voldiff = (z[lat_node] - z[flowdirs[i]]) * grid.dx**2
                    # if the total volume eroded from lat_node is greater than the volume
                    # needed to be removed to make node equal elevation,
                    # then instantaneously remove this height from lat node. already
                    # has timestep in it
                    if vol_lat[lat_node] >= voldiff:
                        self._dzlat[lat_node] = z[flowdirs[i]] - z[lat_node]  # -0.001
                        # after the lateral node is eroded, reset its volume eroded
                        # to zero
                        vol_lat[lat_node] = 0.0

            # multiply dzdt by timestep size and combine with lateral erosion
            # self._dzlat, which is already a length for the calculated time step
            dz = dzdt * dt + self._dzlat
            # change height of landscape
            z[:] += dz
            # update elapsed time
            time = dt + time
            # check to see that you are within 0.01% of the global timestep, if so
            # done, if not continue

            if time > 0.9999 * globdt:
                time = globdt

            else:
                dt = globdt - time
                qs_in = grid.zeros(centering="node")

                # recalculate flow directions
                (da, q) = self._flow_accumulator.accumulate_flow()

                if inlet_on:
                    # if inlet on, reset drainage area and qsin to reflect inlet conditions
                    # this is the drainage area needed for code below with an inlet
                    # set by spatially varible runoff.
                    da = q / grid.dx**2
                    qs_in[inlet_node] = qsinlet
                else:
                    # otherwise, drainage area is just drainage area.
                    da = grid.at_node["drainage_area"]
                s = grid.at_node["flow__upstream_node_order"]
                max_slopes = grid.at_node["topographic__steepest_slope"]
                q = grid.at_node["surface_water__discharge"]
                flowdirs = grid.at_node["flow__receiver_node"]
                interior_s = s[np.where(grid.status_at_node[s] == 0)[0]]
                dwnst_nodes = interior_s.copy()
                dwnst_nodes = dwnst_nodes[::-1]

                lat_nodes = np.zeros(grid.number_of_nodes, dtype=int)
                self._dzlat = np.zeros(grid.number_of_nodes)
                vol_lat_dt = np.zeros(grid.number_of_nodes)
                dzver = np.zeros(grid.number_of_nodes)

        return grid, self._dzlat



================================================
File: lateral_erosion/node_finder.py
================================================
import numpy as np


def angle_finder(grid, dn, cn, rn):
    """Find the interior angle between two vectors on a grid.

    Parameters
    ----------
    grid : ModelGrid
        A landlab grid.
    dn : int or array of int
        Node or nodes at the end of the first vector.
    cn : int or array of int
        Node or nodes at the vertex between vectors.
    rn : int or array of int
        Node or nodes at the end of the second vector.

    Returns
    -------
    float or array of float
        Angle between vectors (in radians).

    Examples
    --------
    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components.lateral_erosion.node_finder import angle_finder

    >>> grid = RasterModelGrid((3, 4))
    >>> np.rad2deg(angle_finder(grid, 8, 5, 0))
    90.0
    >>> np.rad2deg(angle_finder(grid, (8, 9, 10, 6), 5, 6))
    array([135.,  90.,  45.,   0.])
    """
    vertex = np.take(grid.x_of_node, cn), np.take(grid.y_of_node, cn)
    vec_1 = [
        np.take(grid.x_of_node, dn) - vertex[0],
        np.take(grid.y_of_node, dn) - vertex[1],
    ]
    vec_2 = [
        np.take(grid.x_of_node, rn) - vertex[0],
        np.take(grid.y_of_node, rn) - vertex[1],
    ]

    return np.arccos(
        (vec_1[0] * vec_2[0] + vec_1[1] * vec_2[1])
        / np.sqrt((vec_1[0] ** 2 + vec_1[1] ** 2) * (vec_2[0] ** 2 + vec_2[1] ** 2))
    )


def forty_five_node(donor, i, receiver, neighbors, diag_neigh):
    radcurv_angle = 0.67

    lat_node = 0
    # In Landlab 2019: diagonal list goes [NE, NW, SW, SE]. Node list are ordered as [E,N,W,S]
    # if water flows SE-N OR if flow NE-S or E-NW or E-SW, erode west node
    if (
        donor == diag_neigh[0]
        and receiver == neighbors[3]
        or donor == diag_neigh[3]
        and receiver == neighbors[1]
        or donor == neighbors[0]
        and receiver == diag_neigh[2]
        or donor == neighbors[0]
        and receiver == diag_neigh[1]
    ):
        lat_node = neighbors[2]
    # if flow is from SW-N or NW-S or W-NE or W-SE, erode east node
    elif (
        donor == diag_neigh[1]
        and receiver == neighbors[3]
        or donor == diag_neigh[2]
        and receiver == neighbors[1]
        or donor == neighbors[2]
        and receiver == diag_neigh[3]
        or donor == neighbors[2]
        and receiver == diag_neigh[0]
    ):
        lat_node = neighbors[0]
    # if flow is from SE-W or SW-E or S-NE or S-NW, erode north node
    elif (
        donor == diag_neigh[3]
        and receiver == neighbors[2]
        or donor == diag_neigh[2]
        and receiver == neighbors[0]
        or donor == neighbors[3]
        and receiver == diag_neigh[0]
        or donor == neighbors[3]
        and receiver == diag_neigh[1]
    ):
        lat_node = neighbors[1]
    # if flow is from NE-W OR NW-E or N-SE or N-SW, erode south node
    elif (
        donor == diag_neigh[0]
        and receiver == neighbors[2]
        or donor == diag_neigh[1]
        and receiver == neighbors[0]
        or donor == neighbors[1]
        and receiver == diag_neigh[3]
        or donor == neighbors[1]
        and receiver == diag_neigh[2]
    ):
        lat_node = neighbors[3]
    return lat_node, radcurv_angle


def ninety_node(donor, i, receiver, link_list, neighbors, diag_neigh):
    # if flow is 90 degrees
    if donor in diag_neigh and receiver in diag_neigh:
        radcurv_angle = 1.37
        # if flow is NE-SE or NW-SW, erode south node
        if (
            donor == diag_neigh[0]
            and receiver == diag_neigh[3]
            or donor == diag_neigh[1]
            and receiver == diag_neigh[2]
        ):
            lat_node = neighbors[3]
        # if flow is SW-NW or SE-NE, erode north node
        elif (
            donor == diag_neigh[2]
            and receiver == diag_neigh[1]
            or donor == diag_neigh[3]
            and receiver == diag_neigh[0]
        ):
            lat_node = neighbors[1]
        # if flow is SW-SE or NW-NE, erode east node
        elif (
            donor == diag_neigh[2]
            and receiver == diag_neigh[3]
            or donor == diag_neigh[1]
            and receiver == diag_neigh[0]
        ):
            lat_node = neighbors[0]
        # if flow is SE-SW or NE-NW, erode west node
        elif (
            donor == diag_neigh[3]
            and receiver == diag_neigh[2]
            or donor == diag_neigh[0]
            and receiver == diag_neigh[1]
        ):
            lat_node = neighbors[2]
    elif donor not in diag_neigh and receiver not in diag_neigh:
        radcurv_angle = 1.37
        # if flow is from east, erode west node
        if donor == neighbors[0]:
            lat_node = neighbors[2]
        # if flow is from north, erode south node
        elif donor == neighbors[1]:
            lat_node = neighbors[3]
        # if flow is from west, erode east node
        elif donor == neighbors[2]:
            lat_node = neighbors[0]
        # if flow is from south, erode north node
        elif donor == neighbors[3]:
            lat_node = neighbors[1]
    return lat_node, radcurv_angle


def straight_node(donor, i, receiver, neighbors, diag_neigh):
    # ***FLOW LINK IS STRAIGHT, NORTH TO SOUTH***#
    if donor == neighbors[1] or donor == neighbors[3]:
        # print "flow is stright, N-S from ", donor, " to ", flowdirs[i]
        radcurv_angle = 0.23
        # neighbors are ordered E,N,W, S
        # if the west cell is boundary (neighbors=-1), erode from east node
        if neighbors[2] == -1:
            lat_node = neighbors[0]
        elif neighbors[0] == -1:
            lat_node = neighbors[2]
        else:
            # if could go either way, choose randomly. 0 goes East, 1 goes west
            ran_num = np.random.randint(0, 2)
            if ran_num == 0:
                lat_node = neighbors[0]
            if ran_num == 1:
                lat_node = neighbors[2]
    # ***FLOW LINK IS STRAIGHT, EAST-WEST**#
    elif donor == neighbors[0] or donor == neighbors[2]:
        radcurv_angle = 0.23
        #  Node list are ordered as [E,N,W,S]
        # if the north cell is boundary (neighbors=-1), erode from south node
        if neighbors[1] == -1:
            lat_node = neighbors[3]
        elif neighbors[3] == -1:
            lat_node = neighbors[1]
        else:
            # if could go either way, choose randomly. 0 goes south, 1 goes north
            ran_num = np.random.randint(0, 2)
            if ran_num == 0:
                lat_node = neighbors[1]
            if ran_num == 1:
                lat_node = neighbors[3]
    # if flow is straight across diagonal, choose node to erode at random
    elif donor in diag_neigh and receiver in diag_neigh:
        radcurv_angle = 0.23
        if receiver == diag_neigh[0]:
            poss_diag_nodes = neighbors[0 : 1 + 1]
        elif receiver == diag_neigh[1]:
            poss_diag_nodes = neighbors[1 : 2 + 1]
        elif receiver == diag_neigh[2]:
            poss_diag_nodes = neighbors[2 : 3 + 1]
        elif receiver == diag_neigh[3]:
            poss_diag_nodes = [neighbors[3], neighbors[0]]
        ran_num = np.random.randint(0, 2)
        if ran_num == 0:
            lat_node = poss_diag_nodes[0]
        if ran_num == 1:
            lat_node = poss_diag_nodes[1]
    return lat_node, radcurv_angle


def node_finder(grid, i, flowdirs, drain_area):
    """Find lateral neighbor node of the primary node for straight, 45 degree,
    and 90 degree channel segments.

    Parameters
    ----------
    grid : ModelGrid
        A Landlab grid object
    i : int
        node ID of primary node
    flowdirs : array
        Flow direction array
    drain_area : array
        drainage area array

    Returns
    -------
    lat_node : int
        node ID of lateral node
    radcurv_angle : float
        inverse radius of curvature of channel at lateral node
    """
    # receiver node of flow is flowdirs[i]
    receiver = flowdirs[i]

    # find indicies of where flowdirs=i to find donor nodes.
    # will donor nodes always equal the index of flowdir list?
    inflow = np.where(flowdirs == i)

    # if there are more than 1 donors, find the one with largest drainage area

    if len(inflow[0]) > 1:
        drin = drain_area[inflow]
        drmax = max(drin)
        maxinfl = inflow[0][np.where(drin == drmax)]
        # if donor nodes have same drainage area, choose one randomly
        if len(maxinfl) > 1:
            ran_num = np.random.randint(0, len(maxinfl))
            maxinfln = maxinfl[ran_num]
            donor = [maxinfln]
        else:
            donor = maxinfl
        # if inflow is empty, no donor
    elif len(inflow[0]) == 0:
        donor = i
    # else donor is the only inflow
    else:
        donor = inflow[0]
    # now we have chosen donor cell, next figure out if inflow/outflow lines are
    # straight, 45, or 90 degree angle. and figure out which node to erode
    link_list = grid.links_at_node[i]
    # this gives list of active neighbors for specified node
    # the order of this list is: [E,N,W,S]
    neighbors = grid.active_adjacent_nodes_at_node[i]
    # this gives list of all diagonal neighbors for specified node
    # the order of this list is: [NE,NW,SW,SE]
    diag_neigh = grid.diagonal_adjacent_nodes_at_node[i]
    angle_diff = np.rad2deg(angle_finder(grid, donor, i, receiver))

    if (donor == flowdirs[i]) or (donor == i):
        # this is a sink. no lateral ero
        radcurv_angle = 0.0
        lat_node = 0
    elif np.isclose(angle_diff, 0.0) or np.isclose(angle_diff, 180.0):
        [lat_node, radcurv_angle] = straight_node(
            donor, i, receiver, neighbors, diag_neigh
        )
    elif np.isclose(angle_diff, 45.0) or np.isclose(angle_diff, 135.0):
        [lat_node, radcurv_angle] = forty_five_node(
            donor, i, receiver, neighbors, diag_neigh
        )
    elif np.isclose(angle_diff, 90.0):
        [lat_node, radcurv_angle] = ninety_node(
            donor, i, receiver, link_list, neighbors, diag_neigh
        )
    else:
        lat_node = 0
        radcurv_angle = 0.0

    dx = grid.dx
    # INVERSE radius of curvature.
    radcurv_angle = radcurv_angle / dx
    return int(lat_node), radcurv_angle



================================================
File: lithology/README.md
================================================
[![status](http://joss.theoj.org/papers/74487c5a6820fb2fe2898960ad6d2ea0/status.svg)](http://joss.theoj.org/papers/74487c5a6820fb2fe2898960ad6d2ea0)

Welcome to the README for the Lithology component submodule.

This submodule was published as a [Journal of Open Source
Software](http://joss.theoj.org) publication. Click the badge above to be
directed to the paper.

There is a [jupyter notebook in the Landlab Tutorials
repository](https://mybinder.org/v2/gh/landlab/landlab/release?filepath=notebooks/tutorials/lithology/lithology_and_litholayers.ipynb)
that describes the use of this submodule.

If you have any questions, comments, issues, or bugs related to this submodule,
please [open an Issue](https://github.com/landlab/landlab/issues) so we can
respond.



================================================
File: lithology/__init__.py
================================================
from .litholayers import LithoLayers
from .lithology import Lithology

__all__ = ["Lithology", "LithoLayers"]



================================================
File: lithology/litholayers.py
================================================
#!/usr/bin/env python3
"""Create a LithoLayers component with different properties."""

import numpy as np

from landlab.components.lithology.lithology import Lithology


class LithoLayers(Lithology):
    """Create LithoLayers component.

    A LithoLayers is a three dimentional representation of material operated on
    by landlab components. Material can be removed through erosion or added to
    through deposition. Rock types can have multiple attributes (e.g. age,
    erodability or other parameter values, etc).

    If the tracked properties are model grid fields, they will be updated to
    the surface values of the Lithology. If the properties are not grid fields
    then at-node grid fields will be created with their names.

    It is constructed by specifying a series of depths below the surface, an
    anchor point, a series of rock type ids, and the functional form of a
    surface. Depths and IDs are both specified in order of closest
    to the surface to furthest from the surface.

    Additionally, an attribute dictionary specifies the properties of each
    rock type. This dictionary is expected to have the form of:

    .. code-block:: python

        attrs = {"K_sp": {1: 0.001, 2: 0.0001}, "D": {1: 0.01, 2: 0.001}}

    Where ``'K_sp'`` and ``'D'`` are properties to track, and ``1`` and ``2``
    are rock type IDs. The rock type IDs can be any type that is valid as a
    python dictionary key.

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Barnhart, K., Hutton, E., Gasparini, N., Tucker, G. (2018). Lithology: A
    Landlab submodule for spatially variable rock properties. Journal of Open
    Source Software  3(30), 979 - 2. https://dx.doi.org/10.21105/joss.00979

    **Additional References**

    None Listed

    """

    _name = "LithoLayers"

    _unit_agnostic = True

    _cite_as = """
    @article{barnhart2018lithology,
        title = "Lithology: A Landlab submodule for spatially variable rock properties",
        journal = "Journal of Open Source Software",
        volume = "",
        pages = "",
        year = "2018",
        doi = "10.21105/joss.00979",
        author = {Katherine R. Barnhart and Eric Hutton and Nicole M. Gasparini
                  and Gregory E. Tucker},
    }"""

    _info = {}

    def __init__(
        self,
        grid,
        z0s,
        ids,
        attrs,
        x0=0,
        y0=0,
        function=lambda x, y: 0 * x + 0 * y,
        layer_type="EventLayers",
        dz_advection=0,
        rock_id=None,
    ):
        """Create a new instance of a LithoLayers.

        Parameters
        ----------
        grid : Landlab ModelGrid
        z0s : ndarray of shape `(n_layers, )`
            Values of layer depth from surface at horizontal location (x0, y0).
        ids : ndarray of shape `(n_layers, )`
            Values of rock type IDs corresponding to each layer specified in
            **z0s**.
        attrs : dict
            Rock type property dictionary. See class docstring for example of
            required format.
        x0 : float, optional
            x value of anchor point for all layers.
        y0 : float, optional
            y value of anchor point for all layers.
        function : function, optional
            Functional form of layers as a function of two variables, x and y.
            Default value is `lambda x, y: 0*x + 0*y` for flatlying layers.
        layer_type : str, optional
            Type of Landlab layers object used to store the layers. If
            MaterialLayers (default) is specified, then erosion removes material
            and does not create a layer of thickness zero. If EventLayers is
            used, then erosion removes material and creates layers of thickness
            zero. Thus, EventLayers may be appropriate if the user is interested
            in chronostratigraphy.
        dz_advection : float, `(n_nodes, )` shape array, or at-node field array optional
            Change in rock elevation due to advection by some external process.
            This can be changed using the property setter.
        rock_id : value or `(n_nodes, )` shape array, optional
            Rock type id for new material if deposited.
            This can be changed using the property setter.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import LithoLayers
        >>> mg = RasterModelGrid((3, 3))
        >>> z = mg.add_zeros("node", "topographic__elevation")

        Create a LithoLayers with flatlying layers that altrnate between
        layers of type 1 and type 2 rock.

        >>> z0s = [-4, -3, -2, -1, 0, 1, 2, 3, 4]
        >>> ids = [1, 2, 1, 2, 1, 2, 1, 2, 1]
        >>> attrs = {"K_sp": {1: 0.001, 2: 0.0001}}
        >>> lith = LithoLayers(mg, z0s, ids, attrs)
        >>> lith.dz
        array([[1., 1., 1., 1., 1., 1., 1., 1., 1.],
               [1., 1., 1., 1., 1., 1., 1., 1., 1.],
               [1., 1., 1., 1., 1., 1., 1., 1., 1.],
               [1., 1., 1., 1., 1., 1., 1., 1., 1.],
               [0., 0., 0., 0., 0., 0., 0., 0., 0.],
               [0., 0., 0., 0., 0., 0., 0., 0., 0.],
               [0., 0., 0., 0., 0., 0., 0., 0., 0.],
               [0., 0., 0., 0., 0., 0., 0., 0., 0.],
               [0., 0., 0., 0., 0., 0., 0., 0., 0.]])

        Now create a set of layers that dip. Our anchor point will be the
        default value of (x0, y0) = (0, 0)

        >>> lith = LithoLayers(mg, z0s, ids, attrs, function=lambda x, y: x + y)
        >>> lith.dz
        array([[1., 1., 1., 1., 1., 1., 1., 1., 1.],
               [1., 1., 1., 1., 1., 1., 1., 1., 1.],
               [1., 1., 1., 1., 1., 1., 1., 1., 1.],
               [1., 1., 1., 1., 1., 1., 1., 1., 1.],
               [0., 1., 1., 1., 1., 1., 1., 1., 1.],
               [0., 0., 1., 0., 1., 1., 1., 1., 1.],
               [0., 0., 0., 0., 0., 1., 0., 1., 1.],
               [0., 0., 0., 0., 0., 0., 0., 0., 1.],
               [0., 0., 0., 0., 0., 0., 0., 0., 0.]])

        We can get the surface values, and as we'd expect, they alternate as
        the dipping layers are exposed at the surface.

        >>> lith["K_sp"]
        array([0.0001, 0.001 , 0.0001, 0.001 , 0.0001, 0.001 , 0.0001, 0.001 , 0.0001])
        """

        function_args = function.__code__.co_varnames
        if len(function_args) != 2:
            raise ValueError(
                "LithoLayers: function must take exactly two arguments, x and y."
            )

        if np.asarray(z0s).size != np.asarray(ids).size:
            raise ValueError(
                "LithoLayers: Size of layer depths and layer IDs must be the same"
            )

        if np.any(np.diff(z0s) < 0):
            raise ValueError("LithoLayers: Bad layer depth order passed.")

        z_surf = function(grid.x_of_node - x0, grid.y_of_node - y0)

        if hasattr(z_surf, "shape"):
            if z_surf.shape != grid.x_of_node.shape:
                raise ValueError(
                    "LithoLayers: function must return an array of shape (n_nodes,)"
                )
        else:
            raise ValueError(
                "LithoLayers: function must return an array of shape (n_nodes,)"
            )

        layer_thicknesses = []
        layer_ids = []

        num_layers = np.asarray(z0s).size

        last_layer_elev = np.zeros(grid.number_of_nodes)

        # create layers (here listed from the top to the bottom.)
        for i in range(num_layers):
            layer_depth = z_surf + z0s[i]
            layer_depth[layer_depth < 0] = 0

            layer_thickness = layer_depth.copy() - last_layer_elev.copy()

            last_layer_elev = layer_depth.copy()

            layer_thicknesses.append(layer_thickness)
            layer_ids.append(ids[i] * np.ones(z_surf.size))

        super().__init__(
            grid,
            layer_thicknesses,
            layer_ids,
            attrs,
            layer_type=layer_type,
            dz_advection=dz_advection,
            rock_id=rock_id,
        )



================================================
File: lithology/lithology.py
================================================
#!/usr/bin/env python3
"""Create a Lithology object with different properties."""

import numpy as np
import xarray as xr
from scipy.interpolate import interp1d

from landlab import Component
from landlab.layers import EventLayers
from landlab.layers import MaterialLayers
from landlab.utils.return_array import return_array_at_node


class Lithology(Component):
    """Create a Lithology object.

    A Lithology is a three dimentional representation of material operated on
    by landlab components. Material can be removed through erosion or added to
    through deposition. Rock types can have multiple attributes (e.g. age,
    erodability or other parameter values, etc).

    If the tracked properties are model grid fields, they will be updated to
    the surface values of the Lithology. If the properties are not grid fields
    then at-node grid fields will be created with their names. Lithology and
    its derived versions will make a at-node grid field called `rock_type__id`
    to store the rock type id.

    Lithology was designed to be used on its own and to be inherited from and
    improved. Currently one other Lithology variant exists: LithoLayers
    which makes it easy to specify parallel layers of rock with generic layer
    geometries.

    It is constructed by specifying a series of thicknesses and a series of
    rock type IDs. Thicknesses and IDs are both specified in order of closest
    to the surface to furthest from the surface. Thicknesses can either be a
    single value (corresponding to a layer of uniform thickness) or a number-of
    -nodes length array (corresponding to a non-uniform layer).

    Additionally, an attribute dictionary specifies the properties of each
    rock type. This dictionary is expected to have the form of:

    .. code-block:: python

        attrs = {"K_sp": {1: 0.001, 2: 0.0001}, "D": {1: 0.01, 2: 0.001}}

    Where ``'K_sp'`` and ``'D'`` are properties to track, and ``1`` and ``2``
    are rock type IDs. The rock type IDs can be any type that is valid as a
    python dictionary key.

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Barnhart, K., Hutton, E., Gasparini, N., Tucker, G. (2018). Lithology: A
    Landlab submodule for spatially variable rock properties. Journal of Open
    Source Software  3(30), 979 - 2. https://dx.doi.org/10.21105/joss.00979

    **Additional References**

    None Listed

    """

    _name = "Lithology"

    _unit_agnostic = True

    _cite_as = """
    @article{barnhart2018lithology,
        title = "Lithology: A Landlab submodule for spatially variable rock properties",
        journal = "Journal of Open Source Software",
        volume = "",
        pages = "",
        year = "2018",
        doi = "10.21105/joss.00979",
        author = {Katherine R. Barnhart and Eric Hutton and Nicole M. Gasparini
                  and Gregory E. Tucker},
    }"""

    _info = {}

    def __init__(
        self,
        grid,
        thicknesses,
        ids,
        attrs,
        layer_type="MaterialLayers",
        dz_advection=0,
        rock_id=None,
    ):
        """Create a new instance of Lithology.

        Parameters
        ----------
        grid : Landlab ModelGrid
        thicknesses : ndarray of shape `(n_layers, )` or `(n_layers, n_nodes)`
            Values of layer thicknesses from surface to depth. Layers do not
            have to have constant thickness. Layer thickness can be zero,
            though the entirety of Lithology must have non-zero thickness.
        ids : ndarray of shape `(n_layers, )` or `(n_layers, n_nodes)`
            Values of rock type IDs corresponding to each layer specified in
            **thicknesses**. A single layer may have multiple rock types if
            specified by the user.
        attrs : dict
            Rock type property dictionary. See class docstring for example of
            required format.
        layer_type : str, optional
            Type of Landlab layers object used to store the layers. If
            MaterialLayers (default) is specified, then erosion removes material
            and does not create a layer of thickness zero. If EventLayers is
            used, then erosion removes material and creates layers of thickness
            zero. Thus, EventLayers may be appropriate if the user is interested
            in chronostratigraphy.
        dz_advection : float, `(n_nodes, )` shape array, or at-node field array optional
            Change in rock elevation due to advection by some external process.
            This can be changed using the property setter. Dimensions are in
            length, not length per time.
        rock_id : value or `(n_nodes, )` shape array, optional
            Rock type id for new material if deposited.
            This can be changed using the property setter.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import Lithology
        >>> mg = RasterModelGrid((3, 3))
        >>> z = mg.add_zeros("topographic__elevation", at="node")

        Create a Lithology with uniform thicknesses that alternates between
        layers of type 1 and type 2 rock.

        >>> thicknesses = [1, 2, 4, 1]
        >>> ids = [1, 2, 1, 2]
        >>> attrs = {"K_sp": {1: 0.001, 2: 0.0001}}
        >>> lith = Lithology(mg, thicknesses, ids, attrs)

        After creating a Lithology, the model grid will have an at-node grid
        field set to the surface values of 'K_sp'.

        >>> mg.at_node["K_sp"]
        array([0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001])

        The surface values are also properties of the Lithology.

        >>> lith["K_sp"]
        array([0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001])

        We can access information about the Lithology like the total thickness
        or layer thicknesses.

        >>> lith.thickness
        array([8., 8., 8., 8., 8., 8., 8., 8., 8.])
        >>> lith.dz
        array([[1., 1., 1., 1., 1., 1., 1., 1., 1.],
               [4., 4., 4., 4., 4., 4., 4., 4., 4.],
               [2., 2., 2., 2., 2., 2., 2., 2., 2.],
               [1., 1., 1., 1., 1., 1., 1., 1., 1.]])

        This might look confusing -- that the layers are in reverse order, but
        it is OK. The last layers in the Lithology are those that are closest
        to the surface.

        The layers don't all have to have the same thickness as in the prior
        example. If the layers have non-uniform thickness, then they must be
        specified in an array of shape `(n_layer, n_nodes)`. In this case, the
        layer IDs must be specified in either an array of `(n_layer)` or
        `(n_layer, n_nodes)`.

        Here we make a layer that gets thicker as a function of the x value of
        the model grid.

        >>> layer_pattern = (0.5 * mg.x_of_node) + 1.0
        >>> thicknesses = [1 * layer_pattern, 2 * layer_pattern, 4 * layer_pattern]
        >>> ids = [1, 2, 1]
        >>> lith = Lithology(mg, thicknesses, ids, attrs)
        >>> lith.thickness
        array([ 7. , 10.5, 14. ,  7. , 10.5, 14. ,  7. , 10.5, 14. ])
        >>> lith.dz
        array([[4. , 6. , 8. , 4. , 6. , 8. , 4. , 6. , 8. ],
               [2. , 3. , 4. , 2. , 3. , 4. , 2. , 3. , 4. ],
               [1. , 1.5, 2. , 1. , 1.5, 2. , 1. , 1.5, 2. ]])
        """
        super().__init__(grid)

        try:
            self._last_elevation = self._grid["node"]["topographic__elevation"][
                :
            ].copy()
        except KeyError as exc:
            raise ValueError(
                "Lithology requires that topographic__elevation already "
                "exists as an at-node field."
            ) from exc

        # save inital information about thicknesses, layers, attributes, and ids.
        self._init_thicknesses = np.asarray(thicknesses)
        self._attrs = attrs
        self._number_of_init_layers = self._init_thicknesses.shape[0]
        self._properties = list(attrs.keys())
        self._rock_id_name = "rock_type__id"
        # assert that thicknesses and ids are correct and consistent shapes

        # if thickness is a 2d array.
        if self._init_thicknesses.ndim == 2:
            # assert that the 2nd dimension is the same as the number of nodes.
            if self._init_thicknesses.shape[1] != self._grid.number_of_nodes:
                raise ValueError(
                    "Thicknesses provided to Lithology are ",
                    "inconsistent with the ModelGrid.",
                )

            # if IDs is a 2d array assert that it is the same size as thicknesses
            if np.asarray(ids).ndim == 2:
                if self._init_thicknesses.shape != np.asarray(ids).shape:
                    raise ValueError(
                        "Thicknesses and IDs provided to Lithology are ",
                        "inconsistent with each other.",
                    )
                # if tests pass set value of IDs.
                self._layer_ids = np.asarray(ids)

            # if IDS is a 1d array
            elif np.asarray(ids).ndim == 1:
                if np.asarray(ids).size != self._number_of_init_layers:
                    raise ValueError(
                        "Number of IDs provided to Lithology is ",
                        "inconsistent with number of layers provided in "
                        "thicknesses.",
                    )
                # if tests pass, broadcast ids to correct shape.
                self._layer_ids = np.broadcast_to(
                    np.atleast_2d(np.asarray(ids)).T, self._init_thicknesses.shape
                )

            else:
                raise ValueError(
                    "IDs must be of shape `(n_layers, )` or `(n_layers, n_nodes)`. "
                    "Passed array has more than 2 dimensions."
                )

        elif self._init_thicknesses.ndim == 1:
            if self._init_thicknesses.shape != np.asarray(ids).shape:
                raise ValueError(
                    "Thicknesses and IDs provided to Lithology are ",
                    "inconsistent with each other.",
                )
            self._layer_ids = np.asarray(ids)
        else:
            raise ValueError(
                "Thicknesses must be of shape `(n_layers, )` or "
                "`(n_layers, n_nodes)`. Passed array has more than 2 dimensions."
            )

        # assert that attrs are pointing to fields (or create them)
        for at in self._properties:
            if at not in grid.at_node:
                self._grid.add_empty(at, at="node")

        # add a field for the rock type id
        if self._rock_id_name not in self._grid.at_node:
            self._grid.add_empty(self._rock_id_name, at="node")

        # verify that all IDs have attributes.
        self._check_property_dictionary()

        # create a EventLayers instance
        if layer_type == "EventLayers":
            self._layers = EventLayers(
                grid.number_of_nodes, self._number_of_init_layers
            )
        elif layer_type == "MaterialLayers":
            self._layers = MaterialLayers(
                grid.number_of_nodes, self._number_of_init_layers
            )
        else:
            raise ValueError("Lithology passed an invalid option for " "layer type.")

        # From bottom to top, add layers to the Lithology with attributes.
        for i in range(self._number_of_init_layers - 1, -1, -1):
            try:
                self.add_layer(self._init_thicknesses[i, :], self._layer_ids[i, :])
            except IndexError:
                self.add_layer(self._init_thicknesses[i], self._layer_ids[i])

        self.dz_advection = dz_advection
        self.rock_id = rock_id

    def __getitem__(self, name):
        return self._get_surface_values(name)

    @property
    def dz_advection(self):
        """Rate of vertical advection.

        Parameters
        ----------
        dz_advection : float, `(n_nodes, )` shape array, or at-node field array optional
            Change in rock elevation due to advection by some external process.
            This can be changed using the property setter. Dimensions are in
            length, not length per time.

        Returns
        -------
        current rate of vertical advection
        """
        return return_array_at_node(self._grid, self._dz_advection)

    @dz_advection.setter
    def dz_advection(self, dz_advection):
        return_array_at_node(self._grid, dz_advection)  # verify that this will work.
        self._dz_advection = dz_advection

    @property
    def rock_id(self):
        """Rock type for deposition.

        Parameters
        ----------
        rock_id : value or `(n_nodes, )` shape array, optional
            Rock type id for new material if deposited.
            This can be changed using the property setter.

        Returns
        -------
        current type of rock being deposited (if deposition occurs)
        """
        if self._rock_id is None:
            return None
        else:
            return return_array_at_node(self._grid, self._rock_id)

    @rock_id.setter
    def rock_id(self, rock_id):
        return_array_at_node(self._grid, rock_id)  # verify that this will work.
        # verify that all rock types are valid
        self._rock_id = rock_id

    @property
    def ids(self):
        """Rock type IDs used by Lithology."""
        return list(self._ids)

    @property
    def tracked_properties(self):
        """Properties tracked by Lithology.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import Lithology
        >>> mg = RasterModelGrid((3, 3))
        >>> z = mg.add_zeros("topographic__elevation", at="node")
        >>> thicknesses = [1, 2, 4, 1]
        >>> ids = [1, 2, 1, 2]
        >>> attrs = {"K_sp": {1: 0.001, 2: 0.0001}}
        >>> lith = Lithology(mg, thicknesses, ids, attrs)
        >>> lith.tracked_properties
        ['K_sp']
        """
        self._properties.sort()
        return self._properties

    @property
    def properties(self):
        """Properties dictionary used by Lithology.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import Lithology
        >>> mg = RasterModelGrid((3, 3))
        >>> z = mg.add_zeros("topographic__elevation", at="node")
        >>> thicknesses = [1, 2, 4, 1]
        >>> ids = [1, 2, 1, 2]
        >>> attrs = {"K_sp": {1: 0.001, 2: 0.0001}}
        >>> lith = Lithology(mg, thicknesses, ids, attrs)
        >>> lith.properties
        {'K_sp': {1: 0.001, 2: 0.0001}}
        """
        return self._attrs

    @property
    def thickness(self):
        """Total thickness of the Lithology at each node.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import Lithology
        >>> mg = RasterModelGrid((3, 3))
        >>> z = mg.add_zeros("topographic__elevation", at="node")
        >>> thicknesses = [1, 2, 4, 1]
        >>> ids = [1, 2, 1, 2]
        >>> attrs = {"K_sp": {1: 0.001, 2: 0.0001}}
        >>> lith = Lithology(mg, thicknesses, ids, attrs)
        >>> lith.thickness
        array([8., 8., 8., 8., 8., 8., 8., 8., 8.])
        """
        return self._layers.thickness

    @property
    def dz(self):
        """Thickness of each layer in the Lithology at each node.

        The thickness of each layer in the Lithology as an array of shape
        `(number_of_layers, number_of_nodes)`.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import Lithology
        >>> mg = RasterModelGrid((3, 3))
        >>> z = mg.add_zeros("topographic__elevation", at="node")
        >>> thicknesses = [1, 2, 4, 1]
        >>> ids = [1, 2, 1, 2]
        >>> attrs = {"K_sp": {1: 0.001, 2: 0.0001}}
        >>> lith = Lithology(mg, thicknesses, ids, attrs)
        >>> lith.dz
        array([[1., 1., 1., 1., 1., 1., 1., 1., 1.],
               [4., 4., 4., 4., 4., 4., 4., 4., 4.],
               [2., 2., 2., 2., 2., 2., 2., 2., 2.],
               [1., 1., 1., 1., 1., 1., 1., 1., 1.]])
        """
        return self._layers.dz

    @property
    def z_bottom(self):
        """Thickness from the surface to the bottom of each layer in Lithology.

        Thickness from the topographic surface to the bottom of each layer as
        an array of shape `(number_of_layers, number_of_nodes)`.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import Lithology
        >>> mg = RasterModelGrid((3, 3))
        >>> z = mg.add_zeros("topographic__elevation", at="node")
        >>> thicknesses = [1, 2, 4, 1]
        >>> ids = [1, 2, 1, 2]
        >>> attrs = {"K_sp": {1: 0.001, 2: 0.0001}}
        >>> lith = Lithology(mg, thicknesses, ids, attrs)
        >>> lith.z_bottom
        array([[8., 8., 8., 8., 8., 8., 8., 8., 8.],
               [7., 7., 7., 7., 7., 7., 7., 7., 7.],
               [3., 3., 3., 3., 3., 3., 3., 3., 3.],
               [1., 1., 1., 1., 1., 1., 1., 1., 1.]])
        """
        thick = np.broadcast_to(self._layers.thickness, self._layers.z.shape)
        return thick - self._layers.z + self._layers.dz

    @property
    def z_top(self):
        """Thickness from the surface to the top of each layer in Lithology.

        Thickness from the topographic surface to the top of each layer as
        an array of shape `(number_of_layers, number_of_nodes)`.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import Lithology
        >>> mg = RasterModelGrid((3, 3))
        >>> z = mg.add_zeros("topographic__elevation", at="node")
        >>> thicknesses = [1, 2, 4, 1]
        >>> ids = [1, 2, 1, 2]
        >>> attrs = {"K_sp": {1: 0.001, 2: 0.0001}}
        >>> lith = Lithology(mg, thicknesses, ids, attrs)
        >>> lith.z_top
        array([[7., 7., 7., 7., 7., 7., 7., 7., 7.],
               [3., 3., 3., 3., 3., 3., 3., 3., 3.],
               [1., 1., 1., 1., 1., 1., 1., 1., 1.],
               [0., 0., 0., 0., 0., 0., 0., 0., 0.]])
        """
        thick = np.broadcast_to(self._layers.thickness, self._layers.z.shape)
        return thick - self._layers.z

    def _check_property_dictionary(self):
        """Check compatibility of Lithology and property dictionary."""
        ids = []
        for at in self._properties:
            ids.extend(self._attrs[at].keys())
        self._ids = frozenset(np.unique(ids))

        for at in self._properties:
            for i in self._ids:
                if i not in self._attrs[at]:
                    raise ValueError(
                        f"A rock type with ID value {i} was specified in Lithology. "
                        f"No value for this ID was provided in property {at}."
                    )

    def _update_surface_values(self):
        """Update Lithology surface values."""
        # Update surface values for each attribute.
        self._grid["node"][self._rock_id_name][:] = self._surface_rock_type
        for at in self._properties:
            self._grid["node"][at][:] = self[at]

    def add_layer(self, thickness, rock_id=None):
        """Add a new layer to Lithology.

        Parameters
        ----------
        thickness : float or `(n_nodes,)` array
            Positive values deposit material on to Lithology while negative
            values erode Lithology.
        rock_id : single value or `n_nodes` long itterable, optional if only erosion occurs
            Rock type ID for new deposits. Can be single value or an number-
            of-nodes array.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import Lithology
        >>> mg = RasterModelGrid((3, 3))
        >>> z = mg.add_zeros("topographic__elevation", at="node")
        >>> thicknesses = [1, 2, 4, 1]
        >>> ids = [1, 2, 1, 2]

        We can instantiate Lithology with rock type properties we know we will
        use in the future.

        >>> attrs = {"K_sp": {1: 0.001, 2: 0.0001, 3: 0.01}}
        >>> lith = Lithology(mg, thicknesses, ids, attrs)

        Add a layer of thickness 3 and rock type 3.

        >>> lith.add_layer(3, rock_id=3)

        The value of `K_sp` at node is now updated to the value of rock type 3

        >>> mg.at_node["K_sp"]
        array([0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01])

        A negative value will erode. We can also pass a `(n_nodes,) long array
        to erode unevenly. If all parts of the layer erode, then no `rock_id`
        needs to be passed.

        >>> erosion_amount = [-2.0, -2.0, -2.0, -4.0, -4.0, -4.0, -6.0, -6.0, -6.0]
        >>> lith.add_layer(erosion_amount)
        >>> mg.at_node["K_sp"]
        array([0.01  , 0.01  , 0.01  , 0.0001, 0.0001, 0.0001, 0.001 ,
               0.001 , 0.001 ])

        Now different layers are exposed at the surface and the value of `K_sp`
        is spatially variable.
        """
        thickness = np.array(thickness)

        # verify that Lithology will still have thickness after change
        if np.any((self._layers.thickness + thickness) <= 0):
            raise ValueError(
                "add_layer will result in Lithology having a thickness of "
                "zero at at least one node."
            )

        # verify that rock type added exists.
        try:
            all_ids_present = self._ids.issuperset(rock_id)
            new_ids = rock_id
        except TypeError:
            all_ids_present = self._ids.issuperset([rock_id])
            new_ids = [rock_id]

        if not all_ids_present:
            missing_ids = set(new_ids).difference(self._ids)

            if np.any(thickness > 0):
                raise ValueError(
                    "Lithology add_layer was given a rock type id that does "
                    "not yet exist and will need to deposit. Use a valid "
                    f"rock type or add_rock_type. {missing_ids}"
                )

        # add_rock_type
        if rock_id is not None:
            # add layer
            attributes = {self._rock_id_name: rock_id}
            self._layers.add(thickness, **attributes)
        else:
            self._layers.add(thickness)

        # update surface rock type
        self._surface_rock_type = self._layers.get_surface_values(self._rock_id_name)

        # update surface values
        self._update_surface_values()

    def add_property(self, attrs):
        """Add new property to Lithology.

        Parameters
        ----------
        attrs : dict
            Rock attribute dictionary for the new property(s).

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import Lithology
        >>> mg = RasterModelGrid((3, 3))
        >>> z = mg.add_zeros("topographic__elevation", at="node")
        >>> thicknesses = [1, 2, 4, 1]
        >>> ids = [1, 2, 1, 2]
        >>> attrs = {"K_sp": {1: 0.001, 2: 0.0001}}
        >>> lith = Lithology(mg, thicknesses, ids, attrs)
        >>> lith.add_property({"D": {1: 0.03, 2: 0.004}})
        >>> lith.tracked_properties
        ['D', 'K_sp']
        >>> mg.at_node["D"]
        array([0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03])
        """
        for at in attrs:
            if at in self._properties:
                raise ValueError(
                    "add_property is trying to add an existing "
                    f"attribute, this is not permitted. {at}"
                )

            new_rids = attrs[at].keys()
            for rid in new_rids:
                if rid not in self._ids:
                    raise ValueError(
                        f"add_property has an attribute({at}) for rock type {rid!s} "
                        "that no other rock type has. This is not permitted."
                    )

            for rid in self._ids:
                if rid not in new_rids:
                    raise ValueError(
                        "add_property needs a value for id {rid!s} and attribute {at}."
                    )

        for at in attrs:
            if at not in self._grid.at_node:
                self._grid.add_empty(at, at="node")
            self._attrs[at] = attrs[at]
            self._properties.append(at)

        # update surface values
        self._update_surface_values()

    def add_rock_type(self, attrs):
        """Add rock type to Lithology.

        Parameters
        ----------
        attrs : dict
            Rock attribute dictionary for the new rock type(s).

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import Lithology
        >>> mg = RasterModelGrid((3, 3))
        >>> z = mg.add_zeros("topographic__elevation", at="node")
        >>> thicknesses = [1, 2, 4, 1]
        >>> ids = [1, 2, 1, 2]
        >>> attrs = {"K_sp": {1: 0.001, 2: 0.0001}}
        >>> lith = Lithology(mg, thicknesses, ids, attrs)
        >>> lith.add_rock_type({"K_sp": {4: 0.03, 6: 0.004}})
        >>> lith.ids
        [1, 2, 4, 6]
        >>> lith.properties
        {'K_sp': {1: 0.001, 2: 0.0001, 4: 0.03, 6: 0.004}}
        """
        # Check that the new rock type has all existing attributes
        for at in self._properties:
            if at not in attrs:
                raise ValueError(f"The new rock type is missing attribute {at!s}.")
        # And no new attributes
        for at in attrs:
            if at not in self._properties:
                raise ValueError(
                    "The new rock type has an attribute (e{at!s}) "
                    "that no other rock type has. This is not permitted."
                )

        new_ids = []
        for at in attrs:
            att_dict = attrs[at]
            rids = att_dict.keys()
            for rid in rids:
                if rid in self._layer_ids:
                    raise ValueError(
                        "Rock type ID {rid!s} for attribute {at!s} "
                        "has already been added. This is not allowed"
                    )
                else:
                    new_ids.append(rid)
                    self._attrs[at][rid] = att_dict[rid]
        self._ids = self._ids.union(new_ids)

        # update surface values
        self._update_surface_values()

    def update_rock_properties(self, at, rock_id, value):
        """Update rock type attribute.

        Parameters
        ----------
        at : str
            Attribute name
        rock_id : value
            Rock type ID
        value : value
            New value for rock type attribute

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import Lithology
        >>> mg = RasterModelGrid((3, 3))
        >>> z = mg.add_zeros("topographic__elevation", at="node")
        >>> thicknesses = [1, 2, 4, 1]
        >>> ids = [1, 2, 1, 2]
        >>> attrs = {"K_sp": {1: 0.001, 2: 0.0001}}
        >>> lith = Lithology(mg, thicknesses, ids, attrs)

        >>> mg.at_node["K_sp"]
        array([0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001])

        >>> lith.update_rock_properties("K_sp", 1, 0.03)

        >>> mg.at_node["K_sp"]
        array([0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03])
        """
        if at not in self._properties:
            raise ValueError(
                f"Lithology cannot update the value of {at!s} as "
                "this attribute does not exist."
            )

        if not self._ids.issuperset([rock_id]):
            raise ValueError(
                f"Lithology cannot update the value of rock type {rock_id!s} "
                f"for attribute {at!s} as this rock type is not yet defined."
            )

        # set the value in the attribute dictionary
        self._attrs[at][rock_id] = value

        # update surface values
        self._update_surface_values()

    def _get_surface_values(self, at):
        """Get surface values for attribute."""
        return np.array(list(map(self._attrs[at].get, self._surface_rock_type)))

    def rock_cube_to_xarray(self, depths):
        """Construct a 3D rock cube of rock type ID as an xarray dataset.

        Create an xarray dataset in (x, y, z) that shows the rock type with
        depth relative to the current topographic surface.

        Here the z dimension is depth relative to the current topographic
        surface, NOT depth relative to an absolute datum.

        Note also that when this method is called, it will construct the current
        values of lithology with depth, NOT the initial values.

        Parameters
        ----------
        depths : array

        Returns
        -------
        ds : xarray dataset
        """
        depths = np.asarray(depths)
        rock_type = self._layers[self._rock_id_name]
        rock_cube = np.empty((depths.size, self._grid.shape[0], self._grid.shape[1]))

        # at each node point, interpolate between ztop/bottomo correct.y
        for sid in range(self._layers.number_of_stacks):
            coord = np.unravel_index(sid, (self._grid.shape[0], self._grid.shape[1]))
            real_layers = self.dz[:, sid] > 0
            f = interp1d(
                np.flipud(self.z_top[real_layers, sid]),
                np.flipud(rock_type[real_layers, sid]),
                kind="previous",
            )
            vals = f(depths)
            rock_cube[:, coord[0], coord[1]] = vals

        ds = xr.Dataset(
            data_vars={
                "rock_type__id": (
                    ("z", "y", "x"),
                    rock_cube,
                    {"units": "-", "long_name": "Rock Type ID Code"},
                )
            },
            coords={
                "x": (
                    ("x"),
                    self._grid.x_of_node.reshape(self._grid.shape)[0, :],
                    {"units": "meters"},
                ),
                "y": (
                    ("y"),
                    self._grid.y_of_node.reshape(self._grid.shape)[:, 1],
                    {"units": "meters"},
                ),
                "z": (
                    ("z"),
                    depths,
                    {"units": "meters", "long_name": "Depth Below Topographic Surface"},
                ),
            },
        )

        return ds

    def run_one_step(self):
        """Update Lithology.

        The ``run_one_step`` method calculates elevation change of the
        Lithology surface (taking into account any advection due to external
        processes) and then either deposits or erodes based on elevation
        change.

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import Lithology
        >>> mg = RasterModelGrid((3, 3))
        >>> z = mg.add_ones("topographic__elevation", at="node")
        >>> thicknesses = [1, 2, 4, 1]
        >>> ids = [1, 2, 1, 2]
        >>> attrs = {"K_sp": {1: 0.001, 2: 0.0001}}
        >>> lith = Lithology(mg, thicknesses, ids, attrs)
        >>> lith.dz
        array([[1., 1., 1., 1., 1., 1., 1., 1., 1.],
               [4., 4., 4., 4., 4., 4., 4., 4., 4.],
               [2., 2., 2., 2., 2., 2., 2., 2., 2.],
               [1., 1., 1., 1., 1., 1., 1., 1., 1.]])
        >>> lith.thickness
        array([8., 8., 8., 8., 8., 8., 8., 8., 8.])

        If we erode the surface, and then update Lithology, the thickness will
        change.

        >>> z -= 0.5
        >>> lith.run_one_step()
        >>> lith.thickness
        array([7.5, 7.5, 7.5, 7.5, 7.5, 7.5, 7.5, 7.5, 7.5])

        The default of Lithology is to use MaterialLayers from the Landlab
        layers submodule. This means that when we erode, we will remove a layer
        from the layers datastructure if it has no material anywhere.

        >>> lith.dz
        array([[1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ],
               [4. , 4. , 4. , 4. , 4. , 4. , 4. , 4. , 4. ],
               [2. , 2. , 2. , 2. , 2. , 2. , 2. , 2. , 2. ],
               [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]])

        We can see the value of the rock type at the surface.

        >>> mg.at_node["rock_type__id"]
        array([1., 1., 1., 1., 1., 1., 1., 1., 1.])

        If you deposit, a valid rock_id must be provided. If the rock type
        is the same as the current surface value everywhere, then the layers
        will be combined. This rock_id can be provided as part of the init of
        Lithology or by setting a property (as shown below).

        >>> z += 1.5
        >>> lith.rock_id = 1
        >>> lith.run_one_step()
        >>> lith.thickness
        array([9., 9., 9., 9., 9., 9., 9., 9., 9.])

        >>> lith.dz
        array([[1., 1., 1., 1., 1., 1., 1., 1., 1.],
               [4., 4., 4., 4., 4., 4., 4., 4., 4.],
               [2., 2., 2., 2., 2., 2., 2., 2., 2.],
               [2., 2., 2., 2., 2., 2., 2., 2., 2.]])

        This contrasts with the behavior of Lithology if we use EventLayers.
        Next we repeat this example with EventLayers. Note that no matter which
        method you use, the values of the model grid fields will be the same.
        These two methods differ only in the details of the data structure they
        use to store the layer information.

        >>> mg = RasterModelGrid((3, 3))
        >>> z = mg.add_ones("topographic__elevation", at="node")
        >>> thicknesses = [1, 2, 4, 1]
        >>> ids = [1, 2, 1, 2]
        >>> attrs = {"K_sp": {1: 0.001, 2: 0.0001}}
        >>> lith = Lithology(mg, thicknesses, ids, attrs, layer_type="EventLayers")
        >>> lith.dz
        array([[1., 1., 1., 1., 1., 1., 1., 1., 1.],
               [4., 4., 4., 4., 4., 4., 4., 4., 4.],
               [2., 2., 2., 2., 2., 2., 2., 2., 2.],
               [1., 1., 1., 1., 1., 1., 1., 1., 1.]])
        >>> lith.thickness
        array([8., 8., 8., 8., 8., 8., 8., 8., 8.])

        If we erode the surface, and then update Lithology, the thickness
        will change. However, with EventLayers, the ``lith.dz`` structure
        will be different. It will have a layer with thickness zero that
        represents the event of erosion.

        >>> z -= 0.5
        >>> lith.run_one_step()
        >>> lith.thickness
        array([7.5, 7.5, 7.5, 7.5, 7.5, 7.5, 7.5, 7.5, 7.5])
        >>> lith.dz
        array([[1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ],
               [4. , 4. , 4. , 4. , 4. , 4. , 4. , 4. , 4. ],
               [2. , 2. , 2. , 2. , 2. , 2. , 2. , 2. , 2. ],
               [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],
               [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]])

        We can see the value of the rock type at the surface. As expected,
        it is just the same as if we used MaterialLayers.

        >>> mg.at_node["rock_type__id"]
        array([1., 1., 1., 1., 1., 1., 1., 1., 1.])

        If you deposit, a valid rock_id must be provided. Unlike
        MaterialLayers, these two layers will not be combined, even if they
        have the same properties.

        >>> z += 1.5
        >>> lith.rock_id = 1
        >>> lith.run_one_step()
        >>> lith.thickness
        array([9., 9., 9., 9., 9., 9., 9., 9., 9.])

        >>> lith.dz
        array([[1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ],
               [4. , 4. , 4. , 4. , 4. , 4. , 4. , 4. , 4. ],
               [2. , 2. , 2. , 2. , 2. , 2. , 2. , 2. , 2. ],
               [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],
               [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],
               [1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5]])
        """
        # calculate amount of erosion
        elevation_change = self._grid["node"]["topographic__elevation"] - (
            self._last_elevation + self.dz_advection
        )

        # add layer
        self.add_layer(elevation_change, rock_id=self.rock_id)

        # update the last elevation.
        self._last_elevation = self._grid["node"]["topographic__elevation"][:].copy()



================================================
File: marine_sediment_transport/__init__.py
================================================
from .simple_submarine_diffuser import SimpleSubmarineDiffuser

__all__ = ["SimpleSubmarineDiffuser"]



================================================
File: marine_sediment_transport/simple_submarine_diffuser.py
================================================
#! /usr/bin/env python
import numpy as np

from landlab.components import LinearDiffuser

_TINY_DIFFUSIVITY = 1.0e-20


class SimpleSubmarineDiffuser(LinearDiffuser):
    r"""
    Transport marine sediment using a water-depth-dependent diffusion model.

    This component models sediment transport as a diffusion process with a
    coefficient that depends on water depth :math:`h` as follows:

    .. math::
        D(h) = D_0 f_1(h) f_2(h)

    Here :math:`D_0` is the maximum value, corresponding to the input
    parameter :code:`shallow_water_diffusivity`.

    The function :math:`f_1(h)` describes the decrease in transport efficiency
    below the wave base depth :math:`h_w`. It is defined as unity for depth
    above the wave base, and as

    .. math::
        f_1(h) = \exp( -(h - h_w) / h_w)

    for :math:`h > h_w`.

    The function :math:`f_2(h)` handles the transition in transport efficiency
    around the shoreline. If :code:`tidal_range`, :math:`R_t`, is zero, then
    :math:`f_2` is set to unity underwater (:math:`h \ge 0`), and a tiny value
    above water (not zero, because that would cause a divide-by-zero error in
    the base class).
    If :math:`R_t > 0`, then a :math:`tanh` function is used to model
    a smooth decrease in :math:`D` from the low to high tide level:

    .. math::
        f_2(h) = (\tanh ( -h / R_t) + 1) / 2

    with an addition tiny value added to locations above water to avoid
    division by zero.

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import SimpleSubmarineDiffuser
    >>> grid = RasterModelGrid((3, 7), xy_spacing=100.0)
    >>> grid.set_closed_boundaries_at_grid_edges(False, True, False, True)
    >>> topo = grid.add_zeros("topographic__elevation", at="node")
    >>> topo[:] = -10.0
    >>> topo[9:14] = [0.0, 10.0, 10.0, 5.0, 5.0]
    >>> ssd = SimpleSubmarineDiffuser(grid, tidal_range=0.0)
    >>> ssd.run_one_step(dt=5.0)
    >>> topo[8:13]
    array([-9.5,  0. ,  9.5, 10. ,  5. ])
    >>> grid.at_node["sediment_deposit__thickness"][8:13]
    array([ 0.5,  0. , -0.5,  0. ,  0. ])
    """

    _name = "SimpleSubmarineDiffuser"

    _time_units = "y"

    _info = {
        "sea_level__elevation": {
            "dtype": "float",
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "grid",
            "doc": "Sea level elevation",
        },
        "topographic__elevation": {
            "dtype": "float",
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",  # and seafloor
        },
        "water__depth": {
            "dtype": "float",
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "depth of water under current sea level",
        },
        "sediment_deposit__thickness": {
            "dtype": "float",
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Thickness of deposition or erosion in latest time step",
        },
    }

    def __init__(
        self,
        grid,
        sea_level=0.0,
        wave_base=60.0,
        shallow_water_diffusivity=100.0,
        tidal_range=2.0,
        **kwds,
    ):
        """
        Parameters
        ----------
        grid: ModelGrid (RasterModelGrid, HexModelGrid, etc.)
            A landlab grid.
        sea_level: float, optional
            The current sea level (m) (default 0)
        wave_base: float, optional
            Wave base (m) (default 60)
        shallow_water_diffusivity: float, optional
            Diffusivity coefficient for shallow water (m2 / y) (default 100)
        tidal_range: float, optional
            Tidal range (m) (default 2)
        """
        self._wave_base = float(wave_base)
        self._sea_level = sea_level
        grid.at_grid["sea_level__elevation"] = sea_level
        self._sea_level = grid.at_grid["sea_level__elevation"]
        self._shallow_water_diffusivity = shallow_water_diffusivity
        self._tidal_range = tidal_range
        if tidal_range > 0.0:
            self._inverse_tidal_range = 1.0 / tidal_range

        if "kd" not in grid.at_node:
            grid.add_zeros("kd", at="node")
        if "sediment_deposit__thickness" not in grid.at_node:
            grid.add_zeros("sediment_deposit__thickness", at="node")
        if "water__depth" not in grid.at_node:
            grid.add_zeros("water__depth", at="node")
        self._depth = grid.at_node["water__depth"]

        self._time = 0.0

        kwds.setdefault("linear_diffusivity", "kd")
        super().__init__(grid, **kwds)

    @property
    def wave_base(self):
        return self._wave_base

    @wave_base.setter
    def wave_base(self, value):
        self._wave_base = float(value)

    @property
    def shallow_water_diffusivity(self):
        return self._shallow_water_diffusivity

    @shallow_water_diffusivity.setter
    def shallow_water_diffusivity(self, value):
        self._shallow_water_diffusivity = float(value)

    @property
    def time(self):
        return self._time

    @property
    def sea_level(self):
        return self.grid.at_grid["sea_level__elevation"]

    @sea_level.setter
    def sea_level(self, sea_level):
        self.grid.at_grid["sea_level__elevation"] = sea_level

    def depth_function(self, water_depth):
        """
        Return weighting factor for transport.

        If there is no tidal range, then the weight factor is 1 if at or
        below sea level, and 0 if above it. If there is a tidal range, then
        a tanh function is used to weight transport across mean sea level, so
        that there is some degree of transport for water depths within the
        tidal range (less above, more below). The nature of the tanh function
        is such that the transport is about 95% of its maximum value at a depth
        of 1.5x the mean tidal range, and 5% of its maximum value at a height
        of 1.5x the mean tidal range above mean sea level.

        Parameters
        ----------
        water_depth : float array
            Depth of water relative to mean sea level (m) (can be negative)

        Returns
        -------
        df : float array
            Weight factor ranging from 0 to 1.
        """
        if self._tidal_range > 0.0:
            df = (np.tanh(self._inverse_tidal_range * water_depth) + 1.0) / 2.0
        else:
            df = 1.0 * (water_depth >= 0.0)
        return df

    def calc_diffusion_coef(self):
        """
        Calculate and store diffusion coefficient values.

        Returns
        -------
        k : float array
            Diffusion coefficient, m2/y
        """
        sea_level = self.grid.at_grid["sea_level__elevation"]
        self._depth[:] = sea_level - self._grid.at_node["topographic__elevation"]

        deep_water = self._depth > self._wave_base
        land = self._depth < 0.0

        k = self.grid.at_node["kd"]
        k[:] = self._shallow_water_diffusivity * self.depth_function(self._depth)
        k[deep_water] *= np.exp(
            -(self._depth[deep_water] - self._wave_base) / self._wave_base
        )
        k[land] += _TINY_DIFFUSIVITY

        return k

    def run_one_step(self, dt):
        """
        Advance by one time step.

        Parameters
        ----------
        dt : float
            Time-step duration (y)
        """
        z_before = self.grid.at_node["topographic__elevation"].copy()

        self.calc_diffusion_coef()

        super().run_one_step(dt)

        depo = self.grid.at_node["sediment_deposit__thickness"]
        depo[:] = self.grid.at_node["topographic__elevation"] - z_before

        self._time += dt



================================================
File: mass_wasting_runout/__init__.py
================================================
from ..mass_wasting_runout.mass_wasting_runout import MassWastingRunout

__all__ = ["MassWastingRunout"]



================================================
File: mass_wasting_runout/mass_wasting_runout.py
================================================
import warnings

import numpy as np
import pandas as pd

from landlab import Component
from landlab.components import FlowDirectorMFD
from landlab.components.mass_wasting_runout.mass_wasting_saver import MassWastingSaver


class MassWastingRunout(Component):
    """a cellular-automata mass wasting runout model that routes an initial mass
    wasting body (e.g., a landslide) through a watershed, determines erosion and
    aggradation depths, evolves the terrain and regolith and tracks attributes of
    the regolith. This model is intended for modeling the runout extent, topographic
    change and sediment transport caused by a mapped landslide(s) or landslides
    inferred from a landslide hazard map.


    Examples
    ----------
    Import necessary packages and components

    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import FlowDirectorMFD
    >>> from landlab.components.mass_wasting_runout import MassWastingRunout

    Define the topographic__elevation field of a 7 columns by 7 rows, 10-meter
    raster model grid

    >>> dem = np.array(
    ...     [
    ...         [10, 8, 4, 3, 4, 7.5, 10],
    ...         [10, 9, 3.5, 4, 5, 8, 10],
    ...         [10, 9, 6.5, 5, 6, 8, 10],
    ...         [10, 9.5, 7, 6, 7, 9, 10],
    ...         [10, 10, 9.5, 8, 9, 9.5, 10],
    ...         [10, 10, 10, 10, 10, 10, 10],
    ...         [10, 10, 10, 10, 10, 10, 10],
    ...     ]
    ... )

    >>> dem = np.hstack(dem).astype(float)
    >>> mg = RasterModelGrid((7, 7), 10)
    >>> _ = mg.add_field("topographic__elevation", dem, at="node")

    Define boundary conditions

    >>> mg.set_closed_boundaries_at_grid_edges(True, True, True, True)

    Add multiflow direction fields, soil thickness (here set to 1 meter)

    >>> fd = FlowDirectorMFD(mg, diagonals=True, partition_method="slope")
    >>> fd.run_one_step()
    >>> nn = mg.number_of_nodes
    >>> depth = np.ones(nn) * 1
    >>> _ = mg.add_field("soil__thickness", depth, at="node")

    Define the initial landslide. Any mass_wasting_id value >1 is considered a
    landslide. The landslide extent is defined by assigining all nodes withing
    the landslide the same mass_wasting_id value.
    Here, the landslide is represented by a single node (node 38), which assigned
    a mass_wasting_id value of 1:

    >>> mg.at_node["mass__wasting_id"] = np.zeros(nn).astype(int)
    >>> mg.at_node["mass__wasting_id"][np.array([38])] = 1

    Add attributes of the regolith as fields of the raster model grid that will
    be tracked by the model. These could be any attribute in which the tracking
    method used by MassWastingRunout reasonably represents movement of the
    attribute. Here we track the particle diameter and organic content
    of the regolith. Note, a particle__diameter field is required if shear stress
    is determined as a function of grain size.

    >>> np.random.seed(seed=7)
    >>> mg.at_node["particle__diameter"] = np.random.uniform(0.05, 0.25, nn)
    >>> mg.at_node["organic__content"] = np.random.uniform(0.01, 0.10, nn)

    Next define parameter values for MassWastingRunout and instantiate the model:

    >>> Sc = [0.03]  # Sc, note: defined as a list (see below)
    >>> qsc = 0.01  # qsc
    >>> k = 0.02  # k
    >>> h_max = 1
    >>> tracked_attributes = ["particle__diameter", "organic__content"]
    >>> example_square_MWR = MassWastingRunout(
    ...     mg,
    ...     critical_slope=Sc,
    ...     threshold_flux=qsc,
    ...     erosion_coefficient=k,
    ...     tracked_attributes=tracked_attributes,
    ...     effective_qsi=True,
    ...     max_flow_depth_observed_in_field=h_max,
    ...     save=True,
    ... )

    Run MassWastingRunout

    >>> example_square_MWR.run_one_step()

    By subtracting the initial DEM from the final DEM, which has evolvod as
    a consequence of the runout, we can see areas of aggradation (positive values)
    and erosion (negative values). Nodes with non-zero topographic change
    represent the runout extent.

    >>> DEM_initial = mg.at_node["topographic__initial_elevation"]
    >>> DEM_final = mg.at_node["topographic__elevation"]
    >>> DEM_final - DEM_initial
    array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
            0.        ,  0.        ,  0.        ,  0.        ,  0.96579201,
            0.52734339, -0.00778869,  0.        ,  0.        ,  0.        ,
            0.        , -0.00594927, -0.12261762, -0.0027898 ,  0.        ,
            0.        ,  0.        ,  0.        , -0.04562554, -0.10973222,
           -0.05776526,  0.        ,  0.        ,  0.        ,  0.        ,
           -0.01225359, -0.07973101, -0.04888238,  0.        ,  0.        ,
            0.        ,  0.        ,  0.        , -1.        ,  0.        ,
            0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
            0.        ,  0.        ,  0.        ,  0.        ])

    See how the landslide removes all of the regolith at node 38 (the negative -1)

    Look at the final spatial distribution of regolith particle diameter.

    >>> mg.at_node["particle__diameter"]
    array([0.06526166, 0.20598376, 0.13768185, 0.19469304, 0.2455979 ,
           0.15769917, 0.15022409, 0.06441023, 0.1036878 , 0.15619144,
           0.17680799, 0.21074781, 0.12618823, 0.06318727, 0.10762912,
           0.23191871, 0.09295928, 0.14042479, 0.23545374, 0.05497985,
           0.17010978, 0.2400259 , 0.09606058, 0.15969798, 0.23182567,
           0.07663389, 0.15468252, 0.20008197, 0.18380265, 0.14355057,
           0.09096982, 0.14815318, 0.12447694, 0.14548023, 0.12317808,
           0.2175836 , 0.2037295 , 0.11279894, 0.        , 0.10520981,
           0.14056859, 0.12059567, 0.18147989, 0.12407022, 0.1418186 ,
           0.19386482, 0.13259837, 0.23128465, 0.08609032])

    Also note that the attribute value is set to zero at any node in which the regolith
    depth is 0.


    References
    ----------
    Keck, J., Istanbulluoglu, E., Campforts, B., Tucker G., Horner-Devine A.,
    A landslide runout model for sediment transport, landscape evolution and hazard
    assessment applications, submitted to Earth Surface Dynamics (2023)

    """

    _name = "MassWastingRunout"

    _unit_agnostic = False

    _info = {
        "mass__wasting_id": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "interger or float id of each mass wasting area is assigned \
                to all nodes representing the mass wasting area.",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": True,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "soil__thickness": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "soil depth to restrictive layer",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__receiver_proportions": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of proportion of flow sent to each receiver.",
        },
        "topographic__steepest_slope": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "The steepest *downhill* slope",
        },
        "particle__diameter": {
            "dtype": float,
            "intent": "inout",
            "optional": True,
            "units": "m",
            "mapping": "node",
            "doc": "representative particle diameter at each node, this might \
            vary with underlying geology, contributing area or field observations",
        },
    }

    def __init__(
        self,
        grid,
        critical_slope=0.05,
        threshold_flux=0.25,
        erosion_coefficient=0.005,
        tracked_attributes=None,
        deposition_rule="critical_slope",
        grain_shear=True,
        effective_qsi=False,
        settle_deposit=False,
        E_constraint=True,
        save=False,
        typical_flow_thickness_of_erosion_zone=2,
        typical_slope_of_erosion_zone=0.15,
        erosion_exponent=0.2,
        max_flow_depth_observed_in_field=None,
        vol_solids_concentration=0.6,
        density_solids=2650,
        density_fluid=1000,
        gravity=9.81,
        dist_to_full_qsc_constraint=0,
        itL=1000,
        run_id=0,
    ):
        """
        Parameters
        ----------
        grid: landlab raster model grid

        critical_slope: list of floats
            critical slope (angle of repose if no cohesion) of mass
            wasting material , L/L list of length 1 for a basin uniform Sc value
            list of length 2 for hydraulic geometry defined Sc, where the first
            and second values in the list are the coefficient and exponent of a
            user defined function for crictical slope that varies with contributing
            area [m2] to a node (e.g., [0.146,0.051] sets Sc<=0.1 at
            contributing area > ~1100 m2).

        threshold_flux: float
            minimum volumetric flux per unit contour width, [L3/L2/iterataion] or
            [L/iteration]. Flux below this threshold stops at the cell as a deposit

        erosion_coefficient: float
            coefficient used to convert total basal shear stress [kPa] of the runout
            material to a scour depth [m]

        tracked_attributes : list of str or None
            A list of the attribute names (strings) that will be tracked by the
            runout model. Attributes in tracked_attributes must also be a field
            on the model grid and names in list must match the grid field names.
            Default is None.

        deposition_rule : str
            Can be either "critical_slope", "L_metric" or "both".
            "critical_slope" is deposition rule used in Keck et al. 2023.
            "L_metric" is a variation of rule described by Campforts et al. 2020.
            "both" uses the minimum value of both rules.
            Default value is "critical_slope".

        grain_shear : bool
            Indicate whether to define shear stress at the base of the runout material
            as a function of grain size using Equation 13 (True) or the depth-slope
            approximation using Equation 12 (False). Default is True.

        effective_qsi : bool
            Indicate whether to limit erosion and aggradation rates to <= the
            erosion and aggradation rates coorisponding to the maximum observed flow
            depth. All results in Keck et al. 2023 use this constraint. Default is True.

        E_constraint : bool
             Indicate if erosion can not simultaneously occur with aggradation. If True,
             aggradation > 0, then erosion = 0. This is True in Keck et al., 2023.
             Default is True.

        settle_deposit : bool
            Indicate whether to allow deposits to settle before the next model iteration
            is implemented. Settlement is determined the critical slope as evaluated from
            the lowest adjacent node to the deposit. This is not used in Keck et al. 2023
            but tends to allow model to better reproduce smooth, evenly sloped deposits.
            Default is False.

        save : bool
            Save topographic elevation of watershed after each model iteration?
            This uses a lot of memory but is helpful for illustrating runout.
            The default is False.


        Other Parameters
        ----------
        These parameters have a lesser impact on model behavior or may not be
        applicable, depending on model run options.

        typical_flow_thickness_of_erosion_zone: float
            field estimated flow thickness in the erosion-dominatedd reaches of
            the runout path [m], used to estimate erosion_coefficient k using
            erosion_coef_k function. Default value: 3

        typical slope_of_erosion_zone: float
            field or remote sensing estimated slope in the scour dominated reach
            of the runout path [L/L], used to estimate erosion_coefficient k using
            erosion_coef_k function. Default value: 0.4

        erosion_exponent: float
            The exponent of equation 11, that scales erosion depth as a function of
            shear stress. Default value: 0.5

        max_flow_depth_observed: float
            Maximum observed flow depth, over the entire
            runout path [m], h_max in equation 24. Only used effective_qsi is True.
            Default value: 4

        vol_solids_concentration: float
            The ratio of the volume of the solids to the total volume of the flow
            mixture. Default value: 0.6

        density solids: float
            The density of the solids [kg/m3]. Default value: 2650

        density fluid: float
            The density of the fluid [kg/m3]. Default value: 1000

        gravity: float
            Acceleration due to gravity [m2/s]. Default value: 9.81

        dist_to_full_qsc_constraint : float
            distance in meters at which qsc is applied to runout. If the landslide
            initiates on relatively flat terrain, it may be difficult to determine
            a qsc value that allows the model start and deposit in a way that matches
            the observed. In Keck et al. 2023, dist_to_full_qsc_constraint = 0, but
            other landslides may need dist_to_full_qsc_constraint = 20 to 50 meters.

        itL : int
            maximum number of iterations the model runs before it
            is forced to stop. The default is 1000. Ideally, if properly parameterized,
            the model should stop on its own. All modeled runout in Keck et al. 2023
            stopped on its own.

        run_id : float, int or str
            label for landslide run, can be the time or some other identifier. This
            can be updated each time model is implemnted with "run_one_step"

        Returns
        -------
        None
        """
        if isinstance(critical_slope, (float, int)):
            critical_slope = (critical_slope,)

        super().__init__(grid)

        if len(critical_slope) > 1:
            self.variable_slpc = True
            self.a = critical_slope[0]
            self.b = critical_slope[1]
        else:
            self.variable_slpc = False
            self.slpc = critical_slope[0]
        self.qsc = threshold_flux
        self.k = erosion_coefficient
        self._tracked_attributes = tracked_attributes
        self.deposition_rule = deposition_rule
        self.grain_shear = grain_shear
        self.effective_qsi = effective_qsi
        self.settle_deposit = settle_deposit
        self.E_constraint = E_constraint
        self.save = save
        self.h = typical_flow_thickness_of_erosion_zone
        self.s = typical_slope_of_erosion_zone
        self.f = erosion_exponent
        self.qsi_max = max_flow_depth_observed_in_field
        if self.effective_qsi and self.qsi_max is None:
            raise ValueError(
                "Need to define the 'max_flow_depth_observed_in_field'"
                " or set effective_qsi to False"
            )
        self.vs = vol_solids_concentration
        self.ros = density_solids
        self.rof = density_fluid
        self.g = gravity
        self.dist_to_full_qsc_constraint = dist_to_full_qsc_constraint
        self.itL = itL
        self.run_id = run_id

        if tracked_attributes:
            self.track_attributes = True

            # check attributes are included in grid
            for key in self._tracked_attributes:
                if not self._grid.has_field(key, at="node"):
                    raise ValueError(f"{key} not included as field in grid")

            # if using grain size dependent erosion, check
            # particle_diameter is included as an attribute
            if (
                self.grain_shear
                and "particle__diameter" not in self._tracked_attributes
            ):
                raise ValueError(
                    "'particle__diameter' not included as field in grid and/or"
                    " key in tracked_attributes"
                )
        else:
            self.track_attributes = False

        # flow routing option
        # 'square_root_of_slope', see flow director
        self.routing_partition_method = "slope"

        # density of runout mixture
        self.ro_mw = self.vs * self.ros + (1 - self.vs) * self.rof
        # number of model iterations needed to reach dist_to_full_qsc_constraint
        self.d_it = int(self.dist_to_full_qsc_constraint / self._grid.dx)

        # define initial topographic + mass wasting thickness topography
        self._grid.at_node["energy__elevation"] = self._grid.at_node[
            "topographic__elevation"
        ].copy()
        self._grid.at_node["topographic__initial_elevation"] = self._grid.at_node[
            "topographic__elevation"
        ].copy()
        # prepare data containers for saving model images and behavior statistics
        if self.save:
            self.saver = MassWastingSaver(self)
            self.saver.prep_data_containers()

    def run_one_step(self):
        """run MWR"""

        # get all nodes that define the mass wasting events
        mask = self._grid.at_node["mass__wasting_id"] > 0

        # separate the mass wasting event nodes into individual events
        self.mw_ids = np.unique(self._grid.at_node["mass__wasting_id"][mask])
        innL = []  # innL is list of lists of nodes in each mass wasting event
        for mw_id in self.mw_ids:
            ls_mask = self._grid.at_node["mass__wasting_id"] == mw_id
            innL.append(np.hstack(self._grid.nodes)[ls_mask])

        # For each mass wasting event in list:
        for mw_i, inn in enumerate(innL):
            mw_id = self.mw_ids[mw_i]
            self._lsvol = (
                self._grid.at_node["soil__thickness"][inn].sum()
                * self._grid.dx
                * self._grid.dy
            )

            # prepare temporary data containers for each mass wasting event mw_i
            if self.save:
                self.saver.prep_mw_data_containers(mw_i, mw_id)

            # Algorithm 1, prepare initial mass wasting material (debritons) for release
            self._prep_initial_mass_wasting_material(inn, mw_i)

            # self.arndn_r[mw_id].append(self.arndn)
            if self.save:
                # save first set of data to reflect scar created by landslide
                self.saver.save_conditions_before_runout(mw_i, mw_id)

            # Algorith 2, now loop through each receiving nodes,
            # determine next set of recieving nodes,
            # repeat until no more receiving nodes (material deposits)
            self.c = 0  # model iteration counter
            while len(self.arn) > 0 and self.c < self.itL:
                # set qsc: the qsc constraint does not fully apply until runout
                # has traveled dist_to_full_qsc_constraint
                if self.d_it == 0:
                    self.qsc_v = self.qsc
                else:
                    self.qsc_v = self.qsc * (min(self.c / self.d_it, 1))

                # temporary data containers for each iteration of the while loop,
                # that store receiving node, flux and attributes to become the
                # input for the next iteration
                self.arndn_ns = np.array([])  # next iteration donor nodes
                self.arn_ns = np.array([])  # next iteration receiver nodes
                self.arqso_ns = np.array([])  # next iteration flux to receiver nodes
                self.arnL = []  # list of receiver nodes
                self.arqsoL = []  # list of flux out
                self.arndnL = []  # list of donor nodes
                if self.track_attributes:
                    self.aratt_ns = dict.fromkeys(
                        self._tracked_attributes, np.array([])
                    )  #
                    self.arattL = dict.fromkeys(self._tracked_attributes, [])

                # for each unique node in receiving node list self.arn
                self.arn_u = np.unique(self.arn).astype(int)  # unique arn list

                # determine the incoming flux to each node in self.arn_u
                self._determine_qsi()

                # update node elevation plus incoming flow thickness
                # this happens even if using topographic__elevation to route so that
                # the thickness of the debris flow is tracked for plotting
                self._update_E_dem()

                # determine erosion, aggradation, qso and attributes,
                # arranged in array nudat
                self._E_A_qso_determine_attributes()

                # from qso and flow direction at node, determine flux and attributes
                # sent to each receiver node
                self._determine_rn_proportions_attributes()

                # update grid field: topographic__elevation with the values in
                # nudat. Do this after directing flow, because assume deposition
                # does not impact flow direction
                self._update_dem()

                # update topographic slope field
                self._update_topographic_slope()

                # update tracked attribute grid fields
                if self._tracked_attributes:
                    for key in self._tracked_attributes:
                        self._update_attribute_at_node(key)

                # optional settlment of deposits and redistribution of attributes
                if self.settle_deposit:
                    self._settle()
                    self._update_topographic_slope()

                # once all nodes in this iteration have been processed, the lists of receiving
                # nodes (arn), donor nodes (arndn, which are the recieving nodes of this step),
                # outgoing node flux (arqso) and node attributes (artt) are updated
                # for the next iteration
                self.arndn = self.arndn_ns.astype(int)
                self.arn = self.arn_ns.astype(int)
                self.arqso = self.arqso_ns  #
                if self.track_attributes:
                    self.aratt = self.aratt_ns

                if self.save:
                    self.saver.save_conditions_after_one_iteration(mw_i, mw_id)

                # update iteration counter
                self.c += 1

    def _prep_initial_mass_wasting_material(self, inn, mw_i):
        """Algorithm 1 - from an initial source area (landslide), prepare the
        initial lists of receiving nodes and incoming fluxes and attributes
        and remove the source material from the DEM

        Parameters
        ----------
        inn: np.array
             node id's that make up the area of the initial mass wasting area
        mw_i: int
            index of the initial mass wasting area (e.g., if there are two landslides
                                                    the first landslide will be mw_i = 0,
                                                    the second will be mw_i = 0)
        """
        # data containers for initial recieving node, outgoing flux and attributes
        rni = np.array([])
        rqsoi = np.array([])
        if self._tracked_attributes:
            att = dict.fromkeys(self._tracked_attributes, np.array([]))

        # order source area nodes from lowest to highest elevation
        node_z = self._grid.at_node.dataset["topographic__elevation"][inn]
        zdf = pd.DataFrame({"nodes": inn, "z": node_z})
        zdf = zdf.sort_values("z")

        for ci, ni in enumerate(zdf["nodes"].values):
            # regolith (soil) thickness at node. soil thickness in source area
            # represents landslide thickness
            s_t = self._grid.at_node.dataset["soil__thickness"].values[ni]

            # remove soil (landslide) thickness at node
            self._grid.at_node.dataset["topographic__elevation"][ni] = (
                self._grid.at_node.dataset["topographic__elevation"][ni] - s_t
            )

            # update soil thickness at node (now = 0)
            self._grid.at_node["soil__thickness"][ni] = (
                self._grid.at_node["soil__thickness"][ni] - s_t
            )

            if (
                ci > 0
            ):  # use surface slope for first node to start movement of landslide
                # for all other nodes, update slope to reflect material removed from DEM
                self._update_topographic_slope()

            # get receiving nodes of node ni in mw index mw_i
            rn = self._grid.at_node.dataset["flow__receiver_node"].values[ni]
            rn = rn[np.where(rn != -1)]

            # receiving proportion of qso from cell n to each downslope cell
            rp = self._grid.at_node.dataset["flow__receiver_proportions"].values[ni]
            rp = rp[np.where(rp > 0)]  # only downslope cells considered

            # initial mass wasting thickness
            imw_t = s_t
            # get flux out of node ni
            qso = imw_t
            # divide into proportions going to each receiving node
            rqso = rp * qso

            if self._tracked_attributes:
                # get initial mass wasting attributes moving (out) of node ni
                self.att_ar_out = {}
                for key in self._tracked_attributes:
                    att_val = self._grid.at_node.dataset[key].values[ni]
                    # particle diameter to each recieving node
                    self.att_ar_out[key] = np.ones(len(rqso)) * att_val

                    # attribute value is zero at node after reglith leaves
                    self._grid.at_node[key][ni] = 0

                    att[key] = np.concatenate((att[key], self.att_ar_out[key]), axis=0)

            # append receiving node ids, fluxes and attributes to initial lists
            rni = np.concatenate((rni, rn), axis=0)
            rqsoi = np.concatenate((rqsoi, rqso), axis=0)

        self.arndn = np.ones([len(rni)]) * np.nan
        self.arn = rni
        self.arqso = rqsoi
        if self._tracked_attributes:
            self.aratt = att

    def _E_A_qso_determine_attributes(self):
        """mass conservation at a grid cell, implemented using the EAqA
        function below.
        """
        self.D_L = []  # list of deposition depths, if the _settle function is

        # implemented, this is used to determine settlement after deposition
        def EAqA(qsi_r):
            """function for iteratively determining Erosion and Aggradiation depths,
            the outgoing flux (qso) and Attribute values at each affected node and
            attribute values of the outgoing flux.

            thoughts for speeding this up: use cython or try to restructure
            EAqA function so that it can be applied with vector operations,
            then filter all changes to correct cells where computations
            should not have occurred

            Parameters
            ----------
            qsi_r: np array
                a row of the self.qsi_dat array

            """

            n = qsi_r[0]
            qsi = qsi_r[1]

            # maximum slope at node n
            slpn = self._grid.at_node["topographic__steepest_slope"][n].max()

            if self.effective_qsi:
                qsi_ = min(qsi, self.qsi_max)
            else:
                qsi_ = qsi

            # look up critical slope at node n
            if (
                self.variable_slpc
            ):  # if option 1, critical slope is not constant but depends on location
                self.slpc = self.a * self._grid.at_node["drainage_area"][n] ** self.b

            # incoming attributes (weighted average)
            if self._tracked_attributes:
                att_in = self._attributes_in(n, qsi)
            else:
                att_in = None

            rn_g = self._grid.at_node.dataset["flow__receiver_node"].values[n]
            rn_g = rn_g[np.where(rn_g != -1)]

            # if qsi less qsc (equation 1)
            if qsi <= (self.qsc_v):
                A = qsi
                qso = 0
                E = 0
                deta = A
                if self._tracked_attributes:
                    att_up = dict.fromkeys(self._tracked_attributes, 0)
                else:
                    att_up = None
                Tau = 0
                u = 0

            else:
                # aggradation
                A = min(qsi, self._aggradation(qsi, slpn, n))
                if A > 0 and self.E_constraint:
                    E = 0
                    if self._tracked_attributes:
                        att_up = dict.fromkeys(self._tracked_attributes, 0)
                    else:
                        att_up = None
                    Tau = 0
                    u = 0
                else:
                    # erosion
                    E, att_up, Tau, u = self._erosion(n, qsi_, slpn, att_in=att_in)

                ## flux out
                qso = qsi - A + E
                # small qso are considered zero
                qso = np.round(qso, decimals=8)

                # chage elevation
                deta = A - E

            # model behavior tracking
            if self.save:
                self.saver.save_flow_stats(E, A, qsi, slpn, Tau, u)

            # updated attribute values at node
            if self._tracked_attributes:
                n_att = self._attributes_node(n, att_in, E, A)
            else:
                n_att = None

            # list of deposition depths at cells in iteration
            self.D_L.append(A)

            # n_att, att_up, att_in are dictionaries of values of each attribute at
            # the node after erosion and deposition, in the eroded material before depostion
            # and incoming material in qsi respectively
            return deta, qso, qsi, E, A, n_att, att_up, att_in

        # apply EAqA function to all unique nodes in arn (arn_u)
        # create nudat, an np.array of data for updating fields at each node
        ll = np.array([EAqA(r) for r in self.qsi_dat], dtype=object)
        arn_ur = np.reshape(self.qsi_dat[:, 0], (-1, 1))
        self.nudat = np.concatenate((arn_ur, ll), axis=1)

    def _determine_rn_proportions_attributes(self):
        """determine how outgoing flux is partitioned to downslope cells and
        attributes of each parition"""

        def rn_proportions_attributes(nudat_r):
            n = nudat_r[0]
            qso = nudat_r[2]
            qsi = nudat_r[3]
            E = nudat_r[4]
            A = nudat_r[5]

            att_up = nudat_r[7]
            att_in = nudat_r[8]

            # get donor node ids
            dn = self.arndn[self.arn == n]

            # get receiver node idds
            rn = self._grid.at_node.dataset["flow__receiver_node"].values[n]
            rn_ = rn.copy()

            # remove node n and delivery nodes from the receiver node list
            rn_ = rn_[np.where(rn_ != -1)]
            rn_ = rn_[~np.isin(rn_, dn)]

            # these constraints needed to avoid infinite loop
            if qso > 0 and n not in self._grid.boundary_nodes:
                if len(rn_) < 1:  # if no receiver nodes, flux stays at node n
                    rp = np.array([1])
                    rn = np.array([n])
                    rqso = rp * qso
                else:  # flux is proportioned to remaining receiver nodes
                    rp = self._grid.at_node.dataset[
                        "flow__receiver_proportions"
                    ].values[n]
                    rp = rp[np.isin(rn, rn_)]
                    rp = rp / rp.sum()
                    rn = rn_
                    rqso = rp * qso

                # create donor node array, all values are donor node,
                # length equal to number of receiver nodes
                rndn = (np.ones(len(rn)) * n).astype(int)

                # outgoing attributes
                if self._tracked_attributes:
                    att_out = self._attribute_out(att_up, att_in, qsi, E, A)

                    for key in self._tracked_attributes:
                        ratt = np.ones(len(rqso)) * att_out[key]
                        self.aratt_ns[key] = np.concatenate(
                            (self.aratt_ns[key], ratt), axis=0
                        )  # next step receiving node incoming particle diameter list
                        self.arattL[key].append(ratt)

                # store receiving nodes and fluxes in temporary arrays
                self.arndn_ns = np.concatenate(
                    (self.arndn_ns, rndn), axis=0
                )  # next iteration donor nodes
                self.arn_ns = np.concatenate(
                    (self.arn_ns, rn), axis=0
                )  # next iteration receiving nodes
                self.arqso_ns = np.concatenate(
                    (self.arqso_ns, rqso), axis=0
                )  # next iteration qsi
                self.arnL.append(rn)
                self.arqsoL.append(rqso)
                self.arndnL.append(rndn)

        [rn_proportions_attributes(r) for r in self.nudat]

    def _determine_qsi(self):
        """determine flux of incoming material (qsi) to a node.
        returns self.qsi_dat: np array of receiving nodes [column 0],
        and qsi to those nodes [column 1]
        """

        def _qsi(n):
            """sum the incoming flux to node n"""
            qsi = np.sum(self.arqso[self.arn == n])
            return qsi

        ll = np.array([_qsi(n) for n in self.arn_u], dtype=object)
        ll = np.reshape(ll, (-1, 1))
        arn_ur = np.reshape(self.arn_u, (-1, 1))
        self.qsi_dat = np.concatenate((arn_ur, ll), axis=1)

    def _update_E_dem(self):
        """update energy__elevation"""
        n = self.qsi_dat[:, 0].astype(int)
        qsi = self.qsi_dat[:, 1]
        # energy elevation is equal to the topographic elevation plus qsi
        self._grid.at_node["energy__elevation"] = self._grid.at_node[
            "topographic__elevation"
        ].copy()
        self._grid.at_node["energy__elevation"][n] = (
            self._grid.at_node["energy__elevation"].copy()[n] + qsi
        )

    def _update_energy_slope(self):
        """updates the topographic__slope and flow directions grid fields using the
        energy__elevation field. This function is presently not used but may be useful
        for future implementations of MWR"""
        fd = FlowDirectorMFD(
            self._grid,
            surface="energy__elevation",
            diagonals=True,
            partition_method=self.routing_partition_method,
        )
        fd.run_one_step()

    def _update_dem(self):
        """updates the topographic elevation of the landscape dem and soil
        thickness fields"""
        n = self.nudat[:, 0].astype(int)
        deta = self.nudat[:, 1]
        self._grid.at_node["soil__thickness"][n] = (
            self._grid.at_node["soil__thickness"][n] + deta
        )
        self._grid.at_node["topographic__elevation"][n] = (
            self._grid.at_node["topographic__elevation"][n] + deta
        )

    def _update_topographic_slope(self):
        """updates the topographic__slope and flow directions fields using the
        topographic__elevation field"""
        fd = FlowDirectorMFD(
            self._grid,
            surface="topographic__elevation",
            diagonals=True,
            partition_method=self.routing_partition_method,
        )
        fd.run_one_step()

    def _update_attribute_at_node(self, key):
        """for each unique node in receiving node list, update the attribute
        using attribute value determined in the _E_A_qso_determine_attributes method

        Parameters
        ----------
        key: string
            one of the tracked attributes
        """
        n = self.nudat[:, 0].astype(int)
        new_node_pd = np.array([d[key] for d in self.nudat[:, 6]])
        if np.isnan(np.sum(new_node_pd)):
            raise ValueError(f"{key} is {new_node_pd}")
        self._grid.at_node[key][n] = new_node_pd

    def _settle(self):
        """for each unique node in receiving node list, after erosion, aggradation
        and change in node elevation have been determined, check that the height of the node
        is not greater than permitted by angle of repose/critical slope as evaluated from
        the lowest cell. Note, slope is not updated in this function. It is updated
        simultaneously at a later stage during the iteration.
        """
        # for each node in the list, use the slope field, computed from the previous
        # iteration, to compute settlment and settlment direction to adjacent cells
        for ii, n in enumerate(self.arn_u):
            if self.D_L[ii] > 0:  # only settle if node has had deposition...use dif?
                rn = self._grid.at_node.dataset["flow__receiver_node"].values[n]
                # slope to all receiving cells
                slpn = self._grid.at_node["topographic__steepest_slope"][n]

                # only consider downslope cells
                slpn = slpn[np.where(rn != -1)]
                rn = rn[np.where(rn != -1)]

                # critical slope
                if self.variable_slpc:
                    self.slpc = (
                        self.a * self._grid.at_node["drainage_area"][n] ** self.b
                    )

                # only consider all cells that slope > Sc
                rn = rn[slpn > self.slpc]
                slpn = slpn[slpn > self.slpc]

                # if slope to downlsope nodes > Sc, adjust elevation of node n
                if len(rn) >= 1:
                    # destribute material to downslope nodes based on weighted
                    # average slope (same as multiflow direciton proportions,
                    # but here only determined for downslope nodes in which
                    # S  > Sc )
                    sslp = sum(slpn)
                    pp = slpn / sslp

                    # determine the total flux sent to S > Sc downslope cells
                    # mean/min downslope cell elevation
                    zo = self._grid.at_node["topographic__elevation"][rn].min()

                    # node n elevation
                    zi = self._grid.at_node["topographic__elevation"][n]

                    # height of node n using Sc*dx above min downslope elevation
                    slp_h = self.slpc * self._grid.dx

                    # out going sediment depth, determined as half the depth
                    # above the critical slope
                    qso_s = (zi - (zo + slp_h)) / 2

                    if qso_s < 0:  # no negative (this shouldn't be needed because only
                        # nodes greater than slpc considered)
                        qso_s = 0

                    if qso_s > self.D_L[ii]:  # settlement out can not exceed deposit
                        qso_s = self.D_L[ii]

                    qso_s_i = qso_s * pp  # proportion sent to each receiving cell

                    # update the topographic elevation
                    self._grid.at_node["topographic__elevation"][n] = (
                        self._grid.at_node["topographic__elevation"][n] - qso_s
                    )
                    self._grid.at_node["topographic__elevation"][rn] = (
                        self._grid.at_node["topographic__elevation"][rn] + qso_s_i
                    )

                    # update the soil thickness
                    self._grid.at_node["soil__thickness"][n] = (
                        self._grid.at_node["soil__thickness"][n] - qso_s
                    )
                    self._grid.at_node["soil__thickness"][rn] = (
                        self._grid.at_node["soil__thickness"][rn] + qso_s_i
                    )

                    # update tracked attributes for sediment movement during settlement
                    if self._tracked_attributes:
                        # create the att_in dict, this is the same for each of the rn
                        att_in = {}
                        for key in self._tracked_attributes:
                            att_in[key] = self._grid.at_node[key][n]
                        for v, n_ in enumerate(rn):
                            A = qso_s_i[v]
                            n_att_d_ = self._attributes_node(n_, att_in, 0, A)
                            for key in self._tracked_attributes:
                                self._grid.at_node[key][n_] = n_att_d_[key]

    def _erosion(self, n, depth, slpn, att_in=None):
        """if self.grain_shear is True, determines the erosion depth using
        equation (13), otherwise uses equation (12).

        Parameters
        ----------
        n : int
            node id
        depth : float
            erosion depth
        slpn : float
            slope in [l/L]
        att_in: dict
            dictionary of the value of each attribute, this function
            only uses particle__diameter

        Returns
        -------
        E : float
            erosion depth [L]
        att_up : dict
            dictionary of the value of each attribute at node n
        Tau : float
            basal shear stress [Pa]
        u : float
            flow velocity [m/s]
        """
        theta = np.arctan(slpn)  # convert tan(theta) to theta
        # attributes of eroded material
        if self._tracked_attributes:
            att_up = {}
            for key in self._tracked_attributes:
                att_up[key] = self._grid.at_node[key][n]
        else:
            att_up = None

        if self.grain_shear:
            # shear stress approximated as a power function of inertial shear stress
            Dp = att_in["particle__diameter"]
            if depth < Dp:  # grain size dependent erosion breaks if depth<Dp
                Dp = depth * 0.99
            u = flow_velocity(Dp, depth, slpn, self.g)

            Tau = shear_stress_grains(self.vs, self.ros, Dp, depth, slpn, self.g)

            Ec = self._grid.dx * erosion_rate(self.k, Tau, self.f, self._grid.dx)

        else:
            # quasi-static approximation
            Tau = self.ro_mw * self.g * depth * (np.sin(theta))
            Ec = self.k * (Tau) ** self.f
            u = np.nan

        dmx = self._grid.at_node["soil__thickness"][n]

        E = min(dmx, Ec)

        return (E, att_up, Tau, u)

    def _aggradation(self, qsi, slpn, n):
        """determine aggradation depth as a function of a threshold-slope,
        L*qsi or a function of both. Where the L metric is computed following
        Campforts, et al., 2020 but is expressed as 1-(slpn/slpc)**2 rather
        than dx/(1-(slpn/slpc)**2)

        Parameters
        ----------
        qsi : float
            incoming flux [l3/iteration/l2]
        slpn : float
            slope at node in [L/L]
        n : int
            node id

        Returns
        -------
        A : float
            aggradation depth [L]
        """

        if self.deposition_rule == "L_metric":
            A = self._deposit_L_metric(qsi, slpn)
        elif self.deposition_rule == "critical_slope":
            A = self._deposit_friction_angle(qsi, n)
        elif self.deposition_rule == "both":
            A_L = self._deposit_L_metric(qsi, slpn)
            A_f = self._deposit_friction_angle(qsi, n)
            A = min(A_L, A_f)

        return A

    def _determine_zo(self, n, zi, qsi):
        """determine the minimum elevation of the adjacent nodes. If all adjacent
        nodes are higher than the elevation of the node + qsi, zo is set to zi

        Parameters
        ----------
        n : int
            node id
        zi : float
            topographic elevation at node n (eta_n)
        qsi : float
            incoming flux [l3/iteration/l2]

        Returns
        -------
        zo : float
            topographic elevation of the lowest elevation node [l],
            adjacent to node n
        """

        # get adjacent nodes
        adj_n = np.hstack(
            (
                self._grid.adjacent_nodes_at_node[n],
                self._grid.diagonal_adjacent_nodes_at_node[n],
            )
        )

        # exclude closed boundary nodes
        adj_n = adj_n[~np.isin(adj_n, self._grid.closed_boundary_nodes)]

        # elevation of flow surface at node... may not need this
        ei = qsi + zi

        # nodes below elevation of node n
        rn_e = adj_n[self._grid.at_node["topographic__elevation"][adj_n] < ei]

        if len(rn_e) > 0:
            zo = self._grid.at_node["topographic__elevation"][rn_e].min()

        else:  # an obstruction in the DEM
            zo = zi
        return zo

    def _deposit_L_metric(self, qsi, slpn):
        """
        determine the L metric similar to Campforts et al. (2020)

        Parameters
        ----------
        qsi : float
            in coming flux per unit contour width
        slpn : float
            slope of node, measured in downslope direction (downslope is postive)

        Returns
        -------
        A_L : float
            aggradation depth [L]
        """
        Lnum = np.max([(1 - (slpn / self.slpc) ** 2), 0])
        A_L = qsi * Lnum

        return A_L

    def _deposit_friction_angle(self, qsi, n):
        """determine deposition depth following equations 4 though 9

        Parameters
        ----------
        qsi : float
            incoming flux [l3/iteration/l2]
        n : int
            node id

        Returns
        -------
        A_f : float
            aggradation depth [L]
        """
        slp_h = self.slpc * self._grid.dx
        zi = self._grid.at_node["topographic__elevation"][n]
        zo = self._determine_zo(n, zi, qsi)
        rule = (zi - zo) <= (slp_h)

        def eq(qsi, zo, zi, slp_h):
            dx = self._grid.dx
            sc = self.slpc
            s = (zi - zo) / dx
            sd = sc - s
            D1 = sc * dx / 2
            a = 0.5 * dx * sd
            b = D1 - 0.5 * dx * sd
            c = -qsi
            N1 = -b + (((b**2) - 4 * a * c) ** 0.5) / (2 * a)
            N2 = -b - (((b**2) - 4 * a * c) ** 0.5) / (2 * a)
            ndn = np.round(max([N1, N2, 1]))  # aggradation on at least one node
            A = min((1 / ndn) * qsi + ((ndn - 1) / 2) * dx * sd, qsi)
            return A

        if rule:
            A_f = eq(qsi, zo, zi, slp_h)
        else:
            A_f = 0

        if A_f < 0:
            warnings.warn(
                f"negative aggradation!! n={n}, qsi={qsi}, A_f={A_f}, zo={zo}, zi={zi}",
                stacklevel=2,
            )
            # raise(ValueError)
        return A_f

    def _attributes_in(self, n, qsi):
        """determine the weighted average attribute value of the incoming
        flow

        Parameters
        ----------
        n : int
            node id
        qsi : float
            incoming flux [l3/iteration/l2]

        Returns
        -------
        att_in: dict
            dictionary of each attribute value flowing into the node
        """
        if qsi == 0:
            att_in = dict.fromkeys(self._tracked_attributes, 0)
        elif (np.isnan(qsi)) or (np.isinf(qsi)):
            msg = "in-flowing flux is nan or inf"
            raise ValueError(msg)
        else:
            att_in = {}
            for key in self._tracked_attributes:
                att_in[key] = np.sum(
                    (self.aratt[key][self.arn == n]) * (self.arqso[self.arn == n]) / qsi
                )
        return att_in

    def _attributes_node(self, n, att_in, E, A):
        """determine the weighted average attributes of the newly aggraded material
        + the inplace regolith

        Parameters
        ----------
        n : int
            node id
        att_in: dict
            dictionary of the value of each attribute flowing into the node
        E : float
            erosion depth [L]
        A : float
            aggradation depth [L]

        Returns
        -------
        n_att_d: dict
            dictionary of each attribute value at the node after erosion
            and aggradation
        """

        def weighted_avg_at_node(key):
            """determine weighted average attribute at the node. If all soil
            is eroded, attribute value is zero"""
            if A + self._grid.at_node["soil__thickness"][n] - E > 0:
                inatt = self._grid.at_node[key][n]
                n_att = (
                    inatt * (self._grid.at_node["soil__thickness"][n] - E)
                    + att_in[key] * A
                ) / (A + self._grid.at_node["soil__thickness"][n] - E)
            else:
                n_att = 0
            if (n_att < 0) or (np.isnan(n_att)) or (np.isinf(n_att)):
                msg = "node particle diameter is negative, nan or inf"
                raise ValueError(msg)
            return n_att

        n_att_d = {}
        for key in self._tracked_attributes:
            n_att_d[key] = weighted_avg_at_node(key)

        return n_att_d

    def _attribute_out(self, att_up, att_in, qsi, E, A):
        """determine the weighted average attributes of the outgoing
        flux

        Parameters
        ----------
        att_up: dict
            dictionary of each attribute value at the node before erosion
            or aggradation
        att_in: dict
            dictionary of each attribute value flowing into the node
        qsi : float
            incoming flux [l3/iteration/l2]
        E : float
            erosion depth [L]
        A : float
            aggradation depth [L]

        Returns
        -------
        att_out: dict
            dictionary of each attribute value flowing out of the node
        """
        att_out = {}
        for key in self._tracked_attributes:
            att_out[key] = np.sum(
                (att_up[key] * E + att_in[key] * (qsi - A)) / (qsi - A + E)
            )
            check_val = att_out[key]
            if (check_val <= 0) or (np.isnan(check_val)) or (np.isinf(check_val)):
                msg = f"out-flowing {key} is zero, negative, nan or inf"
                raise ValueError(msg)
        return att_out


def flow_velocity(Dp, h, s, g):
    us = (g * h * s) ** 0.5
    u = us * 5.75 * np.log10(h / Dp)
    return u


def shear_stress_grains(vs, ros, Dp, h, s, g):
    theta = np.arctan(s)
    phi = np.arctan(0.32)
    u = flow_velocity(Dp, h, s, g)
    dudz = u / h
    Tcn = np.cos(theta) * vs * ros * (Dp**2) * (dudz**2)
    tau = Tcn * np.tan(phi)
    return tau


def shear_stress_static(vs, ros, rof, h, s, g):
    theta = np.arctan(s)
    rodf = vs * ros + (1 - vs) * rof
    tau = rodf * g * h * (np.sin(theta))
    return tau


def erosion_coef_k(E_l, tau, f, dx):
    k = E_l * dx / (tau**f)
    return k


def erosion_rate(k, tau, f, dx):
    E_l = (k * tau**f) / dx
    return E_l



================================================
File: mass_wasting_runout/mass_wasting_saver.py
================================================
class MassWastingSaver:
    """This class is instantiated and called by MassWastingRunout. It saves
    MWR model ouput. It is only called in MassWastingRunout if save = True"""

    def __init__(self, MassWastingRunout):
        self.MWR = MassWastingRunout

    def prep_data_containers(self):
        # lists and dictionaries for tracking model behavior
        self.EL = []  # entrainment depth / regolith depth
        self.AL = []  # aggradation (deposition) depth
        self.qsiL = []  # incoming flux (qsi)
        self.TauL = []  # basal shear stress
        self.slopeL = []  # slope
        self.velocityL = []  # velocity (if computed)
        self.arqso_r = {}  # flux out
        self.arn_r = {}  # receiver nodes
        self.arndn_r = {}  # donar nodes
        self.aratt_r = {}  # arriving attributes
        self.flowing_volume = (
            {}
        )  # the total volume [m3] of the mobilized runout material
        # dictionaries, that save the entire model grid field each model iteration
        # for all fields listed below
        # usefull for creating movies of how the flow and terrain evolve
        self.runout_evo_maps = {}  # runout material + topographic__elevation
        self.topo_evo_maps = {}  # topographic__elevation
        self.att_r = {}  # attribute value
        self.st_r = {}  # soil__thickness
        self.tss_r = {}  # topographic__steepest_slope
        self.frn_r = {}  # flow__receiver_node
        self.frp_r = {}  # 'flow__receiver_proportions'

    def prep_mw_data_containers(self, mw_i, mw_id):
        # containeers for each unique mass wasting ID
        self.runout_evo_maps[mw_i] = {}
        self.topo_evo_maps[mw_i] = {}
        self.flowing_volume[mw_id] = []
        self.st_r[mw_id] = []
        self.tss_r[mw_id] = []
        self.frn_r[mw_id] = []
        self.frp_r[mw_id] = []
        self.arqso_r[mw_id] = []
        self.arn_r[mw_id] = []
        self.arndn_r[mw_id] = []
        if self.MWR.track_attributes:
            self.att_r[mw_id] = dict.fromkeys(
                self.MWR._tracked_attributes, []
            )  # this becomes the data container for each attribute
            self.aratt_r[mw_id] = dict.fromkeys(self.MWR._tracked_attributes, [])

    def save_conditions_before_runout(self, mw_i, mw_id):
        # save first set of data to reflect scar/depression in DEM created by
        # mass wasting source area
        self.runout_evo_maps[mw_i][0] = self.MWR._grid.at_node[
            "energy__elevation"
        ].copy()
        self.topo_evo_maps[mw_i][0] = self.MWR._grid.at_node[
            "topographic__elevation"
        ].copy()
        self.flowing_volume[mw_id].append(0)
        if self.MWR.track_attributes:
            for key in self.MWR._tracked_attributes:
                self.att_r[mw_id][key].append(
                    self.MWR._grid.at_node[key].copy()
                )  # for each attribute, a copy of entire grid
                self.aratt_r[mw_id][key].append(self.MWR.aratt)
        self.st_r[mw_id].append(self.MWR._grid.at_node["soil__thickness"].copy())
        self.tss_r[mw_id].append(
            self.MWR._grid.at_node["topographic__steepest_slope"].copy()
        )
        self.frn_r[mw_id].append(self.MWR._grid.at_node["flow__receiver_node"].copy())
        self.frp_r[mw_id].append(
            self.MWR._grid.at_node["flow__receiver_proportions"].copy()
        )
        self.arqso_r[mw_id].append(self.MWR.arqso)
        self.arn_r[mw_id].append(self.MWR.arn)
        self.arndn_r[mw_id].append(self.MWR.arndn)

    def save_conditions_after_one_iteration(self, mw_i, mw_id):
        DEMf = self.MWR._grid.at_node["topographic__elevation"].copy()
        DEMdf_r = DEMf - self.MWR._grid.at_node["topographic__initial_elevation"]
        self.flowing_volume[mw_id].append(
            DEMdf_r.sum() * self.MWR._grid.dx * self.MWR._grid.dy
        )
        self.runout_evo_maps[mw_i][self.MWR.c + 1] = self.MWR._grid.at_node[
            "energy__elevation"
        ].copy()
        self.runout_evo_maps[mw_i][self.MWR.c + 1] = self.MWR._grid.at_node[
            "energy__elevation"
        ].copy()
        self.topo_evo_maps[mw_i][self.MWR.c + 1] = self.MWR._grid.at_node[
            "topographic__elevation"
        ].copy()
        if self.MWR.track_attributes:
            for key in self.MWR._tracked_attributes:
                self.att_r[mw_id][key].append(self.MWR._grid.at_node[key].copy())
                self.aratt_r[mw_id][key].append(self.MWR.arattL)
        self.st_r[mw_id].append(self.MWR._grid.at_node["soil__thickness"].copy())
        self.tss_r[mw_id].append(
            self.MWR._grid.at_node["topographic__steepest_slope"].copy()
        )
        self.frn_r[mw_id].append(self.MWR._grid.at_node["flow__receiver_node"].copy())
        self.frp_r[mw_id].append(
            self.MWR._grid.at_node["flow__receiver_proportions"].copy()
        )
        self.arqso_r[mw_id].append(self.MWR.arqsoL)
        self.arn_r[mw_id].append(self.MWR.arnL)
        self.arndn_r[mw_id].append(self.MWR.arndn)

    def save_flow_stats(self, E, A, qsi, slpn, Tau, u):
        self.EL.append(E)
        self.AL.append(A)
        self.qsiL.append(qsi)
        self.slopeL.append(slpn)  # slope
        self.TauL.append(Tau)
        self.velocityL.append(u)  # velocity (if any)



================================================
File: network_sediment_transporter/README.md
================================================
# NetworkSedimentTransporter: move sediment parcels in a river network.

The NetworkSedimentTransporter is a lagrangian model for sediment transport on a river network. This component is the subject of a forthcoming Journal of Open Source Software submission.

## Documentation and installation

Landlab documentation is hosted on this [ReadTheDocs page](https://landlab.csdms.io/),
including instructions to install Landlab. NetworkSedimentTransporter is installed with
Landlab.

NetworkSedimentTransporter documentation is located [here](https://landlab.csdms.io/generated/api/landlab.components.network_sediment_transporter.network_sediment_transporter.html).

## NetworkSedimentTransporter tutorial

Three tutorials exist on NetworkSedimentTransporter. They are Jupyter notebooks accessible in the Landlab notebooks. The following are links to Binder instances of the notebooks. If instead you want to run the notebooks locally, you can clone the landlab repository and find them in the directory `landlab/notebooks/tutorials/network_sediment_transporter`.

- [Part  1: Introduction with a synthetic network](https://mybinder.org/v2/gh/landlab/landlab/release?filepath=notebooks/tutorials/network_sediment_transporter/network_sediment_transporter.ipynb)
- [Part  2: Using a shapefile-based river network](https://mybinder.org/v2/gh/landlab/landlab/release?filepath=notebooks/tutorials/network_sediment_transporter/network_sediment_transporter_shapefile_network.ipynb)
- [Part  3: Plotting options](https://mybinder.org/v2/gh/landlab/landlab/release?filepath=notebooks/tutorials/network_sediment_transporter/network_plotting_examples.ipynb)

The index of all Landlab tutorials on Binder can bee found [here](https://mybinder.org/v2/gh/landlab/landlab/release?filepath=notebooks/welcome.ipynb) using Binder.

## Get or give help

[Open an Issue here](https://github.com/landlab/landlab/issues) where we can
respond to your questions, comments, issues, ideas, or any identified bugs
related to Landlab including NetworkSedimentTransporter.



================================================
File: network_sediment_transporter/__init__.py
================================================
from landlab.components.network_sediment_transporter.bed_parcel_initializers import (
    BedParcelInitializerArea,
)
from landlab.components.network_sediment_transporter.bed_parcel_initializers import (
    BedParcelInitializerDepth,
)
from landlab.components.network_sediment_transporter.bed_parcel_initializers import (
    BedParcelInitializerDischarge,
)
from landlab.components.network_sediment_transporter.bed_parcel_initializers import (
    BedParcelInitializerUserD50,
)
from landlab.components.network_sediment_transporter.network_sediment_transporter import (
    NetworkSedimentTransporter,
)
from landlab.components.network_sediment_transporter.sediment_pulser_at_links import (
    SedimentPulserAtLinks,
)
from landlab.components.network_sediment_transporter.sediment_pulser_each_parcel import (
    SedimentPulserEachParcel,
)

__all__ = [
    "NetworkSedimentTransporter",
    "BedParcelInitializerDischarge",
    "BedParcelInitializerDepth",
    "BedParcelInitializerArea",
    "BedParcelInitializerUserD50",
    "SedimentPulserAtLinks",
    "SedimentPulserEachParcel",
]



================================================
File: network_sediment_transporter/bed_parcel_initializers.py
================================================
"""
Landlab components to initialize river bed sediment "parcels", represented as
items in a landlab :class:`~.DataRecord`, in each link in a river network (represented
by a landlab :class:`~.NetworkModelGrid`). The different *BedParcelInitializers* allow
the user to define the median grain size on a given link several different ways.

.. codeauthor:: Eric Hutton, Allison Pfeiffer, Muneer Ahammad, and Jon Czuba
"""

import warnings

import numpy as np
import scipy.constants

from landlab.core.model_component import Component
from landlab.data_record.data_record import DataRecord
from landlab.grid.network import NetworkModelGrid

_OUT_OF_NETWORK = -2


class BedParcelInitializerBase(Component):
    def __init__(
        self,
        grid,
        time=0.0,
        tau_c_50=0.04,
        rho_sediment=2650.0,
        rho_water=1000.0,
        gravity=scipy.constants.g,
        D84_D50=2.1,
        sed_thickness=2,
        abrasion_rate=0.0,
        median_number_of_starting_parcels=100,
        extra_parcel_attributes=None,
        rng=None,
    ):
        if rng is None:
            rng = np.random.default_rng()
        elif isinstance(rng, int):
            rng = np.random.default_rng(seed=rng)
        self._rng = rng

        if not isinstance(grid, NetworkModelGrid):
            raise TypeError("grid must be a NetworkModelGrid")

        if np.min(sed_thickness) < 0.05:
            warnings.warn(
                f"sed_thickness is unrealistically low ({sed_thickness} * d84)",
                stacklevel=2,
            )

        if np.max(np.abs(tau_c_50 - 0.055)) > 0.35:
            warnings.warn(f"tau_c_50 is unrealistic ({tau_c_50})", stacklevel=2)

        self._time = [time]
        self._grid = grid
        self._tau_c_50 = tau_c_50
        self._rho_sediment = rho_sediment
        self._rho_water = rho_water
        self._gravity = gravity
        self._D84_D50 = D84_D50
        self._abrasion_rate = abrasion_rate
        self._extra_parcel_attributes = extra_parcel_attributes
        self._sed_thickness = sed_thickness
        self._median_number_of_starting_parcels = median_number_of_starting_parcels

    def __call__(self):
        d50 = self.calc_d50()

        d84 = d50 * self._D84_D50

        total_parcel_volume_at_link = calc_total_parcel_volume(
            self._grid.at_link["channel_width"],
            self._grid.at_link["reach_length"],
            d84 * self._sed_thickness,
        )
        max_parcel_volume = _determine_approx_parcel_volume(
            total_parcel_volume_at_link, self._median_number_of_starting_parcels
        )

        variables, items = _parcel_characteristics(
            total_parcel_volume_at_link,
            max_parcel_volume,
            d50,
            self._D84_D50,
            self._rho_sediment,
            self._abrasion_rate,
            self._extra_parcel_attributes,
            rng=self._rng,
        )

        if np.max(d50) > 0.5:
            warnings.warn(
                f"calculated d50 is unrealistically large ({d50} m)", stacklevel=2
            )

        if np.min(d50) < 0.002:
            warnings.warn(
                f"calculated d50 is unrealistically low ({d50} m). The equations used "
                "in this initializer are intended for gravel bedded rivers.",
                stacklevel=2,
            )

        if max_parcel_volume < 0.05:
            warnings.warn(
                f"default parcel volume is extremely small ({max_parcel_volume} m)",
                stacklevel=2,
            )

        return DataRecord(
            self._grid,
            items=items,
            time=self._time,
            data_vars=variables,
            dummy_elements={"link": [_OUT_OF_NETWORK]},
        )

    def calc_d50(self):
        raise NotImplementedError("calc_d50")


class BedParcelInitializerDischarge(BedParcelInitializerBase):
    """Create a landlab :class:`~.DataRecord` to represent parcels of sediment on
    a river network.

    The function takes discharge data for each link as input, as well as channel
    geometry (``channel_width``, ``reach_length``, ``channel_slope``) fields attached
    to the :class:`~.NetworkModelGrid`.

    This function currently estimates median parcel grain size at a link
    according to Snyder et al. (2013), assuming a lognormal parcel grain size
    distribution.

    .. codeauthor:: Eric Hutton, Allison Pfeiffer, Muneer Ahammad

    Parameters
    ----------
    grid : NetworkModelGrid
        *landlab* :class:`~.NetworkModelGrid` to place sediment parcels on.
    time : float, optional
        The initial time to add to the record.
    discharge_at_link: float
        Dominant/formative discharge at each link in the network [m^3 / s].
    mannings_n : float, optional
        Manning's *n* value for all links, used to calculate median parcel grain
        size at a link.
    tau_c_50 : float, optional
        Critical Shields stress for *d50* at dominant discharge for all links, used to
        calculate median parcel grain size.
    rho_sediment : float, optional
        Sediment grain density [kg / m^3].
    rho_water : float, optional
        Density of water [kg / m^3].
    gravity : float, optional
        Acceleration due to gravity [m / s^2].
    D84_D50 : float, optional
        Ratio of *D84:D50,* used to set lognormal distribution of grain size.
    sed_thickness : float, optional
        Sediment thickness in multiples of *d84*.
    abrasion_rate : float, optional
        Abrasion rate of parcels during transport [1/m].
    median_number_of_starting_parcels : int, optional
        Median number of parcels in a link.
    extra_parcel_attributes : str or list of str, optional
        Name of user-defined parcel attribute to be added to parcel data record,
        which will be returned as an empty parcel attribute.

    Examples
    --------
    >>> from landlab import NetworkModelGrid
    >>> from landlab.components.network_sediment_transporter import (
    ...     BedParcelInitializerDischarge,
    ... )

    >>> y_of_node = (0, 100, 200, 200, 300, 400, 400, 125)
    >>> x_of_node = (0, 0, 100, -50, -100, 50, -150, -100)
    >>> nodes_at_link = ((1, 0), (2, 1), (1, 7), (3, 1), (3, 4), (4, 5), (4, 6))

    >>> grid = NetworkModelGrid((y_of_node, x_of_node), nodes_at_link)

    >>> _ = grid.add_full("channel_width", 1.0, at="link")  # m
    >>> _ = grid.add_full("channel_slope", 0.01, at="link")  # m / m
    >>> _ = grid.add_full("reach_length", 100.0, at="link")  # m

    >>> discharge = np.full(grid.number_of_links, 10.0)  # m^3 / s
    >>> initialize_parcels = BedParcelInitializerDischarge(
    ...     grid, discharge_at_link=discharge
    ... )
    >>> parcels = initialize_parcels()
    """

    _name = "BedParcelInitializerDischarge"

    _unit_agnostic = False

    __version__ = "1.0"

    _info = {
        "discharge_at_link": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m^3 / s",
            "mapping": "link",
            "doc": "Dominant/formative discharge at each link in the network",
        },
    }

    def __init__(
        self, grid, time=0.0, discharge_at_link=None, mannings_n=0.035, **kwds
    ):
        if np.max(np.abs(mannings_n - 0.035)) > 0.3:
            warnings.warn(
                f"Manning's n value is unrealistic ({mannings_n})", stacklevel=2
            )

        if discharge_at_link is None:
            raise ValueError("User must provide discharge_at_link")

        if np.size(discharge_at_link) != grid.number_of_links:
            raise ValueError(
                "discharge_at_link should be size number_of_links "
                f"({np.size(discharge_at_link)} != {grid.number_of_links})"
            )

        self._discharge = discharge_at_link
        self._mannings_n = mannings_n

        BedParcelInitializerBase.__init__(self, grid, time=time, **kwds)

    def calc_d50(self):
        return calc_d50_discharge(
            self._grid.at_link["channel_width"],
            self._grid.at_link["channel_slope"],
            discharge=self._discharge,
            mannings_n=self._mannings_n,
            gravity=self._gravity,
            rho_water=self._rho_water,
            rho_sediment=self._rho_sediment,
            tau_c_50=self._tau_c_50,
        )


class BedParcelInitializerDepth(BedParcelInitializerBase):
    """Create a *landlab* :class:`~.DataRecord` to represent parcels of sediment on
    a river network.

    The function takes dominant flow depth for each link as input, as well as channel
    geometry (*channel_width*, *reach_length*, *channel_slope*) fields attached to the
    :class:`~.NetworkModelGrid`.

    This function currently estimates median parcel grain size at a link
    using the formative Shields stress (as in Pfeiffer et al., 2017), assuming
    a lognormal parcel grain size distribution.

    .. codeauthor:: Eric Hutton, Allison Pfeiffer, Muneer Ahammad

    Parameters
    ----------
    grid : ModelGrid
        *landlab* :class:`~.ModelGrid` to place sediment parcels on.
    time : float, optional
        The initial time to add to the record.
    flow_depth_at_link: float, optional
        Dominant/formative flow depth at each link in the network.
    tau_c_multiplier: float, optional
        Coefficient to relate critical and dominant/bankfull/formative Shields
        stress. Dominant/formative/bankfull Shields stress is calculated as
        ``multiplier * critical``.
    tau_c_50 : float, optional
        Critical Shields stress for *d50* at dominant discharge for all links, used to
        calculate median parcel grain size
    rho_sediment : float, optional
        Sediment grain density [kg / m^3].
    rho_water : float, optional
        Density of water [kg / m^3].
    gravity : float, optional
        Acceleration due to gravity [m / s^2].
    D84_D50 : float, optional
        Ratio of *D84:D50*, used to set lognormal distribution of grain size.
    sed_thickness : float, optional
        Sediment thickness in multiples of *d84*.
    abrasion_rate : float, optional
        Abrasion rate of parcels during transport [1 / m].
    median_number_of_starting_parcels : int, optional
        Median number of parcels in a link.
    extra_parcel_attributes : str or list of str, optional
        Name of user-defined parcel attribute to be added to parcel data record,
        which will be returned as an empty parcel attribute.

    Examples
    --------
    >>> from landlab import NetworkModelGrid
    >>> from landlab.components.network_sediment_transporter import (
    ...     BedParcelInitializerDepth,
    ... )

    >>> y_of_node = (0, 100, 200, 200, 300, 400, 400, 125)
    >>> x_of_node = (0, 0, 100, -50, -100, 50, -150, -100)
    >>> nodes_at_link = ((1, 0), (2, 1), (1, 7), (3, 1), (3, 4), (4, 5), (4, 6))
    >>> grid = NetworkModelGrid((y_of_node, x_of_node), nodes_at_link)
    >>> _ = grid.add_full("channel_width", 1.0, at="link")  # m
    >>> _ = grid.add_full("channel_slope", 0.01, at="link")  # m / m
    >>> _ = grid.add_full("reach_length", 100.0, at="link")  # m


    >>> depth = np.full(grid.number_of_links, 1.0)  # m
    >>> initialize_parcels = BedParcelInitializerDepth(grid, flow_depth_at_link=depth)
    >>> parcels = initialize_parcels()
    """

    _name = "BedParcelInitializerDepth"

    _unit_agnostic = False

    __version__ = "1.0"

    _info = {
        "flow_depth_at_link": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "link",
            "doc": "Dominant/formative flow depth at each link in the network",
        },
    }

    def __init__(
        self, grid, time=0.0, flow_depth_at_link=None, tau_c_multiplier=1.0, **kwds
    ):
        if flow_depth_at_link is None:
            raise ValueError("User must provide flow_depth_at_link")

        if np.size(flow_depth_at_link) != grid.number_of_links:
            raise ValueError("flow_depth_at_link should be size number_of_links")

        self._flow_depth = flow_depth_at_link
        self._tau_c_multiplier = tau_c_multiplier

        BedParcelInitializerBase.__init__(self, grid, time=time, **kwds)

    def calc_d50(self):
        return calc_d50_depth(
            self._grid.at_link["channel_slope"],
            flow_depth=self._flow_depth,
            tau_c_multiplier=self._tau_c_multiplier,
            rho_water=self._rho_water,
            rho_sediment=self._rho_sediment,
            tau_c_50=self._tau_c_50,
        )


class BedParcelInitializerArea(BedParcelInitializerBase):
    """Create a *landlab* :class:`~.DataRecord` to represent parcels of sediment on
    a river network.

    The function takes a coefficient and exponent in a grain size-drainage area power
    law scaling relationship, as well as channel attribute (`drainage_area`,
    *channel_width*, *reach_length*, *channel_slope*) fields attached to the
    :class:`~.NetworkModelGrid`.

    This function currently estimates median parcel grain size at a link
    using a power-law scaling relationship between drainage area and median
    grain size (:math:`d_{50} = c A^n`), assuming a lognormal parcel grain size
    distribution.

    .. codeauthor:: Eric Hutton, Allison Pfeiffer, Muneer Ahammad

    Parameters
    ----------
    grid : ModelGrid
        *landlab* :class:`~.ModelGrid` to place sediment parcels on.
    time : float, optional
        The initial time to add to the record.
    drainage_area_coefficient : float, optional
        Coefficient in a power law grain size-drainage area scaling relationship.
    drainage_area_exponent : float, optional
        Exponent in a power law grain size-drainage area scaling relationship.
    rho_sediment : float, optional
        Sediment grain density [kg / m^3].
    rho_water : float, optional
        Density of water [kg / m^3].
    gravity : float, optional
        Acceleration due to gravity [m / s^2].
    D84_D50 : float, optional
        Ratio of *D84:D50*, used to set lognormal distribution of grain size.
    sed_thickness : float, optional
        Sediment thickness in multiples of *d84*.
    abrasion_rate : float, optional
        Abrasion rate of parcels during transport [1 / m].
    median_number_of_starting_parcels : int, optional
        Median number of parcels in a link.
    extra_parcel_attributes : str or list of str, optional
        Name of user-defined parcel attribute to be added to parcel data record,
        which will be returned as an empty parcel attribute.

    Examples
    --------
    >>> from landlab import NetworkModelGrid
    >>> from landlab.components.network_sediment_transporter import (
    ...     BedParcelInitializerArea,
    ... )

    >>> y_of_node = (0, 100, 200, 200, 300, 400, 400, 125)
    >>> x_of_node = (0, 0, 100, -50, -100, 50, -150, -100)
    >>> nodes_at_link = ((1, 0), (2, 1), (1, 7), (3, 1), (3, 4), (4, 5), (4, 6))

    >>> grid = NetworkModelGrid((y_of_node, x_of_node), nodes_at_link)

    >>> _ = grid.add_full("channel_width", 1.0, at="link")  # m
    >>> _ = grid.add_full("channel_slope", 0.01, at="link")  # m / m
    >>> _ = grid.add_full("reach_length", 100.0, at="link")  # m
    >>> _ = grid.add_full("drainage_area", 100.0, at="link")


    >>> initialize_parcels = BedParcelInitializerArea(
    ...     grid, drainage_area_coefficient=0.1, drainage_area_exponent=0.3
    ... )
    >>> parcels = initialize_parcels()
    """

    _name = "BedParcelInitializerArea"

    _unit_agnostic = False

    __version__ = "1.0"

    _info = {
        "time": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "s",
            "mapping": "link",
            "doc": "The initial time to add to the record",
        },
        "drainage_area_coefficient": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "--",
            "mapping": "link",
            "doc": (
                "Coefficient in a power law grain size-drainage area scaling "
                "relationship"
            ),
        },
        "drainage_area_exponent": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "--",
            "mapping": "link",
            "doc": (
                "Exponent in a power law grain size-drainage area scaling "
                "relationship."
            ),
        },
    }

    def __init__(
        self,
        grid,
        time=0.0,
        drainage_area_coefficient=None,
        drainage_area_exponent=None,
        **kwds,
    ):
        if drainage_area_coefficient is None:
            raise ValueError("User must provide drainage_area_coefficient")

        if drainage_area_exponent is None:
            raise ValueError("User must provide drainage_area_exponent")

        self._drainage_area_coefficient = drainage_area_coefficient
        self._drainage_area_exponent = drainage_area_exponent

        BedParcelInitializerBase.__init__(self, grid, time=time, **kwds)

    def calc_d50(self):
        return calc_d50_dArea_scaling(
            self._grid.at_link["drainage_area"],
            a=self._drainage_area_coefficient,
            n=self._drainage_area_exponent,
        )


class BedParcelInitializerUserD50(BedParcelInitializerBase):
    """Create a *landlab* :class:`~.DataRecord` to represent parcels of sediment on
    a river network.

    The function takes either a scalar value or an array (of of length,
    *number_of_links*) to assign the median grain size for parcels on each link in
    the network grid.

    This function creates a lognormal grain size distribution for the parcels
    in the link.

    .. codeauthor:: Eric Hutton, Allison Pfeiffer, Muneer Ahammad

    Parameters
    ----------
    grid : ModelGrid
        landlab :class:`~.ModelGrid` to place sediment parcels on.
    time : float, optional
        The initial time to add to the record.
    user_d50 : float, optional
        Either an array of *d50* (of length *number_of_links*) or a scalar to be
        applied to all links in the network.
    rho_sediment : float, optional
        Sediment grain density [kg / m^3].
    rho_water : float, optional
        Density of water [kg / m^3].
    gravity : float, optional
        Acceleration due to gravity [m / s^2].
    D84_D50 : float, optional
        Ratio of *D84:D50*, used to set lognormal distribution of grain size.
    sed_thickness : float, optional
        Sediment thickness in multiples of *d84*.
    abrasion_rate : float, optional
        Abrasion rate of parcels during transport [1 / m].
    median_number_of_starting_parcels : int, optional
        Median number of parcels in a link.
    extra_parcel_attributes : str or list of str, optional
        Name of user-defined parcel attribute to be added to parcel data record,
        which will be returned as an empty parcel attribute.

    Examples
    --------
    >>> from landlab import NetworkModelGrid
    >>> from landlab.components.network_sediment_transporter import (
    ...     BedParcelInitializerUserD50,
    ... )

    >>> y_of_node = (0, 100, 200, 200, 300, 400, 400, 125)
    >>> x_of_node = (0, 0, 100, -50, -100, 50, -150, -100)
    >>> nodes_at_link = ((1, 0), (2, 1), (1, 7), (3, 1), (3, 4), (4, 5), (4, 6))

    >>> grid = NetworkModelGrid((y_of_node, x_of_node), nodes_at_link)

    >>> _ = grid.add_full("channel_width", 1.0, at="link")  # m
    >>> _ = grid.add_full("channel_slope", 0.01, at="link")  # m / m
    >>> _ = grid.add_full("reach_length", 100.0, at="link")  # m


    >>> initialize_parcels = BedParcelInitializerUserD50(grid, user_d50=0.05)
    >>> parcels = initialize_parcels()
    """

    _name = "BedParcelInitializerUserD50"

    _unit_agnostic = False

    __version__ = "1.0"

    _info = {
        "time": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "s",
            "mapping": "link",
            "doc": "The initial time to add to the record",
        },
        "user_d50": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "link",
            "doc": "Median grain size of the bed sediment in each link",
        },
    }

    def __init__(self, grid, time=0.0, user_d50=None, **kwds):
        if user_d50 is None:
            raise ValueError("User must provide user_d50")

        self._user_d50 = user_d50

        BedParcelInitializerBase.__init__(self, grid, time=time, **kwds)

    def calc_d50(self):
        if np.size(self._user_d50) == 1:  # one d50, all links
            d50 = np.full_like(self._grid.length_of_link, self._user_d50, dtype=float)

            return d50

        elif np.size(self._user_d50) == (
            self._grid.number_of_links
        ):  # different d50 each link
            d50 = self._user_d50

            return d50
        else:
            raise ValueError(
                "user defined D50 must be either a scalar or size(element_id)"
            )


# BedParcelInitializerBase helper functions
def _parcel_characteristics(
    total_parcel_volume_at_link,
    max_parcel_volume,
    d50,
    D84_D50,
    rho_sediment,
    abrasion_rate,
    extra_parcel_attributes,
    rng=None,
):
    if rng is None:
        rng = np.random.default_rng()

    n_parcels_at_link = np.ceil(total_parcel_volume_at_link / max_parcel_volume).astype(
        dtype=int
    )
    if np.min(n_parcels_at_link) < 10:
        warnings.warn(
            f"at least one link has only {n_parcels_at_link} parcels.", stacklevel=2
        )

    element_id = np.empty(np.sum(n_parcels_at_link), dtype=int)

    # volume = np.full(np.sum(n_parcels_at_link), max_parcel_volume, dtype=float)
    volume = np.full_like(element_id, max_parcel_volume, dtype=float)
    grain_size = np.empty_like(element_id, dtype=float)
    offset = 0
    for link, n_parcels in enumerate(n_parcels_at_link):
        element_id[offset : offset + n_parcels] = link
        grain_size[offset : offset + n_parcels] = rng.lognormal(
            np.log(d50[link]), np.log(D84_D50), n_parcels
        )
        volume[offset] = total_parcel_volume_at_link[link] - (
            (n_parcels - 1) * max_parcel_volume
        )  # small remaining volume

        offset += n_parcels
    starting_link = element_id.copy()
    abrasion_rate = np.full_like(element_id, abrasion_rate, dtype=float)
    density = np.full_like(element_id, rho_sediment, dtype=float)

    element_id = np.expand_dims(element_id, axis=1)
    volume = np.expand_dims(volume, axis=1)
    grain_size = np.expand_dims(grain_size, axis=1)

    time_arrival_in_link = np.expand_dims(
        rng.uniform(size=np.sum(n_parcels_at_link)), axis=1
    )
    location_in_link = np.expand_dims(
        rng.uniform(size=np.sum(n_parcels_at_link)), axis=1
    )

    active_layer = np.ones(np.shape(element_id))
    variables = {
        "starting_link": (["item_id"], starting_link),
        "abrasion_rate": (["item_id"], abrasion_rate),
        "density": (["item_id"], density),
        "time_arrival_in_link": (["item_id", "time"], time_arrival_in_link),
        "active_layer": (["item_id", "time"], active_layer),
        "location_in_link": (["item_id", "time"], location_in_link),
        "D": (["item_id", "time"], grain_size),
        "volume": (["item_id", "time"], volume),
    }

    if extra_parcel_attributes is not None:
        for attrib in extra_parcel_attributes:
            variables[attrib] = (["item_id"], np.nan * np.zeros_like(density))

    return variables, {"grid_element": "link", "element_id": element_id}


def _determine_approx_parcel_volume(
    total_parcel_volume_at_link, median_number_of_starting_parcels
):
    """What size parcels should we use?"""
    median_link_volume = np.median(total_parcel_volume_at_link)
    return median_link_volume / median_number_of_starting_parcels


def calc_total_parcel_volume(width, length, sediment_thickness):
    """Simple rectangular prism geometry. Total parcel vol in each link = L*W*H"""

    return width * length * sediment_thickness


# calc_d50 helper functions


def calc_d50_discharge(
    width,
    slope,
    discharge,
    mannings_n,
    gravity,
    rho_water,
    rho_sediment,
    tau_c_50,
):
    """Calculate median grain size via dominant discharge and channel width
    according to Snyder et al. (2013)

    Returns
    -------
    ndarray of float
        d50.

    Examples
    --------
    >>> from numpy.testing import assert_almost_equal

    >>> w = 20
    >>> S = 0.01
    >>> Q = 100
    >>> n = 0.05
    >>> g = 9.81
    >>> rho_w = 1000
    >>> rho_s = 3000
    >>> tau_c_50 = 0.05

    >>> expected_value = (
    ...     rho_w * g * n ** (3 / 5) * Q ** (3 / 5) * w ** (-3 / 5) * S ** (7 / 10)
    ... ) / ((rho_s - rho_w) * g * tau_c_50)
    >>> print(np.round(expected_value, decimals=3))
    0.173
    >>> assert_almost_equal(
    ...     calc_d50_discharge(20, 0.01, 100, 0.05, 9.81, 1000, 3000, 0.05),
    ...     expected_value,
    ... )
    """

    return (
        rho_water
        * gravity
        * mannings_n ** (3 / 5)
        * discharge ** (3 / 5)
        * width ** (-3 / 5)
        * slope ** (7 / 10)
    ) / ((rho_sediment - rho_water) * gravity * tau_c_50)


def calc_d50_depth(
    slope,
    flow_depth,
    tau_c_multiplier,
    rho_water,
    rho_sediment,
    tau_c_50,
):
    """Calculate median grain size via dominant flow depth according to
    Pfeiffer et al. (2017).

    Returns
    -------
    ndarray of float
        *d50*.

    Examples
    --------
    >>> from numpy.testing import assert_almost_equal

    >>> slope = 0.01
    >>> depth = 1
    >>> tau_c_multiplier = 1
    >>> rho_w = 1000
    >>> rho_s = 3000
    >>> tau_c_50 = 0.05

    >>> expected_value = (rho_w * depth * slope) / (
    ...     (rho_s - rho_w) * tau_c_50 * tau_c_multiplier
    ... )
    >>> print(np.round(expected_value, decimals=3))
    0.1
    >>> assert_almost_equal(
    ...     calc_d50_depth(0.01, 1, 1, 1000, 3000, 0.05), expected_value
    ... )
    """

    return (rho_water * flow_depth * slope) / (
        (rho_sediment - rho_water) * tau_c_multiplier * tau_c_50
    )


def calc_d50_dArea_scaling(drainage_area, a, n):
    """Calculate median grain size via power law scaling relationship with drainage
    area.

    Returns
    -------
    ndarray of float
        *d50*.

    Examples
    --------
    >>> from numpy.testing import assert_almost_equal

    >>> drainage_area = 10
    >>> drainage_area_coefficient = 1
    >>> drainage_area_exponent = -0.1

    >>> expected_value = drainage_area_coefficient * (
    ...     drainage_area**drainage_area_exponent
    ... )
    >>> print(np.round(expected_value, decimals=3))
    0.794
    >>> assert_almost_equal(calc_d50_dArea_scaling(10, 1, -0.1), expected_value)
    """
    d50 = a * drainage_area**n

    return d50



================================================
File: network_sediment_transporter/network_sediment_transporter.py
================================================
"""Simulate transport of bed material through a 1-D river network.

Landlab component that simulates the transport of bed material
sediment through a 1-D river network, while tracking the resulting changes
in bed material grain size and river bed elevation. Model framework
described in Czuba (2018). Additions include: particle abrasion, variable
active layer thickness (Wong et al., 2007).

.. codeauthor:: Allison Pfeiffer, Katy Barnhart, Jon Czuba, Eric Hutton
"""

from __future__ import annotations

import warnings
from typing import Literal

import numpy as np
import scipy.constants
from numpy.typing import ArrayLike
from numpy.typing import NDArray

from landlab.components.flow_director.flow_director_steepest import FlowDirectorSteepest
from landlab.core.model_component import Component
from landlab.data_record.aggregators import aggregate_items_as_mean
from landlab.data_record.aggregators import aggregate_items_as_sum
from landlab.data_record.data_record import DataRecord
from landlab.grid.network import NetworkModelGrid

_SUPPORTED_TRANSPORT_METHODS = frozenset(("WilcockCrowe",))
_SUPPORTED_ACTIVE_LAYER_METHODS = frozenset(
    ("WongParker", "GrainSizeDependent", "Constant10cm")
)

_REQUIRED_PARCEL_ATTRIBUTES = frozenset(
    (
        "time_arrival_in_link",
        "abrasion_rate",
        "density",
        "active_layer",
        "location_in_link",
        "D",
        "volume",
    )
)

_ACTIVE = 1
_INACTIVE = 0

_SAND_SIZE = 0.002
_INIT_ACTIVE_LAYER_THICKNESS = 0.03116362


class NetworkSedimentTransporter(Component):
    """Move sediment parcels on a river network.

    Landlab component that simulates the transport of bed material
    sediment through a 1-D river network, while tracking the resulting changes
    in bed material grain size and river bed elevation. Model framework
    described in Czuba (2018). Additions include: particle abrasion, and variable
    active layer thickness (Wong et al., 2007).

    This component cares about units. Its time, length, and mass units are
    seconds, meters, and kilograms, by default. The horizontal unit of the
    grid, and the units of the parameters ``g`` and ``fluid_density`` are
    what specify the component units. In addition, the expressions used
    to calculate the transport have units (Wilcock and Crowe, 2003).

    There is a function that assists in plotting the output of this component.
    It is called :func:`~.plot.plot_network_and_parcels`.  Examples of its usage
    can be found in the `NetworkSedimentTransporter` notebooks (located in the
    "notebooks" folder).

    Examples
    ----------
    >>> import numpy as np
    >>> from landlab.components import FlowDirectorSteepest, NetworkSedimentTransporter
    >>> from landlab import NetworkModelGrid
    >>> from landlab.data_record import DataRecord

    The :class:`~.NetworkSedimentTransporter` moves "parcels" of sediment down a network
    based on a given flow and a given sediment transport formulation. The river
    network is represented by a landlab :class:`~.NetworkModelGrid`.
    Flow direction in the network is determined using a landlab flow director.
    Sediment parcels are represented as items within a landlab
    :class:`~.DataRecord`. The landlab :class:`~.DataRecord` is used to track
    the location, grain size, sediment density, and total volume of each parcel.

    Create a :class:`~.NetworkModelGrid` to represent the river channel network.
    In this case, the grid is a single line of 4 nodes connected by 3 links. Each
    link represents a reach of river.

    >>> y_of_node = (0, 0, 0, 0)
    >>> x_of_node = (0, 100, 200, 300)
    >>> nodes_at_link = ((0, 1), (1, 2), (2, 3))
    >>> nmg = NetworkModelGrid((y_of_node, x_of_node), nodes_at_link)

    Add required channel and topographic variables to the
    :class:`~.network.NetworkModelGrid`.

    >>> _ = nmg.add_field("bedrock__elevation", [3.0, 2.0, 1.0, 0.0], at="node")  # m
    >>> _ = nmg.add_field("reach_length", [100.0, 100.0, 100.0], at="link")  # m
    >>> _ = nmg.add_field("channel_width", (15 * np.ones(nmg.size("link"))), at="link")
    >>> _ = nmg.add_field("flow_depth", (2 * np.ones(nmg.size("link"))), at="link")  # m

    Add ``"topographic__elevation"`` to the grid because the
    :class:`~.FlowDirectorSteepest` will look to it to
    determine the direction of sediment transport through the network. Each
    time we run the `NetworkSedimentTransporter` the topography will be
    updated based on the bedrock elevation and the distribution of alluvium.

    >>> _ = nmg.add_field(
    ...     "topographic__elevation",
    ...     np.copy(nmg.at_node["bedrock__elevation"]),
    ...     at="node",
    ... )

    Run :class:`~.FlowDirectorSteepest` to determine the direction of sediment
    transport through the network.

    >>> flow_director = FlowDirectorSteepest(nmg)
    >>> flow_director.run_one_step()

    Define the starting time and the number of timesteps for this model run.

    >>> timesteps = 10
    >>> time = [0.0]

    Define the sediment characteristics that will be used to create the parcels
    :class:`~.DataRecord`.

    >>> items = {"grid_element": "link", "element_id": np.array([[0]])}

    >>> variables = {
    ...     "starting_link": (["item_id"], np.array([0])),
    ...     "abrasion_rate": (["item_id"], np.array([0])),
    ...     "density": (["item_id"], np.array([2650])),
    ...     "time_arrival_in_link": (["item_id", "time"], np.array([[0]])),
    ...     "active_layer": (["item_id", "time"], np.array([[1]])),
    ...     "location_in_link": (["item_id", "time"], np.array([[0]])),
    ...     "D": (["item_id", "time"], np.array([[0.05]])),
    ...     "volume": (["item_id", "time"], np.array([[1]])),
    ... }

    Create the sediment parcel :class:`~.DataRecord`. In this case, we are creating
    a single sediment parcel with all of the required attributes.

    >>> one_parcel = DataRecord(
    ...     nmg,
    ...     items=items,
    ...     time=time,
    ...     data_vars=variables,
    ...     dummy_elements={"link": [NetworkSedimentTransporter.OUT_OF_NETWORK]},
    ... )

    Instantiate the model run

    >>> nst = NetworkSedimentTransporter(
    ...     nmg,
    ...     one_parcel,
    ...     flow_director,
    ...     bed_porosity=0.03,
    ...     g=9.81,
    ...     fluid_density=1000,
    ...     transport_method="WilcockCrowe",
    ...     active_layer_method="WongParker",
    ... )

    >>> dt = 60  # (seconds) 1 min timestep

    Run the model

    >>> for _ in range(timesteps):
    ...     nst.run_one_step(dt)
    ...

    We can the link location of the parcel at each timestep

    >>> one_parcel.dataset.element_id.values
    array([[0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 2.]])

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Pfeiffer, A., Barnhart, K., Czuba, J., & Hutton, E. (2020).
    NetworkSedimentTransporter: A Landlab component for bed material transport
    through river networks. Journal of Open Source Software, 5(53).

    **Additional References**

    Czuba, J. A. (2018). A Lagrangian framework for exploring complexities
    of mixed-size sediment transport in gravel-bedded river networks.
    Geomorphology, 321, 146-152.

    Wilcock, P. R., & Crowe, J. C. (2003). Surface-based transport model
    for mixed-size sediment. Journal of Hydraulic Engineering, 129(2), 120-128.

    Wong, M., Parker, G., DeVries, P., Brown, T. M., & Burges, S. J. (2007).
    Experiments on dispersion of tracer stones under lower‐regime plane‐bed
    equilibrium bed load transport. Water Resources Research, 43(3).
    """

    _name = "NetworkSedimentTransporter"

    _unit_agnostic = False

    __version__ = "1.0"

    _info = {
        "bedrock__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "elevation of the bedrock surface",
        },
        "channel_slope": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/m",
            "mapping": "link",
            "doc": "Slope of the river channel through each reach",
        },
        "channel_width": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "link",
            "doc": "Flow width of the channel, assuming constant width",
        },
        "flow_depth": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "link",
            "doc": "Flow depth of the channel",
        },
        "reach_length": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "link",
            "doc": "Length of each reach",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
    }

    #: Indicates a parcel is out of network
    OUT_OF_NETWORK = NetworkModelGrid.BAD_INDEX - 1

    def __init__(
        self,
        grid: NetworkModelGrid,
        parcels: DataRecord,
        flow_director: FlowDirectorSteepest,
        bed_porosity: float = 0.3,
        g: float = scipy.constants.g,
        fluid_density: float = 1000.0,
        transport_method: Literal["WilcockCrowe"] = "WilcockCrowe",
        active_layer_method: Literal[
            "WongParker", "GrainSizeDependent", "Constant10cm"
        ] = "WongParker",
        active_layer_d_multiplier: int = 2,
        slope_threshold: float = 1e-4,
        k_transp_dep_abr: float | None = None,
    ) -> None:
        """
        Parameters
        ----------
        grid: NetworkModelGrid
            A :class:`~.NetworkModelGrid` in which links are stream channel segments.
        parcels: DataRecord
            A landlab :class:`~.DataRecord` describing the characteristics and
            location of sediment "parcels".  At any given timestep, each parcel is
            located at a specified point along (location_in_link) a particular link
            (element_id). Each parcel has a total sediment volume (volume), sediment
            grain size (D), sediment density (density), and bed material abrasion rate
            (abrasion_rate). During a given timestep, parcels may be in the
            "active layer" of most recently deposited sediment
            (active_layer = 1), or they may be buried and not subject to
            transport (active_layer = 0). Whether a sediment parcel is active
            or not is determined based on flow conditions and parcel attributes
            in :meth:`~.NetworkSedimentTransporter.run_one_step`.
        flow_director: :class:`~.FlowDirectorSteepest`
            A landlab flow director. Currently, must be :class:`~.FlowDirectorSteepest`.
        bed_porosity: float, optional
            Proportion of void space between grains in the river channel bed.
        g: float, optional
            Acceleration due to gravity [m / s^2].
        fluid_density: float, optional
            Density of the fluid (generally, water) in which sediment is
            moving [kg / m^3].
        transport_method: {"WilcockCrowe"}, optional
            Sediment transport equation option.
        active_layer_method: {"WongParker", "GrainSizeDependent", "Constant10cm"}, optional
            Option for treating sediment active layer as a constant or variable.
        slope_threshold: float, optional
            Minimum channel slope at any given link. Slopes lower than this
            value will default to the threshold.
        k_transp_dep_abr: float, optional
            Coefficient of enhanced abrasion at high bedload transport stage, as in
            Pfeiffer et al. (2022)
        """
        if not isinstance(grid, NetworkModelGrid):
            raise ValueError("grid must be NetworkModelGrid")

        # run super. this will check for required inputs specified by _info
        super().__init__(grid)

        # check key information about the parcels, including that all required
        # attributes are present.
        if not isinstance(parcels, DataRecord):
            raise ValueError("parcels must be an instance of DataRecord")

        for rpa in _REQUIRED_PARCEL_ATTRIBUTES:
            if rpa not in parcels.dataset:
                raise ValueError(f"{rpa} must be assigned to the parcels")

        # save key information about the parcels.
        self._parcels = parcels
        self._num_parcels = self._parcels.number_of_items
        self._time_variable_parcel_attributes = frozenset(
            ("time_arrival_in_link", "active_layer", "location_in_link", "D", "volume")
        )

        # assert that the flow director is a component and is of type
        # FlowDirectorSteepest
        if not isinstance(flow_director, FlowDirectorSteepest):
            raise ValueError("flow_director must be FlowDirectorSteepest.")

        # save reference to flow director
        self._fd = flow_director

        # verify and save the bed porosity.
        if bed_porosity < 0.0 or bed_porosity > 1:
            raise ValueError(f"bed_porosity must be between 0 and 1 ({bed_porosity})")
        self._bed_porosity = bed_porosity

        # save or create other key properties.
        self._g = g
        self._fluid_density = fluid_density
        self._time_idx = 0
        self._time = 0.0
        self._distance_traveled_cumulative = np.zeros(self._num_parcels)
        self._slope_threshold = slope_threshold
        self._k_transp_dep_abr = k_transp_dep_abr

        # check the transport method is valid.
        if transport_method in _SUPPORTED_TRANSPORT_METHODS:
            self._transport_method = transport_method
        else:
            raise ValueError(
                f"{transport_method}: Valid transport method not supported."
            )

        # update the update_transport_time function to be the correct function
        # for the transport method.
        if self._transport_method == "WilcockCrowe":
            self._update_transport_time = self._calc_transport_wilcock_crowe

        if active_layer_method in _SUPPORTED_ACTIVE_LAYER_METHODS:
            self._active_layer_method = active_layer_method
        else:
            raise ValueError(
                f"{active_layer_method}: Active layer method not supported."
            )

        if self._active_layer_method == "GrainSizeDependent":
            self._active_layer_d_multiplier = active_layer_d_multiplier

        # save reference to key fields
        self._width = self._grid.at_link["channel_width"]
        self._topographic__elevation = self._grid.at_node["topographic__elevation"]

        # create field for channel_slope and topographic__elevation if they
        # don't yet exist.
        self.initialize_output_fields()
        self._channel_slope = self._grid.at_link["channel_slope"]

        # Adjust topographic elevation based on the parcels present.
        # Note that at present FlowDirector is just used for network connectivity.
        # get alluvium depth and calculate topography from br+alluvium, then update slopes.

        self._create_new_parcel_time()
        self._calculate_mean_D_and_rho()

        self._partition_active_and_storage_layers()
        self._adjust_node_elevation()
        self._update_channel_slopes()

    @property
    def time(self) -> float:
        """Return current time."""
        return self._time

    @property
    def d_mean_active(self) -> float:
        """Mean parcel grain size of active parcels aggregated at link."""
        return self._d_mean_active

    @property
    def rhos_mean_active(self) -> float:
        """Mean parcel density of active parcels aggregated at link."""
        return self._rhos_mean_active

    def _create_new_parcel_time(self) -> None:
        """If we are going to track parcels through time in
        :class:`~.DataRecord`, we need to add a new time column to the parcels
        dataframe. This method simply copies over the attributes of the parcels
        from the former timestep.  Attributes will be updated over the course of
        this step.
        """

        if self._time_idx != 0:
            self._parcels.add_record(time=[self._time])

            self._parcels.ffill_grid_element_and_id()

            # copy parcel attributes forward in time.
            for at in self._time_variable_parcel_attributes:
                self._parcels.dataset[at].values[:, self._time_idx] = (
                    self._parcels.dataset[at].values[:, self._time_idx - 1]
                )

        self._this_timesteps_parcels = np.zeros_like(
            self._parcels.dataset.element_id, dtype=bool
        )
        self._this_timesteps_parcels[:, -1] = True

        parcels_off_grid = (
            self._parcels.dataset.element_id[:, -1] == self.OUT_OF_NETWORK
        )
        self._this_timesteps_parcels[parcels_off_grid, -1] = False

        self._num_parcels = self._parcels.number_of_items
        # ^ needs to run just in case we've added more parcels

    def _update_channel_slopes(self) -> None:
        """Re-calculate channel slopes during each timestep."""

        for i in range(self._grid.number_of_links):
            upstream_node_id = self._fd.upstream_node_at_link()[i]
            downstream_node_id = self._fd.downstream_node_at_link()[i]

            self._channel_slope[i] = _recalculate_channel_slope(
                self._grid.at_node["topographic__elevation"][upstream_node_id],
                self._grid.at_node["topographic__elevation"][downstream_node_id],
                self._grid.at_link["reach_length"][i],
                self._slope_threshold,
            )

    def _calculate_mean_D_and_rho(self) -> None:
        """Calculate mean grain size and density on each link"""
        self._rhos_mean_active = aggregate_items_as_mean(
            self._parcels.dataset["element_id"].values[:, -1].astype(int),
            self._parcels.dataset["density"].values.reshape(-1),
            weights=self._parcels.dataset["volume"].values[:, -1],
            size=self._grid.number_of_links,
        )
        self._d_mean_active = aggregate_items_as_mean(
            self._parcels.dataset["element_id"].values[:, -1].astype(int),
            self._parcels.dataset["D"].values.reshape(-1),
            weights=self._parcels.dataset["volume"].values[:, -1],
            size=self._grid.number_of_links,
        )

    def _partition_active_and_storage_layers(self) -> None:
        """For each parcel in the network, determines whether it is in the
        active or storage layer during this timestep, then updates node
        elevations.
        """
        self._vol_tot = aggregate_items_as_sum(
            self._parcels.dataset["element_id"].values[:, -1].astype(int),
            self._parcels.dataset["volume"].values[:, -1],
            size=self._grid.number_of_links,
        )

        if self._active_layer_method == "WongParker":
            # Wong et al. (2007) approximation for active layer thickness.
            # NOTE: calculated using grain size and grain density calculated for
            # the active layer grains in each link at the **previous** timestep.
            # This circumvents the need for an iterative scheme to determine grain
            # size of the active layer before determining which grains are in the
            # active layer.

            # calculate tau
            tau = (
                self._fluid_density
                * self._g
                * self._grid.at_link["channel_slope"]
                * self._grid.at_link["flow_depth"]
            )

            # calcuate taustar
            taustar = np.zeros_like(tau)
            np.divide(
                tau,
                (self._rhos_mean_active - self._fluid_density)
                * self._g
                * self._d_mean_active,
                where=self._rhos_mean_active > self._fluid_density,
                out=taustar,
            )

            # calculate active layer thickness (in units of m)
            self._active_layer_thickness = (
                0.515
                * self._d_mean_active
                * (3.09 * (taustar - 0.0549).clip(0.0, None) ** 0.56)
            )

        elif self._active_layer_method == "GrainSizeDependent":
            # Set all active layers to a multiple of the lnk mean grain size
            self._active_layer_thickness = (
                self._d_mean_active * self._active_layer_d_multiplier
            )

        elif self._active_layer_method == "Constant10cm":
            # Set all active layers to 10 cm thickness.
            self._active_layer_thickness = 0.1 * np.ones_like(self._d_mean_active)

        # If links have no parcels, we still need to assign them an active layer
        # thickness..
        links_with_no_active_layer = np.isnan(self._active_layer_thickness)
        self._active_layer_thickness[links_with_no_active_layer] = np.mean(
            self._active_layer_thickness[links_with_no_active_layer == 0]
        )  # assign links with no parcels an average value

        if np.sum(np.isfinite(self._active_layer_thickness)) == 0:
            self._active_layer_thickness.fill(_INIT_ACTIVE_LAYER_THICKNESS)
            # handles the case of the first timestep -- assigns a modest value

        capacity = (
            self._grid.at_link["channel_width"]
            * self._grid.at_link["reach_length"]
            * self._active_layer_thickness
        )  # in units of m^3

        active_inactive = _INACTIVE * np.ones(self._num_parcels)

        current_link = self._parcels.dataset.element_id.values[:, -1].astype(int)
        time_arrival = self._parcels.dataset.time_arrival_in_link.values[:, -1]
        volumes = self._parcels.dataset.volume.values[:, -1]

        for i in range(self._grid.number_of_links):
            if self._vol_tot[i] > 0:
                # only do this check capacity if parcels are in link
                # First In Last Out.

                # Find parcels on this link.
                this_links_parcels = np.where(current_link == i)[0]

                # sort them by arrival time.
                time_arrival_sort = np.flip(
                    np.argsort(
                        time_arrival[this_links_parcels],
                        0,
                    )
                )
                parcel_id_time_sorted = this_links_parcels[time_arrival_sort]

                # calculate the cumulative volume (in sorted order).
                cumvol = np.cumsum(volumes[parcel_id_time_sorted])

                # determine which parcels are within capacity and set those to
                # active.
                make_active = parcel_id_time_sorted[cumvol <= capacity[i]]

                active_inactive[make_active] = _ACTIVE

        self._parcels.dataset.active_layer[:, -1] = active_inactive

        # set active here. reference it below in wilcock crowe
        self._active_parcel_records = (
            self._parcels.dataset.active_layer == _ACTIVE
        ) * (self._this_timesteps_parcels)

        parcel_volumes = self._parcels.dataset.volume.values[:, -1].copy()
        parcel_volumes[~self._active_parcel_records.values[:, -1].astype(bool)] = 0.0

        self._vol_act = aggregate_items_as_sum(
            self._parcels.dataset["element_id"].values[:, -1].astype(int),
            parcel_volumes,
            size=self._grid.number_of_links,
        )

        self._vol_stor = (
            self._vol_tot - self._vol_act
        )  # stored parcel rock volume (bug fix AP 4/25/24)

    def _adjust_node_elevation(self) -> None:
        """Adjusts slope for each link based on parcel motions from last
        timestep and additions from this timestep.
        """

        number_of_contributors = np.sum(
            self._fd.flow_link_incoming_at_node() == 1, axis=1
        )
        downstream_link_id = self._fd.link_to_flow_receiving_node

        upstream_contributing_links_at_node = np.where(
            self._fd.flow_link_incoming_at_node() == 1, self._grid.links_at_node, -1
        )

        # Update the node topographic elevations depending on the quantity of stored sediment
        for n in range(self._grid.number_of_nodes):
            if number_of_contributors[n] > 0:  # we don't update head node elevations
                upstream_links = upstream_contributing_links_at_node[n]
                real_upstream_links = upstream_links[
                    upstream_links != self._grid.BAD_INDEX
                ]
                width_of_upstream_links = self._grid.at_link["channel_width"][
                    real_upstream_links
                ]
                length_of_upstream_links = self._grid.at_link["reach_length"][
                    real_upstream_links
                ]

                if downstream_link_id[n] == self._grid.BAD_INDEX:
                    # I'm sure there's a better way to do this, but...
                    length_of_downstream_link = 0
                    width_of_downstream_link = 0
                else:
                    length_of_downstream_link = self._grid.at_link["reach_length"][
                        downstream_link_id
                    ][n]
                    width_of_downstream_link = self._grid.at_link["channel_width"][
                        downstream_link_id
                    ][n]

                alluvium__depth = _calculate_alluvium_depth(
                    self._vol_stor[downstream_link_id][n],
                    width_of_upstream_links,
                    length_of_upstream_links,
                    width_of_downstream_link,
                    length_of_downstream_link,
                    self._bed_porosity,
                )

                self._grid.at_node["topographic__elevation"][n] = (
                    self._grid.at_node["bedrock__elevation"][n] + alluvium__depth
                )

    def _calc_transport_wilcock_crowe(self) -> None:
        """Method to determine the transport time for each parcel in the active
        layer using a sediment transport equation.

        Note: could have options here (e.g. Wilcock and Crowe, FLVB, MPM, etc)
        """
        # Initialize _pvelocity, the virtual velocity of each parcel
        # (link length / link travel time)
        self._pvelocity = np.zeros(self._num_parcels)

        # parcel attribute arrays from DataRecord

        Darray = self._parcels.dataset.D[:, self._time_idx]
        Activearray = self._parcels.dataset.active_layer[:, self._time_idx].values
        Rhoarray = self._parcels.dataset.density.values
        Volarray = self._parcels.dataset.volume[:, self._time_idx].values
        # link that the parcel is currently in
        Linkarray = self._parcels.dataset.element_id[:, self._time_idx].values

        R = (Rhoarray - self._fluid_density) / self._fluid_density

        # parcel attribute arrays to populate below
        frac_sand_array = np.zeros(self._num_parcels)
        vol_act_array = np.zeros(self._num_parcels)
        Sarray = np.zeros(self._num_parcels)
        Harray = np.zeros(self._num_parcels)
        Larray = np.zeros(self._num_parcels)

        D_mean_activearray = np.full(self._num_parcels, np.nan)
        active_layer_thickness_array = np.full(self._num_parcels, np.nan)

        # find active sand
        # since find active already sets all prior timesteps to False, we
        # can use D for all timesteps here.
        findactivesand = (
            self._parcels.dataset.D < _SAND_SIZE
        ) * self._active_parcel_records

        parcel_volumes = self._parcels.dataset.volume.values[:, -1].copy()
        parcel_volumes[~findactivesand[:, -1].astype(bool)] = 0.0

        vol_act_sand = aggregate_items_as_sum(
            self._parcels.dataset["element_id"].values[:, -1].astype(int),
            parcel_volumes,
            size=self._grid.number_of_links,
        )

        frac_sand = np.zeros_like(self._vol_act)
        frac_sand[self._vol_act != 0.0] = (
            vol_act_sand[self._vol_act != 0.0] / self._vol_act[self._vol_act != 0.0]
        )
        frac_sand[np.isnan(frac_sand)] = 0.0

        # Calc attributes for each link, map to parcel arrays
        for i in range(self._grid.number_of_links):
            active_here = np.nonzero(
                np.logical_and(Linkarray == i, Activearray == _ACTIVE)
            )[0]
            d_act_i = Darray[active_here]
            vol_act_i = Volarray[active_here]
            rhos_act_i = Rhoarray[active_here]
            vol_act_tot_i: float = np.sum(vol_act_i)

            self._d_mean_active[i] = np.sum(d_act_i * vol_act_i) / (vol_act_tot_i)
            if vol_act_tot_i > 0:
                self._rhos_mean_active[i] = np.sum(rhos_act_i * vol_act_i) / (
                    vol_act_tot_i
                )
            else:
                self._rhos_mean_active[i] = np.nan
            D_mean_activearray[Linkarray == i] = self._d_mean_active[i]
            frac_sand_array[Linkarray == i] = frac_sand[i]
            vol_act_array[Linkarray == i] = self._vol_act[i]
            Sarray[Linkarray == i] = self._grid.at_link["channel_slope"][i]
            Harray[Linkarray == i] = self._grid.at_link["flow_depth"][i]
            Larray[Linkarray == i] = self._grid.at_link["reach_length"][i]
            active_layer_thickness_array[Linkarray == i] = self._active_layer_thickness[
                i
            ]

        # Wilcock and Crowe calculate transport for all parcels (active and inactive)
        taursg = _calculate_reference_shear_stress(
            self._fluid_density, R, self._g, D_mean_activearray, frac_sand_array
        )

        frac_parcel = np.nan * np.zeros_like(Volarray)
        frac_parcel[vol_act_array != 0.0] = (
            Volarray[vol_act_array != 0.0] / vol_act_array[vol_act_array != 0.0]
        )

        b = 0.67 / (1.0 + np.exp(1.5 - Darray / D_mean_activearray))

        tau = np.atleast_1d(self._fluid_density * self._g * Harray * Sarray)

        taur = taursg * (Darray / D_mean_activearray) ** b
        tautaur = tau / taur
        self._tautaur = tautaur.copy()  # use below for xport dependent abrasion
        tautaur_cplx = tautaur.astype(np.complex128)
        # ^ work around needed b/c np fails with non-integer powers of negative numbers

        W = 0.002 * np.power(tautaur_cplx.real, 7.5)
        W[tautaur >= 1.35] = 14 * np.power(
            (1 - (0.894 / np.sqrt(tautaur_cplx.real[tautaur >= 1.35]))), 4.5
        )

        active_parcel_idx = Activearray == _ACTIVE

        # compute parcel virtual velocity, m/s
        self._pvelocity[active_parcel_idx] = (
            W.real[active_parcel_idx]
            * (tau[active_parcel_idx] ** (3.0 / 2.0))
            * frac_parcel[active_parcel_idx]
            / (self._fluid_density ** (3.0 / 2.0))
            / self._g
            / R[active_parcel_idx]
            / active_layer_thickness_array[active_parcel_idx]
        )

        self._pvelocity[np.isnan(self._pvelocity)] = 0.0

        if np.max(self._pvelocity) > 1:
            warnings.warn(
                "NetworkSedimentTransporter: Maximum parcel virtual velocity"
                f" exceeds 1 m/s ({np.max(self._pvelocity)})",
                stacklevel=2,
            )

        # Assign those things to the grid -- might be useful for plotting
        self._grid.at_link["sediment_total_volume"] = self._vol_tot
        self._grid.at_link["sediment__active__volume"] = self._vol_act
        self._grid.at_link["sediment__active__sand_fraction"] = frac_sand

    def _move_parcel_downstream(self, dt: float) -> None:
        """Update parcel location for each parcel in the active layer."""

        # determine where parcels are starting
        current_link = self._parcels.dataset.element_id.values[:, -1].astype(int)
        self.current_link = current_link

        # determine location within link where parcels are starting.
        location_in_link = self._parcels.dataset.location_in_link.values[:, -1]

        # determine how far each parcel needs to travel this timestep.
        distance_to_travel_this_timestep = self._pvelocity * dt
        # total distance traveled in dt at parcel virtual velocity
        # Note: movement in current and any DS links at this dt is at the same
        # velocity as in the current link perhaps modify in the future

        # Accumulate the total distance traveled by a parcel for abrasion rate
        # calculations.
        if np.size(self._distance_traveled_cumulative) != np.size(
            distance_to_travel_this_timestep
        ):
            dist_array = distance_to_travel_this_timestep
            dist_array[: self._num_parcels] += distance_to_travel_this_timestep
            self._distance_traveled_cumulative = dist_array
        else:
            self._distance_traveled_cumulative += distance_to_travel_this_timestep
            # ^ accumulates total distanced traveled for testing abrasion

        # active parcels on the network:
        in_network = (
            self._parcels.dataset.element_id.values[:, self._time_idx]
            != self.OUT_OF_NETWORK
        )
        active = distance_to_travel_this_timestep > 0.0
        active_parcel_ids = np.nonzero(in_network * active)[0]

        distance_left_to_travel = distance_to_travel_this_timestep.copy()
        while np.any(distance_left_to_travel > 0.0):
            # Step 1: Move parcels downstream.
            on_network = current_link != self.OUT_OF_NETWORK

            # Get current link lengths:
            current_link_lengths = self._grid.at_link["reach_length"][current_link]

            # Determine where they are in the current link.
            distance_to_exit_current_link = current_link_lengths * (
                1.0 - location_in_link
            )

            # Identify which ones will come to rest in the current link.
            rest_this_link = (
                (distance_left_to_travel < distance_to_exit_current_link)
                * on_network
                * (distance_left_to_travel > 0.0)
            )

            # Deal with those staying in the current link.
            if np.any(rest_this_link):
                # print('  {x} coming to rest'.format(x=np.sum(rest_this_link)))

                # for those staying in this link, calculate the location in link
                # (note that this is a proportional distance). AND change
                # distance_left_to_travel to 0.0
                location_in_link[rest_this_link] = 1.0 - (
                    (
                        distance_to_exit_current_link[rest_this_link]
                        - distance_left_to_travel[rest_this_link]
                    )
                    / current_link_lengths[rest_this_link]
                )

                distance_left_to_travel[rest_this_link] = 0.0

            # Deal with those moving to a downstream link.
            moving_downstream = (
                (distance_left_to_travel >= distance_to_exit_current_link)
                * on_network
                * (distance_left_to_travel > 0.0)
            )
            if np.any(moving_downstream):
                # print('  {x} next link'.format(x=np.sum(moving_downstream)))
                # change location in link to 0
                location_in_link[moving_downstream] = 0.0

                # decrease distance to travel.
                distance_left_to_travel[
                    moving_downstream
                ] -= distance_to_exit_current_link[moving_downstream]

                # change current link to the downstream link.

                # get the downstream link at link:
                downstream_node = self._fd.downstream_node_at_link()[current_link]
                downstream_link = self._fd.link_to_flow_receiving_node[downstream_node]

                # assign new values to current link.
                current_link[moving_downstream] = downstream_link[moving_downstream]

                # find and address those links who have moved out of network.
                moved_oon = downstream_link == self._grid.BAD_INDEX

                if np.any(moved_oon):
                    # print('  {x} exiting network'.format(x=np.sum(moved_oon)))

                    current_link[moved_oon] = self.OUT_OF_NETWORK
                    # assign location in link of np.nan to those which moved oon
                    location_in_link[moved_oon] = np.nan
                    distance_left_to_travel[moved_oon] = 0.0

        # Step 2: Parcel is at rest... Now update its information.

        # transport dependent abrasion - update alphas for xport dependence
        if self._k_transp_dep_abr is not None:
            self._abrasion_rate_xport_dep = _calculate_transport_dep_abrasion_rate(
                self._parcels.dataset.abrasion_rate,
                self._k_transp_dep_abr,
                self._parcels.dataset.density.values,
                self._fluid_density,
                self._parcels.dataset.D.values[:, self._time_idx],
                self._tautaur,
            )
            abrasion_now = self._abrasion_rate_xport_dep.copy()
        else:
            abrasion_now = self._parcels.dataset.abrasion_rate.copy()

        # reduce D and volume due to abrasion
        vol = _calculate_parcel_volume_post_abrasion(
            self._parcels.dataset.volume[active_parcel_ids, self._time_idx],
            distance_to_travel_this_timestep[active_parcel_ids],
            abrasion_now[active_parcel_ids],
        )

        D = _calculate_parcel_grain_diameter_post_abrasion(
            self._parcels.dataset.D[active_parcel_ids, self._time_idx],
            self._parcels.dataset.volume[active_parcel_ids, self._time_idx],
            vol,
        )

        # update parcel attributes

        # arrival time in link
        self._parcels.dataset.time_arrival_in_link[
            active_parcel_ids, self._time_idx
        ] = self._time_idx

        # location in link
        self._parcels.dataset.location_in_link[active_parcel_ids, self._time_idx] = (
            location_in_link[active_parcel_ids]
        )

        self._parcels.dataset.element_id[active_parcel_ids, self._time_idx] = (
            current_link[active_parcel_ids]
        )
        #                self._parcels.dataset.active_layer[p, self._time_idx] = 1
        # ^ reset to 1 (active) to be recomputed/determined at next timestep
        self._parcels.dataset.D[active_parcel_ids, self._time_idx] = D
        self._parcels.dataset.volume[active_parcel_ids, self._time_idx] = vol

    def run_one_step(self, dt: float) -> None:
        """Run :class:`~.NetworkSedimentTransporter` forward in time.

        When the :class:`~.NetworkSedimentTransporter` runs forward in time
        the following steps occur:

        1. A new set of records is created in the Parcels that corresponds to
           the new time
        2. If parcels are on the network then:

           a. Active parcels are identified based on entrainment critera.
           b. Effective bed slope is calculated based on inactive parcel volumes.
           c. Transport rate is calculated.
           d. Active parcels are moved based on the tranport rate.

        Parameters
        ----------
        dt : float
            Duration of time to run the :class:`~.NetworkSedimentTransporter` forward.

        Raises
        ------
        RuntimeError
            If no parcels remain on the grid.

        """
        self._time += dt
        self._time_idx += 1
        self._create_new_parcel_time()

        if self._this_timesteps_parcels.any():
            self._partition_active_and_storage_layers()
            self._adjust_node_elevation()
            self._update_channel_slopes()
            self._update_transport_time()
            self._move_parcel_downstream(dt)

        else:
            raise RuntimeError("No more parcels on grid")


# %% Methods referenced above, separated for purposes of testing


def _recalculate_channel_slope(
    z_up: float, z_down: float, dx: float, threshold: float
) -> float:
    """Recalculate channel slope based on elevation.

    Parameters
    ----------
    z_up : float
        Upstream elevation.
    z_down : float
        Downstream elevation.
    dz : float
        Distance.
    threshold : float
        Minimum channel slope threshold.

    Examples
    --------
    >>> _recalculate_channel_slope(10.0, 0.0, 10.0, 0.0001)
    1.0
    >>> _recalculate_channel_slope(0.0, 0.0, 10.0, 0.0001)
    0.0001
    >>> _recalculate_channel_slope(0.0, 10.0, 10.0, 0.0001)
    0.0
    """
    chan_slope = (z_up - z_down) / dx

    if chan_slope < 0.0:
        chan_slope = 0.0
        warnings.warn(
            "NetworkSedimentTransporter: Negative channel slope"
            f" encountered ({chan_slope})",
            UserWarning,
            stacklevel=2,
        )

    elif chan_slope < threshold:
        chan_slope = threshold

    return chan_slope


def _calculate_alluvium_depth(
    stored_volume: float,
    width_of_upstream_links: ArrayLike,
    length_of_upstream_links: ArrayLike,
    width_of_downstream_link: float,
    length_of_downstream_link: float,
    porosity: float,
) -> float:
    """Calculate alluvium depth based on adjacent link inactive parcel volumes.

    Parameters
    ----------
    stored_volume : float
        Total volume of inactive parcels in this link.
    width_of_upstream_links : float
        Channel widths of upstream links.
    length_of_upstream_link : float
        Channel lengths of upstream links.
    width_of_downstream_link : float
        Channel widths of downstream links.
    length_of_downstream_link : float
        Channel lengths of downstream links.
    porosity: float
        Channel bed sediment porosity.

    Examples
    --------
    >>> _calculate_alluvium_depth(100, [0.5, 1], [10, 10], 1, 10, 0.2)
    10.0
    >>> _calculate_alluvium_depth(24, np.array([0.1, 3]), np.array([10, 10]), 1, 1, 0.5)
    3.0
    >>> _calculate_alluvium_depth(24, np.array([0.1, 3]), np.array([10, 10]), 1, 1, 2)
    Traceback (most recent call last):
    ValueError: negative alluvium depth (-1.5)
    """
    width_of_upstream_links = np.asarray(width_of_upstream_links)
    length_of_upstream_links = np.asarray(length_of_upstream_links)

    alluvium_depth = (
        2
        * stored_volume
        / (
            np.sum(width_of_upstream_links * length_of_upstream_links)
            + width_of_downstream_link * length_of_downstream_link
        )
        / (1 - porosity)
    )

    if alluvium_depth < 0.0:
        raise ValueError(f"negative alluvium depth ({alluvium_depth})")

    return alluvium_depth


def _calculate_reference_shear_stress(
    fluid_density: float,
    R: ArrayLike,
    g: float,
    mean_active_grain_size: ArrayLike,
    frac_sand: ArrayLike,
) -> float:
    """Calculate reference Shields stress (taursg) using the sand content of
    the bed surface, as per Wilcock and Crowe (2003).

    Parameters
    ----------
    fluid_density : float
        Density of fluid (generally, water).
    R: float
        Excess density ratio ``(sediment_density - fluid_density) / fluid_density``
    g: float
        Gravitational acceleration.
    mean_active_grain_size: float
        Mean grain size of the 'active' sediment parcels.
    frac_sand: float
        Fraction of the bed surface grain size composed of sand sized parcels.

    Examples
    --------
    >>> import numpy as np

    >>> np.isclose(_calculate_reference_shear_stress(1, 1, 1, 1, 0), 0.036)
    True
    >>> np.isclose(_calculate_reference_shear_stress(1000, 1.65, 9.8, 0.1, 0.9), 33.957)
    True
    """
    taursg = (
        fluid_density
        * np.asarray(R)
        * g
        * np.asarray(mean_active_grain_size)
        * (0.021 + 0.015 * np.exp(-20.0 * np.asarray(frac_sand)))
    )

    if np.any(np.asarray(taursg < 0)):
        raise ValueError(
            "NetworkSedimentTransporter: Reference Shields stress is negative"
        )

    return taursg


def _calculate_transport_dep_abrasion_rate(
    alpha: ArrayLike,
    k: ArrayLike,
    rhos: ArrayLike,
    rhow: ArrayLike,
    D: ArrayLike,
    tautaur: ArrayLike,
) -> NDArray[float]:
    """Calculate abrasion rate for each parcel in the network, accounting for
    increases in abrasion due to high bedload transport rates.

    Parameters
    ----------
    alpha : array
        Baseline abrasion rate for each parcel.
    k : float
        Transport-dependent coefficient. 0 = no transport-dependence. Standard
        values ~15-45.
    rhos : array
        Sediment density for each parcel
    rhow : float
        density of fluid
    D : array
        Grain diameter for each parcel
    tautaur : array
        Ratio of the Shields stress to reference (critical) Shields stress for
        each pacel.

    Returns
    -------
    ndarray of float
        Abrasion rate of parcels.

    Examples
    --------
    >>> _calculate_transport_dep_abrasion_rate(
    ...     [1, 1, 1, 1, 1],
    ...     55,
    ...     [1, 1, 1, 1, 1],
    ...     1000.0,
    ...     [1, 1, 1, 1, 1],
    ...     [1, 1, 1, 1, 1],
    ... )
    array([1.,  1.,  1.,  1.,  1.])
    >>> _calculate_transport_dep_abrasion_rate(0, 55, 1, 1000.0, 1, 1)
    array([0.])
    >>> _calculate_transport_dep_abrasion_rate(1, 8, [2.0], 1, 1, [4.3])
    array([9.])
    >>> _calculate_transport_dep_abrasion_rate(1, -8, [2.0], 1, 1, 4.3)
    Traceback (most recent call last):
    ValueError: transport dependent abrasion decreased abrasion rate (should increase)
    """
    alpha = np.atleast_1d(alpha)
    rhos = np.atleast_1d(rhos)
    D = np.atleast_1d(D)
    tautaur = np.atleast_1d(tautaur)

    abrasion_rate_xport_dep = np.asarray(alpha, dtype=float).copy()
    abrasion_rate_xport_dep[tautaur > 3.3] = alpha[tautaur > 3.3] * (
        1
        + k
        * ((rhos[tautaur > 3.3] - rhow) / rhow)
        * D[tautaur > 3.3]
        * (tautaur[tautaur > 3.3] - 3.3)
    )

    if np.any(alpha > abrasion_rate_xport_dep):
        raise ValueError(
            "transport dependent abrasion decreased abrasion rate (should increase)"
        )

    return abrasion_rate_xport_dep


def _calculate_parcel_volume_post_abrasion(
    starting_volume: ArrayLike, travel_distance: ArrayLike, abrasion_rate: ArrayLike
) -> NDArray[float]:
    """Calculate parcel volumes after abrasion, according to Sternberg
    exponential abrasion.

    Parameters
    ----------
    starting_volume : float or array
        Starting volume of each parcel.
    travel_distance: float or array
        Travel distance for each parcel during this timestep, in ___.
    abrasion_rate: float or array
        Volumetric abrasion exponent (1/m).

    Examples
    --------
    >>> _calculate_parcel_volume_post_abrasion(10, 100, 0.003)
    7.4081822068171785
    >>> _calculate_parcel_volume_post_abrasion(10, 300, 0.1)
    9.357622968840175e-13
    >>> _calculate_parcel_volume_post_abrasion(10, 300, -3)
    Traceback (most recent call last):
    ValueError: parcel volume has increased due to abrasion
    """
    starting_volume = np.asarray(starting_volume)
    travel_distance = np.asarray(travel_distance)
    abrasion_rate = np.asarray(abrasion_rate)

    volume = starting_volume * np.exp(travel_distance * (-abrasion_rate))

    if np.any(volume > starting_volume):
        raise ValueError("parcel volume has increased due to abrasion")

    return volume


def _calculate_parcel_grain_diameter_post_abrasion(
    starting_diameter: ArrayLike,
    pre_abrasion_volume: ArrayLike,
    post_abrasion_volume: ArrayLike,
) -> NDArray[float]:
    """Calculate parcel grain diameters after abrasion, according to Sternberg
    exponential abrasion.

    Parameters
    ----------
    starting_diameter : float or array
        Starting volume of each parcel.
    pre_abrasion_volume: float or array
        Parcel volume before abrasion.
    post_abrasion_volume: float or array
        Parcel volume after abrasion.

    Examples
    --------
    >>> import numpy as np
    >>> from numpy.testing import assert_almost_equal

    If no abrasion happens, we should get the same value.

    >>> _calculate_parcel_grain_diameter_post_abrasion(10, 1, 1)
    10.0

    If some abrasion happens, test the value.

    >>> starting_diameter = 10
    >>> pre_abrasion_volume = 2
    >>> post_abrasion_volume = 1
    >>> expected_value = starting_diameter * (
    ...     post_abrasion_volume / pre_abrasion_volume
    ... ) ** (1.0 / 3.0)
    >>> print(np.round(expected_value, decimals=3))
    7.937
    >>> assert_almost_equal(
    ...     _calculate_parcel_grain_diameter_post_abrasion(10, 2, 1), expected_value
    ... )
    """
    starting_diameter = np.asarray(starting_diameter)
    post_abrasion_volume = np.asarray(post_abrasion_volume)
    pre_abrasion_volume = np.asarray(pre_abrasion_volume)

    abraded_grain_diameter = starting_diameter * (
        post_abrasion_volume / pre_abrasion_volume
    ) ** (1.0 / 3.0)

    return abraded_grain_diameter



================================================
File: network_sediment_transporter/sediment_pulser_at_links.py
================================================
import numpy as np

from landlab.components.network_sediment_transporter.sediment_pulser_base import (
    SedimentPulserBase,
)
from landlab.data_record.data_record import DataRecord

_OUT_OF_NETWORK = -2


class SedimentPulserAtLinks(SedimentPulserBase):
    """Send a pulse of parcels to specific links in a channel network

    :class:`~.SedimentPulserAtLinks` is instantiated by specifying the
    :class:`~.NetworkModelGrid` it will pulse the parcels into and the time(s) when
    a pulse is allowed to occur.  It inherits attributes and functions from the
    :class:`~.SedimentPulserBase`.

    :class:`~.SedimentPulserAtLinks` is run (adds parcels to ``DataRecord``) by
    calling the instance with a list of links and a list of the number of parcels
    added to each link.

    If parcel attributes are constant with time and uniform
    across the basin, these constant-uniform-attributes can be defined
    when :class:`~.SedimentPulserAtLinks` is instantiated. If parcel attributes vary
    with location and time, the user specifies the varying parcel attributes
    each time the instance is called with a list for each attribute of length
    equal to the number of links included in the pulse.


    .. codeauthor:: Jeff Keck, Allison Pfeiffer, Shelby Ahrendt
                    (with help from Eric Hutton and Katy Barnhart)


    Examples
    --------
    >>> import numpy as np
    >>> from landlab import NetworkModelGrid

    Create the network model grid the parcels will be added to.

    >>> y_of_node = (0, 100, 200, 200, 300, 400, 400, 125)
    >>> x_of_node = (0, 0, 100, -50, -100, 50, -150, -100)
    >>> nodes_at_link = ((1, 0), (2, 1), (1, 7), (3, 1), (3, 4), (4, 5), (4, 6))
    >>> grid = NetworkModelGrid((y_of_node, x_of_node), nodes_at_link)
    >>> grid.at_link["channel_width"] = np.full(grid.number_of_links, 1.0)  # m
    >>> grid.at_link["channel_slope"] = np.full(grid.number_of_links, 0.01)  # m / m
    >>> grid.at_link["reach_length"] = np.full(grid.number_of_links, 100.0)  # m

    Define a function that contains which times a pulse is allowed to occur.
    This function says a pulse can occur at any time

    >>> def time_to_pulse(time):
    ...     return True
    ...

    Instantiate :class:`~.SedimentPulserAtLinks`

    >>> make_pulse = SedimentPulserAtLinks(grid, time_to_pulse=time_to_pulse)

    Run the instance with inputs for the time, link location and number of
    parcels. Other attributes will use the default values in the base class

    >>> time = 11
    >>> links = [2, 6]
    >>> n_parcels_at_link = [2, 3]
    >>> parcels = make_pulse(
    ...     time=time, links=links, n_parcels_at_link=n_parcels_at_link
    ... )

    Check the element_id of each parcel

    >>> print(parcels.dataset["element_id"].values)
    [[2]
     [2]
     [6]
     [6]
     [6]]
    """

    _name = "SedimentPulserAtLinks"

    _unit_agnostic = False

    _info = {}  # works with the DataRecord

    def __init__(
        self,
        grid,
        time_to_pulse=None,
        parcels=None,
        D50=0.05,
        D84_D50=2.1,
        rho_sediment=2650.0,
        parcel_volume=0.5,
        abrasion_rate=0.0,
        rng=None,
    ):
        """Create :class:`~.SedimentPulserAtLinks`.

        Parameters
        ----------

        grid : ModelGrid
            landlab :class:`~.ModelGrid` to place sediment parcels on.
        time_to_pulse: function, optional
            The condition when a pulse occurs using the ``_pulse_characteristics``
            method. If not specified, a pulse occurs whenever the instance is run
        parcels: landlab DataRecord
            Tracks parcel location and variables.
        D50: float, optional
            Median grain size [m].
        D84_D50: float, optional
            Ratio of 84th percentile grain size to the median grain size.
        rho_sediment : float, optional
            Sediment grain density [kg / m^3].
        parcel_volume : float
            Parcel volume [m^3]
        abrasion_rate: float
            Volumetric abrasion exponent [1 / m]
        """
        if rng is None:
            rng = np.random.default_rng()
        elif isinstance(rng, int):
            rng = np.random.default_rng(seed=rng)
        self._rng = rng

        SedimentPulserBase.__init__(
            self,
            grid,
            parcels=parcels,
            D50=D50,
            D84_D50=D84_D50,
            rho_sediment=rho_sediment,
            parcel_volume=parcel_volume,
            abrasion_rate=abrasion_rate,
        )

        # set time_to_pulse to True if not specified
        if time_to_pulse is None:
            self._time_to_pulse = lambda time: True
        else:
            self._time_to_pulse = time_to_pulse

    def __call__(
        self,
        time,
        links=None,
        n_parcels_at_link=None,
        D50=None,
        D84_D50=None,
        rho_sediment=None,
        parcel_volume=None,
        abrasion_rate=None,
    ):
        """
        specify the time, link(s) and attributes of pulses added to a
        :class:`~.NetworkModelGrid` at stochastically determined locations within the
        link(s).

        Parameters
        ----------
        time : integer or datetime64
            Time that the pulse is occurs.
        links : list of int
            Link ID # that parcels are added to.
        n_parcels_at_link: list of int
            Number of parcels added to each link listed in links
        D50 : list of float, optional
            Median grain size of parcels added to each link listed in links [m].
        D84_D50 : list of float, optional
            Ratio of 84th percentile grain size to the median grain size.
        rho_sediment: list of float, optional
            Density of grains [kg / m^3].
        parcel_volume : list of float, optional
            Volume of each parcel added to link listed in links [m^3].
        abrasion_rate: list of float, optional
            Rate that grain size decreases with distance along channel [mm / km?].

        Returns
        -------
        parcels
            :class:`~.DataRecord` containing all information on each individual parcel.

        """

        # check user provided links and number of parcels sent to each link
        assert (
            links is not None and n_parcels_at_link is not None
        ), "must provide links and number of parcels entered into each link"

        links = np.array(links)
        n_parcels_at_link = np.array(n_parcels_at_link)

        # any parameters not specified with __Call__ method use default values
        # specified in the base class
        if D50 is None:
            D50 = np.full_like(links, self._D50, dtype=float)
        else:
            D50 = np.array(D50)

        if D84_D50 is None:
            D84_D50 = np.full_like(links, self._D84_D50, dtype=float)
        else:
            D84_D50 = np.array(D84_D50)

        if rho_sediment is None:
            rho_sediment = np.full_like(links, self._rho_sediment, dtype=float)
        else:
            rho_sediment = np.array(rho_sediment)

        if parcel_volume is None:
            parcel_volume = np.full_like(links, self._parcel_volume, dtype=float)
        else:
            parcel_volume = np.array(parcel_volume)

        if abrasion_rate is None:
            abrasion_rate = np.full_like(links, self._abrasion_rate, dtype=float)
        else:
            abrasion_rate = np.array(abrasion_rate)

        # before running, check that no inputs < 0
        # check for negative inputs
        if (
            np.array([D50, D84_D50, rho_sediment, parcel_volume, abrasion_rate]) < 0
        ).any():
            raise AssertionError("parcel attributes cannot be less than zero")
        # before running, check if time to pulse
        if not self._time_to_pulse(time):
            # if not time to pulse, return the existing parcels
            print("user provided time not a time-to-pulse, parcels have not changed")

            return self._parcels

        # create items and variables for DataRecord
        variables, items = self._sediment_pulse_stochastic(
            time,
            links,
            n_parcels_at_link,
            parcel_volume,
            D50,
            D84_D50,
            abrasion_rate,
            rho_sediment,
        )

        # if DataRecord does not exist, create one
        if self._parcels is None:
            self._parcels = DataRecord(
                self._grid,
                items=items,
                time=[time],
                data_vars=variables,
                dummy_elements={"link": [_OUT_OF_NETWORK]},
            )
        # else, add parcels to existing DataRecrod
        else:
            self._parcels.add_item(time=[time], new_item=items, new_item_spec=variables)

        return self._parcels

    def _sediment_pulse_stochastic(
        self,
        time,
        links,
        n_parcels_at_link,
        parcel_volume,
        D50,
        D84_D50,
        abrasion_rate,
        rho_sediment,
    ):
        """Convert lists of link ids and link parcel parameters to a dataset
        that describes the network location and attributes of each individual parcel

        Returns
        -------
        dict
            Dictionary with keys and data in format for :class:`~.DataRecord`.

        """

        # Create np array for each paracel attribute. Length of array is equal
        # to the number of parcels

        # link id, D50 and volume
        element_id = np.empty(np.sum(n_parcels_at_link), dtype=int)
        grain_size = np.empty(np.sum(n_parcels_at_link))
        volume = np.empty(np.sum(n_parcels_at_link))
        offset = 0
        for link, n_parcels in enumerate(n_parcels_at_link):
            element_id[offset : offset + n_parcels] = links[link]
            grain_size[offset : offset + n_parcels] = self._rng.lognormal(
                np.log(D50[link]), np.log(D84_D50[link]), n_parcels
            )
            volume[offset : offset + n_parcels] = parcel_volume[link] % n_parcels
            offset += n_parcels
        starting_link = element_id.copy()

        # abrasion rate and density
        abrasion_rate_L = []
        density_L = []
        for c, ei in enumerate(np.unique(element_id)):
            element_id_subset = element_id[element_id == ei]
            abrasion_rate_L = abrasion_rate_L + list(
                np.full_like(element_id_subset, abrasion_rate[c], dtype=float)
            )
            density_L = density_L + list(
                np.full_like(element_id_subset, rho_sediment[c], dtype=float)
            )
        abrasion_rate = np.array(abrasion_rate_L)
        density = np.array(density_L)

        element_id = np.expand_dims(element_id, axis=1)
        grain_size = np.expand_dims(grain_size, axis=1)
        volume = np.expand_dims(volume, axis=1)

        # time of arrivial (time instance called)
        time_arrival_in_link = np.full(np.shape(element_id), time, dtype=float)

        # link location (distance from link inlet / link length) is stochastically
        # determined
        location_in_link = np.expand_dims(
            self._rng.uniform(size=np.sum(n_parcels_at_link)), axis=1
        )

        # All parcels in pulse are in the active layer (1) rather than subsurface (0)
        active_layer = np.ones(np.shape(element_id))

        # specify that parcels are in the links of the network model grid
        grid_element = ["link"] * np.size(element_id)
        grid_element = np.expand_dims(grid_element, axis=1)

        return {
            "starting_link": (["item_id"], starting_link),
            "abrasion_rate": (["item_id"], abrasion_rate),
            "density": (["item_id"], density),
            "time_arrival_in_link": (["item_id", "time"], time_arrival_in_link),
            "active_layer": (["item_id", "time"], active_layer),
            "location_in_link": (["item_id", "time"], location_in_link),
            "D": (["item_id", "time"], grain_size),
            "volume": (["item_id", "time"], volume),
        }, {"grid_element": grid_element, "element_id": element_id}



================================================
File: network_sediment_transporter/sediment_pulser_base.py
================================================
from landlab.core.model_component import Component
from landlab.grid.network import NetworkModelGrid


class SedimentPulserBase(Component):
    """Base class of :class:`~.SedimentPulserAtLinks` and :class:`~.SedimentPulserEachParcel`.

    :class:`~.SedimentPulserAtLinks` and :class:`~.SedimentPulserEachParcel` run the
    landlab :class:`~.DataRecord` :meth:`~.DataRecord.add_item` method on a
    :class:`~.DataRecord` configured for :class:`~.NetworkSedimentTransporter`.


    .. codeauthor: Jeff Keck, Allison Pfeiffer, Shelby Ahrendt
                   (with help from Eric Hutton and Katy Barnhart)

    Parameters
    ----------
    grid : ModelGrid
        landlab *ModelGrid* to place sediment parcels on.
    parcels: landlab DataRecord
        Tracks parcel location and variables
    D50: float, optional
        median grain size [m]
    D84_D50: float, optional
        ratio of 84th percentile grain size to the median grain size
    rho_sediment : float, optional
        Sediment grain density [kg / m^3].
    parcel_volume : float, optional
        parcel volume used for all parcels that do not have a specified volume
    abrasion_rate: float, optional
        volumetric abrasion exponent [1/m]


    Examples
    --------
    >>> import numpy as np
    >>> from landlab import NetworkModelGrid

    >>> y_of_node = (0, 100, 200, 200, 300, 400, 400, 125)
    >>> x_of_node = (0, 0, 100, -50, -100, 50, -150, -100)
    >>> nodes_at_link = ((1, 0), (2, 1), (1, 7), (3, 1), (3, 4), (4, 5), (4, 6))
    >>> grid = NetworkModelGrid((y_of_node, x_of_node), nodes_at_link)
    >>> grid.at_link["channel_width"] = np.full(grid.number_of_links, 1.0)  # m
    >>> grid.at_link["channel_slope"] = np.full(grid.number_of_links, 0.01)  # m / m
    >>> grid.at_link["reach_length"] = np.full(grid.number_of_links, 100.0)  # m
    >>> make_pulse_base = SedimentPulserBase(grid)
    >>> make_pulse_base._parcels

    SedimentPulserBase does not have any methods for adding a pulse

    >>> a_pulse = make_pulse_base()
    Traceback (most recent call last):
    ...
    NotImplementedError: the base component has no call method
    """

    _name = "SedimentPulserBase"

    _unit_agnostic = False

    _info = {}  # works with the DataRecord

    def __init__(
        self,
        grid,
        parcels=None,
        D50=0.05,
        D84_D50=2.1,
        rho_sediment=2650.0,
        parcel_volume=0.5,
        abrasion_rate=0.0,
    ):
        self._grid = grid
        self._parcels = parcels
        self._D50 = D50
        self._D84_D50 = D84_D50
        self._rho_sediment = rho_sediment
        self._parcel_volume = parcel_volume
        self._abrasion_rate = abrasion_rate

        if not isinstance(grid, NetworkModelGrid):
            raise ValueError(
                "NetworkSedimentTransporter: grid must be NetworkModelGrid"
            )

    def __call__(self):
        """__call__ is not implemented for this component."""
        raise NotImplementedError("the base component has no call method")



================================================
File: network_sediment_transporter/sediment_pulser_each_parcel.py
================================================
import warnings

import numpy as np

from landlab.components.network_sediment_transporter.sediment_pulser_base import (
    SedimentPulserBase,
)
from landlab.data_record.data_record import DataRecord

_OUT_OF_NETWORK = -2


class SedimentPulserEachParcel(SedimentPulserBase):
    """Send pulses of sediment to specific point locations within the channel
    network and divide the pulses into parcels. Pulses may be any volume.
    Parcels must be less than or equal to a user specified maximum volume.

    SedimentPulserEachParcel is instantiated by specifying the network model grid
    it will pulse the parcels into

    SedimentPulserEachParcel is run (adds parcels to DataRecrod) by calling the
    SedimentPulserEachParcel instance with the time that pulses are added to
    the channel network and a sediment pulse table (PulseDF)

    PulseDF is a pandas dataframe. At a minimum, the dataframe must have columns 'Link#'
    'normalized_downstream_distance' and 'pulse_volume'. Optionally, the parcel
    volume that the pulse is divided into and grain characteristics of each pulse
    can also be specified in PulseDF.


    .. codeauthor: Jeff Keck, Allison Pfeiffer, Shelby Ahrendt
                   (with help from Eric Hutton and Katy Barnhart)


    Examples
    --------
    >>> import numpy as np
    >>> import pandas as pd
    >>> from landlab import NetworkModelGrid

    Create the network model grid. Pulses are added to the links of the network
    model grid.

    >>> y_of_node = (0, 100, 200, 200, 300, 400, 400, 125)
    >>> x_of_node = (0, 0, 100, -50, -100, 50, -150, -100)
    >>> nodes_at_link = ((1, 0), (2, 1), (1, 7), (3, 1), (3, 4), (4, 5), (4, 6))
    >>> grid = NetworkModelGrid((y_of_node, x_of_node), nodes_at_link)
    >>> grid.at_link["channel_width"] = np.full(grid.number_of_links, 1.0)  # m
    >>> grid.at_link["channel_slope"] = np.full(grid.number_of_links, 0.01)  # m / m
    >>> grid.at_link["reach_length"] = np.full(grid.number_of_links, 100.0)  # m


    Instantiate 'SedimentPulserEachParcel'

    >>> make_pulse = SedimentPulserEachParcel(grid)

    Define the PulseDF and time of the pulse

    >>> PulseDF = pd.DataFrame(
    ...     {
    ...         "pulse_volume": [0.2, 1, 1.1, 0.5],
    ...         "link_#": [1, 3, 5, 2],
    ...         "normalized_downstream_distance": [0.8, 0.7, 0.5, 0.2],
    ...     }
    ... )
    >>> time = 7

    Run the instance

    >>> parcels = make_pulse(time, PulseDF)

    This should yield a UserWarning: Parcels not provided, created a new DataRecord

    check element_id of each parcel

    >>> print(parcels.dataset["element_id"].values)
    [[1]
    [3]
    [3]
    [5]
    [5]
    [5]
    [2]]

    """

    _name = "SedimentPulserEachParcel"

    _unit_agnostic = False

    _info = {}  # works with the DataRecord

    def __init__(
        self,
        grid,
        parcels=None,
        D50=0.05,
        D84_D50=2.1,
        rho_sediment=2650.0,
        parcel_volume=0.5,
        abrasion_rate=0.0,
        rng=None,
    ):
        """
        instantiate SedimentPulserEachParcel

        Parameters
        ----------
        grid : ModelGrid
            landlab *ModelGrid* to place sediment parcels on.
        parcels: landlab DataRecord, optional
            Tracks parcel location and attributes
        D50: float, optional
            median grain size [m]
        D84_D50: float, optional
            ratio of 84th percentile grain size to the median grain size
        rho_sediment : float, optional
            Sediment grain density [kg/m^3].
        parcel_volume : float, optional
            parcel volume [m^3]
        abrasion_rate: float, optional
            volumetric abrasion exponent [1/m]
        """
        if rng is None:
            rng = np.random.default_rng()
        elif isinstance(rng, int):
            rng = np.random.default_rng(seed=rng)
        self._rng = rng

        SedimentPulserBase.__init__(
            self,
            grid,
            parcels=parcels,
            D50=D50,
            D84_D50=D84_D50,
            rho_sediment=rho_sediment,
            parcel_volume=parcel_volume,
            abrasion_rate=abrasion_rate,
        )

    def __call__(self, time, PulseDF=None):
        """specify the location and attributes of each pulse of material added to
        a Network Model Grid DataRecord

        Parameters
        ----------
        time : integer or datetime64 value equal to nst.time
            time that the pulse is triggered in the network sediment transporter
        PulseDF : pandas dataframe
            each row contains information on the deposition location and volume of
            a single pulse of sediment. The pulse is divided into 'n' number of
            parcels, where 'n' equals the np.ceil(pulse volume / parcel volume)
            For details on the format of the DataFrame, see the docstring for
            function _sediment_pulse_dataframe


        Returns
        -------
            self._parcels
                a DataRecord containing all information on each individual parcel
        """
        # If no PulseDF provided, raise error. Should at least provide an empty PulseDF
        if PulseDF is None:
            raise ValueError("PulseDF was not specified")

        if (
            PulseDF.empty is True
        ):  # if empty, pulser stops, returns the existing parcels, call stops
            warnings.warn("Pulse DataFrame is EMPTY", stacklevel=2)
            return self._parcels

        variables, items = self._sediment_pulse_dataframe(
            time,  # create variabels and and items needed to create the data record
            PulseDF,
        )

        if self._parcels is None:  # if no parcels, create parcels
            self._parcels = DataRecord(
                self._grid,
                items=items,
                time=[time],
                data_vars=variables,
                dummy_elements={"link": [_OUT_OF_NETWORK]},
            )

            warnings.warn(
                "Parcels not provided, created a new DataRecord", stacklevel=2
            )

        else:  # else use the add item method to add parcels
            self._parcels.add_item(time=[time], new_item=items, new_item_spec=variables)

        return self._parcels

    def _sediment_pulse_dataframe(self, time, PulseDF):
        """Convert PulseDF to a :class:`~.DataRecord` formatted for the
        :class:`~.NetworkSedimentTransporter`.

        Parameters
        ----------
        time : integer or datetime64

        PulseDF : pandas dataframe

            The PulseDF must include the following columns:
                'link_#', 'pulse_volume', 'normalized_downstream_distance'

            Optionally, PulseDF can include the following columns:
               'D50', 'D84_D50', 'abrasion_rate', 'rho_sediment', 'parcel_volume'

            Values in each columne are defined as follows:

            'link_#': int - link number pulse enters the channel network
            'pulse_volume: float - total volume of the pulse [m^3]
            'normalized_downstream_distance': float - distance from link inlet
                                                      divided by link length
            'D50': float - median grain size [m]
            'D84_D50': float - grain-size standard deviation [m]
            'abrasion_rate': float - rate that grain size decreases with
                                     distance along channel [mm/km?]
            'rho_sediment': float - density grains [kg/m^3]
            'parcel_volume': float - maximum volume of one parcel [m^3]


            if the optional columns are not included in PulseDF, those parameters
            are assumed uniform across the basin, constant with time and equal
            to the corrisponding class variables.

        Returns
        -------
        tuple: (variables, items)
            variables: dictionary, attribute values for all new parcels
            item_id: dictionary, model grid element and index of element of each parcel

        """
        # split pulse into parcels.
        p_np = []  # list of number of parcels in each pulse
        volume = np.array([])  # list of parcel volumes from all pulses
        for _index, row in PulseDF.iterrows():
            # set the maximum allowable parcel volume using either
            # the default value or value in PulseDF
            if "parcel_volume" in PulseDF:
                mpv = row["parcel_volume"]
            else:
                mpv = self._parcel_volume

            # split the pulse into parcels
            if row["pulse_volume"] < mpv:
                # only one partial parcel volume
                v_p = np.array([row["pulse_volume"]])
            else:
                # number of whole parcels
                n_wp = int(np.floor(row["pulse_volume"] / mpv))
                # array of volumes, whole parcels
                v_wp = np.ones(n_wp) * mpv
                # volume of last parcel, a partial parcel
                v_pp = np.array([row["pulse_volume"] % mpv])
                # array of all parcel volumes
                # partial parcel included if volume > 0
                if v_pp > 0:
                    v_p = np.concatenate((v_wp, v_pp))
                else:
                    v_p = v_wp
            volume = np.concatenate((volume, v_p))
            p_np.append(len(v_p))
        volume = np.expand_dims(volume, axis=1)

        # link location
        link_distance_ratio = np.array([])
        for i, val in enumerate(PulseDF["normalized_downstream_distance"].values):
            # parcels from the same pulse enter channel at the same point
            link_distance_ratio = np.concatenate(
                (link_distance_ratio, np.ones(p_np[i]) * val)
            )
        location_in_link = np.expand_dims(link_distance_ratio, axis=1)

        # element id and starting link
        element_id = np.array([])
        for i, row in PulseDF.iterrows():
            element_id = np.concatenate((element_id, np.ones(p_np[i]) * row["link_#"]))
        starting_link = element_id.copy()
        element_id = np.expand_dims(element_id.astype(int), axis=1)

        # specify that parcels are in the links of the network model grid
        grid_element = ["link"] * np.size(element_id)
        grid_element = np.expand_dims(grid_element, axis=1)

        # time of arrivial (time instance called)
        time_arrival_in_link = np.full(np.shape(element_id), time, dtype=float)

        # All parcels in pulse are in the active layer (1) rather than subsurface (0)
        active_layer = np.ones(np.shape(element_id))

        if "rho_sediment" in PulseDF.columns:
            density = np.array([])
            for i, row in PulseDF.iterrows():
                density = np.concatenate(
                    (density, np.ones(p_np[i]) * row["rho_sediment"])
                )
        else:
            density = self._rho_sediment * np.ones(np.shape(starting_link))

        if "abrasion_rate" in PulseDF.columns:
            abrasion_rate = np.array([])
            for i, row in PulseDF.iterrows():
                abrasion_rate = np.concatenate(
                    (abrasion_rate, np.ones(p_np[i]) * row["abrasion_rate"])
                )
        else:
            abrasion_rate = self._abrasion_rate * np.ones(np.shape(starting_link))

        if "D50" in PulseDF.columns and "D84_D50" in PulseDF.columns:
            grain_size = np.array([])
            for i, row in PulseDF.iterrows():
                # det D50 and D84_D50
                n_parcels = p_np[i]
                D50 = row["D50"]
                D84_D50 = row["D84_D50"]
                grain_size_pulse = self._rng.lognormal(
                    np.log(D50), np.log(D84_D50), n_parcels
                )
                grain_size = np.concatenate((grain_size, grain_size_pulse))
        else:
            n_parcels = sum(p_np)
            D50 = self._D50
            D84_D50 = self._D84_D50
            grain_size = self._rng.lognormal(np.log(D50), np.log(D84_D50), n_parcels)

        grain_size = np.expand_dims(grain_size, axis=1)

        return {
            "starting_link": (["item_id"], starting_link),
            "abrasion_rate": (["item_id"], abrasion_rate),
            "density": (["item_id"], density),
            "time_arrival_in_link": (["item_id", "time"], time_arrival_in_link),
            "active_layer": (["item_id", "time"], active_layer),
            "location_in_link": (["item_id", "time"], location_in_link),
            "D": (["item_id", "time"], grain_size),
            "volume": (["item_id", "time"], volume),
        }, {"grid_element": grid_element, "element_id": element_id}



================================================
File: nonlinear_diffusion/Perron_nl_diffuse.py
================================================
import numpy as np
import scipy.sparse as sparse
import scipy.sparse.linalg as linalg

from landlab import Component

# Things to add: 1. Explicit stability check.
# 2. Implicit handling of scenarios where kappa*dt exceeds critical step -
#    subdivide dt automatically.


class PerronNLDiffuse(Component):
    """Nonlinear diffusion, following Perron (2011).

    This module uses Taylor Perron's implicit (2011) method to solve the
    nonlinear hillslope diffusion equation across a rectangular, regular grid
    for a single timestep. Note it works with the mass flux implicitly, and
    thus does not actually calculate it. Grid must be at least 5x5.

    Boundary condition handling assumes each edge uses the same BC for each of
    its nodes.
    This component cannot yet handle looped boundary conditions, but all others
    should be fine.

    This component has KNOWN STABILITY ISSUES which will be resolved in a
    future release; use at your own risk.

    The primary method of this class is :func:`run_one_step`.

    Examples
    --------
    >>> from landlab.components import PerronNLDiffuse
    >>> from landlab import RasterModelGrid
    >>> import numpy as np
    >>> mg = RasterModelGrid((5, 5))
    >>> z = mg.add_zeros("topographic__elevation", at="node")
    >>> nl = PerronNLDiffuse(mg, nonlinear_diffusivity=1.0)
    >>> dt = 100.0
    >>> nt = 20
    >>> uplift_rate = 0.001
    >>> for i in range(nt):
    ...     z[mg.core_nodes] += uplift_rate * dt
    ...     nl.run_one_step(dt)
    ...
    >>> z_target = [
    ...     [0.0, 0.0, 0.0, 0.0, 0.0],
    ...     [0.0, 0.00778637, 0.0075553, 0.00778637, 0.0],
    ...     [0.0, 0.0075553, 0.0078053, 0.0075553, 0.0],
    ...     [0.0, 0.00778637, 0.0075553, 0.00778637, 0.0],
    ...     [0.0, 0.0, 0.0, 0.0, 0.0],
    ... ]
    >>> np.allclose(z.reshape(mg.shape), z_target)
    True

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    Perron, J. (2011). Numerical methods for nonlinear hillslope transport laws.
    Journal of Geophysical Research  116(F2), 23 - 13.
    https://dx.doi.org/10.1029/2010jf001801

    """

    _name = "PerronNLDiffuse"

    _unit_agnostic = True

    _info = {
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        }
    }

    def __init__(
        self,
        grid,
        nonlinear_diffusivity=0.01,
        S_crit=33.0 * np.pi / 180.0,
        rock_density=2700.0,
        sed_density=2700.0,
    ):
        """
        Parameters
        ----------
        grid : RasterModelGrid
            A Landlab raster grid
        nonlinear_diffusivity : float, array or field name
            The nonlinear diffusivity
        S_crit : float (radians)
            The critical hillslope angle
        rock_density : float (kg*m**-3)
            The density of intact rock
        sed_density : float (kg*m**-3)
            The density of the mobile (sediment) layer
        """
        super().__init__(grid)

        self._bc_set_code = self._grid.bc_set_code
        self._values_to_diffuse = "topographic__elevation"
        self._kappa = nonlinear_diffusivity
        self._rock_density = rock_density
        self._sed_density = sed_density
        self._S_crit = S_crit
        self._uplift = 0.0
        self._delta_x = grid.dx
        self._delta_y = grid.dy
        self._one_over_delta_x = 1.0 / self._delta_x
        self._one_over_delta_y = 1.0 / self._delta_y
        self._one_over_delta_x_sqd = self._one_over_delta_x**2.0
        self._one_over_delta_y_sqd = self._one_over_delta_y**2.0
        self._b = 1.0 / self._S_crit**2.0

        ncols = grid.number_of_node_columns
        self._ncols = ncols
        nrows = grid.number_of_node_rows
        self._nrows = nrows
        nnodes = grid.number_of_nodes
        self._nnodes = nnodes
        ninteriornodes = grid.number_of_interior_nodes
        ncorenodes = ninteriornodes - 2 * (ncols + nrows - 6)
        self._ninteriornodes = ninteriornodes
        self._interior_grid_width = ncols - 2
        self._core_cell_width = ncols - 4

        self._interior_corners = np.array(
            [ncols + 1, 2 * ncols - 2, nnodes - 2 * ncols + 1, nnodes - ncols - 2]
        )
        _left_list = np.array(range(2 * ncols + 1, nnodes - 2 * ncols, ncols))
        # ^these are still real IDs
        _right_list = np.array(range(3 * ncols - 2, nnodes - 2 * ncols, ncols))
        _bottom_list = np.array(range(ncols + 2, 2 * ncols - 2))
        _top_list = np.array(range(nnodes - 2 * ncols + 2, nnodes - ncols - 2))
        self._left_list = _left_list
        self._right_list = _right_list
        self._bottom_list = _bottom_list
        self._top_list = _top_list

        self._core_nodes = self._coreIDtoreal(np.arange(ncorenodes, dtype=int))
        self._corenodesbyintIDs = self._realIDtointerior(self._core_nodes)
        self._ncorenodes = len(self._core_nodes)

        self._corner_interior_IDs = self._realIDtointerior(self._interior_corners)
        # ^i.e., interior corners as interior IDs
        self._bottom_interior_IDs = self._realIDtointerior(np.array(_bottom_list))
        self._top_interior_IDs = self._realIDtointerior(np.array(_top_list))
        self._left_interior_IDs = self._realIDtointerior(np.array(_left_list))
        self._right_interior_IDs = self._realIDtointerior(np.array(_right_list))

        # build an ID map to let us easily map the variables of the core nodes
        # onto the operating matrix:
        # This array is ninteriornodes long, but the IDs it contains are
        # REAL IDs
        operating_matrix_ID_map = np.empty((ninteriornodes, 9))
        self._interior_IDs_as_real = self._interiorIDtoreal(np.arange(ninteriornodes))
        for j in range(ninteriornodes):
            i = self._interior_IDs_as_real[j]
            operating_matrix_ID_map[j, :] = np.array(
                [
                    (i - ncols - 1),
                    (i - ncols),
                    (i - ncols + 1),
                    (i - 1),
                    i,
                    (i + 1),
                    (i + ncols - 1),
                    (i + ncols),
                    (i + ncols + 1),
                ]
            )
        self._operating_matrix_ID_map = operating_matrix_ID_map
        self._operating_matrix_core_int_IDs = self._realIDtointerior(
            operating_matrix_ID_map[self._corenodesbyintIDs, :]
        )
        # ^shape(ncorenodes,9)
        # see below for corner and edge maps

        # Build masks for the edges and corners to be applied to the operating
        # matrix map.
        # Antimasks are the boundary nodes, masks are "normal"
        self._topleft_mask = [1, 2, 4, 5]
        topleft_antimask = [0, 3, 6, 7, 8]
        self._topright_mask = [0, 1, 3, 4]
        topright_antimask = [2, 5, 6, 7, 8]
        self._bottomleft_mask = [4, 5, 7, 8]
        bottomleft_antimask = [0, 1, 2, 3, 6]
        self._bottomright_mask = [3, 4, 6, 7]
        bottomright_antimask = [0, 1, 2, 5, 8]
        self._corners_masks = np.vstack(
            (
                self._bottomleft_mask,
                self._bottomright_mask,
                self._topleft_mask,
                self._topright_mask,
            )
        )
        # ^(each_corner,mask_for_each_corner)
        self._corners_antimasks = np.vstack(
            (
                bottomleft_antimask,
                bottomright_antimask,
                topleft_antimask,
                topright_antimask,
            )
        )
        # ^so shape becomes (4,5)
        self._left_mask = [1, 2, 4, 5, 7, 8]
        self._left_antimask = [0, 3, 6]
        self._top_mask = [0, 1, 2, 3, 4, 5]
        self._top_antimask = [6, 7, 8]
        self._right_mask = [0, 1, 3, 4, 6, 7]
        self._right_antimask = [2, 5, 8]
        self._bottom_mask = [3, 4, 5, 6, 7, 8]
        self._bottom_antimask = [0, 1, 2]
        self._antimask_corner_position = [0, 2, 2, 4]
        # ^this is the position w/i the corner antimasks that the true corner
        # actually occupies

        self._modulator_mask = np.array(
            [-ncols - 1, -ncols, -ncols + 1, -1, 0, 1, ncols - 1, ncols, ncols + 1]
        )

        self.updated_boundary_conditions()

    def updated_boundary_conditions(self):
        """Call if grid BCs are updated after component instantiation."""
        grid = self._grid
        nrows = self._nrows
        ncols = self._ncols
        # ^Set up terms for BC handling (still feels very clumsy)
        bottom_edge = grid.nodes_at_bottom_edge[1:-1]
        top_edge = grid.nodes_at_top_edge[1:-1]
        left_edge = grid.nodes_at_left_edge[1:-1]
        right_edge = grid.nodes_at_right_edge[1:-1]
        self._bottom_flag = 1
        self._top_flag = 1
        self._left_flag = 1
        self._right_flag = 1
        # self._corner_flags = [1,1,1,1] #In ID order, so BL,BR,TL,TR
        if np.all(grid.status_at_node[bottom_edge] == 4):
            # ^This should be all of them, or none of them
            self._bottom_flag = 4
        elif np.all(grid.status_at_node[bottom_edge] == 3):
            self._bottom_flag = 3
        elif np.all(grid.status_at_node[bottom_edge] == 2):
            self._bottom_flag = 2
        elif np.all(grid.status_at_node[bottom_edge] == 1):
            pass
        else:
            raise NameError(
                "Different cells on the same grid edge have "
                "different boundary statuses"
            )
            # Note this could get fraught if we need to open a cell to let
            # water flow out...
        if np.all(grid.status_at_node[top_edge] == 4):
            self._top_flag = 4
        elif np.all(grid.status_at_node[top_edge] == 3):
            self._top_flag = 3
        elif np.all(grid.status_at_node[top_edge] == 2):
            self._top_flag = 2
        elif np.all(grid.status_at_node[top_edge] == 1):
            pass
        else:
            raise NameError(
                "Different cells on the same grid edge have "
                "different boundary statuses"
            )
        if np.all(grid.status_at_node[left_edge] == 4):
            self._left_flag = 4
        elif np.all(grid.status_at_node[left_edge] == 3):
            self._left_flag = 3
        elif np.all(grid.status_at_node[left_edge] == 2):
            self._left_flag = 2
        elif np.all(grid.status_at_node[left_edge] == 1):
            pass
        else:
            raise NameError(
                "Different cells on the same grid edge have "
                "different boundary statuses"
            )
        if np.all(grid.status_at_node[right_edge] == 4):
            self._right_flag = 4
        elif np.all(grid.status_at_node[right_edge] == 3):
            self._right_flag = 3
        elif np.all(grid.status_at_node[right_edge] == 2):
            self._right_flag = 2
        elif np.all(grid.status_at_node[right_edge] == 1):
            pass
        else:
            raise NameError(
                "Different cells on the same grid edge have "
                "different boundary statuses"
            )

        self._fixed_grad_BCs_present = (
            self._bottom_flag == 2
            or self._top_flag == 2
            or self._left_flag == 2
            or self._right_flag == 2
        )
        self._looped_BCs_present = (
            self._bottom_flag == 3
            or self._top_flag == 3
            or self._left_flag == 3
            or self._right_flag == 3
        )
        if (
            self._fixed_grad_BCs_present
            and self._values_to_diffuse != grid.fixed_gradient_of
        ):
            raise ValueError(
                "Boundary conditions set in the grid don't "
                "apply to the data the diffuser is trying to "
                "work with"
            )

        if np.any(grid.status_at_node == 2):
            self._fixed_grad_offset_map = np.empty(nrows * ncols, dtype=float)
            self._fixed_grad_anchor_map = np.empty_like(self._fixed_grad_offset_map)
            self._fixed_grad_offset_map[
                grid.fixed_gradient_node_properties["boundary_node_IDs"]
            ] = grid.fixed_gradient_node_properties["values_to_add"]

        self._corner_flags = grid.status_at_node[[0, ncols - 1, -ncols, -1]]

        op_mat_just_corners = self._operating_matrix_ID_map[
            self._corner_interior_IDs, :
        ]
        op_mat_cnr0 = op_mat_just_corners[0, self._bottomleft_mask]
        op_mat_cnr1 = op_mat_just_corners[1, self._bottomright_mask]
        op_mat_cnr2 = op_mat_just_corners[2, self._topleft_mask]
        op_mat_cnr3 = op_mat_just_corners[3, self._topright_mask]
        op_mat_just_active_cnrs = np.vstack(
            (op_mat_cnr0, op_mat_cnr1, op_mat_cnr2, op_mat_cnr3)
        )
        self._operating_matrix_corner_int_IDs = self._realIDtointerior(
            op_mat_just_active_cnrs
        )
        # ^(4corners,4nodesactivepercorner)
        self._operating_matrix_bottom_int_IDs = self._realIDtointerior(
            self._operating_matrix_ID_map[self._bottom_interior_IDs, :][
                :, self._bottom_mask
            ]
        )
        # ^(nbottomnodes,6activenodeseach)
        self._operating_matrix_top_int_IDs = self._realIDtointerior(
            self._operating_matrix_ID_map[self._top_interior_IDs, :][:, self._top_mask]
        )
        self._operating_matrix_left_int_IDs = self._realIDtointerior(
            self._operating_matrix_ID_map[self._left_interior_IDs, :][
                :, self._left_mask
            ]
        )
        self._operating_matrix_right_int_IDs = self._realIDtointerior(
            self._operating_matrix_ID_map[self._right_interior_IDs, :][
                :, self._right_mask
            ]
        )

    def _gear_timestep(self, timestep_in, new_grid):
        """This method allows the gearing between the model run step and the
        component (shorter) step.

        The method becomes unstable if S>Scrit, so we test to prevent
        this. We implicitly assume the initial condition does not
        contain slopes > Scrit. If the method persistently explodes,
        this may be the problem.
        """
        extended_elevs = np.empty(self._grid.number_of_nodes + 1, dtype=float)
        extended_elevs[-1] = np.nan
        node_neighbors = self._grid.active_adjacent_nodes_at_node
        extended_elevs[:-1] = new_grid["node"][self._values_to_diffuse]
        max_offset = np.nanmax(
            np.fabs(
                extended_elevs[:-1][node_neighbors]
                - extended_elevs[:-1].reshape((self._grid.number_of_nodes, 1))
            )
        )
        if max_offset > np.tan(self._S_crit) * min(self._grid.dx, self._grid.dy):
            # ^using S not tan(S) adds a buffer - but not appropriate
            self._internal_repeats = (
                int(
                    max_offset
                    // (np.tan(self._S_crit) * min(self._grid.dx, self._grid.dy))
                )
                + 1
            )
            # now we rig it so the actual timestep is an integer divisor
            # of T_in:
            self._delta_t = timestep_in / self._internal_repeats
            self._uplift_per_step = (
                new_grid["node"][self._values_to_diffuse]
                - self._grid["node"][self._values_to_diffuse]
            ) / self._internal_repeats
            if self._internal_repeats > 10000:
                raise ValueError(
                    """Uplift rate is too high; solution is not
                                 stable!!"""
                )
        else:
            self._internal_repeats = 1
            self._delta_t = timestep_in
            self._uplift_per_step = (
                new_grid["node"][self._values_to_diffuse]
                - self._grid["node"][self._values_to_diffuse]
            )
        return self._delta_t

    def _set_variables(self, grid):
        """This function sets the variables needed for update().

        Now vectorized, shouold run faster. At the moment, this method
        can only handle fixed value BCs.
        """
        n_interior_nodes = grid.number_of_interior_nodes

        # Initialize the local builder lists
        _mat_RHS = np.zeros(n_interior_nodes)

        try:
            elev = grid["node"][self._values_to_diffuse]
        except KeyError as exc:
            raise NameError("elevations not found in grid!") from exc
        try:
            _delta_t = self._delta_t
        except AttributeError as exc:
            raise NameError(
                "Timestep not set! Call _gear_timestep(tstep) "
                "after initializing the component, but before running it."
            ) from exc
        _one_over_delta_x = self._one_over_delta_x
        _one_over_delta_x_sqd = self._one_over_delta_x_sqd
        _one_over_delta_y = self._one_over_delta_y
        _one_over_delta_y_sqd = self._one_over_delta_y_sqd
        _kappa = self._kappa
        _b = self._b
        _S_crit = self._S_crit
        _core_nodes = self._core_nodes
        corenodesbyintIDs = self._corenodesbyintIDs
        operating_matrix_core_int_IDs = self._operating_matrix_core_int_IDs
        operating_matrix_corner_int_IDs = self._operating_matrix_corner_int_IDs
        _interior_corners = self._interior_corners
        corners_antimasks = self._corners_antimasks
        corner_interior_IDs = self._corner_interior_IDs
        modulator_mask = self._modulator_mask
        corner_flags = self._corner_flags
        bottom_interior_IDs = self._bottom_interior_IDs
        top_interior_IDs = self._top_interior_IDs
        left_interior_IDs = self._left_interior_IDs
        right_interior_IDs = self._right_interior_IDs
        bottom_antimask = self._bottom_antimask
        _bottom_list = self._bottom_list
        top_antimask = self._top_antimask
        _top_list = self._top_list
        left_antimask = self._left_antimask
        _left_list = self._left_list
        right_antimask = self._right_antimask
        _right_list = self._right_list

        # Need to modify the "effective" values of the edge nodes if any of
        # the edges are inactive:
        if self._bottom_flag == 4:
            bottom_edge, inside_bottom_edge = grid.nodes[(0, 1), :]
            elev[bottom_edge] = elev[inside_bottom_edge]
            # corners are special cases, and assumed linked to the bottom and
            # top edge BCs...
            elev[bottom_edge[0]] = elev[inside_bottom_edge[1]]
            elev[bottom_edge[-1]] = elev[inside_bottom_edge[-2]]
        if self._top_flag == 4:
            top_edge, inside_top_edge = grid.nodes[(-1, -2), :]
            elev[top_edge] = elev[inside_top_edge]
            # corners are special cases, and assumed linked to the bottom and
            # top edge BCs...
            elev[top_edge[0]] = elev[inside_top_edge[1]]
            elev[top_edge[-1]] = elev[inside_top_edge[-2]]
        if self._left_flag == 4:
            left_edge = grid.nodes[1:-1, 0]
            inside_left_edge = grid.nodes[1:-1, 1]
            elev[left_edge] = elev[inside_left_edge]
        if self._right_flag == 4:
            right_edge = grid.nodes[1:-1, -1]
            inside_right_edge = grid.nodes[1:-1, -2]
            elev[right_edge] = elev[inside_right_edge]

        # replacing loop:
        cell_neighbors = grid.active_adjacent_nodes_at_node
        # ^E,N,W,S
        cell_diagonals = grid.diagonal_adjacent_nodes_at_node  # NE,NW,SW,SE
        # ^this should be dealt with by active_neighbors... (skips bad nodes)

        _z_x = (
            (elev[cell_neighbors[:, 0]] - elev[cell_neighbors[:, 2]])
            * 0.5
            * _one_over_delta_x
        )
        _z_y = (
            (elev[cell_neighbors[:, 1]] - elev[cell_neighbors[:, 3]])
            * 0.5
            * _one_over_delta_y
        )
        _z_xx = (
            elev[cell_neighbors[:, 0]] - 2.0 * elev + elev[cell_neighbors[:, 2]]
        ) * _one_over_delta_x_sqd
        _z_yy = (
            elev[cell_neighbors[:, 1]] - 2.0 * elev + elev[cell_neighbors[:, 3]]
        ) * _one_over_delta_y_sqd
        _z_xy = (
            (
                elev[cell_diagonals[:, 0]]
                - elev[cell_diagonals[:, 1]]
                - elev[cell_diagonals[:, 3]]
                + elev[cell_diagonals[:, 2]]
            )
            * 0.25
            * _one_over_delta_x
            * _one_over_delta_y
        )
        _d = 1.0 / (1.0 - _b * (_z_x * _z_x + _z_y * _z_y))

        _abd_sqd = _kappa * _b * _d * _d
        _F_ij = -2.0 * _kappa * _d * (
            _one_over_delta_x_sqd + _one_over_delta_y_sqd
        ) - 4.0 * _abd_sqd * (
            _z_x * _z_x * _one_over_delta_x_sqd + _z_y * _z_y * _one_over_delta_y_sqd
        )
        _F_ijminus1 = (
            _kappa * _d * _one_over_delta_x_sqd
            - _abd_sqd * _z_x * (_z_xx + _z_yy) * _one_over_delta_x
            - 4.0
            * _abd_sqd
            * _b
            * _d
            * (_z_x * _z_x * _z_xx + _z_y * _z_y * _z_yy + 2.0 * _z_x * _z_y * _z_xy)
            * _z_x
            * _one_over_delta_x
            - 2.0
            * _abd_sqd
            * (
                _z_x * _z_xx * _one_over_delta_x
                - _z_x * _z_x * _one_over_delta_x_sqd
                + _z_y * _z_xy * _one_over_delta_x
            )
        )
        _F_ijplus1 = (
            _kappa * _d * _one_over_delta_x_sqd
            + _abd_sqd * _z_x * (_z_xx + _z_yy) * _one_over_delta_x
            + 4.0
            * _abd_sqd
            * _b
            * _d
            * (_z_x * _z_x * _z_xx + _z_y * _z_y * _z_yy + 2.0 * _z_x * _z_y * _z_xy)
            * _z_x
            * _one_over_delta_x
            + 2.0
            * _abd_sqd
            * (
                _z_x * _z_xx * _one_over_delta_x
                + _z_x * _z_x * _one_over_delta_x_sqd
                + _z_y * _z_xy * _one_over_delta_x
            )
        )
        _F_iminus1j = (
            _kappa * _d * _one_over_delta_y_sqd
            - _abd_sqd * _z_y * (_z_xx + _z_yy) * _one_over_delta_y
            - 4.0
            * _abd_sqd
            * _b
            * _d
            * (_z_x * _z_x * _z_xx + _z_y * _z_y * _z_yy + 2.0 * _z_x * _z_y * _z_xy)
            * _z_y
            * _one_over_delta_y
            - 2.0
            * _abd_sqd
            * (
                _z_y * _z_yy * _one_over_delta_y
                - _z_y * _z_y * _one_over_delta_y_sqd
                + _z_x * _z_xy * _one_over_delta_y
            )
        )
        _F_iplus1j = (
            _kappa * _d * _one_over_delta_y_sqd
            + _abd_sqd * _z_y * (_z_xx + _z_yy) * _one_over_delta_y
            + 4.0
            * _abd_sqd
            * _b
            * _d
            * (_z_x * _z_x * _z_xx + _z_y * _z_y * _z_yy + 2.0 * _z_x * _z_y * _z_xy)
            * _z_y
            * _one_over_delta_y
            + 2.0
            * _abd_sqd
            * (
                _z_y * _z_yy * _one_over_delta_y
                + _z_y * _z_y * _one_over_delta_y_sqd
                + _z_x * _z_xy * _one_over_delta_y
            )
        )
        _F_iplus1jplus1 = _abd_sqd * _z_x * _z_y * _one_over_delta_x * _one_over_delta_y
        _F_iminus1jminus1 = _F_iplus1jplus1
        _F_iplus1jminus1 = -_F_iplus1jplus1
        _F_iminus1jplus1 = _F_iplus1jminus1

        _equ_RHS_calc_frag = (
            _F_ij * elev
            + _F_ijminus1 * elev[cell_neighbors[:, 2]]
            + _F_ijplus1 * elev[cell_neighbors[:, 0]]
            + _F_iminus1j * elev[cell_neighbors[:, 3]]
            + _F_iplus1j * elev[cell_neighbors[:, 1]]
            + _F_iminus1jminus1 * elev[cell_diagonals[:, 2]]
            + _F_iplus1jplus1 * elev[cell_diagonals[:, 0]]
            + _F_iplus1jminus1 * elev[cell_diagonals[:, 1]]
            + _F_iminus1jplus1 * elev[cell_diagonals[:, 3]]
        )

        # NB- all _z_... and _F_... variables are nnodes long, and thus use
        # real IDs (tho calcs will be flawed for Bnodes)

        # RHS of equ 6 (see para [20])
        _func_on_z = self._rock_density / self._sed_density * self._uplift + _kappa * (
            (_z_xx + _z_yy) / (1.0 - (_z_x * _z_x + _z_y * _z_y) / _S_crit * _S_crit)
            + 2.0
            * (_z_x * _z_x * _z_xx + _z_y * _z_y * _z_yy + 2.0 * _z_x * _z_y * _z_xy)
            / (
                _S_crit
                * _S_crit
                * (1.0 - (_z_x * _z_x + _z_y * _z_y) / _S_crit * _S_crit) ** 2.0
            )
        )

        # Remember, the RHS is getting wiped each loop as part of
        # self._set_variables()
        # _mat_RHS is ninteriornodes long, but were only working on a
        # ncorenodes long subset here
        _mat_RHS[corenodesbyintIDs] += elev[_core_nodes] + _delta_t * (
            _func_on_z[_core_nodes] - _equ_RHS_calc_frag[_core_nodes]
        )
        low_row = (
            np.vstack((_F_iminus1jminus1, _F_iminus1j, _F_iminus1jplus1)) * -_delta_t
        )
        mid_row = np.vstack(
            (-_delta_t * _F_ijminus1, 1.0 - _delta_t * _F_ij, -_delta_t * _F_ijplus1)
        )
        top_row = np.vstack((_F_iplus1jminus1, _F_iplus1j, _F_iplus1jplus1)) * -_delta_t
        nine_node_map = np.vstack((low_row, mid_row, top_row)).T
        # ^Note shape is (nnodes,9); it's realID indexed
        core_op_mat_row = np.repeat(corenodesbyintIDs, 9)
        core_op_mat_col = operating_matrix_core_int_IDs.astype(int).flatten()
        core_op_mat_data = nine_node_map[_core_nodes, :].flatten()

        # Now the interior corners; BL,BR,TL,TR
        _mat_RHS[corner_interior_IDs] += elev[_interior_corners] + _delta_t * (
            _func_on_z[_interior_corners] - _equ_RHS_calc_frag[_interior_corners]
        )
        corners_op_mat_row = np.repeat(self._corner_interior_IDs, 4)
        corners_op_mat_col = operating_matrix_corner_int_IDs.astype(int).flatten()
        corners_op_mat_data = nine_node_map[_interior_corners, :][
            (np.arange(4).reshape((4, 1)), self._corners_masks)
        ].flatten()
        # ^1st index gives (4,9), 2nd reduces to (4,4), then flattened
        for i in range(4):  # loop over each corner, as so few
            # Note that this ONLY ADDS THE VALUES FOR THE TRUE GRID CORNERS.
            # The sides get done in the edge tests, below.
            if corner_flags[i] == 1:
                true_corner = self._antimask_corner_position[i]
                _mat_RHS[corner_interior_IDs[i]] -= _delta_t * np.sum(
                    nine_node_map[_interior_corners[i], :][
                        corners_antimasks[i, true_corner]
                    ]
                    * elev[
                        _interior_corners[i]
                        + modulator_mask[corners_antimasks[i, true_corner]]
                    ]
                )
            elif corner_flags[i] == 4 or corner_flags[i] == 3:
                # ^inactive boundary cell
                # Actually the easiest case! Equivalent to fixed gradient,
                # but the gradient is zero, so material only goes in the linked
                # cell. And because it's a true corner, that linked cell
                # doesn't appear in the interior matrix at all!
                pass
            elif corner_flags[i] == 2:
                true_corner = self._antimask_corner_position[i]
                _mat_RHS[corner_interior_IDs[i]] -= _delta_t * np.sum(
                    nine_node_map[_interior_corners[i], :][
                        corners_antimasks[i, true_corner]
                    ]
                    * self._fixed_gradient_offset_map[
                        _interior_corners[i]
                        + modulator_mask[corners_antimasks[i, true_corner]]
                    ]
                )
            else:
                raise NameError(
                    """Sorry! This module cannot yet handle fixed
                    gradient or looped BCs..."""
                )
            # Todo: handle these BCs properly, once the grid itself works with
            # them.
            # Can follow old routines; see self.set_bc_cell() commented out
            # method below.

        # Now the edges
        _mat_RHS[bottom_interior_IDs] += elev[_bottom_list] + _delta_t * (
            _func_on_z[_bottom_list] - _equ_RHS_calc_frag[_bottom_list]
        )
        _mat_RHS[top_interior_IDs] += elev[_top_list] + _delta_t * (
            _func_on_z[_top_list] - _equ_RHS_calc_frag[_top_list]
        )
        _mat_RHS[left_interior_IDs] += elev[_left_list] + _delta_t * (
            _func_on_z[_left_list] - _equ_RHS_calc_frag[_left_list]
        )
        _mat_RHS[right_interior_IDs] += elev[_right_list] + _delta_t * (
            _func_on_z[_right_list] - _equ_RHS_calc_frag[_right_list]
        )
        bottom_op_mat_row = np.repeat(bottom_interior_IDs, 6)
        top_op_mat_row = np.repeat(top_interior_IDs, 6)
        left_op_mat_row = np.repeat(left_interior_IDs, 6)
        right_op_mat_row = np.repeat(right_interior_IDs, 6)
        bottom_op_mat_col = self._operating_matrix_bottom_int_IDs.astype(int).flatten()
        top_op_mat_col = self._operating_matrix_top_int_IDs.astype(int).flatten()
        left_op_mat_col = self._operating_matrix_left_int_IDs.astype(int).flatten()
        right_op_mat_col = self._operating_matrix_right_int_IDs.astype(int).flatten()
        bottom_op_mat_data = nine_node_map[_bottom_list, :][
            :, self._bottom_mask
        ].flatten()
        top_op_mat_data = nine_node_map[_top_list, :][:, self._top_mask].flatten()
        left_op_mat_data = nine_node_map[_left_list, :][:, self._left_mask].flatten()
        right_op_mat_data = nine_node_map[_right_list, :][:, self._right_mask].flatten()

        if self._bottom_flag == 1:
            # goes to RHS only
            _mat_RHS[bottom_interior_IDs] -= _delta_t * np.sum(
                nine_node_map[_bottom_list, :][:, bottom_antimask]
                * elev[
                    _bottom_list.reshape((len(_bottom_list), 1))
                    + (modulator_mask[bottom_antimask]).reshape((1, 3))
                ],
                axis=1,
            )
            # ^note the broadcasting to (nedge,3) in final fancy index
            # ...& the corners
            edges = [(1, 2), (0, 1), (0, 0), (0, 0)]
            for i in [0, 1]:
                edge_list = edges[i]
                _mat_RHS[corner_interior_IDs[i]] -= _delta_t * np.sum(
                    nine_node_map[_interior_corners[i], :][
                        corners_antimasks[i, edge_list]
                    ]
                    * elev[
                        _interior_corners[i]
                        + modulator_mask[corners_antimasks[i, edge_list]]
                    ]
                )
            # make dummy array objects for the x,y coords in coo creation of
            # _operating_matrix
            bottom_op_mat_row_add = np.empty(0)
            bottom_op_mat_col_add = np.empty(0)
            bottom_op_mat_data_add = np.empty(0)
        elif self._bottom_flag == 4 or self._bottom_flag == 2:
            # ^i.e., fixed zero gradient (4) or more general case...
            bottom_op_mat_row_add = np.empty(bottom_interior_IDs.size * 3 + 6)
            bottom_op_mat_col_add = np.empty(bottom_interior_IDs.size * 3 + 6)
            bottom_op_mat_data_add = np.empty(bottom_interior_IDs.size * 3 + 6)
            # Equivalent to fixed gradient, but the gradient is zero, so
            # material only goes in the linked cell(i.e., each cell in the
            # op_mat edges points back to itself).
            bottom_op_mat_row_add[: (bottom_interior_IDs.size * 3)] = np.repeat(
                bottom_interior_IDs, 3
            )
            bottom_op_mat_col_add[: (bottom_interior_IDs.size * 3)] = (
                self._realIDtointerior(
                    self._operating_matrix_ID_map[self._bottom_interior_IDs, :][
                        :, self._bottom_mask[0:3]
                    ]
                ).flatten()
            )
            bottom_op_mat_data_add[: (bottom_interior_IDs.size * 3)] = (
                _delta_t
                * (nine_node_map[_bottom_list, :][:, bottom_antimask]).flatten()
            )
            # ...& the corners
            this_corner_coords = np.array([0, 1])
            # order is bottom 2 lower left, bottom 2 lower right, lower left
            # true corner, lower right true corner.
            bottom_op_mat_row_add[-6:-2] = np.repeat(
                corner_interior_IDs[this_corner_coords], 2
            )
            bottom_op_mat_col_add[-6:-2] = self._operating_matrix_corner_int_IDs[
                this_corner_coords.reshape((2, 1)), this_corner_coords
            ].flatten()
            bottom_op_mat_row_add[-2:] = corner_interior_IDs[this_corner_coords]
            bottom_op_mat_col_add[-2:] = self._operating_matrix_corner_int_IDs[
                (this_corner_coords[0], this_corner_coords[0]),
                (this_corner_coords[1], this_corner_coords[1]),
            ].flatten()
            bottom_op_mat_data_add[-6:-4] = (
                _delta_t
                * nine_node_map[_interior_corners[0], :][
                    corners_antimasks[0, [1, 2]]
                ].flatten()
            )
            bottom_op_mat_data_add[-4:-2] = (
                _delta_t
                * nine_node_map[_interior_corners[1], :][
                    corners_antimasks[1, [0, 1]]
                ].flatten()
            )
            bottom_op_mat_data_add[-2] = (
                _delta_t
                * nine_node_map[_interior_corners[0], :][corners_antimasks[0, 0]]
            )
            bottom_op_mat_data_add[-1] = (
                _delta_t
                * nine_node_map[_interior_corners[1], :][corners_antimasks[1, 2]]
            )
            if self._bottom_flag == 2:
                # Read the offsets from the map we made in the __init__,
                # use them as constant terms, incorporated into RHS
                _mat_RHS[bottom_interior_IDs] -= _delta_t * np.sum(
                    nine_node_map[_bottom_list, :][:, bottom_antimask]
                    * self._fixed_gradient_offset_map[
                        _bottom_list.reshape((len(_bottom_list), 1))
                        + (modulator_mask[bottom_antimask]).reshape((1, 3))
                    ],
                    axis=1,
                )
                # ^note the broadcasting to (nedge,3) in final fancy index
                # ...& the corners
                edges = [(1, 2), (0, 1), (0, 0), (0, 0)]
                for i in [0, 1]:
                    edge_list = edges[i]
                    _mat_RHS[corner_interior_IDs[i]] -= _delta_t * np.sum(
                        nine_node_map[_interior_corners[i], :][
                            corners_antimasks[i, edge_list]
                        ]
                        * self._fixed_gradient_offset_map[
                            _interior_corners[i]
                            + modulator_mask[corners_antimasks[i, edge_list]]
                        ]
                    )
        elif self._bottom_flag == 3:
            # This will handle both top and bottom BCs...
            bottom_op_mat_row_add = np.empty(bottom_interior_IDs.size * 3 + 6)
            bottom_op_mat_col_add = np.empty(bottom_interior_IDs.size * 3 + 6)
            bottom_op_mat_data_add = np.empty(bottom_interior_IDs.size * 3 + 6)
            bottom_op_mat_row_add[: (bottom_interior_IDs.size * 3)] = np.repeat(
                bottom_interior_IDs, 3
            )
            # ^...put the values in the same places in the operating matrix...
            bottom_op_mat_col_add[: (bottom_interior_IDs.size * 3)] = (
                self._realIDtointerior(
                    self._operating_matrix_ID_map[self._top_interior_IDs, :][
                        :, self._top_mask[3:6]
                    ]
                ).flatten()
            )
            bottom_op_mat_data_add[: (bottom_interior_IDs.size * 3)] = (
                _delta_t
                * (nine_node_map[_bottom_list, :][:, bottom_antimask]).flatten()
            )
            # ^...but the values refer to the TOP of the grid
            top_op_mat_row_add = np.empty(top_interior_IDs.size * 3 + 6)
            top_op_mat_col_add = np.empty(top_interior_IDs.size * 3 + 6)
            top_op_mat_data_add = np.empty(top_interior_IDs.size * 3 + 6)
            top_op_mat_row_add[: (top_interior_IDs.size * 3)] = np.repeat(
                top_interior_IDs, 3
            )
            top_op_mat_col_add[: (top_interior_IDs.size * 3)] = self._realIDtointerior(
                self._operating_matrix_ID_map[self._bottom_interior_IDs, :][
                    :, self._bottom_mask[0:3]
                ]
            ).flatten()
            top_op_mat_data_add[: (top_interior_IDs.size * 3)] = (
                _delta_t * (nine_node_map[_top_list, :][:, top_antimask]).flatten()
            )
            # & the corners
            bottom_corner_coords = np.array([0, 1])
            top_corner_coords = np.array([2, 3])
            bottom_op_mat_row_add[-6:-2] = np.repeat(
                corner_interior_IDs[bottom_corner_coords], 2
            )
            bottom_op_mat_col_add[-6:-2] = self._operating_matrix_corner_int_IDs[
                top_corner_coords.reshape((2, 1)), top_corner_coords
            ].flatten()
            bottom_op_mat_row_add[-2:] = corner_interior_IDs[bottom_corner_coords]
            bottom_op_mat_col_add[-2:] = self._operating_matrix_corner_int_IDs[
                (top_corner_coords[0], top_corner_coords[0]),
                (top_corner_coords[1], top_corner_coords[1]),
            ].flatten()
            bottom_op_mat_data_add[-6:-4] = (
                _delta_t
                * nine_node_map[_interior_corners[0], :][
                    corners_antimasks[0, [1, 2]]
                ].flatten()
            )
            bottom_op_mat_data_add[-4:-2] = (
                _delta_t
                * nine_node_map[_interior_corners[1], :][
                    corners_antimasks[1, [0, 1]]
                ].flatten()
            )
            bottom_op_mat_data_add[-2] = (
                _delta_t
                * nine_node_map[_interior_corners[0], :][corners_antimasks[0, 0]]
            )
            bottom_op_mat_data_add[-1] = (
                _delta_t
                * nine_node_map[_interior_corners[1], :][corners_antimasks[1, 2]]
            )
            top_op_mat_row_add[-6:-2] = np.repeat(
                corner_interior_IDs[top_corner_coords], 2
            )
            top_op_mat_col_add[-6:-2] = self._operating_matrix_corner_int_IDs[
                bottom_corner_coords.reshape((2, 1)), bottom_corner_coords
            ].flatten()
            top_op_mat_row_add[-2:] = corner_interior_IDs[top_corner_coords]
            top_op_mat_col_add[-2:] = self._operating_matrix_corner_int_IDs[
                (bottom_corner_coords[0], bottom_corner_coords[0]),
                (bottom_corner_coords[1], bottom_corner_coords[1]),
            ].flatten()
            top_op_mat_data_add[-6:-4] = (
                _delta_t
                * nine_node_map[_interior_corners[2], :][
                    corners_antimasks[2, [3, 4]]
                ].flatten()
            )
            top_op_mat_data_add[-4:-2] = (
                _delta_t
                * nine_node_map[_interior_corners[3], :][
                    corners_antimasks[3, [2, 3]]
                ].flatten()
            )
            top_op_mat_data_add[-2] = (
                _delta_t
                * nine_node_map[_interior_corners[2], :][corners_antimasks[2, 2]]
            )
            top_op_mat_data_add[-1] = (
                _delta_t
                * nine_node_map[_interior_corners[3], :][corners_antimasks[3, 4]]
            )
        else:
            raise NameError(
                """Something is very wrong with your boundary
                            conditions...!"""
            )

        if self._top_flag == 1:
            # goes to RHS only
            _mat_RHS[top_interior_IDs] -= _delta_t * np.sum(
                nine_node_map[_top_list, :][:, top_antimask]
                * elev[
                    _top_list.reshape((len(_top_list), 1))
                    + (modulator_mask[top_antimask]).reshape((1, 3))
                ],
                axis=1,
            )
            # ...& the corners
            edges = [(0, 0), (0, 0), (3, 4), (2, 3)]
            for i in [2, 3]:
                edge_list = edges[i]
                _mat_RHS[corner_interior_IDs[i]] -= _delta_t * np.sum(
                    nine_node_map[_interior_corners[i], :][
                        corners_antimasks[i, edge_list]
                    ]
                    * elev[
                        _interior_corners[i]
                        + modulator_mask[corners_antimasks[i, edge_list]]
                    ]
                )
            top_op_mat_row_add = np.empty(0)
            top_op_mat_col_add = np.empty(0)
            top_op_mat_data_add = np.empty(0)
        elif self._top_flag == 4 or self._top_flag == 2:
            top_op_mat_row_add = np.empty(top_interior_IDs.size * 3 + 6)
            top_op_mat_col_add = np.empty(top_interior_IDs.size * 3 + 6)
            top_op_mat_data_add = np.empty(top_interior_IDs.size * 3 + 6)
            # Equivalent to fixed gradient, but the gradient is zero, so
            # material only goes in the linked cell(i.e., each cell in the
            # op_mat edges points back to itself).
            top_op_mat_row_add[: (top_interior_IDs.size * 3)] = np.repeat(
                top_interior_IDs, 3
            )
            top_op_mat_col_add[: (top_interior_IDs.size * 3)] = self._realIDtointerior(
                self._operating_matrix_ID_map[self._top_interior_IDs, :][
                    :, self._top_mask[3:6]
                ]
            ).flatten()
            top_op_mat_data_add[: (top_interior_IDs.size * 3)] = (
                _delta_t * (nine_node_map[_top_list, :][:, top_antimask]).flatten()
            )
            # ...& the corners
            this_corner_coords = np.array([2, 3])
            top_op_mat_row_add[-6:-2] = np.repeat(
                corner_interior_IDs[this_corner_coords], 2
            )
            top_op_mat_col_add[-6:-2] = self._operating_matrix_corner_int_IDs[
                this_corner_coords.reshape((2, 1)), this_corner_coords
            ].flatten()
            top_op_mat_row_add[-2:] = corner_interior_IDs[this_corner_coords]
            top_op_mat_col_add[-2:] = self._operating_matrix_corner_int_IDs[
                (this_corner_coords[0], this_corner_coords[0]),
                (this_corner_coords[1], this_corner_coords[1]),
            ].flatten()
            top_op_mat_data_add[-6:-4] = (
                _delta_t
                * nine_node_map[_interior_corners[2], :][
                    corners_antimasks[2, [3, 4]]
                ].flatten()
            )
            top_op_mat_data_add[-4:-2] = (
                _delta_t
                * nine_node_map[_interior_corners[3], :][
                    corners_antimasks[3, [2, 3]]
                ].flatten()
            )
            top_op_mat_data_add[-2] = (
                _delta_t
                * nine_node_map[_interior_corners[2], :][corners_antimasks[2, 2]]
            )
            top_op_mat_data_add[-1] = (
                _delta_t
                * nine_node_map[_interior_corners[3], :][corners_antimasks[3, 4]]
            )
            if self._top_flag == 2:
                _mat_RHS[top_interior_IDs] -= _delta_t * np.sum(
                    nine_node_map[_top_list, :][:, top_antimask]
                    * self._fixed_gradient_offset_map[
                        _top_list.reshape((len(_top_list), 1))
                        + (modulator_mask[top_antimask]).reshape((1, 3))
                    ],
                    axis=1,
                )
                # ...& the corners
                edges = [(0, 0), (0, 0), (3, 4), (2, 3)]
                for i in [2, 3]:
                    edge_list = edges[i]
                    _mat_RHS[corner_interior_IDs[i]] -= _delta_t * np.sum(
                        nine_node_map[_interior_corners[i], :][
                            corners_antimasks[i, edge_list]
                        ]
                        * self._fixed_gradient_offset_map[
                            _interior_corners[i]
                            + modulator_mask[corners_antimasks[i, edge_list]]
                        ]
                    )
        elif self._top_flag == 3:
            pass  # dealt with above
        else:
            raise NameError(
                """Something is very wrong with your boundary
                            conditions...!"""
            )

        if self._left_flag == 1:
            # goes to RHS only
            _mat_RHS[left_interior_IDs] -= _delta_t * np.sum(
                nine_node_map[_left_list, :][:, left_antimask]
                * elev[
                    _left_list.reshape((len(_left_list), 1))
                    + (modulator_mask[left_antimask]).reshape((1, 3))
                ],
                axis=1,
            )
            # ...& the corners
            edges = [(3, 4), (0, 0), (0, 1), (0, 0)]
            for i in [0, 2]:
                edge_list = edges[i]
                _mat_RHS[corner_interior_IDs[i]] -= _delta_t * np.sum(
                    nine_node_map[_interior_corners[i], :][
                        corners_antimasks[i, edge_list]
                    ]
                    * elev[
                        _interior_corners[i]
                        + modulator_mask[corners_antimasks[i, edge_list]]
                    ]
                )
            left_op_mat_row_add = np.empty(0)
            left_op_mat_col_add = np.empty(0)
            left_op_mat_data_add = np.empty(0)
        elif self._left_flag == 4 or self._left_flag == 2:
            left_op_mat_row_add = np.empty(left_interior_IDs.size * 3 + 4)
            left_op_mat_col_add = np.empty(left_interior_IDs.size * 3 + 4)
            left_op_mat_data_add = np.empty(left_interior_IDs.size * 3 + 4)
            # Equivalent to fixed gradient, but the gradient is zero, so
            # material only goes in the linked cell(i.e., each cell in the
            # op_mat edges points back to itself).
            left_op_mat_row_add[: (left_interior_IDs.size * 3)] = np.repeat(
                left_interior_IDs, 3
            )
            left_op_mat_col_add[: (left_interior_IDs.size * 3)] = (
                self._realIDtointerior(
                    self._operating_matrix_ID_map[self._left_interior_IDs, :][
                        :, self._left_mask[::2]
                    ]
                ).flatten()
            )
            left_op_mat_data_add[: (left_interior_IDs.size * 3)] = (
                _delta_t * (nine_node_map[_left_list, :][:, left_antimask]).flatten()
            )
            # ...& the corners
            this_corner_coords = np.array([0, 2])
            left_op_mat_row_add[-4:] = np.repeat(
                corner_interior_IDs[this_corner_coords], 2
            )
            left_op_mat_col_add[-4:] = self._operating_matrix_corner_int_IDs[
                this_corner_coords.reshape((2, 1)), this_corner_coords
            ].flatten()
            left_op_mat_data_add[-4:-2] = (
                _delta_t
                * nine_node_map[_interior_corners[0], :][
                    corners_antimasks[0, [3, 4]]
                ].flatten()
            )
            left_op_mat_data_add[-2:] = (
                _delta_t
                * nine_node_map[_interior_corners[2], :][
                    corners_antimasks[2, [0, 1]]
                ].flatten()
            )
            if self._left_flag == 2:
                _mat_RHS[left_interior_IDs] -= _delta_t * np.sum(
                    nine_node_map[_left_list, :][:, left_antimask]
                    * self._fixed_gradient_offset_map[
                        _left_list.reshape((len(_left_list), 1))
                        + (modulator_mask[left_antimask]).reshape((1, 3))
                    ],
                    axis=1,
                )
                # ...& the corners
                edges = [(3, 4), (0, 0), (0, 1), (0, 0)]
                for i in [0, 2]:
                    edge_list = edges[i]
                    _mat_RHS[corner_interior_IDs[i]] -= _delta_t * np.sum(
                        nine_node_map[_interior_corners[i], :][
                            corners_antimasks[i, edge_list]
                        ]
                        * self._fixed_gradient_offset_map[
                            _interior_corners[i]
                            + modulator_mask[corners_antimasks[i, edge_list]]
                        ]
                    )
        elif self._left_flag == 3:
            left_op_mat_row_add = np.empty(left_interior_IDs.size * 3 + 4)
            left_op_mat_col_add = np.empty(left_interior_IDs.size * 3 + 4)
            left_op_mat_data_add = np.empty(left_interior_IDs.size * 3 + 4)
            left_op_mat_row_add[: (left_interior_IDs.size * 3)] = np.repeat(
                left_interior_IDs, 3
            )
            left_op_mat_col_add[: (left_interior_IDs.size * 3)] = (
                self._realIDtointerior(
                    self._operating_matrix_ID_map[self._right_interior_IDs, :][
                        :, self._right_mask[1::2]
                    ]
                ).flatten()
            )
            left_op_mat_data_add[: (left_interior_IDs.size * 3)] = (
                _delta_t * (nine_node_map[_left_list, :][:, left_antimask]).flatten()
            )
            right_op_mat_row_add = np.empty(right_interior_IDs.size * 3 + 4)
            right_op_mat_col_add = np.empty(right_interior_IDs.size * 3 + 4)
            right_op_mat_data_add = np.empty(right_interior_IDs.size * 3 + 4)
            right_op_mat_row_add[: (right_interior_IDs.size * 3)] = np.repeat(
                right_interior_IDs, 3
            )
            right_op_mat_col_add[: (right_interior_IDs.size * 3)] = (
                self._realIDtointerior(
                    self._operating_matrix_ID_map[self._left_interior_IDs, :][
                        :, self._left_mask[::2]
                    ]
                ).flatten()
            )
            right_op_mat_data_add[: (right_interior_IDs.size * 3)] = (
                _delta_t * (nine_node_map[_right_list, :][:, right_antimask]).flatten()
            )
            # & the corners
            left_corner_coords = np.array([0, 2])
            right_corner_coords = np.array([1, 3])
            left_op_mat_row_add[-4:] = np.repeat(
                corner_interior_IDs[left_corner_coords], 2
            )
            left_op_mat_col_add[-4:] = self._operating_matrix_corner_int_IDs[
                right_corner_coords.reshape((2, 1)), right_corner_coords
            ].flatten()
            left_op_mat_data_add[-4:-2] = (
                _delta_t
                * nine_node_map[_interior_corners[0], :][
                    corners_antimasks[0, [3, 4]]
                ].flatten()
            )
            left_op_mat_data_add[-2:] = (
                _delta_t
                * nine_node_map[_interior_corners[2], :][
                    corners_antimasks[2, [0, 1]]
                ].flatten()
            )
            right_op_mat_row_add[-4:] = np.repeat(
                corner_interior_IDs[right_corner_coords], 2
            )
            right_op_mat_col_add[-4:] = self._operating_matrix_corner_int_IDs[
                left_corner_coords.reshape((2, 1)), left_corner_coords
            ].flatten()
            right_op_mat_data_add[-4:-2] = (
                _delta_t
                * nine_node_map[_interior_corners[1], :][
                    corners_antimasks[1, [3, 4]]
                ].flatten()
            )
            right_op_mat_data_add[-2:] = (
                _delta_t
                * nine_node_map[_interior_corners[3], :][
                    corners_antimasks[3, [0, 1]]
                ].flatten()
            )
        else:
            raise NameError(
                """Something is very wrong with your boundary
                            conditions...!"""
            )

        if self._right_flag == 1:
            # goes to RHS only
            _mat_RHS[right_interior_IDs] -= _delta_t * np.sum(
                nine_node_map[_right_list, :][:, right_antimask]
                * elev[
                    _right_list.reshape((len(_right_list), 1))
                    + (modulator_mask[right_antimask]).reshape((1, 3))
                ],
                axis=1,
            )
            # ...& the corners
            edges = [(0, 0), (3, 4), (0, 0), (0, 1)]
            for i in [1, 3]:
                edge_list = edges[i]
                _mat_RHS[corner_interior_IDs[i]] -= _delta_t * np.sum(
                    nine_node_map[_interior_corners[i], :][
                        corners_antimasks[i, edge_list]
                    ]
                    * elev[
                        _interior_corners[i]
                        + modulator_mask[corners_antimasks[i, edge_list]]
                    ]
                )
            right_op_mat_row_add = np.empty(0)
            right_op_mat_col_add = np.empty(0)
            right_op_mat_data_add = np.empty(0)
        elif self._right_flag == 4 or self._right_flag == 2:
            right_op_mat_row_add = np.empty(right_interior_IDs.size * 3 + 4)
            right_op_mat_col_add = np.empty(right_interior_IDs.size * 3 + 4)
            right_op_mat_data_add = np.empty(right_interior_IDs.size * 3 + 4)
            # Equivalent to fixed gradient, but the gradient is zero, so
            # material only goes in the linked cell(i.e., each cell in the
            # op_mat edges points back to itself).
            right_op_mat_row_add[: (right_interior_IDs.size * 3)] = np.repeat(
                right_interior_IDs, 3
            )
            right_op_mat_col_add[: (right_interior_IDs.size * 3)] = (
                self._realIDtointerior(
                    self._operating_matrix_ID_map[self._right_interior_IDs, :][
                        :, self._right_mask[1::2]
                    ]
                ).flatten()
            )
            right_op_mat_data_add[: (right_interior_IDs.size * 3)] = (
                _delta_t * (nine_node_map[_right_list, :][:, right_antimask]).flatten()
            )
            # ...& the corners
            this_corner_coords = np.array([1, 3])
            right_op_mat_row_add[-4:] = np.repeat(
                corner_interior_IDs[this_corner_coords], 2
            )
            right_op_mat_col_add[-4:] = self._operating_matrix_corner_int_IDs[
                this_corner_coords.reshape((2, 1)), this_corner_coords
            ].flatten()
            right_op_mat_data_add[-4:-2] = (
                _delta_t
                * nine_node_map[_interior_corners[1], :][
                    corners_antimasks[1, [3, 4]]
                ].flatten()
            )
            right_op_mat_data_add[-2:] = (
                _delta_t
                * nine_node_map[_interior_corners[3], :][
                    corners_antimasks[3, [0, 1]]
                ].flatten()
            )
            if self._right_flag == 2:
                _mat_RHS[right_interior_IDs] -= _delta_t * np.sum(
                    nine_node_map[_right_list, :][:, right_antimask]
                    * self._fixed_gradient_offset_map[
                        _right_list.reshape((len(_right_list), 1))
                        + (modulator_mask[right_antimask]).reshape((1, 3))
                    ],
                    axis=1,
                )
                # ...& the corners
                edges = [(0, 0), (3, 4), (0, 0), (0, 1)]
                for i in [1, 3]:
                    edge_list = edges[i]
                    _mat_RHS[corner_interior_IDs[i]] -= _delta_t * np.sum(
                        nine_node_map[_interior_corners[i], :][
                            corners_antimasks[i, edge_list]
                        ]
                        * self._fixed_gradient_offset_map[
                            _interior_corners[i]
                            + modulator_mask[corners_antimasks[i, edge_list]]
                        ]
                    )
        elif self._top_flag == 3:
            pass  # dealt with above
        else:
            raise NameError(
                """Something is very wrong with your boundary
                            conditions...!"""
            )

        # new approach using COO sparse matrix requires we build the matrix
        # only now...
        self._operating_matrix = sparse.coo_matrix(
            (
                np.concatenate(
                    (
                        core_op_mat_data,
                        corners_op_mat_data,
                        bottom_op_mat_data,
                        top_op_mat_data,
                        left_op_mat_data,
                        right_op_mat_data,
                        bottom_op_mat_data_add,
                        top_op_mat_data_add,
                        left_op_mat_data_add,
                        right_op_mat_data_add,
                    )
                ),
                (
                    np.concatenate(
                        (
                            core_op_mat_row,
                            corners_op_mat_row,
                            bottom_op_mat_row,
                            top_op_mat_row,
                            left_op_mat_row,
                            right_op_mat_row,
                            bottom_op_mat_row_add,
                            top_op_mat_row_add,
                            left_op_mat_row_add,
                            right_op_mat_row_add,
                        )
                    ),
                    np.concatenate(
                        (
                            core_op_mat_col,
                            corners_op_mat_col,
                            bottom_op_mat_col,
                            top_op_mat_col,
                            left_op_mat_col,
                            right_op_mat_col,
                            bottom_op_mat_col_add,
                            top_op_mat_col_add,
                            left_op_mat_col_add,
                            right_op_mat_col_add,
                        )
                    ),
                ),
            ),
            shape=(n_interior_nodes, n_interior_nodes),
        ).tocsr()
        self._mat_RHS = _mat_RHS

    # These methods translate ID numbers between arrays of differing sizes
    def _realIDtointerior(self, ID):
        ncols = self._ncols
        interior_ID = (ID // ncols - 1) * (ncols - 2) + (ID % ncols) - 1
        if np.any(interior_ID < 0) or np.any(interior_ID >= self._ninteriornodes):
            raise NameError(
                """One of the supplied nodes was outside the
                            interior grid!"""
            )
        else:
            return interior_ID.astype(int)

    def _interiorIDtoreal(self, ID):
        IGW = self._interior_grid_width
        real_ID = (ID // IGW + 1) * self._ncols + (ID % IGW) + 1
        assert np.all(real_ID < self._nnodes)
        return real_ID.astype(int)

    def _realIDtocore(self, ID):
        ncols = self._ncols
        core_ID = (ID // ncols - 2) * (ncols - 4) + (ID % ncols) - 2
        if np.any(core_ID < 0) or np.any(core_ID >= self._ncorenodes):
            raise NameError(
                """One of the supplied nodes was outside the
                            core grid!"""
            )
        else:
            return core_ID.astype(int)

    def _coreIDtoreal(self, ID):
        CCW = self._core_cell_width
        real_ID = (ID // CCW + 2) * self._ncols + (ID % CCW) + 2
        assert np.all(real_ID < self._nnodes)
        return real_ID.astype(int)

    def _interiorIDtocore(self, ID):
        IGW = self._interior_grid_width
        core_ID = (ID // IGW - 1) * (self._ncols - 4) + (ID % IGW) - 1
        if np.any(core_ID < 0) or np.any(core_ID >= self._ncorenodes):
            raise NameError(
                """One of the supplied nodes was outside the
                            core grid!"""
            )
        else:
            return core_ID.astype(int)

    def _coreIDtointerior(self, ID):
        CCW = self._core_cell_width
        interior_ID = (ID // CCW + 1) * (self._ncols - 2) + (ID % CCW) + 1
        assert np.all(interior_ID < self._ninteriornodes)
        return interior_ID.astype(int)

    def run_one_step(self, dt):
        """Run the diffuser for one timestep, dt.

        This is the primary method of the class.

        Parameters
        ----------
        dt : float (time)
            The imposed timestep.
        """
        if self._bc_set_code != self._grid.bc_set_code:
            self.updated_boundary_conditions()
            self._bc_set_code = self._grid.bc_set_code
        else:
            self._gear_timestep(dt, self._grid)
            for _ in range(self._internal_repeats):
                # Initialize the variables for the step:
                self._set_variables(self._grid)
                # Solve interior of grid:
                _interior_elevs = linalg.spsolve(self._operating_matrix, self._mat_RHS)
                # this fn solves Ax=B for x

                # Handle the BC cells; test common cases first for speed
                self._grid["node"][self._values_to_diffuse][
                    self._interior_IDs_as_real
                ] = _interior_elevs

        # if BC==1 or BC==4, don't need to take any action; in both
        # cases the values are unchanged.
        if self._fixed_grad_BCs_present:
            self._grid["node"][self._values_to_diffuse][
                self._grid.fixed_gradient_node_properties["boundary_node_IDs"]
            ] = (
                self._grid["node"][self._values_to_diffuse][
                    self._grid.fixed_gradient_node_properties["anchor_node_IDs"]
                ]
                + self._grid.fixed_gradient_node_properties["values_to_add"]
            )
        if self._looped_BCs_present:
            self._grid["node"][self._values_to_diffuse][
                self._grid.looped_node_properties["boundary_node_IDs"]
            ] = self._grid["node"][self._values_to_diffuse][
                self._grid.looped_node_properties["linked_node_IDs"]
            ]



================================================
File: nonlinear_diffusion/__init__.py
================================================
from .Perron_nl_diffuse import PerronNLDiffuse

__all__ = ["PerronNLDiffuse"]



================================================
File: normal_fault/__init__.py
================================================
from .normal_fault import NormalFault

__all__ = ["NormalFault"]



================================================
File: normal_fault/normal_fault.py
================================================
#!/usr/bin/env python
"""Rock uplift along a normal fault.

Landlab component that implements rock uplift by a normal fault. Note
that this component does not make any attempt to advect topography
laterally.
"""

import numpy as np

from landlab import Component
from landlab import FieldError

TWO_PI = 2.0 * np.pi


class NormalFault(Component):
    """NormalFault implements relative rock motion due to a normal fault.

    The fault can have an arbitrary trace given by two points (x1, y1) and
    (x2, y2) in the `fault_trace` input parameter. These value of these points
    is in model-space coordinates and is not based on node id values or number
    of rows and columns.

    This NormalFault component permits two primary methods for enacting fault
    motion.

    1. **run_one_step**: The throw rate is provided through the
       ``fault_throw_rate_through_time`` parameter. This rate can be constant or
       arbitrary. See the NormalFault tutorial in the landlab tutorials repository
       for an extensive example. In this case, the NormalFault component will keep
       track of the cumulative amount of model-run-time and set the rate based on
       interpolating the provided rate-time history. *NOTE: this means that the
       model run timesteps must align with the time-rate relationship provided to
       NormalFault*. Improving this is on the developers todo list but is of low
       priority.

    2. **run_one_earthquake**: A single uplift event of size dz can be
       specified by this method. If NormalFault is used in this way, any
       specifications provided in the ``fault_throw_rate_through_time`` keyword
       argument will be ignored.

    Note that the NormalFault component does not prevent a user from combining
    the **run_one_step** and **run_one_earthquake** methods. It is encumbent
    upon the user, however, to ensure that these two methods are used in
    combination correctly for their specific use case.

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    None Listed
    """

    _name = "NormalFault"

    _unit_agnostic = True

    _info = {
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": True,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        }
    }

    def __init__(
        self,
        grid,
        faulted_surface="topographic__elevation",
        fault_throw_rate_through_time=(("time", [0]), ("rate", [0.001])),
        fault_dip_angle=90.0,
        fault_trace=(("x1", 0), ("y1", 0), ("x2", 1), ("y2", 1)),
        include_boundaries=False,
    ):
        """Instantiation of a NormalFault.

        Parameters
        ----------
        grid : ModelGrid
        faulted_surface : str or list of str
            Surface that is modified by the NormalFault component. Must be a
            field name or a list of field names if the fault should uplift more
            than one field. Default value is `topographic__elevation`.
            If the faulted surface does not yet exist, it will be ingored. The
            ``run_one_step`` method will check to see an ignored field has been
            added and if it has been, it will modify it.
        fault_throw_rate_through_time : dict, optional
            Dictionary that specifies the time varying throw rate on the fault.
            Expected format is:
            `fault_throw_rate_through_time = {'time': array, 'rate': array}`
            Default value is a constant rate of 0.001 (units not specified).
            This is acomplished by providing the dictionary
            `{'time': [0], 'rate': [0.001]}`. NormalFault uses numpy.interp
            to interpolate the time and rate pattern to the current model time.
            This function uses the first value for all values less than the
            first, and the last value for all values greater than the last, and
            thus providing only one number results in all times getting a rate
            of that value.
        fault_dip_angle : float, optional
            Dip angle of the fault in degrees.  Default value is 90 degrees.
        fault_trace : dictionary, optional
            Dictionary that specifies the coordinates of two locations on the
            fault trace. Expected format is

            .. code-block:: python

                fault_trace = {"x1": float, "y1": float, "x2": float, "y2": float}

            where the vector from `(x1, y1)` to `(x2, y2)` defines the
            strike of the fault trace. The orientation of the fault dip relative
            to the strike follows the right hand rule.
            Default is for the fault to strike NE.
        include_boundaries : boolean, optional
            Flag to indicate if model grid boundaries should be uplifted. If
            set to `True` uplifted model grid boundaries will be set to the
            average value of their upstream nodes. Default value is False.

        Examples
        --------
        Create a grid on which we will run the NormalFault component.

        >>> from landlab import RasterModelGrid
        >>> from landlab.components import NormalFault
        >>> grid = RasterModelGrid((6, 6), xy_spacing=10)

        Add an elevation field.

        >>> z = grid.add_zeros("topographic__elevation", at="node")

        Set the parameter values for the NormalFault component.

        >>> param_dict = {
        ...     "faulted_surface": "topographic__elevation",
        ...     "fault_dip_angle": 90.0,
        ...     "fault_throw_rate_through_time": {
        ...         "time": [0, 9, 10],
        ...         "rate": [0, 0, 0.05],
        ...     },
        ...     "fault_trace": {"y1": 0, "x1": 0, "y2": 30, "x2": 60},
        ...     "include_boundaries": False,
        ... }

        Instantiate a NormalFault component.

        >>> nf = NormalFault(grid, **param_dict)
        >>> nf.faulted_nodes.reshape(grid.shape)
        array([[False, False, False, False, False, False],
               [False,  True, False, False, False, False],
               [False,  True,  True,  True, False, False],
               [False,  True,  True,  True,  True, False],
               [False,  True,  True,  True,  True, False],
               [False, False, False, False, False, False]])

        As we can see, only a subset of the nodes have been identified as
        *faulted nodes*. Because we have set include_boundaries' to False none
        of the boundary nodes are faulted nodes.

        Next we will run the NormalFault for 30 1-year timesteps.

        >>> dt = 1.0
        >>> for i in range(30):
        ...     nf.run_one_step(dt)
        ...
        >>> z.reshape(grid.shape)
        array([[0., 0., 0., 0., 0., 0.],
               [0., 1., 0., 0., 0., 0.],
               [0., 1., 1., 1., 0., 0.],
               [0., 1., 1., 1., 1., 0.],
               [0., 1., 1., 1., 1., 0.],
               [0., 0., 0., 0., 0., 0.]])

        This results in uplift of the faulted nodes, as we would expect.

        If the user knows how much uplift (dz) they want to occur in an event,
        they can use the **run_one_earthquake** function with a specified dz.
        In this case fault_throw_rate_through_time will be ignored.

        >>> nf.run_one_earthquake(dz=100)
        >>> z.reshape(grid.shape)
        array([[  0.,   0.,   0.,   0.,   0.,   0.],
               [  0., 101.,   0.,   0.,   0.,   0.],
               [  0., 101., 101., 101.,   0.,   0.],
               [  0., 101., 101., 101., 101.,   0.],
               [  0., 101., 101., 101., 101.,   0.],
               [  0.,   0.,   0.,   0.,   0.,   0.]])

        Next, we make a very simple landscape model. We need a few components
        and we will set include_boundaries to True.

        >>> from landlab.components import FastscapeEroder, FlowAccumulator
        >>> grid = RasterModelGrid((6, 6), xy_spacing=10)
        >>> z = grid.add_zeros("topographic__elevation", at="node")
        >>> param_dict = {
        ...     "faulted_surface": "topographic__elevation",
        ...     "fault_dip_angle": 90.0,
        ...     "fault_throw_rate_through_time": {
        ...         "time": [0, 900, 1000],
        ...         "rate": [0, 0, 0.05],
        ...     },
        ...     "fault_trace": {"y1": 0, "x1": 0, "y2": 30, "x2": 60},
        ...     "include_boundaries": True,
        ... }

        >>> nf = NormalFault(grid, **param_dict)
        >>> fr = FlowAccumulator(grid)
        >>> fs = FastscapeEroder(grid, K_sp=0.01)

        Run this model for 300 100-year timesteps.

        >>> dt = 100.0
        >>> for i in range(300):
        ...     nf.run_one_step(dt)
        ...     fr.run_one_step()
        ...     fs.run_one_step(dt)
        ...
        >>> z.reshape(grid.shape).round(decimals=2)
        array([[ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],
               [ 5.  ,  5.  ,  0.  ,  0.  ,  0.  ,  0.  ],
               [ 7.39,  7.38,  2.38,  2.89,  0.  ,  0.  ],
               [ 9.36, 11.43,  5.51,  6.42,  3.54,  3.54],
               [15.06, 15.75, 10.6 , 11.42,  8.54,  8.54],
               [15.06, 15.06, 10.7 , 11.42,  8.54,  8.54]])

        The faulted nodes have been uplifted and eroded! Note that here the
        boundary nodes are also uplifted.

        NormalFault keeps track of internal time.

        For example, if a user wanted to only run NormalFault every tenth
        timestep (or some more seismogenically reasonable set of times).

        >>> grid = RasterModelGrid((6, 6), xy_spacing=10)
        >>> z = grid.add_zeros("topographic__elevation", at="node")
        >>> nf = NormalFault(grid, **param_dict)
        >>> fr = FlowAccumulator(grid)
        >>> fs = FastscapeEroder(grid, K_sp=0.01)
        >>> model_time = 0.0
        >>> dt = 100.0
        >>> for i in range(300):
        ...     if i % 10 == 0:
        ...         nf.run_one_step(dt * 10)
        ...     fr.run_one_step()
        ...     fs.run_one_step(dt)
        ...     model_time += dt
        ...
        >>> model_time
        30000.0
        >>> nf.current_time
        30000.0
        """
        fault_throw_rate_through_time = dict(fault_throw_rate_through_time)
        fault_trace = dict(fault_trace)

        super().__init__(grid)

        # save a reference to the grid

        # get the surface to be faulted
        self._surfaces = {}
        self._not_yet_instantiated = []
        if isinstance(faulted_surface, list):
            # if faulted surface is a list, then itterate through multiple
            # surfaces and save
            for surf in faulted_surface:
                try:
                    self._surfaces[surf] = grid.at_node[surf]
                except FieldError:
                    self._not_yet_instantiated.append(surf)
        else:
            self._surfaces[faulted_surface] = grid.at_node[faulted_surface]

        if fault_dip_angle > 90.0:
            raise ValueError(
                "NormaFault fault_dip_angle must be less than 90 " "degrees."
            )

        # get the fault throw parameter values from the parameter dictionary
        self._throw_time = np.array(fault_throw_rate_through_time["time"])
        self._throw_rate = np.array(fault_throw_rate_through_time["rate"])
        self._fault_dip = np.deg2rad(fault_dip_angle)
        self._uplift = self._throw_rate * np.sin(self._fault_dip)

        # Identify in current boundaries will be included
        self._include_boundaries = include_boundaries

        # Instantiate record of current time.
        self._current_time = 0.0

        # get the fault trace dictionary and use to to calculate where the
        # faulted nodes are located.
        self._fault_trace = fault_trace
        dx = self._fault_trace["x2"] - self._fault_trace["x1"]
        dy = self._fault_trace["y2"] - self._fault_trace["y1"]
        self._fault_azimuth = np.mod(np.arctan2(dy, dx), TWO_PI)
        self._fault_anti_azimuth = self._fault_azimuth + np.pi
        # deal with the edge case in which dx == 0
        if dx == 0:
            self._dy_over_dx = 0.0
            self._fault_trace_y_intercept = 0.0
            self._fault_trace_x_intercept = self._fault_trace["x2"]
        else:
            self._dy_over_dx = dy / dx
            self._fault_trace_y_intercept = self._fault_trace["y1"] - (
                self._dy_over_dx * self._fault_trace["x1"]
            )
            self._fault_trace_x_intercept = 0.0

        # set the considered nodes based on whether the boundaries will be
        # included in the faulted terrain.
        if self._include_boundaries:
            potential_nodes = np.arange(self._grid.size("node"))
        else:
            potential_nodes = self._grid.core_nodes

        # select those nodes that are on the correct side of the fault
        dx_pn = self._grid.x_of_node[potential_nodes] - self._fault_trace_x_intercept
        dy_pn = self._grid.y_of_node[potential_nodes] - self._fault_trace_y_intercept
        potential_angles = np.mod(np.arctan2(dy_pn, dx_pn), TWO_PI)
        if self._fault_anti_azimuth <= TWO_PI:
            faulted_node_ids = potential_nodes[
                (
                    (potential_angles > self._fault_azimuth)
                    & (potential_angles <= (self._fault_anti_azimuth))
                )
            ]
        else:
            faulted_node_ids = potential_nodes[
                (
                    (potential_angles > self._fault_azimuth)
                    | (potential_angles <= np.mod(self._fault_anti_azimuth, TWO_PI))
                )
            ]

        # save a n-node array of boolean identifing faulted nodes.
        self._faulted_nodes = np.zeros(self._grid.size("node"), dtype=bool)
        self._faulted_nodes[faulted_node_ids] = True

    @property
    def faulted_nodes(self):
        """At node array indicating which nodes are on the upthrown block."""
        return self._faulted_nodes

    def _check_surfaces(self):
        if len(self._not_yet_instantiated) > 0:
            still_not_instantiated = []
            for surf in self._not_yet_instantiated:
                if surf in self._grid.at_node:
                    self._surfaces[surf] = self._grid.at_node[surf]
                else:
                    still_not_instantiated.append(surf)
            self._not_yet_instantiated = still_not_instantiated

    def run_one_earthquake(self, dz):
        """Run one earthquake with uplift of magnitude ``dz``."""
        self._check_surfaces()

        # save z before uplift only if using include boundaries.
        if self._include_boundaries:
            surfs_before_uplift = {}
            for surf_name in self._surfaces:
                surfs_before_uplift[surf_name] = self._surfaces[surf_name].copy()

        # uplift the faulted_nodes
        for surf_name in self._surfaces:
            self._surfaces[surf_name][self._faulted_nodes] += dz

        # if faulted nodes includes boundaries we must do some extra work because
        # landlab components will typically not erode these boundaries. This means
        # they will be uplifted but not eroded.

        if self._include_boundaries:
            #  here our goal is to set faulted boundaries to average of open
            # node faulted neighbors

            # create boolean of the faulted boundary nodes
            faulted_boundaries = self._faulted_nodes.copy()
            faulted_boundaries[self._grid.core_nodes] = False

            core_nodes = np.zeros(self._grid.size("node"), dtype=bool)
            core_nodes[self._grid.core_nodes] = True

            neighbor_is_core = core_nodes[self._grid.adjacent_nodes_at_node]
            neighbor_is_faulted = self._faulted_nodes[self._grid.adjacent_nodes_at_node]

            neighbor_for_averaging = neighbor_is_faulted & neighbor_is_core

            # Identify nodes that have at least one adjacent node that is both
            # faulted and not a boundary node.
            # average the pre-uplift topography on those adjacent nodes and assign
            # to the boundary node.
            # here we use the pre-uplift elevation because other steps in the model
            # may diminish this topography.

            averaged = neighbor_for_averaging[faulted_boundaries].sum(axis=1) == 1
            if any(averaged):
                averaged_nodes = np.where(faulted_boundaries)[0][np.where(averaged)[0]]
                for surf_name in self._surfaces:
                    elevations_to_average = surfs_before_uplift[surf_name][
                        self._grid.adjacent_nodes_at_node
                    ]
                    elevations_to_average[self._grid.adjacent_nodes_at_node == -1] = (
                        np.nan
                    )
                    elevations_to_average[~neighbor_for_averaging] = np.nan
                    self._surfaces[surf_name][averaged_nodes] = np.nanmean(
                        elevations_to_average[averaged_nodes], axis=1
                    )

            # identify any boundary nodes that are not being averaged. This will
            # happen at the corners on RasterModelGrids. Average over adjacent
            # nodes that are faulted. These nodes will be boundary nodes.
            # here we use the current topography as we will have just updated the
            # adjacent nodes in the prior block.
            if any(~averaged):
                un_averaged_nodes = np.where(faulted_boundaries)[0][
                    np.where(~averaged)[0]
                ]
                for surf_name in self._surfaces:
                    elevations_to_average = self._surfaces[surf_name][
                        self._grid.adjacent_nodes_at_node
                    ]
                    elevations_to_average[self._grid.adjacent_nodes_at_node == -1] = (
                        np.nan
                    )
                    elevations_to_average[~neighbor_is_faulted] = np.nan
                    self._surfaces[surf_name][un_averaged_nodes] = np.nanmean(
                        elevations_to_average[un_averaged_nodes], axis=1
                    )

    def run_one_step(self, dt):
        """Run_one_step method for NormalFault.

        Parameters
        ----------
        dt : float
            Time increment used to advance the NormalFault component.
        current_time : float, optional
            If NormalFault is not being advanced by dt every timestep with all
            components, its internal time may be incorrect, this can be remedied
            by providing a value for current time. Default value is None which
            results in the internal timekeeping not being changed.
        """
        # calculate the current uplift rate
        current_uplift_rate = np.interp(
            self._current_time, self._throw_time, self._throw_rate
        )

        # run one earthquake of size current_uplift_rate * dt
        self.run_one_earthquake(current_uplift_rate * dt)

        # increment time
        self._current_time += dt



================================================
File: overland_flow/__init__.py
================================================
from .generate_overland_flow_Bates import OverlandFlowBates
from .generate_overland_flow_deAlmeida import OverlandFlow
from .generate_overland_flow_implicit_kinwave import KinwaveImplicitOverlandFlow
from .generate_overland_flow_kinwave import KinwaveOverlandFlowModel
from .kinematic_wave_rengers import KinematicWaveRengers
from .linear_diffusion_overland_flow_router import LinearDiffusionOverlandFlowRouter

__all__ = [
    "OverlandFlowBates",
    "OverlandFlow",
    "KinematicWaveRengers",
    "KinwaveImplicitOverlandFlow",
    "KinwaveOverlandFlowModel",
    "LinearDiffusionOverlandFlowRouter",
]



================================================
File: overland_flow/_links.py
================================================
import numpy as np

from ...core.utils import as_id_array
from ...graph.structured_quad.structured_quad import StructuredQuadGraphTopology
from . import _neighbors_at_link


def neighbors_at_link(shape, links):
    """Get neighbor links.

    Examples
    --------
    >>> import numpy as np
    >>> from landlab.components.overland_flow._links import neighbors_at_link

    >>> neighbors_at_link((3, 2), np.arange(7))
    array([[-1,  3, -1, -1],
           [ 2,  4, -1, -1], [-1,  5,  1, -1],
           [-1,  6, -1,  0],
           [ 5,  7, -1,  1], [-1, -1,  4,  2],
           [-1, -1, -1,  3]])
    """
    links = np.asarray(links, dtype=int)
    out = np.full((links.size, 4), -1, dtype=int)
    _neighbors_at_link.neighbors_at_link(links, shape, out)
    return out


def vertical_link_ids(shape):
    """Vertical links in a structured quad grid.

    Parameters
    ----------
    shape : tuple of int
        Shape of grid of nodes.

    Returns
    -------
    (M, N) ndarray :
        Array of link IDs.

    Examples
    --------
    >>> from landlab.components.overland_flow._links import vertical_link_ids
    >>> vertical_link_ids((3, 4))
    array([[ 3,  4,  5,  6],
           [10, 11, 12, 13]])
    """
    layout = StructuredQuadGraphTopology(shape)
    return layout.vertical_links.reshape((shape[0] - 1, shape[1]))


def horizontal_link_ids(shape):
    """Horizontal links in a structured quad grid.

    Parameters
    ----------
    shape : tuple of int
        Shape of grid of nodes.

    Returns
    -------
    (M, N) ndarray :
        Array of link IDs.

    Examples
    --------
    >>> from landlab.components.overland_flow._links import horizontal_link_ids
    >>> horizontal_link_ids((3, 4))
    array([[ 0,  1,  2],
           [ 7,  8,  9],
           [14, 15, 16]])
    """
    layout = StructuredQuadGraphTopology(shape)
    return layout.horizontal_links.reshape((shape[0], shape[1] - 1))


def vertical_south_link_neighbor(shape, vertical_ids, bad_index_value=-1):
    """Link IDs of south, vertical link neighbor.

    Parameters
    ----------
    shape : tuple of int
        Shape of grid of nodes.
    vertical_ids : array of int
        Array of all vertical link ids - MUST BE ARRAY OF LEN(VERTICAL_LINKS)
    bad_index_value: int, optional
        Value assigned to inactive indicies in the array.

    Returns
    -------
    ndarray :
        Link IDs of *south* vertical neighbor links. Length of
        number_of_vertical_links.

    Examples
    --------
    The following example uses this grid::

        *       *       *       *       *
        ^       ^       ^       ^       ^
       22      23      24      25      26
        |       |       |       |       |
        *       *       *       *       *
        ^       ^       ^       ^       ^
       13      14      15      16      17
        |       |       |       |       |
        *       *       *       *       *
        ^       ^       ^       ^       ^
        4       5       6       7       8
        |       |       |       |       |
        *       *       *       *       *

    .. note::

        Only vertical links are shown. When no neighbor is found,
        bad_index_value is returned.

        ``*`` indicates nodes

        Numeric values correspond to the vertical IDs.

    >>> from landlab import RasterModelGrid
    >>> from landlab.components.overland_flow._links import (
    ...     vertical_link_ids,
    ...     vertical_south_link_neighbor,
    ... )
    >>> rmg = RasterModelGrid((4, 5))
    >>> vertical_links = vertical_link_ids(rmg.shape)
    >>> vertical_south_link_neighbor(rmg.shape, vertical_links).reshape((3, 5))
    array([[-1, -1, -1, -1, -1],
           [ 4,  5,  6,  7,  8],
           [13, 14, 15, 16, 17]])
    """
    vertical_links = StructuredQuadGraphTopology(shape).vertical_links
    vertical_links[shape[1] :] = vertical_links[: -shape[1]]
    vertical_links[: shape[1]] = bad_index_value

    return vertical_links


def vertical_west_link_neighbor(shape, vertical_ids, bad_index_value=-1):
    """Link IDs of west, vertical link neighbor.

    Parameters
    ----------
    shape : tuple of int
        Shape of grid of nodes.
    vertical_ids : array of int
        Array of all vertical link ids- MUST BE ARRAY OF LEN(VERTICAL_LINKS)
    bad_index_value: int, optional
        Value assigned to inactive indicies in the array.

    Returns
    -------
    ndarray :
        Link IDs of *west* vertical neighbor links. Length of
        number_of_vertical_links.

    Examples
    --------
    The following example uses this grid::

        *       *       *       *       *
        ^       ^       ^       ^       ^
       22      23      24      25      26
        |       |       |       |       |
        *       *       *       *       *
        ^       ^       ^       ^       ^
       13      14      15      16      17
        |       |       |       |       |
        *       *       *       *       *
        ^       ^       ^       ^       ^
        4       5       6       7       8
        |       |       |       |       |
        *       *       *       *       *

    .. note::

        Only vertical links are shown. When no neighbor is found,
        bad_index_value is returned.

        ``*`` indicates nodes

        Numeric values correspond to the vertical IDs.

    >>> from landlab import RasterModelGrid
    >>> from landlab.components.overland_flow._links import (
    ...     vertical_link_ids,
    ...     vertical_west_link_neighbor,
    ... )
    >>> rmg = RasterModelGrid((4, 5))
    >>> vertical_links = vertical_link_ids(rmg.shape)
    >>> vertical_west_link_neighbor(rmg.shape, vertical_links).reshape((3, 5))
    array([[-1,  4,  5,  6,  7],
           [-1, 13, 14, 15, 16],
           [-1, 22, 23, 24, 25]])
    """
    vertical_links = StructuredQuadGraphTopology(shape).vertical_links.reshape(
        (shape[0] - 1, shape[1])
    )
    vertical_links[:, 1:] = vertical_links[:, :-1]
    vertical_links[:, 0] = bad_index_value

    return vertical_links.reshape(-1)


def vertical_north_link_neighbor(shape, vertical_ids, bad_index_value=-1):
    """Link IDs of north, vertical link neighbor.

    Parameters
    ----------
    shape : tuple of int
        Shape of grid of nodes.
    vertical_ids : array of int
        Array of all vertical link ids- MUST BE ARRAY OF LEN(VERTICAL_LINKS)
    bad_index_value: int, optional
        Value assigned to inactive indicies in the array.

    Returns
    -------
    ndarray :
        Link IDs of *north* vertical neighbor links. Length of
        number_of_vertical_links.

    Examples
    --------
    The following example uses this grid::

        *       *       *       *       *
        ^       ^       ^       ^       ^
       22      23      24      25      26
        |       |       |       |       |
        *       *       *       *       *
        ^       ^       ^       ^       ^
       13      14      15      16      17
        |       |       |       |       |
        *       *       *       *       *
        ^       ^       ^       ^       ^
        4       5       6       7       8
        |       |       |       |       |
        *       *       *       *       *

    .. note::

        Only vertical links are shown. When no neighbor is found,
        bad_index_value is returned.

        ``*`` indicates nodes

        Numeric values correspond to the vertical IDs.

    >>> from landlab import RasterModelGrid
    >>> from landlab.components.overland_flow._links import (
    ...     vertical_link_ids,
    ...     vertical_north_link_neighbor,
    ... )
    >>> rmg = RasterModelGrid((4, 5))
    >>> vertical_ids = vertical_link_ids(rmg.shape)
    >>> vertical_north_link_neighbor(rmg.shape, vertical_ids).reshape((3, 5))
    array([[13, 14, 15, 16, 17],
           [22, 23, 24, 25, 26],
           [-1, -1, -1, -1, -1]])
    """
    vertical_links = StructuredQuadGraphTopology(shape).vertical_links
    vertical_links[: -shape[1]] = vertical_links[shape[1] :]
    vertical_links[-shape[1] :] = bad_index_value

    return vertical_links


def vertical_east_link_neighbor(shape, vertical_ids, bad_index_value=-1):
    """Link IDs of east, vertical link neighbor.

    Parameters
    ----------
    shape : tuple of int
        Shape of grid of nodes.
    vertical_ids : array of int
        Array of all vertical link ids - MUST BE ARRAY OF LEN(VERTICAL_LINKS)
    bad_index_value: int, optional
        Value assigned to inactive indicies in the array.

    Returns
    -------
    ndarray :
        Link IDs of *east* vertical neighbor links. Length of
        number_of_vertical_links.

    Examples
    --------
    The following example uses this grid::

        *       *       *       *       *
        ^       ^       ^       ^       ^
       22      23      24      25      26
        |       |       |       |       |
        *       *       *       *       *
        ^       ^       ^       ^       ^
       13      14      15      16      17
        |       |       |       |       |
        *       *       *       *       *
        ^       ^       ^       ^       ^
        4       5       6       7       8
        |       |       |       |       |
        *       *       *       *       *

    .. note::

        Only vertical links are shown. When no neighbor is found,
        bad_index_value is returned.

        ``*`` indicates nodes

        Numeric values correspond to the vertical IDs.

    >>> from landlab import RasterModelGrid
    >>> from landlab.components.overland_flow._links import (
    ...     vertical_link_ids,
    ...     vertical_east_link_neighbor,
    ... )
    >>> rmg = RasterModelGrid((4, 5))
    >>> vertical_links = vertical_link_ids(rmg.shape)
    >>> vertical_east_link_neighbor(rmg.shape, vertical_links).reshape((3, 5))
    array([[ 5,  6,  7,  8, -1],
           [14, 15, 16, 17, -1],
           [23, 24, 25, 26, -1]])
    """
    vertical_links = StructuredQuadGraphTopology(shape).vertical_links.reshape(
        (shape[0] - 1, shape[1])
    )
    vertical_links[:, :-1] = vertical_links[:, 1:]
    vertical_links[:, -1] = bad_index_value

    return vertical_links.base


def active_link_ids(shape, node_status):
    """Get active links.

    Parameters
    ----------
    shape : tuple of int
        Shape of grid of nodes.
    node_status : array_link
        Status of nodes in grid.

    Returns
    -------
    ndarray :
        Links IDs at the active links.

    Examples
    --------
    >>> from landlab.grid import RasterModelGrid
    >>> from landlab.components.overland_flow._links import active_link_ids

    >>> rmg = RasterModelGrid((3, 4))
    >>> rmg.set_closed_boundaries_at_grid_edges(True, True, True, True)

    >>> status = rmg.status_at_node
    >>> status.reshape(rmg.shape)
    array([[4, 4, 4, 4],
           [4, 0, 0, 4],
           [4, 4, 4, 4]], dtype=uint8)

    >>> active_link_ids((3, 4), status)
    array([8])
    """
    return as_id_array(np.where(is_active_link(shape, node_status))[0])


def is_active_link(shape, node_status):
    """Link IDs of active links.

    Parameters
    ----------
    shape : tuple of int
        Shape of grid of nodes.
    node_status : array_link
        Status of nodes in grid.

    Returns
    -------
    ndarray :
        Links IDs at the active links.

    Examples
    --------
    >>> from landlab.components.overland_flow._links import is_active_link
    >>> from landlab.grid.nodestatus import NodeStatus

    >>> status = [
    ...     [NodeStatus.CLOSED, NodeStatus.CLOSED, NodeStatus.CLOSED],
    ...     [NodeStatus.CLOSED, NodeStatus.CORE, NodeStatus.CLOSED],
    ...     [NodeStatus.CLOSED, NodeStatus.CORE, NodeStatus.CLOSED],
    ...     [NodeStatus.CLOSED, NodeStatus.CLOSED, NodeStatus.CLOSED],
    ... ]
    >>> is_active_link((4, 3), status)
    array([False, False,
           False, False, False,
           False, False,
           False, True, False,
           False, False,
           False, False, False,
           False, False])
    """
    from ...grid.linkstatus import is_active_link

    node_status = np.asarray(node_status).reshape(-1)

    if np.prod(shape) != node_status.size:
        raise ValueError(
            "node status array does not match size of grid "
            "(%d != %d)" % (np.prod(shape), len(node_status))
        )

    # status_at_link_start = node_status.flat[node_id_at_link_start(shape)]
    # status_at_link_end = node_status.flat[node_id_at_link_end(shape)]

    # status_at_link = node_status[StructuredQuadGraphTopology(shape).nodes_at_link]

    return is_active_link(node_status[StructuredQuadGraphTopology(shape).nodes_at_link])


def vertical_active_link_ids(shape, active_ids, bad_index_value=-1):
    """ID of vertical active links.

    Parameters
    ----------
    shape : tuple of int
        Shape of grid of nodes.
    active_ids : array of int
        Array of all active link ids
    bad_index_value: int, optional
        Value assigned to inactive indicies in the array.

    Returns
    -------
    ndarray :
        Link IDs at the VERTICAL active links. Length of
        number_of_vertical_links.

    Examples
    --------
    The following example uses this grid::

          *---I-->*---I-->*---I-->*---I-->*
          ^       ^       ^       ^       ^
          I       I       I       I       I
          |       |       |       |       |
          *---I-->o---H-->o---H-->o---I-->*
          ^       ^       ^       ^       ^
          I       6       7       8       I
          |       |       |       |       |
          *---I-->o---H-->o---H-->o---I-->*
          ^       ^       ^       ^       ^
          I       I       I       I       I
          |       |       |       |       |
          *---I-->*---I-->*---I-->*---I-->*

    .. note::

        ``*`` indicates the nodes that are set to `NodeStatus.CLOSED`

        ``o`` indicates the nodes that are set to `NodeStatus.CORE`

        ``I`` indicates the links that are set to `LinkStatus.INACTIVE`

        ``H`` indicates horizontal active ids, which are ignored by this
        function

        Numeric values correspond to the vertical `LinkStatus.ACTIVE` IDs.

    >>> from landlab import RasterModelGrid
    >>> from landlab.components.overland_flow._links import (
    ...     active_link_ids,
    ...     vertical_active_link_ids,
    ... )

    >>> rmg = RasterModelGrid((4, 5))
    >>> active_ids = active_link_ids((4, 5), rmg.status_at_node)
    >>> active_ids
    array([ 5,  6,  7,
            9, 10, 11, 12,
           14, 15, 16,
           18, 19, 20, 21,
           23, 24, 25])

    >>> vertical_active_link_ids((4, 5), active_ids).reshape((3, 5))
    array([[-1,  5,  6,  7, -1],
           [-1, 14, 15, 16, -1],
           [-1, 23, 24, 25, -1]])

    >>> rmg.set_closed_boundaries_at_grid_edges(True, True, True, True)
    >>> status = rmg.status_at_node
    >>> active_ids = active_link_ids((4, 5), status)
    >>> vertical_active_link_ids((4, 5), active_ids).reshape((3, 5))
    array([[-1, -1, -1, -1, -1],
           [-1, 14, 15, 16, -1],
           [-1, -1, -1, -1, -1]])
    """
    number_of_vertical_links = (shape[0] - 1) * shape[1]
    out = np.full(number_of_vertical_links, bad_index_value, dtype=int)
    vertical_ids = active_ids[np.where(is_vertical_link(shape, active_ids))]

    out[nth_vertical_link(shape, vertical_ids)] = vertical_ids
    return out


def _number_of_links(shape):
    """Number of links in a structured quad grid.

    Parameters
    ----------
    shape : tuple of int
        Shape of grid of nodes.

    Returns
    -------
    int :
        Number of links in grid.

    Examples
    --------
    >>> from landlab.components.overland_flow._links import _number_of_links
    >>> _number_of_links((3, 4))
    17
    """
    return (shape[0] - 1) * shape[1] + shape[0] * (shape[1] - 1)
    # return number_of_vertical_links(shape) + number_of_horizontal_links(shape)


def number_of_vertical_links(shape):
    """Number of vertical links in a structured quad grid.

    Parameters
    ----------
    shape : tuple of int
        Shape of grid of nodes.

    Returns
    -------
    int :
        Number of vertical links in grid.

    Examples
    --------
    >>> from landlab.components.overland_flow._links import number_of_vertical_links
    >>> number_of_vertical_links((3, 4))
    8
    """
    return (shape[0] - 1) * shape[1]


def number_of_horizontal_links(shape):
    """Number of horizontal links in a structured quad grid.

    Parameters
    ----------
    shape : tuple of int
        Shape of grid of nodes.

    Returns
    -------
    int :
        Number of horizontal links in grid.

    Examples
    --------
    >>> from landlab.components.overland_flow._links import number_of_horizontal_links
    >>> number_of_horizontal_links((3, 4))
    9
    """
    return shape[0] * (shape[1] - 1)


def is_vertical_link(shape, links):
    """Test if links are vertical.

    Parameters
    ----------
    shape : tuple of int
        Shape of grid of nodes.
    links : array of int
        Array of link ids to test.

    Returns
    -------
    ndarray of bool
        `True` for links that are vertical.

    Examples
    --------
    >>> from landlab.components.overland_flow._links import (
    ...     is_vertical_link,
    ...     _number_of_links,
    ... )
    >>> import numpy as np
    >>> shape = (3, 4)
    >>> links = np.arange(_number_of_links(shape))
    >>> is_vertical_link(shape, links)
    array([False, False, False,  True,  True,  True,  True,
           False, False, False,  True,  True,  True,  True,
           False, False, False])
    """
    return ((links % (2 * shape[1] - 1)) >= shape[1] - 1) & (
        links < _number_of_links(shape)
    )


def nth_vertical_link(shape, links):
    """Convert link ID to vertical link ID.

    Parameters
    ----------
    shape : tuple of int
        Shape of grid of nodes.
    links : array of int
        Array of link ids to test.

    Returns
    -------
    ndarray of int
        The link ID as the nth vertical links.

    Examples
    --------
    >>> from landlab.components.overland_flow._links import nth_vertical_link
    >>> shape = (3, 4)
    >>> nth_vertical_link(shape, 4)
    1
    >>> nth_vertical_link(shape, (3, 4, 11))
    array([0, 1, 5])
    """
    links = np.asarray(links, dtype=int)
    return as_id_array(
        (links // (2 * shape[1] - 1)) * shape[1]
        + links % (2 * shape[1] - 1)
        - (shape[1] - 1)
    )


def horizontal_active_link_ids(shape, active_ids, bad_index_value=-1):
    """ID of horizontal active links.

    Parameters
    ----------
    shape : tuple of int
        Shape of grid of nodes.
    active_ids : array of int
        Array of all active link ids
    bad_index_value: int, optional
        Value assigned to inactive indicies in the array.

    Returns
    -------
    ndarray :
        Link IDs at the HORIZONTAL active links. Length of
        number_of_horizontal_links.

    Examples
    --------

    The following example uses this grid::

          *---I-->*---I-->*---I-->*---I-->*
          ^       ^       ^       ^       ^
          I       I       I       I       I
          |       |       |       |       |
          *---I-->o--24-->o--25-->o---I-->*
          ^       ^       ^       ^       ^
          I       V       V       V       I
          |       |       |       |       |
          *---I-->o--20-->o--21-->o---I-->*
          ^       ^       ^       ^       ^
          I       I       I       I       I
          |       |       |       |       |
          *---I-->*---I-->*---I-->*---I-->*

    .. note::

        ``*`` indicates the nodes that are set to `NodeStatus.CLOSED`

        ``o`` indicates the nodes that are set to `NodeStatus.CORE`

        ``I`` indicates the links that are set to `LinkStatus.INACTIVE`

        ``V`` indicates vertical active ids, which are ignored by this
        function.

        Numeric values correspond to the horizontal `LinkStatus.ACTIVE`  ID.

    >>> from landlab import RasterModelGrid
    >>> from landlab.components.overland_flow._links import (
    ...     active_link_ids,
    ...     horizontal_active_link_ids,
    ... )

    >>> rmg = RasterModelGrid((4, 5))
    >>> rmg.set_closed_boundaries_at_grid_edges(True, True, True, True)

    >>> status = rmg.status_at_node
    >>> status.reshape(rmg.shape)
    array([[4, 4, 4, 4, 4],
           [4, 0, 0, 0, 4],
           [4, 0, 0, 0, 4],
           [4, 4, 4, 4, 4]], dtype=uint8)
    >>> active_ids = active_link_ids((4, 5), status)

    >>> horizontal_active_link_ids((4, 5), active_ids).reshape((4, 4))
    array([[-1, -1, -1, -1],
           [-1, 10, 11, -1],
           [-1, 19, 20, -1],
           [-1, -1, -1, -1]])
    """
    number_of_horizontal_links = shape[0] * (shape[1] - 1)
    out = np.full(number_of_horizontal_links, bad_index_value, dtype=int)
    horizontal_ids = active_ids[np.where(~is_vertical_link(shape, active_ids))]

    out[nth_horizontal_link(shape, horizontal_ids)] = horizontal_ids
    return out


def nth_horizontal_link(shape, links):
    """Convert link ID to horizontal link ID.

    Parameters
    ----------
    shape : tuple of int
        Shape of grid of nodes.
    links : array of int
        Array of link ids to test.

    Returns
    -------
    ndarray of int
        The link ID as the nth horizontal links.

    Examples
    --------
    >>> from landlab.components.overland_flow._links import nth_horizontal_link
    >>> shape = (3, 4)
    >>> nth_horizontal_link(shape, 16)
    8
    >>> nth_horizontal_link(shape, (1, 7, 8))
    array([1, 3, 4])
    """
    links = np.asarray(links, dtype=int)
    return as_id_array(
        (links // (2 * shape[1] - 1)) * (shape[1] - 1) + links % (2 * shape[1] - 1)
    )


def is_horizontal_link(shape, links):
    """Test if a link is horizontal.

    Parameters
    ----------
    shape : tuple of int
        Shape of grid of nodes.
    links : array of int
        Array of link ids to test.

    Returns
    -------
    ndarray of bool
        `True` for links that are horizontal.

    Examples
    --------
    >>> from landlab.components.overland_flow._links import (
    ...     is_horizontal_link,
    ...     _number_of_links,
    ... )
    >>> import numpy as np
    >>> shape = (3, 4)
    >>> links = np.arange(_number_of_links(shape))
    >>> is_horizontal_link(shape, links)
    array([ True,  True,  True, False, False, False, False,
            True,  True,  True, False, False, False, False,
            True,  True,  True])
    """
    return (~is_vertical_link(shape, links)) & (links < _number_of_links(shape))


def horizontal_west_link_neighbor(shape, horizontal_ids, bad_index_value=-1):
    """ID of west, horizontal link neighbor.

    Parameters
    ----------
    shape : tuple of int
        Shape of grid of nodes.
    horizontal_ids : array of int
        Array of all horizontal link ids - *must be of len(horizontal_links)*
    bad_index_value: int, optional
        Value assigned to inactive indicies in the array.

    Returns
    -------
    ndarray :
        Link IDs of west horizontal neighbor links. Length of
        number_of_horizontal_links.

    Examples
    --------
    The following example uses this grid::

        *--27-->*--28-->*--29-->*--30-->*



        *--18-->*--19-->*--20-->*--21-->*



        *---9-->*--10-->*--11-->*--12-->*



        *---0-->*---1-->*---2-->*---3-->*

    .. note::

        Only horizontal links are shown. When no neighbor is found,
        bad_index_value is returned.

        ``*`` indicates nodes

        Numeric values correspond to the horizontal IDs.

    >>> from landlab import RasterModelGrid
    >>> from landlab.components.overland_flow._links import (
    ...     horizontal_link_ids,
    ...     horizontal_west_link_neighbor,
    ... )
    >>> rmg = RasterModelGrid((4, 5))
    >>> horizontal_links = horizontal_link_ids(rmg.shape).flatten()
    >>> horizontal_west_link_neighbor(rmg.shape, horizontal_links)
    array([-1,  0,  1,  2, -1,  9, 10, 11, -1, 18, 19, 20, -1, 27, 28, 29])
    """
    links = np.roll(horizontal_ids.reshape((shape[0], shape[1] - 1)), 1, axis=1)
    links[:, 0] = bad_index_value

    return links.reshape(-1)


def horizontal_east_link_neighbor(shape, horizontal_ids, bad_index_value=-1):
    """IDs of east, horizontal link neighbor.

    Parameters
    ----------
    shape : tuple of int
        Shape of grid of nodes.
    horizontal_ids : array of int
        Array of all horizontal link ids - *must be of len(horizontal_links)*
    bad_index_value: int, optional
        Value assigned to inactive indicies in the array.

    Returns
    -------
    ndarray :
        Link IDs of east horizontal neighbor links. Length of
        number_of_horizontal_links.

    Examples
    --------
    The following example uses this grid::

        *--27-->*--28-->*--29-->*--30-->*



        *--18-->*--19-->*--20-->*--21-->*



        *---9-->*--10-->*--11-->*--12-->*



        *---0-->*---1-->*---2-->*---3-->*

    .. note::

        Only horizontal links are shown. When no neighbor is found,
        bad_index_value is returned.

        ``*`` indicates nodes

        Numeric values correspond to the horizontal `LinkStatus.ACTIVE` IDs.

    >>> from landlab import RasterModelGrid
    >>> from landlab.components.overland_flow._links import (
    ...     horizontal_link_ids,
    ...     horizontal_east_link_neighbor,
    ... )
    >>> rmg = RasterModelGrid((4, 5))
    >>> horizontal_links = horizontal_link_ids(rmg.shape).flatten()
    >>> horizontal_east_link_neighbor(rmg.shape, horizontal_links)
    array([ 1,  2,  3, -1, 10, 11, 12, -1, 19, 20, 21, -1, 28, 29, 30, -1])
    """
    links = np.roll(horizontal_ids.reshape((shape[0], shape[1] - 1)), -1, axis=1)
    links[:, -1] = -1

    return links.reshape(-1)


def horizontal_north_link_neighbor(shape, horizontal_ids, bad_index_value=-1):
    """ID of north, horizontal link neighbor.

    Parameters
    ----------
    shape : tuple of int
        Shape of grid of nodes.
    horizontal_ids : array of int
        Array of all horizontal link ids - *must be of len(horizontal_links)*
    bad_index_value: int, optional
        Value assigned to inactive indicies in the array.

    Returns
    -------
    ndarray :
        Link IDs of north horizontal neighbor links. Length of
        number_of_horizontal_links.

    Examples
    --------
    The following example uses this grid::

        *--27-->*--28-->*--29-->*--30-->*



        *--18-->*--19-->*--20-->*--21-->*



        *---9-->*--10-->*--11-->*--12-->*



        *---0-->*---1-->*---2-->*---3-->*

    .. note::

        Only horizontal links are shown. When no neighbor is found,
        bad_index_value is returned.

        ``*`` indicates nodes

        Numeric values correspond to the horizontal `LinkStatus.ACTIVE` IDs.

    >>> from landlab import RasterModelGrid
    >>> from landlab.components.overland_flow._links import (
    ...     horizontal_link_ids,
    ...     horizontal_north_link_neighbor,
    ... )
    >>> rmg = RasterModelGrid((4, 5))
    >>> horizontal_links = horizontal_link_ids(rmg.shape).flatten()
    >>> horizontal_north_link_neighbor(rmg.shape, horizontal_links)
    array([ 9, 10, 11, 12, 18, 19, 20, 21, 27, 28, 29, 30, -1, -1, -1, -1])
    """
    links = np.roll(horizontal_ids.reshape((shape[0], shape[1] - 1)), -1, axis=0)
    links[-1, :] = bad_index_value

    return links.reshape(-1)


def horizontal_south_link_neighbor(shape, horizontal_ids, bad_index_value=-1):
    """ID of south horizontal link neighbor.

    Parameters
    ----------
    shape : tuple of int
        Shape of grid of nodes.
    horizontal_ids : array of int
        Array of all horizontal link ids *must be of len(horizontal_links)*.
    bad_index_value: int, optional
        Value assigned to inactive indicies in the array.

    Returns
    -------
    ndarray :
        Link IDs of south horizontal neighbor links. Length of
        number_of_horizontal_links.

    Examples
    --------
    The following example uses this grid::

        *--27-->*--28-->*--29-->*--30-->*



        *--18-->*--19-->*--20-->*--21-->*



        *---9-->*--10-->*--11-->*--12-->*



        *---0-->*---1-->*---2-->*---3-->*

    .. note::

        Only horizontal links are shown. When no neighbor is found,
        bad_index_value is returned.

        ``*`` indicates nodes

        Numeric values correspond to the horizontal IDs.

    >>> from landlab import RasterModelGrid
    >>> from landlab.components.overland_flow._links import (
    ...     horizontal_link_ids,
    ...     horizontal_north_link_neighbor,
    ... )
    >>> rmg = RasterModelGrid((4, 5))
    >>> horizontal_links = horizontal_link_ids(rmg.shape).flatten()
    >>> horizontal_south_link_neighbor(rmg.shape, horizontal_links)
    array([-1, -1, -1, -1,  0,  1,  2,  3,  9, 10, 11, 12, 18, 19, 20, 21])
    """
    links = np.roll(horizontal_ids.reshape((shape[0], shape[1] - 1)), 1, axis=0)
    links[0, :] = bad_index_value

    return links.reshape(-1)



================================================
File: overland_flow/_neighbors_at_link.pyx
================================================
cimport cython

# https://cython.readthedocs.io/en/stable/src/userguide/fusedtypes.html
ctypedef fused id_t:
    cython.integral
    long long


@cython.boundscheck(False)
def neighbors_at_link(
    const id_t [:] links,
    shape,
    id_t [:, :] out,
):
    cdef int stride
    cdef int n_links
    cdef int link
    cdef int i
    cdef bint is_top, is_bottom, is_left, is_right

    stride = 2 * shape[1] - 1
    n_links = (shape[0] - 1) * shape[1] + shape[0] * (shape[1] - 1)

    for i in range(links.shape[0]):
        link = links[i]

        is_top = link > (n_links - stride)
        is_bottom = link < stride
        is_left = link % stride == 0 or (link + shape[1]) % stride == 0
        is_right = (link - (shape[1] - 2)) % stride == 0 or (link + 1) % stride == 0

        if not is_right:
            out[i, 0] = link + 1

        if not is_top:
            out[i, 1] = link + stride

        if not is_left:
            out[i, 2] = link - 1

        if not is_bottom:
            out[i, 3] = link - stride



================================================
File: overland_flow/generate_overland_flow_Bates.py
================================================
"""generate_overland_flow.py.

This component simulates overland flow using
the 2-D numerical model of shallow-water flow
over topography using the Bates et al. (2010)
algorithm for storage-cell inundation modeling.

Written by Jordan Adams, based on code written by Greg Tucker.

Last updated: April 21, 2016
"""

import numpy as np
import scipy.constants

from landlab import Component


class OverlandFlowBates(Component):
    """Simulate overland flow using Bates et al. (2010).

    Landlab component that simulates overland flow using the Bates et al.,
    (2010) approximations of the 1D shallow water equations to be used for 2D
    flood inundation modeling.

    This component calculates discharge, depth and shear stress after some
    precipitation event across any raster grid. Default input file is named
    "overland_flow_input.txt' and is contained in the
    landlab.components.overland_flow folder.

    Parameters
    ----------
    grid : RasterGridModel
        A grid.
    input_file : str
        Contains necessary and optional inputs. If not given, default input
        file is used.

        -  Manning's n is *required*.
        -  Storm duration is needed *if* rainfall_duration is not passed in the
           initialization
        -  Rainfall intensity is needed *if* rainfall_intensity is not passed
           in the initialization
        -  Model run time can be provided in initialization. If not it is set
           to the storm duration

    h_init : float, optional
        Some initial depth in the channels. Default = 0.001 m
    g : float, optional
        Gravitational acceleration, :math:`m / s^2`
    alpha : float, optional
        Non-dimensional time step factor from Bates et al., (2010)
    rho : integer, optional
        Density of water, :math:`kg / m^3`
    ten_thirds : float, optional
        Precalculated value of :math:`10 / 3` which is used in the
        implicit shallow water equation.

    Examples
    --------
    >>> DEM_name = "DEM_name.asc"
    >>> (rg, z) = read_esri_ascii(DEM_name)  # doctest: +SKIP
    >>> of = OverlandFlowBates(rg)  # doctest: +SKIP

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    Bates, P., Horritt, M., Fewtrell, T. (2010). A simple inertial formulation
    of the shallow water equations for efficient two-dimensional flood
    inundation modelling Journal of Hydrology  387(1-2), 33-45.
    https://dx.doi.org/10.1016/j.jhydrol.2010.03.027

    """

    _name = "OverlandFlowBates"

    _unit_agnostic = False

    _info = {
        "surface_water__depth": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of water on the surface",
        },
        "surface_water__discharge": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m3/s",
            "mapping": "link",
            "doc": "Volumetric discharge of surface water",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
    }

    def __init__(
        self,
        grid,
        h_init=0.00001,
        alpha=0.7,
        mannings_n=0.03,
        g=scipy.constants.g,
        rainfall_intensity=0.0,
    ):
        super().__init__(grid)

        # First we copy our grid

        self._h_init = h_init
        self._alpha = alpha
        self._mannings_n = mannings_n
        self._g = g
        self._rainfall_intensity = rainfall_intensity

        # Now setting up fields at the links...
        # For water discharge

        self._surface_water__discharge = grid.add_zeros(
            "surface_water__discharge",
            at="link",
            units=self._info["surface_water__discharge"]["units"],
        )

        # Pre-calculated values included for speed.
        self._ten_thirds = 10.0 / 3.0
        self._mannings_n_squared = self._mannings_n * self._mannings_n

        # Start time of simulation is at 1.0 s
        self._elapsed_time = 1.0

        # Assigning a class variable to the water depth field and adding the
        # initial thin water depth
        self._h = self._grid["node"]["surface_water__depth"] = (
            self._grid["node"]["surface_water__depth"] + self._h_init
        )

        # Assigning a class variable to the water discharge field.
        self._q = self._grid["link"]["surface_water__discharge"]

        # Assiging a class variable to the elevation field.
        self._z = self._grid.at_node["topographic__elevation"]

    @property
    def surface_water__discharge(self):
        """The discharge of water on active links."""
        return self._surface_water__discharge

    @property
    def h(self):
        """The depth of water at each node."""
        return self._h

    @property
    def dt(self):
        """dt: component timestep."""
        return self._dt

    @dt.setter
    def dt(self, dt):
        assert dt > 0
        self._dt = dt

    def calc_time_step(self):
        # Adaptive time stepper from Bates et al., 2010 and de Almeida et al.,
        # 2012
        self._dt = (
            self._alpha
            * self._grid.dx
            / np.sqrt(self._g * np.amax(self._grid.at_node["surface_water__depth"]))
        )

        return self._dt

    def overland_flow(self, dt=None):
        """For one time step, this generates 'overland flow' across a given
        grid by calculating discharge at each node.

        Using the depth slope product, shear stress is calculated at every
        node.

        Outputs water depth, discharge and shear stress values through time at
        every point in the input grid.


        Parameters
        ----------
        grid : RasterModelGrid
            A grid.
        dt : float, optional
            Time step. Either set when called or the component will do it for
            you.
        """

        # If no dt is provided, one will be calculated using
        # self._gear_time_step()
        if dt is None:
            self.calc_time_step()

        # In case another component has added data to the fields, we just reset
        # our water depths, topographic elevations and water discharge
        # variables to the fields.
        # self._h = self._grid['node']['surface_water__depth']
        self._z = self._grid["node"]["topographic__elevation"]
        self._q = self._grid["link"]["surface_water__discharge"]

        # Here we identify the core nodes and active link ids for later use.
        self._core_nodes = self._grid.core_nodes
        self._active_links = self._grid.active_links

        # Per Bates et al., 2010, this solution needs to find the difference
        # between the highest water surface in the two cells and the highest
        # bed elevation
        zmax = self._grid.map_max_of_link_nodes_to_link(self._z)
        w = self._h + self._z
        wmax = self._grid.map_max_of_link_nodes_to_link(w)
        hflow = wmax[self._grid.active_links] - zmax[self._grid.active_links]

        # Now we calculate the slope of the water surface elevation at active
        # links
        water_surface_slope = self._grid.calc_grad_at_link(w)[self._grid.active_links]

        # Here we calculate discharge at all active links using Eq. 11 from
        # Bates et al., 2010
        self._q[self._active_links] = (
            self._q[self._active_links]
            - self._g * hflow * self._dt * water_surface_slope
        ) / (
            1.0
            + self._g
            * hflow
            * self._dt
            * self._mannings_n_squared
            * abs(self._q[self._active_links])
            / hflow**self._ten_thirds
        )

        # Update our water depths
        dhdt = self._rainfall_intensity - self._grid.calc_flux_div_at_node(self._q)

        self._h[self._core_nodes] = (
            self._h[self._core_nodes] + dhdt[self._core_nodes] * self._dt
        )

        # And reset our field values with the newest water depth and discharge.
        self._grid.at_node["surface_water__depth"] = self._h
        self._grid.at_link["surface_water__discharge"] = self._q



================================================
File: overland_flow/generate_overland_flow_deAlmeida.py
================================================
"""Landlab component that simulates overland flow.

This component simulates overland flow using the 2-D numerical model of
shallow-water flow over topography using the de Almeida et al., 2012
algorithm for storage-cell inundation modeling.

.. codeauthor:: Jordan Adams

Examples
--------
>>> import numpy as np
>>> from landlab import RasterModelGrid
>>> from landlab.components.overland_flow import OverlandFlow

Create a grid on which to calculate overland flow.

>>> grid = RasterModelGrid((4, 5))

The grid will need some data to provide the overland flow component. To
check the names of the fields that provide input to the overland flow
component use the *input_var_names* class property.

>>> OverlandFlow.input_var_names
('surface_water__depth', 'topographic__elevation')

Create fields of data for each of these input variables.

>>> grid.at_node["topographic__elevation"] = [
...     [0.0, 0.0, 0.0, 0.0, 0.0],
...     [1.0, 1.0, 1.0, 1.0, 1.0],
...     [2.0, 2.0, 2.0, 2.0, 2.0],
...     [3.0, 3.0, 3.0, 3.0, 3.0],
... ]
>>> grid.at_node["surface_water__depth"] = [
...     [0.0, 0.0, 0.0, 0.0, 0.0],
...     [0.0, 0.0, 0.0, 0.0, 0.0],
...     [0.0, 0.0, 0.0, 0.0, 0.0],
...     [0.1, 0.1, 0.1, 0.1, 0.1],
... ]

Instantiate the `OverlandFlow` component to work on this grid, and run it.

>>> of = OverlandFlow(grid, steep_slopes=True)
>>> of.run_one_step()

After calculating the overland flow, new fields have been added to the
grid. Use the *output_var_names* property to see the names of the fields that
have been changed.

>>> of.output_var_names
('surface_water__depth', 'surface_water__discharge', 'water_surface__gradient')

The `surface_water__depth` field is defined at nodes.

>>> of.var_loc("surface_water__depth")
'node'
>>> grid.at_node["surface_water__depth"]
array([1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,
       1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,
       1.0000e-05, 2.0010e-02, 2.0010e-02, 2.0010e-02, 1.0000e-05,
       1.0001e-01, 1.0001e-01, 1.0001e-01, 1.0001e-01, 1.0001e-01])

The `surface_water__discharge` field is defined at links. Because our initial
topography was a dipping plane, there is no water discharge in the horizontal
direction, only toward the bottom of the grid.

>>> of.var_loc("surface_water__discharge")
'link'
>>> q = grid.at_link["surface_water__discharge"]
>>> np.all(q[grid.horizontal_links] == 0.0)
True
>>> np.all(q[grid.vertical_links] <= 0.0)
True

The *water_surface__gradient* is also defined at links.

>>> of.var_loc("water_surface__gradient")
'link'
>>> grid.at_link["water_surface__gradient"]
array([0. , 0. , 0. , 0. ,
       0. , 1. , 1. , 1. , 0. ,
       0. , 0. , 0. , 0. ,
       0. , 1. , 1. , 1. , 0. ,
       0. , 0. , 0. , 0. ,
       0. , 1.1, 1.1, 1.1, 0. ,
       0. , 0. , 0. , 0. ])
"""

import numpy as np
import scipy.constants

from landlab import Component
from landlab import FieldError

from . import _links as links

_SEVEN_OVER_THREE = 7.0 / 3.0


def _active_links_at_node(grid, *args):
    """_active_links_at_node([node_ids]) Active links of a node.

    .. note::

        This function returns links that are in *clockwise* order,
        rather than the standard *counterclockwise* ordering that
        landlab uses everywhere else.

    Parameters
    ----------
    grid : RasterModelGrid
        A grid.
    node_ids : int or list of ints
        ID(s) of node(s) for which to find connected active links

    Returns
    -------
    (4, N) ndarray
        The ids of active links attached to grid nodes with
        *node_ids*. If *node_ids* is not given, return links for all of the
        nodes in the grid. Link ids are listed in clockwise order starting
        with the south link. Diagonal links are never returned.

    Examples
    --------
    >>> from landlab import RasterModelGrid

    >>> grid = RasterModelGrid((3, 4))
    >>> grid.links_at_node[5]
    array([ 8, 11,  7,  4])
    >>> _active_links_at_node(grid, (5, 6))
    array([[ 4,  5],
           [ 7,  8],
           [11, 12],
           [ 8,  9]])
    >>> _active_links_at_node(grid)
    array([[-1, -1, -1, -1, -1,  4,  5, -1, -1, 11, 12, -1],
           [-1, -1, -1, -1, -1,  7,  8,  9, -1, -1, -1, -1],
           [-1,  4,  5, -1, -1, 11, 12, -1, -1, -1, -1, -1],
           [-1, -1, -1, -1,  7,  8,  9, -1, -1, -1, -1, -1]])

    :meta landlab: deprecated, info-link, info-node
    """
    active_links_at_node = grid.links_at_node.copy()
    active_links_at_node[grid.active_link_dirs_at_node == 0] = -1
    active_links_at_node = active_links_at_node[:, (3, 2, 1, 0)]

    if len(args) == 0:
        return active_links_at_node.T
    elif len(args) == 1:
        node_ids = np.broadcast_arrays(args[0])[0]
        return active_links_at_node[node_ids, :].T
    else:
        raise ValueError("only zero or one arguments accepted")


class OverlandFlow(Component):
    """Simulate overland flow using de Almeida approximations.

    Landlab component that simulates overland flow using the de Almeida
    et al., 2012 approximations of the 1D shallow water equations to be used
    for 2D flood inundation modeling.

    This component calculates discharge, depth and shear stress after some
    precipitation event across any raster grid. Default input file is named
    "overland_flow_input.txt' and is contained in the
    landlab.components.overland_flow folder.

    The primary method of this class is :func:`run_one_step`.

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Adams, J., Gasparini, N., Hobley, D., Tucker, G., Hutton, E., Nudurupati,
    S., Istanbulluoglu, E. (2017). The Landlab v1. 0 OverlandFlow component:
    a Python tool for computing shallow-water flow across watersheds.
    Geoscientific Model Development  10(4), 1645.
    https://dx.doi.org/10.5194/gmd-10-1645-2017

    **Additional References**

    de Almeida, G., Bates, P., Freer, J., Souvignet, M. (2012). Improving the
    stability of a simple formulation of the shallow water equations for 2-D
    flood modeling. Water Resources Research 48(5)
    https://dx.doi.org/10.1029/2011wr011570

    """

    _name = "OverlandFlow"

    _unit_agnostic = False

    _cite_as = """@article{adams2017landlab,
        title={The Landlab v1. 0 OverlandFlow component: a Python
            tool for computing shallow-water flow across watersheds},
        author={Adams, Jordan M and Gasparini, Nicole M and
            Hobley, Daniel EJ and Tucker, Gregory E and
            Hutton, Eric WH and Nudurupati, Sai S and
            Istanbulluoglu, Erkan},
        journal={Geoscientific Model Development},
        volume={10},
        number={4},
        pages={1645},
        year={2017},
        publisher={Copernicus GmbH}
        }
    """

    _info = {
        "surface_water__depth": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of water on the surface",
        },
        "surface_water__discharge": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m3/s",
            "mapping": "link",
            "doc": "Volumetric discharge of surface water",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "water_surface__gradient": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "link",
            "doc": "Downstream gradient of the water surface.",
        },
    }

    def __init__(
        self,
        grid,
        default_fixed_links=False,
        h_init=0.00001,
        alpha=0.7,
        mannings_n=0.03,
        g=scipy.constants.g,
        theta=0.8,
        rainfall_intensity=0.0,
        steep_slopes=False,
    ):
        """Create an overland flow component.

        Parameters
        ----------
        grid : RasterModelGrid
            A landlab grid.
        h_init : float, optional
            Thicknes of initial thin layer of water to prevent divide by zero
            errors (m).
        alpha : float, optional
            Time step coeffcient, described in Bates et al., 2010 and
            de Almeida et al., 2012.
        mannings_n : float, optional
            Manning's roughness coefficient.
        g : float, optional
            Acceleration due to gravity (m/s^2).
        theta : float, optional
            Weighting factor from de Almeida et al., 2012.
        rainfall_intensity : float or array of float, optional
            Rainfall intensity. Default is zero.
        steep_slopes : bool, optional
            Modify the algorithm to handle steeper slopes at the expense of
            speed. If model runs become unstable, consider setting to True.
        """
        super().__init__(grid)

        # First we copy our grid

        self._h_init = h_init
        self._alpha = alpha

        if isinstance(mannings_n, str):
            self._mannings_n = self._grid.at_link[mannings_n]
        else:
            self._mannings_n = mannings_n

        self._g = g
        self._theta = theta
        self.rainfall_intensity = rainfall_intensity
        self._steep_slopes = steep_slopes

        # Now setting up fields at the links...
        # For water discharge
        try:
            self._q = grid.add_zeros(
                "surface_water__discharge",
                at="link",
                units=self._info["surface_water__discharge"]["units"],
            )

        except FieldError:
            # Field was already set; still, fill it with zeros
            self._q = grid.at_link["surface_water__discharge"]
            self._q.fill(0.0)

        # For water depths calculated at links
        try:
            self._h_links = grid.add_zeros(
                "surface_water__depth",
                at="link",
                units=self._info["surface_water__depth"]["units"],
            )
        except FieldError:
            self._h_links = grid.at_link["surface_water__depth"]
            self._h_links.fill(0.0)
        self._h_links += self._h_init

        self._h = grid.at_node["surface_water__depth"]
        self._h += self._h_init

        # For water surface slopes at links
        try:
            self._water_surface_slope = grid.add_zeros(
                "water_surface__gradient", at="link"
            )
        except FieldError:
            self._water_surface_slope = grid.at_link["water_surface__gradient"]
            self._water_surface_slope.fill(0.0)

        # Start time of simulation is at 1.0 s
        self._elapsed_time = 1.0

        self._dt = None
        self._dhdt = grid.zeros()

        # When we instantiate the class we recognize that neighbors have not
        # been found. After the user either calls self.set_up_neighbor_array
        # or self.overland_flow this will be set to True. This is done so
        # that every iteration of self.overland_flow does NOT need to
        # reinitalize the neighbors and saves computation time.
        self._neighbor_flag = False

        # When looking for neighbors, we automatically ignore inactive links
        # by default. However, what about when we want to look at fixed links
        # too? By default, we ignore these, but if they are important to your
        # model and will be updated in your driver loop, they can be used by
        # setting the flag in the initialization of  the class to 'True'
        self._default_fixed_links = default_fixed_links

        # Assiging a class variable to the elevation field.
        self._z = self._grid.at_node["topographic__elevation"]

    @property
    def h(self):
        """The depth of water at each node."""
        return self._h

    @property
    def dt(self):
        """dt: Component timestep."""
        return self._dt

    @dt.setter
    def dt(self, dt):
        if dt <= 0:
            raise ValueError("timestep dt must be positive")

        self._dt = dt

    @property
    def rainfall_intensity(self):
        """rainfall_intensity: the rainfall rate [m/s]

        Must be positive.
        """
        return self._rainfall_intensity

    @rainfall_intensity.setter
    def rainfall_intensity(self, new_val):
        if np.any(new_val < 0.0):
            raise ValueError("Rainfall intensity must be positive")

        self._rainfall_intensity = new_val

    def calc_time_step(self):
        """Calculate time step.

        Adaptive time stepper from Bates et al., 2010 and de Almeida et
        al., 2012
        """
        self._dt = (
            self._alpha
            * self._grid.dx
            / np.sqrt(self._g * np.amax(self._grid.at_node["surface_water__depth"]))
        )

        return self._dt

    def set_up_neighbor_arrays(self):
        """Create and initialize link neighbor arrays.

        Set up arrays of neighboring horizontal and vertical links that
        are needed for the de Almeida solution.
        """
        # First we identify all active links

        self._active_ids = links.active_link_ids(
            self._grid.shape, self._grid.status_at_node
        )

        self._active_links_at_open_bdy = _active_links_at_node(
            self.grid, self.grid.open_boundary_nodes
        ).transpose()

        self._active_links_at_open_bdy = self._active_links_at_open_bdy[
            np.where(self._active_links_at_open_bdy > -1)
        ]

        # And then find all horizontal link IDs (Active and Inactive)
        self._horizontal_ids = links.horizontal_link_ids(self._grid.shape)

        # And make the array 1-D
        self._horizontal_ids = self._horizontal_ids.flatten()

        # Find all horizontal active link ids
        self._horizontal_active_link_ids = links.horizontal_active_link_ids(
            self._grid.shape, self._active_ids
        )

        # Now we repeat this process for the vertical links.
        # First find the vertical link ids and reshape it into a 1-D array
        self._vertical_ids = links.vertical_link_ids(self._grid.shape).flatten()

        # Find the *active* verical link ids
        self._vertical_active_link_ids = links.vertical_active_link_ids(
            self._grid.shape, self._active_ids
        )

        if self._default_fixed_links is True:
            fixed_link_ids = links.fixed_link_ids(
                self._grid.shape, self._grid.status_at_node
            )
            fixed_horizontal_links = links.horizontal_fixed_link_ids(
                self._grid.shape, fixed_link_ids
            )
            fixed_vertical_links = links.vertical_fixed_link_ids(
                self._grid.shape, fixed_link_ids
            )
            self._horizontal_active_link_ids = np.maximum(
                self._horizontal_active_link_ids, fixed_horizontal_links
            )
            self._vertical_active_link_ids = np.maximum(
                self._vertical_active_link_ids, fixed_vertical_links
            )
            self._active_neighbors = find_active_neighbors_for_fixed_links(self._grid)

        self._vert_bdy_ids = self._active_links_at_open_bdy[
            links.is_vertical_link(self._grid.shape, self._active_links_at_open_bdy)
        ]

        self._vert_bdy_ids = links.nth_vertical_link(
            self._grid.shape, self._vert_bdy_ids
        )

        self._horiz_bdy_ids = self._active_links_at_open_bdy[
            links.is_horizontal_link(self._grid.shape, self._active_links_at_open_bdy)
        ]

        self._horiz_bdy_ids = links.nth_horizontal_link(
            self._grid.shape, self._horiz_bdy_ids
        )

        # Using the active vertical link ids we can find the north
        # and south vertical neighbors
        self._north_neighbors = links.vertical_north_link_neighbor(
            self._grid.shape, self._vertical_active_link_ids
        )
        self._south_neighbors = links.vertical_south_link_neighbor(
            self._grid.shape, self._vertical_active_link_ids
        )

        # Using the horizontal active link ids, we can find the west and
        # east neighbors
        self._west_neighbors = links.horizontal_west_link_neighbor(
            self._grid.shape, self._horizontal_active_link_ids
        )
        self._east_neighbors = links.horizontal_east_link_neighbor(
            self._grid.shape, self._horizontal_active_link_ids
        )

        # replace bdy condition links
        (ids,) = np.where(self._west_neighbors[self._horiz_bdy_ids] == -1)
        ids = self._horiz_bdy_ids[ids]
        self._west_neighbors[ids] = self._horizontal_active_link_ids[ids]

        (ids,) = np.where(self._east_neighbors[self._horiz_bdy_ids] == -1)
        ids = self._horiz_bdy_ids[ids]
        self._east_neighbors[ids] = self._horizontal_active_link_ids[ids]

        (ids,) = np.where(self._north_neighbors[self._vert_bdy_ids] == -1)
        ids = self._vert_bdy_ids[ids]
        self._north_neighbors[ids] = self._vertical_active_link_ids[ids]

        (ids,) = np.where(self._south_neighbors[self._vert_bdy_ids] == -1)
        ids = self._vert_bdy_ids[ids]
        self._south_neighbors[ids] = self._vertical_active_link_ids[ids]

        # Set up arrays for discharge in the horizontal & vertical directions.
        self._q_horizontal = np.zeros(
            links.number_of_horizontal_links(self._grid.shape)
        )
        self._q_vertical = np.zeros(links.number_of_vertical_links(self._grid.shape))

        # Once the neighbor arrays are set up, we change the flag to True!
        self._neighbor_flag = True

    def overland_flow(self, dt=None):
        """Generate overland flow across a grid.

        For one time step, this generates 'overland flow' across a given grid
        by calculating discharge at each node.

        Using the depth slope product, shear stress is calculated at every
        node.

        Outputs water depth, discharge and shear stress values through time at
        every point in the input grid.
        """
        # DH adds a loop to enable an imposed tstep while maintaining stability
        local_elapsed_time = 0.0
        if dt is None:
            dt = np.inf  # to allow the loop to begin
        while local_elapsed_time < dt:
            dt_local = self.calc_time_step()
            # Can really get into trouble if nothing happens but we still run:
            if not dt_local < np.inf:
                break
            if local_elapsed_time + dt_local > dt:
                dt_local = dt - local_elapsed_time
            self._dt = dt_local

            # First, we check and see if the neighbor arrays have been
            # initialized
            if self._neighbor_flag is False:
                self.set_up_neighbor_arrays()

            # In case another component has added data to the fields, we just
            # reset our water depths, topographic elevations and water
            # discharge variables to the fields.
            self._h = self._grid["node"]["surface_water__depth"]
            self._z = self._grid["node"]["topographic__elevation"]
            self._q = self._grid["link"]["surface_water__discharge"]
            self._h_links = self._grid["link"]["surface_water__depth"]

            # Here we identify the core nodes and active links for later use.
            self._core_nodes = self._grid.core_nodes
            self._active_links = self._grid.active_links

            # Per Bates et al., 2010, this solution needs to find difference
            # between the highest water surface in the two cells and the
            # highest bed elevation
            zmax = self._grid.map_max_of_link_nodes_to_link(self._z)
            w = self._h + self._z
            wmax = self._grid.map_max_of_link_nodes_to_link(w)
            hflow = wmax[self._grid.active_links] - zmax[self._grid.active_links]

            # Insert this water depth into an array of water depths at the
            # links.
            self._h_links[self._active_links] = hflow

            # Now we calculate the slope of the water surface elevation at
            # active links
            self._water_surface__gradient = self._grid.calc_grad_at_link(w)[
                self._grid.active_links
            ]

            # And insert these values into an array of all links
            self._water_surface_slope[self._active_links] = (
                self._water_surface__gradient
            )
            # If the user chooses to set boundary links to the neighbor value,
            # we set the discharge array to have the boundary links set to
            # their neighbor value
            if self._default_fixed_links is True:
                self._q[self._grid.fixed_links] = self._q[self._active_neighbors]

            # Now we can calculate discharge. To handle links with neighbors
            # that do not exist, we will do a fancy indexing trick. Non-
            # existent links or inactive links have an index of '-1', which in
            # Python, looks to the end of a list or array. To accommodate these
            # '-1' indices, we will simply insert an value of 0.0 discharge (in
            # units of L^2/T) to the end of the discharge array.
            self._q = np.append(self._q, [0])

            horiz = self._horizontal_ids
            vert = self._vertical_ids
            # Now we calculate discharge in the horizontal direction
            try:
                self._q[horiz] = (
                    self._theta * self._q[horiz]
                    + (1.0 - self._theta)
                    / 2.0
                    * (self._q[self._west_neighbors] + self._q[self._east_neighbors])
                    - self._g
                    * self._h_links[horiz]
                    * self._dt
                    * self._water_surface_slope[horiz]
                ) / (
                    1
                    + self._g
                    * self._dt
                    * self._mannings_n**2.0
                    * abs(self._q[horiz])
                    / self._h_links[horiz] ** _SEVEN_OVER_THREE
                )

                # ... and in the vertical direction
                self._q[vert] = (
                    self._theta * self._q[vert]
                    + (1 - self._theta)
                    / 2.0
                    * (self._q[self._north_neighbors] + self._q[self._south_neighbors])
                    - self._g
                    * self._h_links[vert]
                    * self._dt
                    * self._water_surface_slope[vert]
                ) / (
                    1
                    + self._g
                    * self._dt
                    * self._mannings_n**2.0
                    * abs(self._q[vert])
                    / self._h_links[vert] ** _SEVEN_OVER_THREE
                )

            except ValueError:
                self._mannings_n = self._grid["link"]["mannings_n"]
                # if manning's n in a field
                # calc discharge in horizontal
                self._q[horiz] = (
                    self._theta * self._q[horiz]
                    + (1.0 - self._theta)
                    / 2.0
                    * (self._q[self._west_neighbors] + self._q[self._east_neighbors])
                    - self._g
                    * self._h_links[horiz]
                    * self._dt
                    * self._water_surface_slope[horiz]
                ) / (
                    1
                    + self._g
                    * self._dt
                    * self._mannings_n[horiz] ** 2.0
                    * abs(self._q[horiz])
                    / self._h_links[horiz] ** _SEVEN_OVER_THREE
                )

                # ... and in the vertical direction
                self._q[vert] = (
                    self._theta * self._q[vert]
                    + (1 - self._theta)
                    / 2.0
                    * (self._q[self._north_neighbors] + self._q[self._south_neighbors])
                    - self._g
                    * self._h_links[vert]
                    * self._dt
                    * self._water_surface_slope[self._vertical_ids]
                ) / (
                    1
                    + self._g
                    * self._dt
                    * self._mannings_n[vert] ** 2.0
                    * abs(self._q[vert])
                    / self._h_links[vert] ** _SEVEN_OVER_THREE
                )

            # Now to return the array to its original length (length of number
            # of all links), we delete the extra 0.0 value from the end of the
            # array.
            self._q = np.delete(self._q, len(self._q) - 1)

            # Updating the discharge array to have the boundary links set to
            # their neighbor
            if self._default_fixed_links is True:
                self._q[self._grid.fixed_links] = self._q[self._active_neighbors]

            if self._steep_slopes is True:
                # To prevent water from draining too fast for our time steps...
                # Our Froude number.
                Fr = 1.0
                # Our two limiting factors, the froude number and courant
                # number.
                # Looking a calculated q to be compared to our Fr number.
                calculated_q = (self._q / self._h_links) / np.sqrt(
                    self._g * self._h_links
                )

                # Looking at our calculated q and comparing it to Courant no.,
                q_courant = self._q * self._dt / self._grid.dx

                # Water depth split equally between four links..
                water_div_4 = self._h_links / 4.0

                # IDs where water discharge is positive...
                (positive_q,) = np.where(self._q > 0)

                # ... and negative.
                (negative_q,) = np.where(self._q < 0)

                # Where does our calculated q exceed the Froude number? If q
                # does exceed the Froude number, we are getting supercritical
                # flow and discharge needs to be reduced to maintain stability.
                (Froude_logical,) = np.where((calculated_q) > Fr)
                (Froude_abs_logical,) = np.where(abs(calculated_q) > Fr)

                # Where does our calculated q exceed the Courant number and
                # water depth divided amongst 4 links? If the calculated q
                # exceeds the Courant number and is greater than the water
                # depth divided by 4 links, we reduce discharge to maintain
                # stability.
                (water_logical,) = np.where(q_courant > water_div_4)
                (water_abs_logical,) = np.where(abs(q_courant) > water_div_4)

                # Where are these conditions met? For positive and negative q,
                # there are specific rules to reduce q. This step finds where
                # the discharge values are positive or negative and where
                # discharge exceeds the Froude or Courant number.
                self._if_statement_1 = np.intersect1d(positive_q, Froude_logical)
                self._if_statement_2 = np.intersect1d(negative_q, Froude_abs_logical)
                self._if_statement_3 = np.intersect1d(positive_q, water_logical)
                self._if_statement_4 = np.intersect1d(negative_q, water_abs_logical)

                # Rules 1 and 2 reduce discharge by the Froude number.
                self._q[self._if_statement_1] = self._h_links[self._if_statement_1] * (
                    np.sqrt(self._g * self._h_links[self._if_statement_1]) * Fr
                )

                self._q[self._if_statement_2] = 0.0 - (
                    self._h_links[self._if_statement_2]
                    * np.sqrt(self._g * self._h_links[self._if_statement_2])
                    * Fr
                )

                # Rules 3 and 4 reduce discharge by the Courant number.
                self._q[self._if_statement_3] = (
                    (self._h_links[self._if_statement_3] * self._grid.dx) / 5.0
                ) / self._dt

                self._q[self._if_statement_4] = (
                    0.0
                    - (self._h_links[self._if_statement_4] * self._grid.dx / 5.0)
                    / self._dt
                )

            # Once stability has been restored, we calculate the change in
            # water depths on all core nodes by finding the difference between
            # inputs (rainfall) and the inputs/outputs (flux divergence of
            # discharge)
            self._dhdt = self._rainfall_intensity - self._grid.calc_flux_div_at_node(
                self._q
            )

            # Updating our water depths...
            self._h[self._core_nodes] = (
                self._h[self._core_nodes] + self._dhdt[self._core_nodes] * self._dt
            )

            # To prevent divide by zero errors, a minimum threshold water depth
            # must be maintained. To reduce mass imbalances, this is set to
            # find locations where water depth is smaller than h_init (default
            # is 0.001) and the new value is self._h_init * 10^-3. This was set
            # as it showed the smallest amount of mass creation in the grid
            # during testing.
            if self._steep_slopes is True:
                self._h[self._h < self._h_init] = self._h_init * 10.0**-3

            # And reset our field values with the newest water depth and
            # discharge.
            self._grid.at_node["surface_water__depth"] = self._h
            self._grid.at_link["surface_water__discharge"] = self._q
            #
            #
            #  self._helper_q = self._grid.map_upwind_node_link_max_to_node(self._q)
            #  self._helper_s = self._grid.map_upwind_node_link_max_to_node(
            #      self._water_surface_slope)
            #
            #  self._helper_q = self._grid.map_max_of_link_nodes_to_link(self._helper_q)
            #  self._helper_s = self._grid.map_max_of_link_nodes_to_link(self._helper_s)
            #
            #  self._grid['link']['surface_water__discharge'][
            #     self._active_links_at_open_bdy] = self._helper_q[
            #     self._active_links_at_open_bdy]
            #
            #  self._grid['link']['water_surface__gradient'][
            #     self._active_links_at_open_bdy] = self._helper_s[
            #     self._active_links_at_open_bdy]
            # Update nodes near boundary locations - nodes adjacent to
            # boundaries may have discharge and water surface slopes
            # artifically reduced due to boundary effects. This step removes
            # those errors.

            if dt is np.inf:
                break
            local_elapsed_time += self._dt

    def run_one_step(self, dt=None):
        """Generate overland flow across a grid.

        For one time step, this generates 'overland flow' across a given grid
        by calculating discharge at each node.

        Using the depth slope product, shear stress is calculated at every
        node.

        Outputs water depth, discharge and shear stress values through time at
        every point in the input grid.
        """
        self.overland_flow(dt=dt)

    def discharge_mapper(self, input_discharge, convert_to_volume=False):
        """Maps discharge value from links onto nodes.

        This method takes the discharge values on links and determines the
        links that are flowing INTO a given node. The fluxes moving INTO a
        given node are summed.

        This method ignores all flow moving OUT of a given node.

        This takes values from the OverlandFlow component (by default) in
        units of [L^2/T]. If the convert_to_cms flag is raised as True, this
        method converts discharge to units [L^3/T] - as of Aug 2016, only
        operates for square RasterModelGrid instances.

        The output array is of length grid.number_of_nodes and can be used
        with the Landlab imshow_grid plotter.

        Returns a numpy array (discharge_vals)
        """

        discharge_vals = np.zeros(self._grid.number_of_links)
        discharge_vals[:] = input_discharge[:]

        if convert_to_volume:
            discharge_vals *= self._grid.dx

        discharge_vals = (
            discharge_vals[self._grid.links_at_node] * self._grid.link_dirs_at_node
        )

        discharge_vals = discharge_vals.flatten()

        discharge_vals[np.where(discharge_vals < 0)] = 0.0

        discharge_vals = discharge_vals.reshape(self._grid.number_of_nodes, 4)

        discharge_vals = discharge_vals.sum(axis=1)

        return discharge_vals


def find_active_neighbors_for_fixed_links(grid):
    """Find active link neighbors for every fixed link.

    Specialized link ID function used to ID the active links that neighbor
    fixed links in the vertical and horizontal directions.

    If the user wants to assign fixed gradients or values to the fixed
    links dynamically, this function identifies the nearest active_link
    neighbor.

    Each fixed link can either have 0 or 1 active neighbor. This function
    finds if and where that active neighbor is and stores those IDs in
    an array.

    Parameters
    ----------
    grid : RasterModelGrid
        A landlab grid.

    Returns
    -------
    ndarray of int, shape `(*, )`
        Flat array of links.


    Examples
    --------
    >>> from landlab import NodeStatus, RasterModelGrid

    >>> grid = RasterModelGrid((4, 5))
    >>> grid.status_at_node[:5] = NodeStatus.FIXED_GRADIENT
    >>> grid.status_at_node[::5] = NodeStatus.FIXED_GRADIENT
    >>> grid.status_at_node.reshape(grid.shape)
    array([[2, 2, 2, 2, 2],
           [2, 0, 0, 0, 1],
           [2, 0, 0, 0, 1],
           [2, 1, 1, 1, 1]], dtype=uint8)

    >>> grid.fixed_links
    array([ 5,  6,  7,  9, 18])
    >>> grid.active_links
    array([10, 11, 12, 14, 15, 16, 19, 20, 21, 23, 24, 25])

    >>> find_active_neighbors_for_fixed_links(grid)
    array([14, 15, 16, 10, 19])

    >>> rmg = RasterModelGrid((4, 7))

    >>> rmg.at_node["topographic__elevation"] = rmg.zeros(at="node")
    >>> rmg.at_link["topographic__slope"] = rmg.zeros(at="link")
    >>> rmg.status_at_node[rmg.perimeter_nodes] = rmg.BC_NODE_IS_FIXED_GRADIENT
    >>> find_active_neighbors_for_fixed_links(rmg)
    array([20, 21, 22, 23, 24, 14, 17, 27, 30, 20, 21, 22, 23, 24])
    """
    neighbors = links.neighbors_at_link(grid.shape, grid.fixed_links).flat
    return neighbors[np.isin(neighbors, grid.active_links)]



================================================
File: overland_flow/generate_overland_flow_implicit_kinwave.py
================================================
"""Landlab component for overland flow using a local implicit solution to the
kinematic-wave approximation.

Created on Fri May 27 14:26:13 2016

@author: gtucker
"""

import numpy as np
from scipy.optimize import newton

from landlab import Component
from landlab.components import FlowAccumulator


def water_fn(x, a, b, c, d, e):
    r"""Evaluates the solution to the water-depth equation.

    Called by scipy.newton() to find solution for :math:`x`
    using Newton's method.

    Parameters
    ----------
    x : float
        Water depth at new time step.
    a : float
        "alpha" parameter (see below)
    b : float
        Weighting factor on new versus old time step. :math:`b=1` means purely
        implicit solution with all weight on :math:`H` at new time
        step. :math:`b=0` (not recommended) would mean purely explicit.
    c : float
        Water depth at old time step (time step :math:`t` instead
        of :math:`t+1`)
    d : float
        Depth-discharge exponent; normally either 5/3 (Manning) or 3/2 (Chezy)
    e : float
        Water inflow volume per unit cell area in one time step.


    This equation represents the implicit solution for water depth
    :math:`H` at the next time step. In the code below, it is
    formulated in a generic way.  Written using more familiar
    terminology, the equation is:

    .. math::

        H - H_0 + \alpha ( w H + (w-1) H_0)^d - \Delta t (R + Q_{in} / A)

    .. math::

        \alpha = \frac{\Delta t \sum S^{1/2}}{C_f A}

    where :math:`H` is water depth at the given node at the new
    time step, :math:`H_0` is water depth at the prior time step,
    :math:`w` is a weighting factor, :math:`d` is the depth-discharge
    exponent (2/3 or 1/2), :math:`\Delta t` is time-step duration,
    :math:`R` is local runoff rate, :math:`Q_{in}` is inflow
    discharge, :math:`A` is cell area, :math:`C_f` is a
    dimensional roughness coefficient, and :math:`\sum S^{1/2}`
    represents the sum of square-root-of-downhill-gradient over
    all outgoing (downhill) links.
    """
    return x - c + a * (b * x + (b - 1.0) * c) ** d - e


class KinwaveImplicitOverlandFlow(Component):
    r"""Calculate shallow water flow over topography.

    Landlab component that implements a two-dimensional kinematic wave model.
    This is a form of the 2D shallow-water equations in which energy slope is
    assumed to equal bed slope. The solution method is locally implicit, and
    works as follows. At each time step, we iterate from upstream to downstream
    over the topography. Because we are working downstream, we can assume that
    we know the total water inflow to a given cell. We solve the following mass
    conservation equation at each cell:

    .. math::

        (H^{t+1} - H^t)/\Delta t = Q_{in}/A - Q_{out}/A + R

    where :math:`H` is water depth, :math:`t` indicates time step
    number, :math:`\Delta t` is time step duration, :math:`Q_{in}` is
    total inflow discharge, :math:`Q_{out}` is total outflow
    discharge, :math:`A` is cell area, and :math:`R` is local
    runoff rate (precipitation minus infiltration; could be
    negative if runon infiltration is occurring).

    The specific outflow discharge leaving a cell along one of its faces is:

    .. math::

        q = (1/C_r) H^\alpha S^{1/2}

    where :math:`C_r` is a roughness coefficient (such as
    Manning's n), :math:`\alpha` is an exponent equal to :math:`5/3`
    for the Manning equation and :math:`3/2` for the Chezy family,
    and :math:`S` is the downhill-positive gradient of the link
    that crosses this particular face. Outflow discharge is zero
    for links that are flat or "uphill" from the given node.
    Total discharge out of a cell is then the sum of (specific
    discharge x face width) over all outflow faces

    .. math::

        Q_{out} = \sum_{i=1}^N (1/C_r) H^\alpha S_i^{1/2} W_i

    where :math:`N` is the number of outflow faces (i.e., faces
    where the ground slopes downhill away from the cell's node),
    and :math:`W_i` is the width of face :math:`i`.

    We use the depth at the cell's node, so this simplifies to:

    .. math::

        Q_{out} = (1/C_r) H'^\alpha \sum_{i=1}^N S_i^{1/2} W_i

    We define :math:`H` in the above as a weighted sum of
    the "old" (time step :math:`t`) and "new" (time step :math:`t+1`)
    depth values:

    .. math::

        H' = w H^{t+1} + (1-w) H^t

    If :math:`w=1`, the method is fully implicit. If :math:`w=0`,
    it is a simple forward explicit method.

    When we combine these equations, we have an equation that includes the
    unknown :math:`H^{t+1}` and a bunch of terms that are known.
    If :math:`w\ne 0`, it is a nonlinear equation in :math:`H^{t+1}`,
    and must be solved iteratively. We do this using a root-finding
    method in the scipy.optimize library.

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> rg = RasterModelGrid((4, 5), xy_spacing=10.0)
    >>> z = rg.add_zeros("topographic__elevation", at="node")
    >>> kw = KinwaveImplicitOverlandFlow(rg)
    >>> kw.runoff_rate  # default value
    1.0
    >>> kw.roughness  # default value
    0.01
    >>> rg.at_node["surface_water__depth"][6:9]
    array([0., 0., 0.])

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    None Listed

    """

    _name = "KinwaveImplicitOverlandFlow"

    _unit_agnostic = False

    _info = {
        "surface_water__depth": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of water on the surface",
        },
        "surface_water_inflow__discharge": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m3/s",
            "mapping": "node",
            "doc": "water volume inflow rate to the cell around each node",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__gradient": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/m",
            "mapping": "link",
            "doc": "Gradient of the ground surface",
        },
    }

    def __init__(
        self,
        grid,
        runoff_rate=1.0,
        roughness=0.01,
        changing_topo=False,
        depth_exp=1.5,
        weight=1.0,
    ):
        """Initialize the KinwaveImplicitOverlandFlow.

        Parameters
        ----------
        grid : ModelGrid
            Landlab ModelGrid object
        runoff_rate : array_like of float
            Precipitation rate, mm/hr.
        roughness : array_like of float
            Manning's roughness coefficient(s); units depend on depth_exp.
        changing_topo : boolean, optional
            Flag indicating whether topography changes between time steps
        depth_exp : float
            Exponent on water depth in velocity equation (3/2 for Darcy/Chezy,
            5/3 for Manning)
        weight : float
            Weighting on depth at new time step versus old time step (1 = all
            implicit; 0 = explicit)
        """
        super().__init__(grid)

        # Store parameters
        self.runoff_rate = runoff_rate
        self.roughness = roughness
        self._changing_topo = changing_topo
        self._depth_exp = depth_exp
        self._weight = weight

        # Get elevation field
        self._elev = grid.at_node["topographic__elevation"]

        # Create fields...
        self.initialize_output_fields()

        self._depth = grid.at_node["surface_water__depth"]
        self._slope = grid.at_link["topographic__gradient"]
        self._disch_in = grid.at_node["surface_water_inflow__discharge"]

        # This array holds, for each node, the sum of sqrt(slope) x face width
        # for each link/face.
        self._grad_width_sum = grid.zeros(at="node")

        # This array holds the prefactor in the algebraic equation that we
        # will find a solution for.
        self._alpha = grid.zeros(at="node")

        # Instantiate flow router
        self._flow_accum = FlowAccumulator(
            grid,
            surface="topographic__elevation",
            flow_director="MFD",
            partition_method="square_root_of_slope",
        )

        # Flag to let us know whether this is our first iteration
        self._first_iteration = True

    @property
    def runoff_rate(self):
        """Runoff rate at nodes."""
        # Return a read-only view of the runoff_rate array
        read_only_runoff = self._runoff_rate.view()
        read_only_runoff.flags["WRITEABLE"] = False
        return read_only_runoff

    @runoff_rate.setter
    def runoff_rate(self, new_rate):
        new_rate = np.array(new_rate)
        if new_rate.size == 1:
            if new_rate < 0.0:
                raise ValueError("runoff_rate must be positive")
        else:
            if np.any(new_rate[self._grid.core_nodes] < 0.0):
                raise ValueError("runoff_rate must be positive")
        self._runoff_rate = new_rate

    @property
    def roughness(self):
        """Roughness at nodes."""
        # Return a read-only view of the roughness array
        read_only_roughness = self._roughness.view()
        read_only_roughness.flags["WRITEABLE"] = False
        return read_only_roughness

    @roughness.setter
    def roughness(self, new_rough):
        new_rough = np.array(new_rough)
        if new_rough.size == 1:
            if new_rough < 0.0:
                raise ValueError("roughness must be positive")
        else:
            if np.any(new_rough[self._grid.core_nodes] < 0.0):
                raise ValueError("roughness must be positive")
        self._roughness = new_rough

    @property
    def depth(self):
        """The depth of water at each node."""
        return self._depth

    def run_one_step(self, dt):
        """Calculate water flow for a time period `dt`."""

        # If it's our first iteration, or if the topography may be changing,
        # do flow routing and calculate square root of slopes at links
        if self._changing_topo or self._first_iteration:
            # Calculate the ground-surface slope
            self._slope[self._grid.active_links] = self._grid.calc_grad_at_link(
                self._elev
            )[self._grid.active_links]

            # Take square root of slope magnitude for use in velocity eqn
            self._sqrt_slope = np.sqrt(np.abs(self._slope))

            # Re-route flow, which gives us the downstream-to-upstream
            # ordering
            self._flow_accum.accumulate_flow()
            self._nodes_ordered = self._grid.at_node["flow__upstream_node_order"]
            self._flow_lnks = self._grid.at_node["flow__link_to_receiver_node"]

            # (Re)calculate, for each node, sum of sqrt(gradient) x width
            self._grad_width_sum[:] = 0.0
            for i in range(self._flow_lnks.shape[1]):
                self._grad_width_sum[:] += (
                    self._sqrt_slope[self._flow_lnks[:, i]]
                    * self._grid.length_of_face[
                        self._grid.face_at_link[self._flow_lnks[:, i]]
                    ]
                )

            # Calculate values of alpha, which is defined as
            #
            #   $\alpha = \frac{\Sigma W S^{1/2} \Delta t}{A C_r}$
            cores = self._grid.core_nodes

            # Calculate alpha; try for roughness as a float, else as array of floats
            if np.ndim(self._roughness) == 0:
                roughness_at_core_nodes = self._roughness
            else:
                roughness_at_core_nodes = self._roughness[cores]

            self._alpha[cores] = (
                self._grad_width_sum[cores]
                * dt
                / (self._grid.area_of_cell[self._grid.cell_at_node[cores]])
                / roughness_at_core_nodes
            )

        # Zero out inflow discharge
        self._disch_in[:] = 0.0

        # Upstream-to-downstream loop
        for i in range(len(self._nodes_ordered) - 1, -1, -1):
            n = self._nodes_ordered[i]
            if self._grid.status_at_node[n] == 0:
                # Solve for new water depth
                aa = self._alpha[n]
                cc = self._depth[n]

                # Calculate parameter ee; try for runoff_rate as a float, else as
                # array of floats
                if np.ndim(self._runoff_rate) == 0:
                    runoff_at_nodes = self._runoff_rate / 3.6e6
                else:
                    runoff_at_nodes = self._runoff_rate[n] / 3.6e6

                ee = (dt * runoff_at_nodes) + (
                    dt
                    * self._disch_in[n]
                    / self._grid.area_of_cell[self._grid.cell_at_node[n]]
                )

                self._depth[n] = newton(
                    water_fn,
                    self._depth[n],
                    args=(aa, self._weight, cc, self._depth_exp, ee),
                )

                # Calc outflow
                Heff = self._weight * self._depth[n] + (1.0 - self._weight) * cc

                # Calculate outflow; try for roughness as a float, else as array of floats
                if np.ndim(self._roughness) == 0:
                    roughness_at_nodes = self._roughness
                else:
                    roughness_at_nodes = self._roughness[n]

                outflow = (
                    (Heff**self._depth_exp)
                    * self._grad_width_sum[n]
                    / roughness_at_nodes
                )  # this is manning/chezy/darcy

                # Send flow downstream. Here we take total inflow discharge
                # and partition it among the node's neighbors. For this, we use
                # the flow director's "proportions" array, which contains, for
                # each node, the proportion of flow that heads out toward each
                # of its N neighbors. The proportion is zero if the neighbor is
                # uphill; otherwise, it is S^1/2 / sum(S^1/2). If for example
                # we have a raster grid, there will be four neighbors and four
                # proportions, some of which may be zero and some between 0 and
                # 1.
                self._disch_in[self._grid.adjacent_nodes_at_node[n]] += (
                    outflow * self._flow_accum.flow_director._proportions[n]
                )

                # TODO: the above is enough to implement the solution for flow
                # depth, but it does not provide any information about flow
                # velocity or discharge on links. This could be added as an
                # optional method, perhaps done just before output.



================================================
File: overland_flow/generate_overland_flow_kinwave.py
================================================
"""Landlab component for overland flow using the kinematic-wave approximation.

Created on Fri May 27 14:26:13 2016

@author: gtucker
"""

import numpy as np

from landlab import Component


class KinwaveOverlandFlowModel(Component):
    """Calculate water flow over topography.

    Landlab component that implements a two-dimensional
    kinematic wave model. This is an extremely simple, unsophisticated
    model, originally built simply to demonstrate the component creation
    process. Limitations to the present version include: infiltration is
    handled very crudely, the called is responsible for picking a stable
    time step size (no adaptive time stepping is used in the `run_one_step`
    method), precipitation rate is constant for a given duration (then zero),
    and all parameters are uniform in space. Also, the terrain is assumed
    to be stable over time. Caveat emptor!

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> rg = RasterModelGrid((4, 5), xy_spacing=10.0)
    >>> z = rg.add_zeros("topographic__elevation", at="node")
    >>> s = rg.add_zeros("topographic__gradient", at="link")
    >>> kw = KinwaveOverlandFlowModel(rg)
    >>> kw.vel_coef
    100.0
    >>> rg.at_node["surface_water__depth"]
    array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0.])

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    None Listed

    """

    _name = "KinwaveOverlandFlowModel"

    _unit_agnostic = False

    _info = {
        "surface_water__depth": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of water on the surface",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__gradient": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m/m",
            "mapping": "link",
            "doc": "Gradient of the ground surface",
        },
        "water__specific_discharge": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m2/s",
            "mapping": "link",
            "doc": "flow discharge component in the direction of the link",
        },
        "water__velocity": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/s",
            "mapping": "link",
            "doc": "flow velocity component in the direction of the link",
        },
    }

    def __init__(
        self,
        grid,
        precip_rate=1.0,
        precip_duration=1.0,
        infilt_rate=0.0,
        roughness=0.01,
    ):
        """Initialize the KinwaveOverlandFlowModel.

        Parameters
        ----------
        grid : ModelGrid
            Landlab ModelGrid object
        precip_rate : float, optional (defaults to 1 mm/hr)
            Precipitation rate, mm/hr
        precip_duration : float, optional (defaults to 1 hour)
            Duration of precipitation, hours
        infilt_rate : float, optional (defaults to 0)
            Maximum rate of infiltration, mm/hr
        roughness : float, defaults to 0.01
            Manning roughness coefficient, s/m^1/3
        """
        super().__init__(grid)

        # Store parameters and do unit conversion
        self._current_time = 0

        self._precip = precip_rate / 3600000.0  # convert to m/s
        self._precip_duration = precip_duration * 3600.0  # h->s
        self._infilt = infilt_rate / 3600000.0  # convert to m/s
        self._vel_coef = 1.0 / roughness  # do division now to save time

        # Create fields...
        #   Elevation
        self._elev = grid.at_node["topographic__elevation"]

        #   Slope
        self._slope = grid.at_link["topographic__gradient"]

        self.initialize_output_fields()
        self._depth = grid.at_node["surface_water__depth"]
        self._vel = grid.at_link["water__velocity"]
        self._disch = grid.at_link["water__specific_discharge"]

        # Calculate the ground-surface slope (assume it won't change)
        self._slope[self._grid.active_links] = self._grid.calc_grad_at_link(self._elev)[
            self._grid.active_links
        ]
        self._sqrt_slope = np.sqrt(self._slope)
        self._sign_slope = np.sign(self._slope)

    @property
    def vel_coef(self):
        """Velocity coefficient.

        (1/roughness)
        """
        return self._vel_coef

    def run_one_step(self, dt):
        """Calculate water flow for a time period `dt`.

        Default units for dt are *seconds*.
        """
        # Calculate water depth at links. This implements an "upwind" scheme
        # in which water depth at the links is the depth at the higher of the
        # two nodes.
        H_link = self._grid.map_value_at_max_node_to_link(
            "topographic__elevation", "surface_water__depth"
        )

        # Calculate velocity using the Manning equation.
        self._vel = (
            -self._sign_slope * self._vel_coef * H_link**0.66667 * self._sqrt_slope
        )

        # Calculate discharge
        self._disch[:] = H_link * self._vel

        # Flux divergence
        dqda = self._grid.calc_flux_div_at_node(self._disch)

        # Rate of change of water depth
        if self._current_time < self._precip_duration:
            ppt = self._precip
        else:
            ppt = 0.0
        dHdt = ppt - self._infilt - dqda

        # Update water depth: simple forward Euler scheme
        self._depth[self._grid.core_nodes] += dHdt[self._grid.core_nodes] * dt

        # Very crude numerical hack: prevent negative water depth
        self._depth[np.where(self._depth < 0.0)[0]] = 0.0

        self._current_time += dt


if __name__ == "__main__":
    import doctest

    doctest.testmod()



================================================
File: overland_flow/kinematic_wave_rengers.py
================================================
#!/usr/bin/env python

import numpy as np

from landlab import Component
from landlab import RasterModelGrid


class KinematicWaveRengers(Component):
    """
    This code is based on an overland flow model by Francis Rengers and
    colleagues, after Julien et al., 1995. It uses an explicit face-centered
    solution to a depth-varying Manning's equation, broadly following, e.g.,
    Mugler et al., 2011.
    It was implemented in Landlab by DEJH, March '16. Please cite
    Rengers et al., 2016, Model Predictions of Water Runoff in Steep
    Catchments after Wildfire, WRR.

    Note: You will not have a good day if you have pits present in your topo
    before routing flow with this component. Fill pits before instantiating
    the component (or call :func:`update_topographic_params` once you have
    filled after instantiation).

    Note this module assumes that the topography DOES NOT change during the
    run. If it does, call :func:`update_topographic_params` to update the
    component to the new topo.

    Boundary condition control can be... interesting with this component.
    Be sure to close boundaries you do not wish water to leave - or enter! -
    through. To allow free water discharge from the grid edge it is
    recommended to use fixed gradient boundary conditions at the open edges.
    The component will then set the fixed gradient as equal to the underlying
    topographic gradient throughout the run.

    It is also possible to fix the water depth at the open edge, but this
    is not really recommended.

    Construction::

        KinematicWaveRengers(grid, mannings_n=0.03, critical_flow_depth=0.003,
                             mannings_epsilon=0.33333333, dt_max=0.3,
                             max_courant=0.2, min_surface_water_depth=1.e-8)

    Parameters
    ----------
    grid : RasterModelGrid
        A grid.
    mannings_n : float
        A value to use for Manning's n in the Manning discharge equation.
    critical_flow_depth : float (m)
        An index flow depth for the depth-varying Manning's equation,
        controlling the depth at which the effective Manning's n begins to
        increase.
    mannings_epsilon : float
        An exponent for the depth-varying Manning's equation, controlling the
        rate of increase of effective Manning's n at small flow depths.
    dt_max : float or None (s)
        The largest permitted internal timestep for the component. If the
        Courant criterion produces a more restrictive condition, that will be
        used instead.
    max_courant : float
        The maximum permitted Courant number for the courant stability
        criterion.
    min_surface_water_depth : float (m)
        A water depth below which surface water thickness may never fall, to
        ensure model stabilty.

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import KinematicWaveRengers
    >>> mg = RasterModelGrid((5, 10), 10.0)
    >>> mg.status_at_node[mg.nodes_at_left_edge] = mg.BC_NODE_IS_FIXED_GRADIENT
    >>> mg.status_at_node[mg.nodes_at_top_edge] = mg.BC_NODE_IS_CLOSED
    >>> mg.status_at_node[mg.nodes_at_bottom_edge] = mg.BC_NODE_IS_CLOSED
    >>> mg.status_at_node[mg.nodes_at_right_edge] = mg.BC_NODE_IS_CLOSED
    >>> _ = mg.add_field("node", "topographic__elevation", 0.05 * mg.node_x)
    >>> _ = mg.add_empty("node", "surface_water__depth")
    >>> mg.at_node["surface_water__depth"].fill(1.0e-8)
    >>> dt = 60.0  # 1 min intervals
    >>> rain_intensities = (1.0e-5, 1.0e-5, 1.0e-5, 1.0e-5, 1.0e-5)
    >>> kw = KinematicWaveRengers(mg)
    >>> for i in rain_intensities:
    ...     kw.run_one_step(dt, rainfall_intensity=i)
    ...
    >>> mg.at_node["surface_water__depth"]
    array([1.00000000e-08, 1.00000000e-08, 1.00000000e-08,
           1.00000000e-08, 1.00000000e-08, 1.00000000e-08,
           1.00000000e-08, 1.00000000e-08, 1.00000000e-08,
           1.00000000e-08, 2.95578314e-03, 2.95578314e-03,
           2.90945761e-03, 2.82912876e-03, 2.70127141e-03,
           2.51202011e-03, 2.24617193e-03, 1.88032853e-03,
           1.35451064e-03, 1.00000000e-08, 2.95578314e-03,
           2.95578314e-03, 2.90945761e-03, 2.82912876e-03,
           2.70127141e-03, 2.51202011e-03, 2.24617193e-03,
           1.88032853e-03, 1.35451064e-03, 1.00000000e-08,
           2.95578314e-03, 2.95578314e-03, 2.90945761e-03,
           2.82912876e-03, 2.70127141e-03, 2.51202011e-03,
           2.24617193e-03, 1.88032853e-03, 1.35451064e-03,
           1.00000000e-08, 1.00000000e-08, 1.00000000e-08,
           1.00000000e-08, 1.00000000e-08, 1.00000000e-08,
           1.00000000e-08, 1.00000000e-08, 1.00000000e-08,
           1.00000000e-08, 1.00000000e-08])
    """

    _name = "KinematicWaveRengers"

    _unit_agnostic = False

    _info = {
        "surface_water__depth": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of water on the surface",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "surface_water__discharge": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m**3/s",
            "mapping": "node",
            "doc": "Volumetric discharge of surface water",
        },
        "surface_water__velocity": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/s",
            "mapping": "node",
            "doc": "Speed of water flow above the surface",
        },
    }

    def __init__(
        self,
        grid,
        mannings_n=0.03,
        critical_flow_depth=0.003,
        mannings_epsilon=0.33333333,
        dt_max=0.3,
        max_courant=0.2,
        min_surface_water_depth=1.0e-8,
    ):
        """Initialize the kinematic wave approximation overland flow component."""
        super().__init__(grid)

        if not isinstance(self.grid, RasterModelGrid):
            ValueError("KinematicWaveRengers: grid must be regular")

        if np.isclose(dt_max, 0.0):
            raise ValueError("KinematicWaveRengers: dt must be > 0.0")

        active = np.nonzero(self.grid.status_at_node != self.grid.BC_NODE_IS_CLOSED)
        self._h = self.grid.at_node["surface_water__depth"]
        self._active = active
        self._hc = critical_flow_depth
        self._n = mannings_n
        self._negepsilon = -mannings_epsilon

        self._dt_max = dt_max
        self._min_surface_water_depth = min_surface_water_depth
        self._active_depths = self.grid.at_node["surface_water__depth"][active]
        all_grads = self.grid.calc_grad_at_link("topographic__elevation")
        hoz_grads = self.grid.map_mean_of_horizontal_active_links_to_node(all_grads)
        vert_grads = self.grid.map_mean_of_vertical_active_links_to_node(all_grads)
        self._hozslopept5 = np.fabs(hoz_grads[active]) ** 0.5
        self._vertslopept5 = np.fabs(vert_grads[active]) ** 0.5
        self._velx = self.grid.zeros(at="node", dtype=float)
        self._vely = self.grid.zeros(at="node", dtype=float)
        self._qy = np.zeros(grid.number_of_nodes + 1, dtype=float)
        self._qx = np.zeros(grid.number_of_nodes + 1, dtype=float)
        self._poshozgrads = hoz_grads > 0.0
        self._posvertgrads = vert_grads > 0.0
        if np.isclose(self.grid.dx, self.grid.dy):
            self._equaldims = True
            self._courant_prefactor = max_courant * self.grid.dx
        else:
            self._equaldims = False
            self._courant_prefactor = max_courant * self.grid.dx * self.grid.dy
        self._neighbors = self.grid.adjacent_nodes_at_node.copy()
        self._neighbors[self._neighbors == self.grid.BAD_INDEX] = -1
        self._water_balance = []
        self._actives_BCs = (
            self.grid.status_at_node[active] == self.grid.BC_NODE_IS_FIXED_VALUE
        )
        self._actives_BCs_water_depth = self._h[active][self._actives_BCs]
        fixed_grad_nodes = self.grid.fixed_gradient_boundary_nodes.copy()
        fixed_grad_anchors = self.grid.fixed_gradient_boundary_node_anchor_node

        # ^add this value to the anchor nodes to update the BCs
        # these also need to be mapped to active_IDs:
        blank_nodes = self.grid.zeros(at="node", dtype=bool)
        blank_nodes[fixed_grad_nodes] = True
        self._fixed_grad_nodes_active = np.where(blank_nodes[active])[0]
        blank_nodes.fill(False)
        blank_nodes[fixed_grad_anchors] = True
        self._fixed_grad_anchors_active = np.where(blank_nodes[active])[0]

        # create outputs
        self.initialize_output_fields()

    def run_one_step(
        self,
        dt,
        rainfall_intensity=0.00001,
        update_topography=False,
        track_min_depth=False,
    ):
        """Update fields with current hydrologic conditions.

        Parameters
        ----------
        rain_intensity : float or array (m/s)
            The rainfall intensity across the grid (water input rate at each
            node).
        update_topography : bool
            Set to true if the topography of the grid evolves during the run.
        track_min_depth : bool
            At *very* low rainfall inputs, there is a possibility this
            component could allow creation of small amounts of water mass.
            Set to true to track this mass, and use the :func:`water_balance`
            property to investigate its evolution through time.
        """
        elapsed_time_in_dt = 0.0  # this is only since the start of the timestep
        active = self._active
        self._hnew = self._h[active]
        _hnew = self._hnew
        if update_topography:
            self.update_topographic_params()
        while elapsed_time_in_dt < dt:
            internal_dt = self.calc_grads_and_timesteps(
                update_topography, track_min_depth
            )
            remaining_dt = dt - elapsed_time_in_dt
            # now reduce timestep is needed if limited by total tstep length
            internal_dt = min(internal_dt, remaining_dt).clip(0.0)
            # this section uses our final-array-val-is-zero trick
            _qx_left = self._qx[self._neighbors[:, 2]].clip(min=0.0)
            _qx_right = self._qx[self._neighbors[:, 0]].clip(max=0.0)
            _qy_top = self._qy[self._neighbors[:, 1]].clip(min=0.0)
            _qy_bottom = self._qy[self._neighbors[:, 3]].clip(max=0.0)
            # FR's rainfall handling was here. We're going to assume that the
            # component is being driven by a "LL style" rainfall record, where
            # the provided rainfall_intensity is constant across the provide
            # dt. If it's not, it needs to be handled outside the component.

            # now add the rainfall input
            if type(rainfall_intensity) is not np.ndarray:
                _hnew += internal_dt * rainfall_intensity
            else:
                _hnew += internal_dt * rainfall_intensity[active]
            # set the BCs
            _hnew[self._actives_BCs] = self._actives_BCs_water_depth
            # flux it round
            _hnew -= internal_dt / self.grid.dx * np.fabs(self._qy[active])
            _hnew -= internal_dt / self.grid.dy * np.fabs(self._qx[active])
            _hnew += internal_dt / self.grid.dx * (_qy_top - _qy_bottom)[active]
            _hnew += internal_dt / self.grid.dy * (_qx_left - _qx_right)[active]
            _hnew[self._fixed_grad_nodes_active] = _hnew[
                self._fixed_grad_anchors_active
            ]
            # update the internal clock
            elapsed_time_in_dt += internal_dt

        # update the actual fields
        self._h[active] = _hnew
        self.grid.at_node["surface_water__velocity"][:] = np.sqrt(
            np.square(self._velx) + np.square(self._vely)
        )
        self.grid.at_node["surface_water__discharge"][:] = np.sqrt(
            np.square(self._qx[:-1]) + np.square(self._qy[:-1])
        )

    def calc_grads_and_timesteps(self, update_topography, track_min_depth):
        """
        Perform the first part of the calculation for the main run, mainly
        velocities and fluxes. The main objective of this part of the
        calculation is to derive the stable timestep for the run.

        Parameters
        ----------
        update_topography : bool
            If False, the underlying surface topography is assumed unchanged
            since the last run.
        track_min_depth : bool
            If True, the internal list _water_balance will be appended with
            the volumetric fractional change in mass balance during the run.

        Returns
        -------
        internal_dt : float
            The internally stable timestep that will be used on this loop.
        """
        active = self._active
        _hnew = self._hnew
        if update_topography:
            self.update_topographic_params()
        # assert the minimum water depth - this could introduce an element of
        # mass gain, but should remain minor
        _hnew.clip(self._min_surface_water_depth, out=_hnew)
        if track_min_depth:
            self._water_balance.append(
                (_hnew - self._h[active]).sum() / self._h[active].sum()
            )
        n = self._n * (_hnew / self._hc) ** self._negepsilon
        twothirds_hnewbyn = _hnew**0.66666666 / n
        self._vely[active] = twothirds_hnewbyn * self._vertslopept5
        self._velx[active] = twothirds_hnewbyn * self._hozslopept5
        self._vely[self._posvertgrads] *= -1.0
        self._velx[self._poshozgrads] *= -1.0
        self._qy[active] = self._vely[active] * _hnew  # m**2/s
        self._qx[active] = self._velx[active] * _hnew  # m**2/s
        max_vely = np.fabs(self._vely).max()
        max_velx = np.fabs(self._velx).max()
        if self._equaldims:
            courant_dt = self._courant_prefactor / (max_velx + max_vely)
        else:
            # note prefactor is NOT THE SAME as above in this case
            courant_dt = self._courant_prefactor / (
                self.grid.dy * max_velx + self.grid.dx * max_vely
            )
        if self._dt_max is not None:
            internal_dt = np.min((self._dt_max, courant_dt))
        else:
            internal_dt = courant_dt
        self._internal_dt = internal_dt

        return internal_dt

    def update_topographic_params(self):
        """
        If the topo changes during the run, change the held params used by
        :func:`run_one_step`.
        """
        active = np.where(self.grid.status_at_node != self.grid.BC_NODE_IS_CLOSED)[0]
        all_grads = self.grid.calculate_gradients_at_links("topographic__elevation")
        hoz_grads = self.grid.map_mean_of_horizontal_active_links_to_node(all_grads)
        vert_grads = self.grid.map_mean_of_vertical_active_links_to_node(all_grads)
        self._hozslopept5 = np.fabs(hoz_grads[active]) ** 0.5
        self._vertslopept5 = np.fabs(vert_grads[active]) ** 0.5
        self._poshozgrads = hoz_grads > 0.0
        self._posvertgrads = vert_grads > 0.0
        fixed_grad_nodes = self.grid.fixed_gradient_boundary_nodes
        fixed_grad_anchors = self.grid.fixed_gradient_boundary_node_anchor_node
        # ^add this value to the anchor nodes to update the BCs
        # these also need to be mapped to active_IDs:
        blank_nodes = self.grid.zeros(at="node", dtype=bool)
        blank_nodes[fixed_grad_nodes] = True
        self._fixed_grad_nodes_active = np.where(blank_nodes[active])[0]
        blank_nodes.fill(False)
        blank_nodes[fixed_grad_anchors] = True
        self._fixed_grad_anchors_active = np.where(blank_nodes[active])[0]
        # check is the grid topology has changed...
        if not np.all(np.equal(self._active, active)):
            self._active = active
            self._velx.fill(0.0)
            self._vely.fill(0.0)
            self._qy.fill(0.0)
            self._qx.fill(0.0)
            self._neighbors = self.grid.adjacent_nodes_at_node.copy()
            self._neighbors[self._neighbors == self.grid.BAD_INDEX] = -1
            self._actives_BCs = (
                self.grid.status_at_node[active] == self.grid.BC_NODE_IS_FIXED_VALUE
            )
            self._actives_BCs_water_depth = self._h[self._actives_BCs]

    @property
    def water_balance(self):
        """
        Return a list of the fractional gain/loss of water mass during the
        run, if it was tracked using the track_min_depth flag.
        """
        if self._water_balance == []:
            raise ValueError("No record of water balance was found!")
        else:
            return self._water_balance

    @property
    def internal_timestep(self):
        """
        Return the internal timestep last used by the kinematic wave component.
        """
        try:
            return self._internal_dt
        except AttributeError:
            # the component hasn't started running yet
            _ = self.calc_grads_and_timesteps(False, False)
            return self._internal_dt



================================================
File: overland_flow/linear_diffusion_overland_flow_router.py
================================================
"""Landlab component for overland flow using the linearized diffusion-wave approximation.

Created on Fri May 27 14:26:13 2016

@author: gtucker
"""

import numpy as np

from landlab import Component

_FOUR_THIRDS = 4.0 / 3.0
_SEVEN_THIRDS = 7.0 / 3.0
_MICRO_DEPTH = 1.0e-6  # tiny water depth to avoid blowup in time-step estimator


class LinearDiffusionOverlandFlowRouter(Component):
    r"""Calculate water flow over topography.

    Landlab component that implements a two-dimensional, linearized
    diffusion-wave model. The diffusion-wave approximation is a simplification
    of the shallow-water equations that omits the momentum terms. The flow
    velocity is calculated using the local water-surface slope as an
    approximation of the energy slope. With this linearized form, flow velocity
    is calculated using a linearized Manning equation, with the water-surface
    slope being used as the slope factor. There are two governing equations, one
    that represents conservation of water mass:

    ..math::

        \frac{\partial H}{\partial t} = (P - I) - \nabla\cdot\mathbf{q}

    where :math:`H(x,y,t)` is local water depth, :math:`t` is time, :math:`P`
    is precipitation rate, :math:`I` is infiltration rate, and :math:`\mathbf{q}`
    is specific water discharge, which equals depth times depth-averaged
    velocity. The other governing equation represents specific discharge as a
    function of gravity, pressure, and friction:

    ..math::

        \mathbf{q} = \frac{H^{4/3}}{n^2 U_c} \nabla w

    where :math:`n` is the friction factor ("Manning's n"), :math:`U_c` is a
    characteristic flow velocity, and :math:`w` is water-surface height, which
    is the sum of topographic elevation plus water depth.

    Infiltration rate should decline smoothly to zero as surface water depth
    approaches zero. To ensure this, infiltration rate is calculated as

    ..math::

        I = I_c \left( 1 - e^{-H / H_i} ) \right)

    where :math:`H_i` is a characteristic water depth. The concept here is that
    when :math:`H \le H_i`, small spatial variations in water depth will leave
    parts of the ground un-ponded and therefore not subject to any infiltration.
    Mathematically, :math:`I \approx 0.95 I_c` when :math:`H = 3H_i`.

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> grid = RasterModelGrid((3, 3))
    >>> _ = grid.add_zeros("topographic__elevation", at="node")
    >>> olf = LinearDiffusionOverlandFlowRouter(grid, roughness=0.1)
    >>> round(olf.vel_coef)
    100
    >>> olf.rain_rate
    1e-05
    >>> olf.rain_rate = 1.0e-4
    >>> olf.run_one_step(dt=10.0)
    >>> grid.at_node["surface_water__depth"][4]
    0.001

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    None Listed

    """

    _name = "LinearDiffusionOverlandFlowRouter"

    _unit_agnostic = True

    _info = {
        "surface_water__depth": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of water on the surface",
        },
        "surface_water__depth_at_link": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "link",
            "doc": "Depth of water on the surface at grid links",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "water__specific_discharge": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m2/s",
            "mapping": "link",
            "doc": "flow discharge component in the direction of the link",
        },
        "water__velocity": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/s",
            "mapping": "link",
            "doc": "flow velocity component in the direction of the link",
        },
        "water_surface__gradient": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/s",
            "mapping": "link",
            "doc": "Downstream gradient of the water surface.",
        },
        "water_surface__elevation": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Elevation of the water surface.",
        },
    }

    def __init__(
        self,
        grid,
        roughness=0.01,
        rain_rate=1.0e-5,
        infilt_rate=0.0,
        infilt_depth_scale=0.001,
        velocity_scale=1.0,
        cfl_factor=0.2,
    ):
        """Initialize the LinearDiffusionOverlandFlowRouter.

        Parameters
        ----------
        grid : ModelGrid
            Landlab ModelGrid object
        roughness : float, defaults to 0.01
            Manning roughness coefficient, s/m^1/3
        rain_rate : float, optional (defaults to 36 mm/hr)
            Rainfall rate, m/s
        infilt_depth_scale : float, defaults to 0.001
            Depth scale for water infiltration, m
        infilt_rate : float, optional (defaults to 0)
            Maximum rate of infiltration, m/s
        velocity_scale : float, defaults to 1
            Characteristic flow velocity, m/s
        cfl_factor : float, optional (defaults to 0.2)
            Time-step control factor: fraction of maximum estimated time-step
            that is actually used (must be <=1)
        """
        super().__init__(grid)

        if roughness <= 0.0:
            raise ValueError(f"roughness must be greater than zero {roughness}")
        if rain_rate < 0.0:
            raise ValueError(f"rain_rate must not be negative {rain_rate}")
        if infilt_depth_scale <= 0.0:
            raise ValueError(
                f"infilt_depth_scale must be greater than zero {infilt_depth_scale}"
            )
        if infilt_rate < 0.0:
            raise ValueError(f"infilt_rate must not be negative {infilt_rate}")
        if velocity_scale <= 0.0:
            raise ValueError(
                f"velocity_scale must be greater than zero {velocity_scale}"
            )
        if cfl_factor > 1.0 or cfl_factor <= 0.0:
            raise ValueError(f"cfl_factor must >0, <=1 {cfl_factor}")

        # Store parameters and do unit conversion
        self._rain = rain_rate
        self._infilt = infilt_rate
        self._infilt_depth_scale = infilt_depth_scale
        self._vel_coef = 1.0 / (roughness**2 * velocity_scale)

        self._elev = grid.at_node["topographic__elevation"]

        self.initialize_output_fields()
        self._depth = grid.at_node["surface_water__depth"]
        self._depth_at_link = grid.at_link["surface_water__depth_at_link"]
        self._vel = grid.at_link["water__velocity"]
        self._disch = grid.at_link["water__specific_discharge"]
        self._wsgrad = grid.at_link["water_surface__gradient"]
        self._water_surf_elev = grid.at_node["water_surface__elevation"]

        self._inactive_links = grid.status_at_link == grid.BC_LINK_IS_INACTIVE

        self._cfl_param = cfl_factor * 0.5 * np.amin(grid.length_of_link) ** 2

    @property
    def rain_rate(self):
        """Rainfall rate"""
        return self._rain

    @rain_rate.setter
    def rain_rate(self, value):
        if value < 0.0:
            raise ValueError(f"rain_rate must be positive {value}")
        self._rain = value

    @property
    def vel_coef(self):
        """Velocity coefficient.

        (1/(roughness^2 x velocity_scale)
        """
        return self._vel_coef

    def _cfl_time_step(self):
        """Calculate maximum time-step size using CFL criterion for explicit
        FTCS diffusion."""
        max_water_depth = np.amax(self._depth, initial=_MICRO_DEPTH)
        max_diffusivity = self._vel_coef * max_water_depth**_SEVEN_THIRDS
        return self._cfl_param / max_diffusivity

    def update_for_one_iteration(self, iter_dt):
        """Update state variables for one iteration of duration iter_dt."""

        # Calculate the water-surface elevation
        self._water_surf_elev[:] = self._elev + self._depth

        # Calculate water depth at links. This implements an "upwind" scheme
        # in which water depth at the links is the depth at the higher of the
        # two nodes.
        self._grid.map_value_at_max_node_to_link(
            self._water_surf_elev, "surface_water__depth", out=self._depth_at_link
        )

        # Calculate the water-surface gradient and impose any closed boundaries
        self.grid.calc_grad_at_link(self._water_surf_elev, out=self._wsgrad)
        self._wsgrad[self._inactive_links] = 0.0

        # Calculate velocity using the linearized Manning equation.
        self._vel[:] = (
            -self._vel_coef * self._depth_at_link**_FOUR_THIRDS * self._wsgrad
        )

        # Calculate discharge
        self._disch[:] = self._depth_at_link * self._vel

        # Flux divergence
        dqda = self._grid.calc_flux_div_at_node(self._disch)

        # Rates of infiltration and runoff
        infilt = self._infilt * (1.0 - np.exp(-self._depth / self._infilt_depth_scale))

        # Rate of change of water depth
        dHdt = self._rain - infilt - dqda

        # Update water depth: simple forward Euler scheme
        self._depth[self._grid.core_nodes] += dHdt[self._grid.core_nodes] * iter_dt

        # Very crude numerical hack: prevent negative water depth (TODO: better)
        self._depth.clip(min=0.0, out=self._depth)

    def run_one_step(self, dt):
        """Calculate water flow for a time period `dt`.

        Default units for dt are *seconds*. We use a time-step subdivision
        algorithm that ensures step size is always below CFL limit.
        """
        remaining_time = dt
        while remaining_time > 0.0:
            dtmax = self._cfl_time_step()  # biggest possible time step size
            dt_this_iter = min(dtmax, remaining_time)  # step we'll actually use
            self.update_for_one_iteration(dt_this_iter)
            remaining_time -= dt_this_iter



================================================
File: pet/__init__.py
================================================
from .potential_evapotranspiration_field import PotentialEvapotranspiration

__all__ = ["PotentialEvapotranspiration"]



================================================
File: pet/potential_evapotranspiration_field.py
================================================
import numpy as np

from landlab import Component

_VALID_METHODS = {"Constant", "PriestleyTaylor", "MeasuredRadiationPT", "Cosine"}


def _assert_method_is_valid(method):
    if method not in _VALID_METHODS:
        raise ValueError("%s: Invalid method name" % method)


class PotentialEvapotranspiration(Component):
    """
    Potential Evapotranspiration Component calculates spatially distributed
    potential evapotranspiration based on input radiation factor (spatial
    distribution of incoming radiation) using chosen method such as constant
    or Priestley Taylor. Ref: Xiaochi et. al. 2013 for 'Cosine' method and
    ASCE-EWRI Task Committee Report Jan 2005 for 'PriestleyTaylor' method.
    Note: Calling 'PriestleyTaylor' method would generate/overwrite shortwave &
    longwave radiation fields.

    .. codeauthor:: Sai Nudurupati and Erkan Istanbulluoglu

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> from landlab.components.pet import PotentialEvapotranspiration

    >>> grid = RasterModelGrid((5, 4), xy_spacing=(0.2, 0.2))
    >>> grid["cell"]["radiation__ratio_to_flat_surface"] = np.array(
    ...     [0.38488566, 0.38488566, 0.33309785, 0.33309785, 0.37381705, 0.37381705]
    ... )
    >>> PET = PotentialEvapotranspiration(grid)
    >>> PET.name
    'PotentialEvapotranspiration'
    >>> PET.input_var_names
    ('radiation__ratio_to_flat_surface',)
    >>> sorted(PET.output_var_names)
    ['radiation__incoming_shortwave_flux',
     'radiation__net_flux',
     'radiation__net_longwave_flux',
     'radiation__net_shortwave_flux',
     'surface__potential_evapotranspiration_rate']
    >>> sorted(PET.units)
    [('radiation__incoming_shortwave_flux', 'W/m^2'),
     ('radiation__net_flux', 'W/m^2'),
     ('radiation__net_longwave_flux', 'W/m^2'),
     ('radiation__net_shortwave_flux', 'W/m^2'),
     ('radiation__ratio_to_flat_surface', 'None'),
     ('surface__potential_evapotranspiration_rate', 'mm')]
    >>> PET.grid.number_of_cell_rows
    3
    >>> PET.grid.number_of_cell_columns
    2
    >>> PET.grid is grid
    True
    >>> pet_rate = grid.at_cell["surface__potential_evapotranspiration_rate"]
    >>> np.allclose(pet_rate, 0.0)
    True
    >>> PET.current_time = 0.5
    >>> PET.update()
    >>> np.allclose(pet_rate, 0.0)
    False

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    ASCE-EWRI: The ASCE standardized reference evapotranspiration equation, in:
    Standardization of Reference Evapotranspiration Task Committee Final Report,
    edited by: Allen, R. G., Walter, I. A., Elliot, R. L., Howell, T. A.,
    Itenﬁsu, D., Jensen, M. E., and Snyder, R. L., Technical Committee report
    to the Environmental and Water Resources Institute of the American Society
    of Civil Engineers from the Task Committee on Standardization of Reference
    Evapotranspiration, Reston, VA, USA, 2005.

    Zhou, X., Istanbulluoglu, E., and Vivoni, E. R.: Modeling the
    ecohydrological role of aspect-controlled radiation on tree-grass-shrub
    coexistence in a semiarid climate, Water Resour. Res., 49, 2872– 2895,
    doi:10.1002/wrcr.20259, 2013.

    """

    _name = "PotentialEvapotranspiration"

    _unit_agnostic = False

    _info = {
        "radiation__incoming_shortwave_flux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "W/m^2",
            "mapping": "cell",
            "doc": "incident shortwave radiation",
        },
        "radiation__net_flux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "W/m^2",
            "mapping": "cell",
            "doc": "net radiation",
        },
        "radiation__net_longwave_flux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "W/m^2",
            "mapping": "cell",
            "doc": "net incident longwave radiation",
        },
        "radiation__net_shortwave_flux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "W/m^2",
            "mapping": "cell",
            "doc": "net incident shortwave radiation",
        },
        "radiation__ratio_to_flat_surface": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "None",
            "mapping": "cell",
            "doc": (
                "ratio of incident shortwave radiation on sloped "
                "surface to flat surface"
            ),
        },
        "surface__potential_evapotranspiration_rate": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "mm",
            "mapping": "cell",
            "doc": "potential sum of evaporation and potential transpiration",
        },
    }

    def __init__(
        self,
        grid,
        method="Cosine",
        priestley_taylor_const=1.26,
        albedo=0.6,
        latent_heat_of_vaporization=28.34,
        psychometric_const=0.066,
        stefan_boltzmann_const=0.0000000567,
        solar_const=1366.67,
        latitude=34.0,
        elevation_of_measurement=300,
        adjustment_coeff=0.18,
        lt=0.0,
        nd=365.0,
        MeanTmaxF=12.0,
        delta_d=5.0,
        current_time=None,
        const_potential_evapotranspiration=12.0,
        Tmin=0.0,
        Tmax=1.0,
        Tavg=0.5,
        obs_radiation=350.0,
    ):
        """
        Parameters
        ----------
        grid: RasterModelGrid
            A grid.
        method: {'Constant', 'PriestleyTaylor', 'MeasuredRadiationPT', 'Cosine'}, optional
            Priestley Taylor method will spit out radiation outputs too.
        priestley_taylor_constant: float, optional
            Alpha used in Priestley Taylor method.
        albedo: float, optional
            Albedo.
        latent_heat_of_vaporization: float, optional
            Latent heat of vaporization for water Pwhv (Wd/(m*mm^2)).
        psychometric_const: float, optional
            Psychometric constant (kPa (deg C)^-1).
        stefan_boltzmann_const: float, optional
            Stefan Boltzmann's constant (W/(m^2K^-4)).
        solar_const: float, optional
            Solar constant (W/m^2).
        latitude: float, optional
            Latitude (radians).
        elevation_of_measurement: float, optional
            Elevation at which measurement was taken (m).
        adjustment_coeff: float, optional
            adjustment coeff to predict Rs from air temperature (deg C)^-0.5.
        lt: float, optional
            lag between peak TmaxF and solar forcing (days).
        nd: float, optional
            Number of days in year (days).
        MeanTmaxF: float, optional
            Mean annual rate of TmaxF (mm/d).
        delta_d: float, optional
            Calibrated difference between max & min daily TmaxF (mm/d).
        current_time: float, required only for 'Cosine' method
            Current time (Years)
        const_potential_evapotranspiration: float, optional for
            'Constant' method
            Constant PET value to be spatially distributed.
        Tmin: float, required for 'Priestley Taylor' method
            Minimum temperature of the day (deg C)
        Tmax: float, required for 'Priestley Taylor' method
            Maximum temperature of the day (deg C)
        Tavg: float, required for 'Priestley Taylor' and 'MeasuredRadiationPT'
            methods
            Average temperature of the day (deg C)
        obs_radiation float, required for 'MeasuredRadiationPT' method
            Observed radiation (W/m^2)
        """
        super().__init__(grid)

        self.current_time = current_time
        self.const_potential_evapotranspiration = const_potential_evapotranspiration
        self.Tmin = Tmin
        self.Tmax = Tmax
        self.Tavg = Tavg
        self.obs_radiation = obs_radiation

        self._method = method
        # For Priestley Taylor
        self._alpha = priestley_taylor_const
        self._a = albedo
        self._pwhv = latent_heat_of_vaporization
        self._y = psychometric_const
        self._sigma = stefan_boltzmann_const
        self._Gsc = solar_const
        self._phi = (np.pi / 180.0) * latitude
        self._z = elevation_of_measurement
        self._Krs = adjustment_coeff
        self._LT = lt
        self._ND = nd
        self._TmaxF_mean = MeanTmaxF
        self._DeltaD = delta_d
        _assert_method_is_valid(self._method)

        self.initialize_output_fields()

        self._cell_values = self._grid["cell"]

    @property
    def const_potential_evapotranspiration(self):
        """Constant PET value to be spatially distributed.

        Used by 'Constant' method.
        """
        return self._const_potential_evapotranspiration

    @const_potential_evapotranspiration.setter
    def const_potential_evapotranspiration(self, const_potential_evapotranspiration):
        self._const_potential_evapotranspiration = const_potential_evapotranspiration

    @property
    def obs_radiation(self):
        """Observed radiation (W/m^2)

        obs_radiation float, required for 'MeasuredRadiationPT' method.
        """
        return self._obs_radiation

    @obs_radiation.setter
    def obs_radiation(self, obs_radiation):
        self._obs_radiation = obs_radiation

    @property
    def Tmin(self):
        """Minimum temperature of the day (deg C)

        Tmin: float, required for 'Priestley Taylor' method.
        """
        return self._Tmin

    @Tmin.setter
    def Tmin(self, Tmin):
        self._Tmin = Tmin

    @property
    def Tmax(self):
        """Maximum temperature of the day (deg C)

        Tmax: float, required for 'Priestley Taylor' method.
        """
        return self._Tmax

    @Tmax.setter
    def Tmax(self, Tmax):
        self._Tmax = Tmax

    @property
    def Tavg(self):
        """Average temperature of the day (deg C)

        Tavg: float, required for 'Priestley Taylor' and 'MeasuredRadiationPT'
        methods.
        """
        return self._Tavg

    @Tavg.setter
    def Tavg(self, Tavg):
        self._Tavg = Tavg

    def update(self):
        """Update fields with current conditions.

        If the 'Constant' method is used, this method looks to the value of
        the ``const_potential_evapotranspiration`` property.

        If the 'PriestleyTaylor' method is used, this method looks to the
        values of the ``Tmin``, ``Tmax``, and ``Tavg`` properties.

        If the 'MeasuredRadiationPT' method is use this method looks to the
        values of the ``Tavg`` and ``obs_radiation`` property.
        """

        if self._method == "Constant":
            self._PET_value = self._const_potential_evapotranspiration
        elif self._method == "PriestleyTaylor":
            self._PET_value = self._PriestleyTaylor(
                self._current_time, self._Tmax, self._Tmin, self._Tavg
            )
            self._cell_values["radiation__incoming_shortwave_flux"] = (
                self._Rs * self._cell_values["radiation__ratio_to_flat_surface"]
            )
            self._cell_values["radiation__net_shortwave_flux"] = (
                self._Rns * self._cell_values["radiation__ratio_to_flat_surface"]
            )
            self._cell_values["radiation__net_longwave_flux"] = (
                self._Rnl * self._cell_values["radiation__ratio_to_flat_surface"]
            )
            self._cell_values["radiation__net_flux"] = (
                self._Rn * self._cell_values["radiation__ratio_to_flat_surface"]
            )
        elif self._method == "MeasuredRadiationPT":
            Robs = self._obs_radiation
            self._PET_value = self._MeasuredRadPT(self._Tavg, (1 - self._a) * Robs)
        elif self._method == "Cosine":
            self._J = np.floor(
                (self._current_time - np.floor(self._current_time)) * 365.0
            )
            self._PET_value = max(
                (
                    self._TmaxF_mean
                    + self._DeltaD
                    / 2.0
                    * np.cos(
                        (2 * np.pi) * (self._J - self._LT - self._ND / 2) / self._ND
                    )
                ),
                0.0,
            )

        self._PET = (
            self._PET_value * self._cell_values["radiation__ratio_to_flat_surface"]
        )
        self._cell_values["surface__potential_evapotranspiration_rate"][:] = self._PET

    def _PriestleyTaylor(self, current_time, Tmax, Tmin, Tavg):
        # Julian Day - ASCE-EWRI Task Committee Report, Jan-2005 - Eqn 25, (52)
        self._J = np.floor((current_time - np.floor(current_time)) * 365)
        # Saturation Vapor Pressure - ASCE-EWRI Task Committee Report,
        # Jan-2005 - Eqn 6, (37)
        self._es = 0.6108 * np.exp((17.27 * Tavg) / (237.7 + Tavg))

        # Actual Vapor Pressure - ASCE-EWRI Task Committee Report,
        # Jan-2005 - Eqn 8, (38)
        self._ea = 0.6108 * np.exp((17.27 * Tmin) / (237.7 + Tmin))

        # Slope of Saturation Vapor Pressure - ASCE-EWRI Task Committee Report,
        # Jan-2005 - Eqn 5, (36)
        self._delta = (4098.0 * self._es) / ((237.3 + Tavg) ** 2.0)

        # Solar Declination Angle - ASCE-EWRI Task Committee Report,
        # Jan-2005 - Eqn 24,(51)
        self._sdecl = 0.409 * np.sin(((np.pi / 180.0) * self._J) - 1.39)

        # Inverse Relative Distance Factor - ASCE-EWRI Task Committee Report,
        # Jan-2005 - Eqn 23,(50)
        self._dr = 1 + (0.033 * np.cos(np.pi / 180.0 * self._J))

        # To calculate ws - ASCE-EWRI Task Committee Report,
        # Jan-2005 - Eqn 29,(61)
        self._x = 1.0 - (((np.tan(self._phi)) ** 2.0) * (np.tan(self._sdecl) ** 2.0))
        if self._x <= 0:
            self._x = 0.00001
            # Sunset Hour Angle - ASCE-EWRI Task Committee Report,
            # Jan-2005 - Eqn 28,(60)
        self._ws = (np.pi / 2.0) - np.arctan(
            (-1 * np.tan(self._phi) * np.tan(self._sdecl)) / (self._x**2.0)
        )

        # Extraterrestrial radmodel.docx - ASCE-EWRI Task Committee Report,
        # Jan-2005 - Eqn 21, (48)
        # 11.57 converts 1 MJ/m^2/day to W/m^2
        self._Ra = (
            11.57
            * (24.0 / np.pi)
            * 4.92
            * self._dr
            * (
                (self._ws * np.sin(self._phi) * np.sin(self._sdecl))
                + (np.cos(self._phi) * np.cos(self._sdecl) * (np.sin(self._ws)))
            )
        )

        # Clear-sky Solar Radiation - ASCE-EWRI Task Committee Report,
        # Jan-2005 - Eqn 19, (47)
        self._Rso = (0.75 + ((2.0 * (10 ** (-5.0))) * self._z)) * self._Ra
        self._Rs = min(self._Krs * self._Ra * np.sqrt(Tmax - Tmin), self._Rso)

        # Net Short Wave Radiation - ASCE-EWRI Task Committee Report,
        # Jan-2005 - Eqn 16, (43)
        self._Rns = self._Rs * (1 - self._a)

        # Relative Cloudiness - ASCE-EWRI Task Committee Report,
        # Jan-2005 - Page 20,35
        if self._Rso > 0:
            self._u = self._Rs / self._Rso
        else:
            self._u = 0

        if self._u < 0.3:
            self._u = 0.3
        elif self._u > 1:
            self._u = 1.0

        # Cloudiness Function - ASCE-EWRI Task Committee Report,
        # Jan-2005 - Eqn 18, (45)
        self._fcd = (1.35 * self._u) - 0.35

        # Net Long Wave Radiation - ASCE-EWRI Task Committee Report,
        # Jan-2005 - Eqn 17, (44)
        self._Rnl = (
            self._sigma
            * self._fcd
            * (
                0.34
                - (0.14 * np.sqrt(self._ea))
                * (((Tmax + 273.16) ** 4.0 + (Tmin + 273.16) ** 4.0) / 2.0)
            )
        )

        # Net Radiation - ASCE-EWRI Task Committee Report,
        # Jan-2005 - Eqn 15, (42)
        self._Rn = self._Rns - self._Rnl

        self._ETp = max(
            self._alpha
            * (self._delta / (self._delta + self._y))
            * (self._Rn / self._pwhv),
            0,
        )

        return self._ETp

    def _MeasuredRadPT(self, Tavg, Rnobs):
        # Saturation Vapor Pressure - ASCE-EWRI Task Committee Report,
        # Jan-2005 - Eqn 6, (37)
        self._es = 0.6108 * np.exp((17.27 * Tavg) / (237.7 + Tavg))

        # Slope of Saturation Vapor Pressure - ASCE-EWRI Task Committee Report,
        # Jan-2005 - Eqn 5, (36)
        self._delta = (4098.0 * self._es) / ((237.3 + Tavg) ** 2.0)
        self._ETp = max(
            self._alpha
            * (self._delta / (self._delta + self._y))
            * (Rnobs / self._pwhv),
            0,
        )
        return self._ETp



================================================
File: plant_competition_ca/__init__.py
================================================
from landlab.components.plant_competition_ca.plant_competition_ca import VegCA

__all__ = ["VegCA"]



================================================
File: plant_competition_ca/plant_competition_ca.py
================================================
import numpy as np

from landlab import Component

_VALID_METHODS = {"Grid"}
GRASS = 0
SHRUB = 1
TREE = 2
BARE = 3
SHRUBSEEDLING = 4
TREESEEDLING = 5


def assert_method_is_valid(method):
    if method not in _VALID_METHODS:
        raise ValueError("%s: Invalid method name" % method)


class VegCA(Component):
    """Landlab component that simulates inter-species plant competition using a
    2D cellular automata model.

    This code is based on Cellular Automata Tree Grass Shrub Simulator (CATGraSS).
    It simulates spatial competition of multiple plant functional types through
    establishment and mortality. In the current code, tree, grass and
    shrubs are used.

    Ref: Zhou, X., Istanbulluoglu, E., & Vivoni, E. R. (2013). Modeling the
    ecohydrological role of aspect controlled radiation on tree grass shrub
    coexistence in a semiarid climate. Water Resources Research,
    49(5), 2872-2895.

    .. codeauthor:: Sai Nudurupati and Erkan Istanbulluoglu

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import VegCA
    >>> grid = RasterModelGrid((5, 4), xy_spacing=(0.2, 0.2))
    >>> VegCA.name
    'Cellular Automata Plant Competition'
    >>> sorted(VegCA.output_var_names)
    ['plant__age', 'plant__live_index']
    >>> sorted(VegCA.units)
    [('plant__age', 'Years'),
     ('plant__live_index', 'None'),
     ('vegetation__cumulative_water_stress', 'None'),
     ('vegetation__plant_functional_type', 'None')]
    >>> grid["cell"]["vegetation__plant_functional_type"] = np.arange(
    ...     0, grid.number_of_cells, dtype=int
    ... )
    >>> grid["cell"]["vegetation__cumulative_water_stress"] = np.ones(
    ...     grid.number_of_cells
    ... )
    >>> ca_veg = VegCA(grid)
    >>> ca_veg.grid.number_of_cell_rows
    3
    >>> ca_veg.grid.number_of_cell_columns
    2
    >>> ca_veg.grid is grid
    True
    >>> import numpy as np
    >>> A = np.copy(grid["cell"]["plant__age"])
    >>> ca_veg.update()
    >>> np.all(grid["cell"]["plant__age"] == A)
    False

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    Zhou, X., Istanbulluoglu, E., and Vivoni, E. R.: Modeling the
    ecohydrological role of aspect-controlled radiation on tree-grass-shrub
    coexistence in a semiarid climate, Water Resour. Res., 49, 2872– 2895,
    doi:10.1002/wrcr.20259, 2013.

    """

    _name = "Cellular Automata Plant Competition"

    _unit_agnostic = False

    _info = {
        "plant__age": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "Years",
            "mapping": "cell",
            "doc": "Age of plant",
        },
        "plant__live_index": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "None",
            "mapping": "cell",
            "doc": "1 - vegetation__cumulative_water_stress",
        },
        "vegetation__cumulative_water_stress": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "None",
            "mapping": "cell",
            "doc": "cumulative vegetation__water_stress over the growing season",
        },
        "vegetation__plant_functional_type": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "None",
            "mapping": "cell",
            "doc": (
                "classification of plants (int), grass=0, shrub=1, tree=2, "
                "bare=3, shrub_seedling=4, tree_seedling=5"
            ),
        },
    }

    def __init__(
        self,
        grid,
        Pemaxg=0.35,
        ING=2.0,
        ThetaGrass=0.62,
        PmbGrass=0.05,
        Pemaxsh=0.2,
        ThetaShrub=0.8,
        PmbShrub=0.01,
        tpmaxShrub=600,
        Pemaxtr=0.25,
        ThetaTree=0.72,
        PmbTree=0.01,
        tpmaxTree=350,
        ThetaShrubSeedling=0.64,
        PmbShrubSeedling=0.03,
        tpmaxShrubSeedling=18,
        ThetaTreeSeedling=0.64,
        PmbTreeSeedling=0.03,
        tpmaxTreeSeedling=18,
        method="Grid",
        Edit_VegCov=True,
    ):
        """
        Parameters
        ----------
        grid: RasterModelGrid
            A grid.
        Pemaxg: float, optional
            Maximal establishment probability of grass.
        ING: float, optional
            Parameter to define allelopathic effect of creosote on grass.
        ThetaGrass: float, optional
            Drought resistance threshold of grass.
        PmbGrass: float, optional
            Background mortality probability of grass.
        Pemaxsh: float, optional
            Maximal establishment probability of shrub.
        ThetaShrub: float, optional
            Drought resistance threshold of shrub.
        PmbShrub: float, optional
            Background mortality probability of shrub.
        tpmaxShrub: float, optional
            Maximum age of shrub (years).
        Pemaxtr: float, optional
            Maximal establishment probability of tree.
        Thetatree: float, optional
            Drought resistance threshold of tree.
        PmbTree: float, optional
            Background mortality probability of tree.
        tpmaxTree: float, optional
            Maximum age of tree (years).
        ThetaShrubSeedling: float, optional
            Drought resistance threshold of shrub seedling.
        PmbShrubSeedling: float, optional
            Background mortality probability of shrub seedling.
        tpmaxShrubSeedling: float, optional
            Maximum age of shrub seedling (years).
        ThetaTreeSeedling: float, optional
            Drought resistance threshold of tree seedling.
        PmbTreeSeedling: float, optional
            Background mortality probability of tree seedling.
        tpmaxTreeSeedling: float, optional
            Maximum age of tree seedling (years).
        method: str, optional
            Method used.
        Edit_VegCov: bool, optional
            If Edit_VegCov=True, an optional field
            'vegetation__boolean_vegetated' will be output, (i.e.) if a cell is
            vegetated the corresponding cell of the field will be 1, otherwise
            it will be 0.

        """
        super().__init__(grid)

        self.Edit_VegCov = Edit_VegCov

        self._Pemaxg = Pemaxg  # Pe-max-grass - max probability
        self._Pemaxsh = Pemaxsh  # Pe-max-shrub
        self._Pemaxtr = Pemaxtr  # Pe-max-tree
        self._INg = ING  # Allelopathic effect on grass from creosotebush
        self._th_g = ThetaGrass  # grass
        self._th_sh = ThetaShrub  # shrub - Creosote
        self._th_tr = ThetaTree  # Juniper pine
        self._th_sh_s = ThetaShrubSeedling  # shrub seedling
        self._th_tr_s = ThetaTreeSeedling  # Juniper pine seedling
        self._Pmb_g = PmbGrass  # Background mortality probability - grass
        self._Pmb_sh = PmbShrub  # shrub
        self._Pmb_tr = PmbTree  # tree
        self._Pmb_sh_s = PmbShrubSeedling  # shrub seedling
        self._Pmb_tr_s = PmbTreeSeedling  # tree seedling
        self._tpmax_sh = tpmaxShrub  # Maximum age - shrub
        self._tpmax_tr = tpmaxTree  # Maximum age - tree
        self._tpmax_sh_s = tpmaxShrubSeedling  # Maximum age - shrub seedling
        self._tpmax_tr_s = tpmaxTreeSeedling  # Maximum age - tree seedling

        self._method = method

        assert_method_is_valid(self._method)

        # if "vegetation__plant_functional_type" not in self._grid.at_cell:
        #     grid["cell"]["vegetation__plant_functional_type"] = np.random.randint(
        #         0, 6, grid.number_of_cells
        #     )

        self.initialize_output_fields()

        self._cell_values = self._grid["cell"]

        VegType = grid["cell"]["vegetation__plant_functional_type"]

        tp = np.zeros(grid.number_of_cells, dtype=int)
        tp[VegType == TREE] = np.random.randint(
            0, self._tpmax_tr, np.where(VegType == TREE)[0].shape
        )
        tp[VegType == SHRUB] = np.random.randint(
            0, self._tpmax_sh, np.where(VegType == SHRUB)[0].shape
        )
        locs_trees = np.where(VegType == TREE)[0]
        locs_shrubs = np.where(VegType == SHRUB)[0]
        VegType[locs_trees[tp[locs_trees] < self._tpmax_tr_s]] = TREESEEDLING
        VegType[locs_shrubs[tp[locs_shrubs] < self._tpmax_sh_s]] = SHRUBSEEDLING
        grid["cell"]["plant__age"] = tp.astype(float)

    @property
    def Edit_VegCov(self):
        """Flag to indicate whether an optional field is created.

        If Edit_VegCov=True, an optional field
        'vegetation__boolean_vegetated' will be output, (i.e.) if a cell
        is vegetated the corresponding cell of the field will be 1,
        otherwise it will be 0.
        """
        return self._Edit_VegCov

    @Edit_VegCov.setter
    def Edit_VegCov(self, Edit_VegCov):
        assert isinstance(Edit_VegCov, bool)
        self._Edit_VegCov = Edit_VegCov

    def update(self, dt=1):
        """Update fields with current loading conditions.

        Parameters
        ----------
        dt: int, optional
            Time elapsed - time step (years).
        """
        self._VegType = self._cell_values["vegetation__plant_functional_type"]
        self._CumWS = self._cell_values["vegetation__cumulative_water_stress"]
        self._live_index = self._cell_values["plant__live_index"]
        self._tp = self._cell_values["plant__age"] + dt

        # Check if shrub and tree seedlings have matured
        shrub_seedlings = np.where(self._VegType == SHRUBSEEDLING)[0]
        tree_seedlings = np.where(self._VegType == TREESEEDLING)[0]
        matured_shrubs = np.where(self._tp[shrub_seedlings] > self._tpmax_sh_s)[0]
        matured_trees = np.where(self._tp[tree_seedlings] > self._tpmax_tr_s)[0]
        self._VegType[shrub_seedlings[matured_shrubs]] = SHRUB
        self._VegType[tree_seedlings[matured_trees]] = TREE
        self._tp[shrub_seedlings[matured_shrubs]] = 0
        self._tp[tree_seedlings[matured_trees]] = 0

        # Establishment
        self._live_index = 1 - self._CumWS  # Plant live index = 1 - WS
        bare_cells = np.where(self._VegType == BARE)[0]
        n_bare = len(bare_cells)
        first_ring = self._grid.looped_neighbors_at_cell[bare_cells]
        second_ring = self._grid.second_ring_looped_neighbors_at_cell[bare_cells]
        veg_type_fr = self._VegType[first_ring]
        veg_type_sr = self._VegType[second_ring]
        Sh_WS_fr = WS_PFT(veg_type_fr, SHRUB, self._live_index[first_ring])
        Tr_WS_fr = WS_PFT(veg_type_fr, TREE, self._live_index[first_ring])
        Tr_WS_sr = WS_PFT(veg_type_sr, TREE, self._live_index[second_ring])

        n = count(veg_type_fr, SHRUB)
        Phi_sh = Sh_WS_fr / 8.0
        Phi_tr = (Tr_WS_fr + Tr_WS_sr / 2.0) / 8.0
        Phi_g = np.mean(self._live_index[np.where(self._VegType == GRASS)])
        Pemaxg = self._Pemaxg * np.ones(n_bare)
        Pemaxsh = self._Pemaxsh * np.ones(n_bare)
        Pemaxtr = self._Pemaxtr * np.ones(n_bare)
        Peg = np.amin(np.vstack((Phi_g / (n * self._INg), Pemaxg)), axis=0)
        Pesh = np.amin(np.vstack((Phi_sh, Pemaxsh)), axis=0)
        Petr = np.amin(np.vstack((Phi_tr, Pemaxtr)), axis=0)
        Select_PFT_E = np.random.choice([GRASS, SHRUBSEEDLING, TREESEEDLING], n_bare)
        # Grass - 0; Shrub Seedling - 4; Tree Seedling - 5
        Pest = np.choose(Select_PFT_E, [Peg, 0, 0, 0, Pesh, Petr])
        # Probability of establishment
        R_Est = np.random.rand(n_bare)
        # Random number for comparison to establish
        Establish = np.int32(np.where(np.greater_equal(Pest, R_Est))[0])
        self._VegType[bare_cells[Establish]] = Select_PFT_E[Establish]
        self._tp[bare_cells[Establish]] = 0

        # Mortality
        plant_cells = np.where(self._VegType != BARE)[0]
        n_plant = len(plant_cells)
        Theta = np.choose(
            self._VegType[plant_cells],
            [self._th_g, self._th_sh, self._th_tr, 0, self._th_sh_s, self._th_tr_s],
        )
        PMd = self._CumWS[plant_cells] - Theta
        PMd[PMd < 0.0] = 0.0
        tpmax = np.choose(
            self._VegType[plant_cells],
            [
                200000,
                self._tpmax_sh,
                self._tpmax_tr,
                0,
                self._tpmax_sh_s,
                self._tpmax_tr_s,
            ],
        )
        PMa = np.zeros(n_plant)
        tp_plant = self._tp[plant_cells]
        tp_greater = np.where(tp_plant > 0.5 * tpmax)[0]
        PMa[tp_greater] = (
            (tp_plant[tp_greater] - 0.5 * tpmax[tp_greater]) / (0.5 * tpmax[tp_greater])
        ) - 1
        PMb = np.choose(
            self._VegType[plant_cells],
            [
                self._Pmb_g,
                self._Pmb_sh,
                self._Pmb_tr,
                0,
                self._Pmb_sh_s,
                self._Pmb_tr_s,
            ],
        )
        PM = PMd + PMa + PMb
        PM[PM > 1.0] = 1.0
        R_Mor = np.random.rand(n_plant)  # Random number for comparison to kill
        Mortality = np.int32(np.where(np.greater_equal(PM, R_Mor))[0])
        self._VegType[plant_cells[Mortality]] = BARE
        self._tp[plant_cells[Mortality]] = 0

        self._cell_values["plant__age"] = self._tp

        if self._Edit_VegCov:
            self._grid["cell"]["vegetation__boolean_vegetated"] = np.zeros(
                self._grid.number_of_cells, dtype=int
            )
            self._grid["cell"]["vegetation__boolean_vegetated"][
                self._VegType != BARE
            ] = 1

        # For debugging purposes
        self._bare_cells = bare_cells
        self._Established = bare_cells[Establish]
        self._plant_cells = plant_cells
        self._Mortified = plant_cells[Mortality]


def count(Arr, value):
    Res = np.zeros(Arr.shape[0], dtype=int)
    x, y = Arr.shape
    for i in range(0, x):
        for j in range(0, y):
            if Arr[i][j] == value:
                Res[i] += 1
    return Res


def WS_PFT(VegType, PlantType, WS):
    Phi = np.zeros(WS.shape[0])
    x, y = WS.shape
    for i in range(0, x):
        for j in range(0, y):
            if VegType[i][j] == PlantType:
                Phi[i] += WS[i][j]
    return Phi



================================================
File: potentiality_flowrouting/__init__.py
================================================
from .route_flow_by_boundary import PotentialityFlowRouter

__all__ = ["PotentialityFlowRouter"]



================================================
File: potentiality_flowrouting/route_flow_by_boundary.py
================================================
"""This is an implementation of Vaughan Voller's experimental boundary method
reduced complexity flow router. Credit: Voller, Hobley, Paola.

Created on Fri Feb 20 09:32:27 2015

@author: danhobley (SiccarPoint), after volle001@umn.edu
"""

# ##in the diagonal case, even closed edges can produce "drag". Is this right?
# Could suppress by mirroring the diagonals

import numpy as np

from landlab import Component
from landlab import LinkStatus
from landlab import RasterModelGrid


class PotentialityFlowRouter(Component):
    """Multidirectional flow routing using a novel method.

    This class implements Voller, Hobley, and Paola's experimental matrix
    solutions for flow routing. The method works by solving for a potential
    field at all nodes on the grid, which enforces both mass conservation
    and flow downhill along topographic gradients. It is order n and highly
    efficient, but does not return any information about flow connectivity.

    Options are permitted to allow "abstract" routing (flow enforced downslope,
    but no particular assumptions are made about the governing equations), or
    routing according to the Chezy or Manning equations. This routine assumes
    that water is distributed evenly over the surface of the cell in deriving
    the depth, and does not assume channelization. You will need to back-
    calculate channel depths for yourself using known widths at each node
    if that is what you want.

    It is VITAL you initialize this component AFTER setting boundary
    conditions.

    If Manning or Chezy specified, the surface_water__depth is the depth of
    flow in the cell, calculated assuming flow occurs over the whole surface.

    Note that this component offers the property `discharges_at_links`. This
    returns the discharges at all links. If method=='D8', this list will
    include diagonal links after the orthogonal links, which is why this
    information is not returned as a field.

    Discharges at nodes are recorded as the outgoing total discharge (i.e.,
    including any contribution from 'water__unit_flux_in').

    The primary method of this class is :func:`run_one_step`.

    Notes
    -----
    This is a "research grade" component, and is subject to dramatic change
    with little warning. No guarantees are made regarding its accuracy or
    utility. It is not recommended for user use yet!

    Examples
    --------
    >>> from landlab import HexModelGrid
    >>> import numpy as np
    >>> mg = HexModelGrid(
    ...     (4, 6), spacing=2.0, node_layout="rect", orientation="vertical"
    ... )
    >>> z = mg.add_zeros("topographic__elevation", at="node")
    >>> Q_in = mg.add_ones("water__unit_flux_in", at="node")
    >>> z += mg.node_y.copy()
    >>> potfr = PotentialityFlowRouter(mg)
    >>> potfr.run_one_step()
    >>> mg.at_node["surface_water__discharge"][mg.core_nodes]
    array([11.70706863, 11.5709712 , 10.41329927,  9.24959728,
            6.65448576,  6.39262702,  5.71410162,  5.04743495])

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    None Listed

    """

    _name = "PotentialityFlowRouter"

    _unit_agnostic = False

    _info = {
        "flow__potential": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m**3/s",
            "mapping": "node",
            "doc": (
                "Value of the hypothetical field 'K', used to force water "
                "flux to flow downhill"
            ),
        },
        "surface_water__depth": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of water on the surface",
        },
        "surface_water__discharge": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m**3/s",
            "mapping": "node",
            "doc": "Volumetric discharge of surface water",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "water__unit_flux_in": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m/s",
            "mapping": "node",
            "doc": (
                "External volume water per area per time input to each node "
                "(e.g., rainfall rate)"
            ),
        },
    }

    _min_slope_thresh = 1.0e-24
    # if your flow isn't connecting up, this probably needs to be reduced

    def __init__(
        self, grid, method="D8", flow_equation="default", Chezys_C=30.0, Mannings_n=0.03
    ):
        """
        Parameters
        ----------
        grid : ModelGrid
            A grid.
        method : {'D8', 'D4'}, optional
            Routing method ('D8' is the default). This keyword has no effect for a
            Voronoi-based grid.
        flow_equation : {'default', 'Manning', 'Chezy'}, optional
            If Manning or Chezy, flow is routed according to the Manning or Chezy
            equation; discharge is allocated to multiple downslope nodes
            proportional to the square root of discharge; and a water__depth field
            is returned. If default, flow is allocated to multiple nodes linearly
            with slope; and the water__depth field is not calculated.
        Chezys_C : float (optional)
            Required if flow_equation == 'Chezy'.
        Mannings_n : float (optional)
            Required if flow_equation == 'Manning'.
        """
        super().__init__(grid)

        if isinstance(grid, RasterModelGrid):
            assert grid.number_of_node_rows >= 3
            assert grid.number_of_node_columns >= 3
            self._raster = True
        else:
            self._raster = False

        self._equation = flow_equation
        assert self._equation in ("default", "Chezy", "Manning")
        if self._equation == "Chezy":
            self._chezy_C = Chezys_C
        elif self._equation == "Manning":
            self._manning_n = Mannings_n
        assert method in ("D8", "D4")
        if method == "D8":
            self._route_on_diagonals = True
        else:
            self._route_on_diagonals = False

        self.initialize_output_fields()

        if self._raster:
            self._equiv_circ_diam = 2.0 * np.sqrt(grid.dx * grid.dy / np.pi)
        else:
            for_cell_areas = 2.0 * np.sqrt(grid.area_of_cell / np.pi)
            mean_A = for_cell_areas.mean()
            self._equiv_circ_diam = for_cell_areas[grid.cell_at_node]
            self._equiv_circ_diam[grid.cell_at_node == -1] = mean_A
        # ^this is the equivalent seen CSWidth of a cell for a flow in a
        # generic 360 direction
        if self._route_on_diagonals and self._raster:
            self._discharges_at_link = np.empty(grid.number_of_d8)
        else:
            self._discharges_at_link = self._grid.empty(at="link")

    def run_one_step(self):
        """Route surface-water flow over a landscape.

        Both convergent and divergent flow can occur.
        """
        grid = self._grid
        self._K = grid.at_node["flow__potential"]
        self._Qw = grid.at_node["surface_water__discharge"]
        z = grid.at_node["topographic__elevation"]
        qwater_in = grid.at_node["water__unit_flux_in"].copy()
        qwater_in[grid.node_at_cell] *= grid.area_of_cell
        prev_K = self._K.copy()
        mismatch = 10000.0
        # do the ortho nodes first, in isolation
        g = grid.calc_grad_at_link(z)
        if self._equation != "default":
            g = np.sign(g) * np.sqrt(np.fabs(g))
            # ^...because both Manning and Chezy actually follow sqrt
            # slope, not slope
        # weight by face width - NO, because diags
        # g *= grid.length_of_face[grid.face_at_link]
        link_grad_at_node_w_dir = g[grid.links_at_node] * grid.active_link_dirs_at_node
        # active_link_dirs strips "wrong" face widths

        # now outgoing link grad sum
        outgoing_sum = (
            np.sum((link_grad_at_node_w_dir).clip(0.0), axis=1) + self._min_slope_thresh
        )
        pos_incoming_link_grads = (-link_grad_at_node_w_dir).clip(0.0)

        if not self._route_on_diagonals or not self._raster:
            while mismatch > 1.0e-6:
                K_link_ends = self._K[grid.adjacent_nodes_at_node]
                incoming_K_sum = (pos_incoming_link_grads * K_link_ends).sum(
                    axis=1
                ) + self._min_slope_thresh
                self._K[:] = (incoming_K_sum + qwater_in) / outgoing_sum
                mismatch = np.sum(np.square(self._K - prev_K))
                prev_K = self._K.copy()

            upwind_K = grid.map_value_at_max_node_to_link(z, self._K)
            self._discharges_at_link[:] = upwind_K * g
            self._discharges_at_link[grid.status_at_link == LinkStatus.INACTIVE] = 0.0
        else:
            # grad on diags:
            gwd = np.empty(grid.number_of_d8, dtype=float)
            gd = gwd[grid.number_of_links :]
            gd[:] = z[grid.nodes_at_diagonal[:, 1]] - z[grid.nodes_at_diagonal[:, 0]]
            gd /= grid.length_of_d8[grid.number_of_links :]
            if self._equation != "default":
                gd[:] = np.sign(gd) * np.sqrt(np.fabs(gd))
            diag_grad_at_node_w_dir = (
                gwd[grid.d8s_at_node[:, 4:]] * self._grid.active_diagonal_dirs_at_node
            )

            outgoing_sum += np.sum(diag_grad_at_node_w_dir.clip(0.0), axis=1)
            pos_incoming_diag_grads = (-diag_grad_at_node_w_dir).clip(0.0)
            while mismatch > 1.0e-6:
                K_link_ends = self._K[grid.adjacent_nodes_at_node]
                K_diag_ends = self._K[grid.diagonal_adjacent_nodes_at_node]
                incoming_K_sum = (
                    (pos_incoming_link_grads * K_link_ends).sum(axis=1)
                    + (pos_incoming_diag_grads * K_diag_ends).sum(axis=1)
                    + self._min_slope_thresh
                )
                self._K[:] = (incoming_K_sum + qwater_in) / outgoing_sum
                mismatch = np.sum(np.square(self._K - prev_K))
                prev_K = self._K.copy()

            # ^this is necessary to suppress stupid apparent link Qs at flow
            # edges, if present.
            upwind_K = grid.map_value_at_max_node_to_link(z, self._K)
            upwind_diag_K = np.where(
                z[grid.nodes_at_diagonal[:, 1]] > z[grid.nodes_at_diagonal[:, 0]],
                self._K[grid.nodes_at_diagonal[:, 1]],
                self._K[grid.nodes_at_diagonal[:, 0]],
            )
            self._discharges_at_link[: grid.number_of_links] = upwind_K * g
            self._discharges_at_link[grid.number_of_links :] = upwind_diag_K * gd
            self._discharges_at_link[grid.status_at_d8 == LinkStatus.FIXED] = 0.0

        np.multiply(self._K, outgoing_sum, out=self._Qw)
        # there is no sensible way to save discharges at links, if we route
        # on diagonals.
        # for now, let's make a property

        # now process uval and vval to give the depths, if Chezy or Manning:
        if self._equation == "Chezy":
            # Chezy: Q = C*Area*sqrt(depth*slope)
            grid.at_node["surface_water__depth"][:] = (
                grid.at_node["flow__potential"] / self._chezy_C / self._equiv_circ_diam
            ) ** (2.0 / 3.0)
        elif self._equation == "Manning":
            # Manning: Q = w/n*depth**(5/3)
            grid.at_node["surface_water__depth"][:] = (
                grid.at_node["flow__potential"]
                * self._manning_n
                / self._equiv_circ_diam
            ) ** 0.6
        else:
            pass

    @property
    def discharges_at_links(self):
        """Return the discharges at links.

        Note that if diagonal routing, this will return number_of_d8.
        Otherwise, it will be number_of_links.
        """
        return self._discharges_at_link



================================================
File: priority_flood_flow_router/README.md
================================================
# FlowDirAccPf: efficient filling and flow routing



``FlowDirAccPf`` is a ``Landlab`` component that provides an alternative and efficent approach to fill or breach DEMs, calculate flow directions and update flow accumulations. The component is restricted to structured grids and contains a wrapper for the RichDEM python package [@barnes2016parallel,@barnes2017parallel]. [``RichDEM``](https://richdem.readthedocs.io/en/latest/intro.html) is a set of hydrologic analysis tools using parallel processing to process large DEMs and calculate hydrologic properties.

#TODO

FlowDirAccPf is introduced [in this paper]()


## Documentation and installation

Landlab documentation is hosted on this [ReadTheDocs page](https://landlab.csdms.io/),
including instructions to install Landlab. ``FlowDirAccPf`` is installed with
Landlab.

#TODO

``FlowDirAccPf`` documentation is located [here](https://landlab.csdms.io/generated/api/landlab.components.priority_flood_flow_router.priority_flood_flow_router.html).

## FlowDirAccPf tutorial

A ``FlowDirAccPf`` tutorial exists in the form of a Jupyter Notebooks accessible
through the following links:

#TODO adjust links when in main branch. [This](https://github.com/BCampforts/landlab/blob/bc/priority_flood/notebooks/tutorials/flow_direction_and_accumulation/PriorityFlood_realDEMs.ipynb) is a direct link to the notebook.

- [Launch the tutorial](https://mybinder.org/v2/gh/BCampforts/landlab/blob/bc/priority_flood/notebooks/tutorials/PriorityFlood/PriorityFlood_realDEMs.ipynb)
as interactive notebook in your browser, with no need to install software,
launched using Binder.
- [A static version of the same tutorial](https://landlab.csdms.io/tutorials/flow_direction_and_accumulation/PriorityFlood_realDEMs.html)
- All Landlab tutorials can be launched from [this directory](https://mybinder.org/v2/gh/landlab/landlab/release?filepath=notebooks/welcome.ipynb) using Binder.

## Get or give help

[Open an Issue here](https://github.com/landlab/landlab/issues) where we can
respond to your questions, comments, issues, ideas, or any identified bugs
related to Landlab including ``FlowDirAccPf``.



================================================
File: priority_flood_flow_router/__init__.py
================================================
from .priority_flood_flow_router import PriorityFloodFlowRouter

__all__ = ["PriorityFloodFlowRouter"]



================================================
File: priority_flood_flow_router/cfuncs.pyx
================================================
cimport cython
import numpy as np

cimport numpy as np

# https://cython.readthedocs.io/en/stable/src/userguide/fusedtypes.html
ctypedef fused id_t:
    cython.integral
    long long


cpdef _D8_flowDir(
    id_t [:] receivers,
    cython.floating [:] distance_receiver,
    cython.floating [:] steepest_slope,
    cython.floating [:] el_dep_free,
    cython.floating [:] el_ori,
    cython.floating [:] dist,
    id_t [:] ngb,
    id_t [:] activeCores,
    id_t [:] activeCells,
    cython.floating [:] el_d,
    long c,
    double dx,
    id_t [:, :] adj_link,
    id_t [:] rec_link,
):
    """
    Calcualte D8 flow dirs
    """
    cdef int idx, i

    # for i in range(0,r*c):
    for i in activeCores:
        ngb[0] = i + 1
        ngb[1] = i + c
        ngb[2] = i - 1
        ngb[3] = i - c
        ngb[4] = i + c + 1
        ngb[5] = i + c - 1
        ngb[6] = i -c - 1
        ngb[7] = i -c + 1

        # Differences after filling can be very small, *1e3 to exaggerate those
        # Set to -1 at boundaries (active cell ==0)
        # If cells have equal slopes, the flow will be directed following
        # Landlab rotational ordening going first to cardial, then to diagonal cells
        el_d[0] = (
            el_dep_free[i] - el_dep_free[i + 1]
        ) * 1e3 * activeCells[i + 1]- 1 + activeCells[i + 1]
        el_d[1] = (
            el_dep_free[i] - el_dep_free[i + c]
        ) * 1e3 * activeCells[i + c] - 1 + activeCells[i + c]
        el_d[2] = (
            el_dep_free[i] - el_dep_free[i -1]
        ) * 1e3 * activeCells[i - 1] - 1 + activeCells[i - 1]
        el_d[3] = (
            el_dep_free[i] - el_dep_free[i - c]
        ) * 1e3 * activeCells[i - c] - 1 + activeCells[i - c]
        el_d[4] = (
            el_dep_free[i] - el_dep_free[i + c + 1]
        ) * 1e3 / np.sqrt(2) * activeCells[i + c + 1] - 1 + activeCells[i + c + 1]
        el_d[5] = (
            el_dep_free[i] - el_dep_free[i + c - 1]
        ) * 1e3 / np.sqrt(2) * activeCells[i + c - 1] - 1 + activeCells[i + c - 1]
        el_d[6] = (
            el_dep_free[i] - el_dep_free[i - c - 1]
        ) * 1e3 / np.sqrt(2) * activeCells[i - c - 1] - 1 + activeCells[i - c - 1]
        el_d[7] = (
            el_dep_free[i] - el_dep_free[i - c + 1]
        ) * 1e3 / np.sqrt(2) * activeCells[i - c + 1] - 1 + activeCells[i - c + 1]

        # check to see if pixel is not a lake, if a lake, drain to itself
        if np.max(el_d) >= 0:
            idx = np.argmax(el_d)
            receivers[i] = ngb[idx]
            distance_receiver[i] = dist[idx]
            rec_link[i] = adj_link[i][idx]
            # Slope over original dem can have negative values, but we set
            # it to zero here in analogy to the other Landlab LakeFiller
            steepest_slope[i] = np.maximum(
                0, (el_ori[i] - el_ori[receivers[i]]) / distance_receiver[i]
            )
        else:
            receivers[i] = i
            rec_link[i] = -1
            steepest_slope[i] = 0

cpdef _D8_FlowAcc(
    cython.floating [:] a,
    cython.floating [:] q,
    id_t [:] stack_flip,
    id_t [:] receivers,
):
    """
    Accumulates drainage area and discharge, permitting transmission losses.
    """
    cdef int donor

    # Work from upstream to downstream.
    for donor in stack_flip:
        rcvr = receivers[donor]
        a[rcvr] += a[donor]
        q[rcvr] += q[donor]



================================================
File: priority_flood_flow_router/priority_flood_flow_router.py
================================================
"""Fill or breach a DEM, accumulate flow and calculate drainage area using
the priority flood algorithm.

PriorityFloodFlowRouter is a wrapper of the RichDEM package:
https://richdem.readthedocs.io/en/latest/flow_metrics.html

The component merges a filling/breaching algorithm, a flow director as well
as a flow accumulator.  Moreover, the component supports the definition of
two flow accumulator fields associated to the same grid.  This prevents the
user from updating the filling/breaching algorithms in between calculation
of flow accumulator one and two.

@author: benjaminCampforts
"""

import copy as cp
from functools import partial

import numpy as np

from landlab import Component
from landlab import FieldError
from landlab import RasterModelGrid
from landlab.grid.nodestatus import NodeStatus
from landlab.utils.return_array import return_array_at_node

from ...utils.suppress_output import suppress_output
from .cfuncs import _D8_FlowAcc
from .cfuncs import _D8_flowDir

# Codes for depression status
_UNFLOODED = 0
_PIT = 1
_CURRENT_LAKE = 2
_FLOODED = 3
# Flow metrics resulting in single flow
PSINGLE_FMs = ["D8", "D4", "Rho8", "Rho4"]
# Flow metrics resulting in multiple flow
PMULTIPLE_FMs = ["Quinn", "Freeman", "Holmgren", "Dinf"]


class PriorityFloodFlowRouter(Component):
    """Component to accumulate flow and calculate drainage area based RICHDEM software package.

    See also: https://richdem.readthedocs.io/en/latest/


    .. note::

        The perimeter nodes  NEVER contribute to the accumulating flux, even
        if the  gradients from them point inwards to the main body of the grid.
        This is because under Landlab definitions, perimeter nodes lack cells, so
        cannot accumulate any discharge.

    *FlowAccumulatorPf* stores as *ModelGrid* fields:

    - *'drainage_area'*: Node array of drainage areas
    - *'flood_status_code'*: Map of flood status (_PIT, _CURRENT_LAKE, _UNFLOODED, or _FLOODED).
    - *'surface_water__discharge'*: Node array of discharges.
    - *'Distance to receiver'*: Distance to receiver
    - *'water__unit_flux_in'*: External volume water per area per time input to each node.
    - *'flow__upstream_node_order'*: Node array containing downstream-to-upstream ordered
      list of node IDs.
    - *'flow__receiver_node'*: Node array of receivers (nodes that receive flow),
      or ITS OWN ID if there is no receiver. This array is 2D for *RouteToMany*
      methods and has the shape *(n-nodes x max number of receivers)*.
    - *'flow__receiver_proportions'*: Node array of flow proportions. This
      array is 2D, for *RouteToMany* methods and has the shape
      *(n-nodes x max number of receivers)*.
    - *'topographic__steepest_slope'*: Node array of downhill slopes from each receiver.
      This array is 2D for *RouteToMany* methods and has the shape
      *(n-nodes x max number of receivers)*.
    - *'flow__link_to_receiver_node'*: Node array of links carrying flow.
    - *'flow__receiver_proportion's*: Node array of proportion of flow sent to each receiver.
    - *'depression_free_elevation'*: Depression free land surface topographic
      elevation, at closed borders, value equals -1.

    The following fields are required when an additional hillslope flowrouting
    scheme is required, can be completed with flow acc and discharge if required:

    - *'hill_flow__upstream_node_order'*: Node array containing downstream-to-upstream
      ordered list of node IDs
    - *'hill_flow__receiver_node'*: Node array of receivers (node that receives flow
      from current node)
    - *'hill_topographic__steepest_slope'*: The steepest *downhill* slope.
    - *'hill_flow__receiver_proportions'*: Node array of proportion of flow sent to each
      receiver

    The primary method of this class is :func:`run_one_step`.

    Parameters
    ----------
    grid : ModelGrid
        A Landlab grid.
    surface : str or array_like, optional
        The surface to direct flow across. An at-node field name or an array
        of length *n_node*.
    flow_metric : str, optional
        String has to be one of 'D8' (O’Callaghan and Mark, 1984), 'Rho8'
        (Fairfield and Leymarie, 1991), 'Quinn' (1991), 'Freeman' (1991),
        'Holmgren' (1994), 'Dinf' (Tarboton, 1997). For details and comparison,
        see https://richdem.readthedocs.io/en/latest/flow_metrics.html
    runoff_rate : str, array_like, or float, optional
        If provided, sets the runoff rate (m / time) and will be assigned to the grid field
        'water__unit_flux_in'. If a spatially and and temporally variable runoff
        rate is desired, pass this field name and update the field through model
        run time. If both the field and argument are present at the time of
        initialization, runoff_rate will *overwrite* the field. If neither are
        set, defaults to spatially constant unit input.
        Both a runoff_rate array and the 'water__unit_flux_in' field are
        permitted to contain negative values, in which case they mimic
        transmission losses rather than e.g. rain inputs.
    update_flow_depressions : bool, optional
        Build-in depression handler. Can be through filling or breaching (see below).
    update_hill_depressions : bool, optional
        Only needed if DEM needs to be filled separately for second (hill flow)
        flow accumulator.  Default behavior is not to execute a separate filling
        procedure in between the first and the second flow accumulator.
    depression_handler : str, optional
        Must be one of 'fill or 'breach'.
        Depression-Filling or breaching algorithm to process depressions

        - 'fill': Depression-Filling.
          Depression-filling is often used to fill in all the depressions
          in a DEM to the level of their lowest outlet or spill-point.
          See also: https://richdem.readthedocs.io/en/latest/depression_filling.html
        - 'breach': Complete Breaching.
          Depression-breaching is used to dig channels from the pit cells
          of a DEM to the nearest cells (in priority-flood sense) outside
          of the depression containing the pit. This resolves the depression
          as all cells in the depression now have a drainage path to the
          edge of the DEM.
          See also: https://richdem.readthedocs.io/en/latest/depression_breaching.html
    exponent : float, optional
        Some methods require an exponent (see flow_metric) Default {1}
    epsilon : bool, optional
        If ``True``, an epsilon gradient is imposed to all flat regions. This ensures
        that there is always a local gradient.
    accumulate_flow : bool, optional
        If ``True`` flow directions and accumulations will be calculated.
        Set to ``False`` when only interested in flow directions
    accumulate_flow_hill : bool, optional
        If ``True`` flow directions and accumulations will be calculated
        for second FD component (Hill). Set to ``False`` when only interested in flow
        directions.
    separate_hill_flow : bool, optional
        For some applications (e.g. *HyLands*) both single and
        multiple flow direction and accumulation is required.
        By calculating them in the same component, we can optimize procedures
        involved with filling and breaching of DEMs
    update_hill_flow_instantaneous : bool, optional
        Update separate hillslope director and accumulator simultaneously on update.
        Set if other operations have to be performed in between updating the
        principle flow properties and the hillslope properties.
    hill_flow_metric : str, optional
        Must be one 'D8' (O’Callaghan and Mark, 1984),'D4' (O’Callaghan and Mark, 1984),
        'Rho8' (Fairfield and Leymarie, 1991), 'Rho4' (Fairfield and Leymarie, 1991),
        'Quinn' (1991) {default},'Freeman' (1991), 'Holmgren' (1994),
        'Dinf' (Tarboton, 1997).
        For details and comparison, see
        https://richdem.readthedocs.io/en/latest/flow_metrics.html
    hill_exponent : float, optional
        Some methods require an exponent (see flow_metric)
    suppress_out : bool, optional
        Suppress verbose of priority flood algorithm


    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Barnes, R., 2017. Parallel non-divergent flow accumulation for trillion
    cell digital elevation models on desktops or clusters. Environmental
    Modelling & Software 92, 202–212. doi: 10.1016/j.envsoft.2017.02.022

    **Additional References**

    https://richdem.readthedocs.io/en/latest/

    """

    _name = "PriorityFloodFlowRouter"

    _unit_agnostic = True

    _info = {
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "drainage_area": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m**2",
            "mapping": "node",
            "doc": "Upstream accumulated surface area contributing to the node's discharge",
        },
        "flood_status_code": {
            "dtype": int,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Map of flood status (_PIT, _CURRENT_LAKE, _UNFLOODED, or _FLOODED).",
        },
        "flow__upstream_node_order": {
            "dtype": int,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array containing downstream-to-upstream ordered list of node IDs",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "surface_water__discharge": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m**3/s",
            "mapping": "node",
            "doc": "Volumetric discharge of surface water",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "water__unit_flux_in": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "m/s",
            "mapping": "node",
            "doc": (
                "External volume water per area per time input to each node "
                "(e.g., rainfall rate)"
            ),
        },
        "topographic__steepest_slope": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "The steepest *downhill* slope",
        },
        "squared_length_adjacent": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": (
                "Length to adjacent nodes, squared (calcualted in advance to "
                "save time during calculation"
            ),
        },
        "flow__receiver_proportions": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of proportion of flow sent to each receiver.",
        },
        "depression_free_elevation": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": (
                "Filled land surface topographic elevation, at closed borders, "
                "value equals -1!"
            ),
        },
        # The following fields are required when an additional
        # hillslope flowrouting scheme is required, can be completed
        # with flow acc and discharge if required
        "hill_drainage_area": {
            "dtype": float,
            "intent": "out",
            "optional": True,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of proportion of flow sent to each receiver.",
        },
        "hill_surface_water__discharge": {
            "dtype": float,
            "intent": "out",
            "optional": True,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of proportion of flow sent to each receiver.",
        },
        "hill_flow__upstream_node_order": {
            "dtype": int,
            "intent": "out",
            "optional": True,
            "units": "-",
            "mapping": "node",
            "doc": "Node array containing downstream-to-upstream ordered list of node IDs",
        },
        "hill_flow__receiver_node": {
            "dtype": int,
            "intent": "out",
            "optional": True,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "hill_topographic__steepest_slope": {
            "dtype": float,
            "intent": "out",
            "optional": True,
            "units": "-",
            "mapping": "node",
            "doc": "The steepest *downhill* slope",
        },
        "hill_flow__receiver_proportions": {
            "dtype": float,
            "intent": "out",
            "optional": True,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of proportion of flow sent to each receiver.",
        },
    }

    def __init__(
        self,
        grid,
        surface="topographic__elevation",
        flow_metric="D8",
        runoff_rate=None,
        update_flow_depressions=True,
        depression_handler="fill",
        exponent=1,
        epsilon=True,
        accumulate_flow=True,
        accumulate_flow_hill=False,
        separate_hill_flow=False,
        update_hill_depressions=False,
        update_hill_flow_instantaneous=True,
        hill_flow_metric="Quinn",
        hill_exponent=1,
        suppress_out=True,
    ):
        """Initialize the FlowAccumulator component.

        Saves the grid, tests grid type, tests input types and
        compatibility for the flow_metric and depression_finder
        keyword arguments, tests the argument of runoff, and
        initializes new fields.
        """
        self._richdem = self.load_richdem()

        super().__init__(grid)
        # Keep a local reference to the grid

        self._suppress_output = partial(
            suppress_output, out=suppress_out, err=suppress_out
        )

        # STEP 1: Testing of input values, supplied either in function call or
        # as part of the grid.
        self._validate_water_inputs(grid, runoff_rate)

        # Grid type testing
        if not isinstance(self._grid, RasterModelGrid):
            raise FieldError(
                "Flow Accumulator Priority flood only works with regular raster grids, "
                "use default Landlab flow accumulator instead"
            )

        # save elevations as class properites.
        self._surface = surface
        self._surface_values = return_array_at_node(grid, surface)

        node_cell_area = self._grid.cell_area_at_node.copy()
        node_cell_area[self._grid.closed_boundary_nodes] = 0.0
        self._node_cell_area = node_cell_area
        self._runoff_rate = runoff_rate

        if (flow_metric in PSINGLE_FMs) or (flow_metric in PMULTIPLE_FMs):
            self._flow_metric = flow_metric
        else:
            raise ValueError(
                "flow metric should be one of these single flow directors: "
                f"{', '.join(repr(x) for x in PSINGLE_FMs)} or multiple flow directors: "
                f"{', '.join(repr(x) for x in PMULTIPLE_FMs)}"
            )
        if (hill_flow_metric in PSINGLE_FMs) or (hill_flow_metric in PMULTIPLE_FMs):
            self._hill_flow_metric = hill_flow_metric
        else:
            raise ValueError(
                "flow metric should be one of these single flow directors:"
                f"{', '.join(repr(x) for x in PSINGLE_FMs)} or multiple flow directors: "
                f"{', '.join(repr(x) for x in PMULTIPLE_FMs)}"
            )

        if depression_handler in ("fill", "breach"):
            self._depression_handler = depression_handler
        else:
            raise ValueError(
                "depression_handler should be one of 'fill' or 'breach'"
                f" (got {depression_handler!r})"
            )

        self._epsilon = epsilon
        self._exponent = exponent
        self._separate_hill_flow = separate_hill_flow
        self._update_hill_flow_instantaneous = update_hill_flow_instantaneous

        self._update_flow_depressions = update_flow_depressions
        self._update_hill_depressions = update_hill_depressions

        self._hill_exponent = hill_exponent

        if self._separate_hill_flow:
            # Adjust dict
            self._info["hill_drainage_area"]["optional"] = False
            self._info["hill_surface_water__discharge"]["optional"] = False
            self._info["hill_flow__upstream_node_order"]["optional"] = False
            self._info["hill_flow__receiver_node"]["optional"] = False
            self._info["hill_topographic__steepest_slope"]["optional"] = False
            self._info["hill_flow__receiver_proportions"]["optional"] = False
        else:
            self._info["hill_drainage_area"]["optional"] = True
            self._info["hill_surface_water__discharge"]["optional"] = True
            self._info["hill_flow__upstream_node_order"]["optional"] = True
            self._info["hill_flow__receiver_node"]["optional"] = True
            self._info["hill_topographic__steepest_slope"]["optional"] = True
            self._info["hill_flow__receiver_proportions"]["optional"] = True

        self._accumulate_flow = accumulate_flow
        self._accumulate_flow_hill = accumulate_flow_hill

        if not self._accumulate_flow:
            self._info["drainage_area"]["optional"] = True
            self._info["surface_water__discharge"]["optional"] = True
        else:
            self._info["drainage_area"]["optional"] = False
            self._info["surface_water__discharge"]["optional"] = False

        if not self._accumulate_flow_hill:
            self._info["hill_drainage_area"]["optional"] = True
            self._info["hill_surface_water__discharge"]["optional"] = True
        else:
            self._info["hill_drainage_area"]["optional"] = False
            self._info["hill_surface_water__discharge"]["optional"] = False

        self.initialize_output_fields()

        # Make aliases
        if self._accumulate_flow:
            self._drainage_area = self.grid.at_node["drainage_area"]
            self._discharges = self.grid.at_node["surface_water__discharge"]
        self._sort = self.grid.at_node["flow__upstream_node_order"]
        # if multiple flow algorithm is made, the dimensions of the slope
        # and receiver fields change (8 colums for all neightbors)
        if flow_metric in PMULTIPLE_FMs:
            self.grid.at_node["topographic__steepest_slope"] = np.zeros(
                (self.grid.number_of_nodes, 8)
            )
            self.grid.at_node["flow__receiver_node"] = np.zeros(
                (self.grid.number_of_nodes, 8), dtype=int
            )
            self.grid.at_node["flow__receiver_proportions"] = np.zeros(
                (self.grid.number_of_nodes, 8)
            )
            self.grid.at_node["flow__link_to_receiver_node"] = np.zeros(
                (self.grid.number_of_nodes, 8)
            )
        self._slope = self.grid.at_node["topographic__steepest_slope"]
        self._rcvs = self.grid.at_node["flow__receiver_node"]
        self._prps = self.grid.at_node["flow__receiver_proportions"]
        self._recvr_link = self.grid.at_node["flow__link_to_receiver_node"]

        if self._separate_hill_flow:
            if self._accumulate_flow_hill:
                self._hill_drainage_area = self.grid.at_node["hill_drainage_area"]
                self._hill_discharges = self.grid.at_node[
                    "hill_surface_water__discharge"
                ]
            if hill_flow_metric in PMULTIPLE_FMs:
                self.grid.at_node["hill_topographic__steepest_slope"] = np.zeros(
                    (self.grid.number_of_nodes, 8)
                )
                self.grid.at_node["hill_flow__receiver_node"] = np.zeros(
                    (self.grid.number_of_nodes, 8), dtype=int
                )
                self.grid.at_node["hill_flow__receiver_proportions"] = np.zeros(
                    (self.grid.number_of_nodes, 8)
                )
            self._hill_slope = self.grid.at_node["hill_topographic__steepest_slope"]
            self._hill_rcvs = self.grid.at_node["hill_flow__receiver_node"]
            self._hill_prps = self.grid.at_node["hill_flow__receiver_proportions"]

        # Create properties specific to RichDEM
        self._create_richdem_properties()

    @staticmethod
    def load_richdem():
        try:
            import _richdem  # noqa: F401
            import richdem
        except ModuleNotFoundError as exc:
            raise ModuleNotFoundError(
                "PriorityFloodFlowRouter requires richdem but richdem is not installed. "
                "You can install richdem either from source "
                "(https://github.com/r-barnes/richdem), or through conda "
                "(conda install richdem -c conda-forge) or pip (pip install richdem)."
            ) from exc
        return richdem

    @property
    def surface_values(self):
        """Values of the surface over which flow is directed."""
        return self._surface_values

    def _changed_surface(self):
        """Check if the surface values have changed.

        If the surface values are stored as a field, it is important to
        check if they have changed since the component was instantiated.
        """
        if isinstance(self._surface, str):
            self._surface_values = return_array_at_node(self._grid, self._surface)

    @property
    def node_drainage_area(self):
        """Return the drainage area."""
        return self._grid["node"]["drainage_area"]

    @property
    def node_water_discharge(self):
        """Return the surface water discharge."""
        return self._grid["node"]["surface_water__discharge"]

    def _create_richdem_properties(self):
        self._depression_free_dem = cp.deepcopy(
            self._richdem.rdarray(
                self._surface_values.reshape(self.grid.shape),
                no_data=-9999,
            )
        )
        self._depression_free_dem.geotransform = [0, 1, 0, 0, 0, -1]

        # Calculate SQUARED length adjacent
        self.grid.at_node["squared_length_adjacent"] = np.concatenate(
            (
                np.ones((self.grid.number_of_nodes, 4)),
                2 * np.ones((self.grid.number_of_nodes, 4)),
            ),
            axis=1,
        )

        self._closed = np.zeros(self._grid.number_of_nodes, dtype=np.uint8)
        self._closed[self._grid.status_at_node == NodeStatus.CLOSED] = 1
        self._closed = self._richdem.rdarray(
            self._closed.reshape(self._grid.shape), no_data=-9999
        )
        self._closed.geotransform = [0, 1, 0, 0, 0, -1]

    def _validate_water_inputs(self, grid, runoff_rate):
        """Test inputs for runoff_rate and water__unit_flux_in."""

        if "water__unit_flux_in" not in grid.at_node:
            grid.add_empty("water__unit_flux_in", at="node")
            runoff_rate = 1.0 if runoff_rate is None else runoff_rate
            grid.at_node["water__unit_flux_in"][:] = runoff_rate
        elif runoff_rate is not None:
            grid.at_node["water__unit_flux_in"][:] = runoff_rate

    def calc_flow_dir_acc(self, hill_flow=False, update_depressions=True):
        """Calculate flow direction and accumulation using the richdem package"""
        if hill_flow:
            flow_metric = self._hill_flow_metric
        else:
            flow_metric = self._flow_metric

        # 1: Remove depressions
        if update_depressions:
            self.remove_depressions(flow_metric=flow_metric)
        if flow_metric == "D8":
            self._FlowAcc_D8(hill_flow=hill_flow)
        else:
            closed_boundary_values = self._depression_free_dem[self._closed == 1]
            self._depression_free_dem[self._closed == 1] = np.inf
            # Calculate flow direction (proportion) and accumulation using RichDEM
            with self._suppress_output():
                props_Pf = self._richdem.FlowProportions(
                    dem=self._depression_free_dem,
                    method=flow_metric,
                    exponent=self._exponent,
                )
            self._depression_free_dem[self._closed == 1] = closed_boundary_values

            # Calculate flow accumulation using RichDEM
            if (hill_flow and self._accumulate_flow_hill) or (
                not hill_flow and self._accumulate_flow
            ):
                self._accumulate_flow_RD(props_Pf, hill_flow=hill_flow)

            # Convert flow proportions to landlab structure
            props_Pf = props_Pf.reshape(
                props_Pf.shape[0] * props_Pf.shape[1], props_Pf.shape[2]
            )
            props_Pf_col0 = props_Pf[:, 0]
            props_Pf = np.column_stack(
                (
                    props_Pf[:, 5],
                    props_Pf[:, 7],
                    props_Pf[:, 1],
                    props_Pf[:, 3],
                    props_Pf[:, 6],
                    props_Pf[:, 8],
                    props_Pf[:, 2],
                    props_Pf[:, 4],
                )
            )
            rcvrs = np.concatenate(
                (
                    self._grid.adjacent_nodes_at_node,
                    self._grid.diagonal_adjacent_nodes_at_node,
                ),
                axis=1,
            )
            rcvrs[props_Pf <= 0] = -1
            val = np.arange(0, props_Pf.shape[0])
            rcvrs[props_Pf_col0 == -1, 0] = val[props_Pf_col0 == -1]

            # Links
            recvr_link = np.array(self._grid.d8s_at_node)
            recvr_link[props_Pf <= 0] = -1

            slope_temp = (
                self._surface_values.reshape(-1, 1) - self._surface_values[rcvrs]
            ) / (self.grid.dx * np.sqrt(self.grid.at_node["squared_length_adjacent"]))

            if flow_metric in PSINGLE_FMs:
                slope_temp[rcvrs == -1] = 0
            else:
                props_Pf[props_Pf_col0 == -1, 0] = 1
                props_Pf = props_Pf.astype(np.float64)  # should be float64
                # Now, make sure sum is 1 in 64 bits
                props_Pf[props_Pf == -1] = 0
                proportion_matrix = np.tile(
                    np.reshape(props_Pf.sum(axis=1), [props_Pf.shape[0], 1]), (1, 8)
                )
                rc64_temp = np.where(
                    proportion_matrix == 0, props_Pf, props_Pf / proportion_matrix
                )
                props_Pf[props_Pf[:, 0] != 1, :] = rc64_temp[props_Pf[:, 0] != 1, :]
                props_Pf[props_Pf == 0] = -1

            if hill_flow:
                if flow_metric in PSINGLE_FMs:
                    ij_at_max = range(len(rcvrs)), np.argmax(rcvrs, axis=1)
                    self._hill_prps[:] = props_Pf[ij_at_max]
                    self._hill_rcvs[:] = rcvrs[ij_at_max]
                    self._hill_slope[:] = slope_temp[ij_at_max]
                else:
                    self._hill_prps[:] = props_Pf
                    self._hill_rcvs[:] = rcvrs
                    self._hill_slope[:] = slope_temp
                    self._hill_slope[rcvrs == -1] = 0

            else:
                if flow_metric in PSINGLE_FMs:
                    ij_at_max = range(len(rcvrs)), np.argmax(rcvrs, axis=1)
                    self._prps[:] = props_Pf[ij_at_max]
                    self._rcvs[:] = rcvrs[ij_at_max]
                    self._slope[:] = slope_temp[ij_at_max]
                    self._recvr_link[:] = recvr_link[ij_at_max]
                else:
                    self._prps[:] = props_Pf
                    self._rcvs[:] = rcvrs
                    self._slope[:] = slope_temp
                    self._slope[rcvrs == -1] = 0
                    self._recvr_link[:] = recvr_link

    def _FlowAcc_D8(self, hill_flow=False):
        """
        Function to calcualte flow accumulation using the D8 flow algorithm.

        Parameters
        ----------
        hill_flow : Boolean, optional
            Defines which instance of flow accumulation is updated.
            If FALSE, the first, default instance is updated.
            If TRUE, the second, hillslope, instance is updated.
            The default is False.

        Returns
        -------
        None.

        """
        c = self.grid.number_of_node_columns
        dx = self.grid.dx
        activeCells = np.array(
            self._grid.status_at_node != NodeStatus.CLOSED + 0, dtype=int
        )
        receivers = np.array(self.grid.status_at_node, dtype=int)
        distance_receiver = np.zeros((receivers.shape), dtype=float)
        cores = self.grid.core_nodes
        activeCores = cores[activeCells[cores] == 1]
        # Make boundaries to save time with conditionals in c loops
        receivers[np.nonzero(self._grid.status_at_node)] = -1
        steepest_slope = np.zeros((receivers.shape), dtype=float)
        el_dep_free = self._depression_free_dem.reshape(self.grid.number_of_nodes)
        el_ori = self._surface_values
        dist = np.multiply(
            [1, 1, 1, 1, np.sqrt(2), np.sqrt(2), np.sqrt(2), np.sqrt(2)], dx
        )
        ngb = np.zeros((8,), dtype=int)
        el_d = np.zeros((8,), dtype=float)

        # Links
        adj_link = np.array(self._grid.d8s_at_node, dtype=int)
        recvr_link = np.zeros((receivers.shape), dtype=int) - 1

        _D8_flowDir(
            receivers,
            distance_receiver,
            steepest_slope,
            np.array(el_dep_free),
            el_ori,
            dist,
            ngb,
            activeCores,
            activeCells,
            el_d,
            c,
            dx,
            adj_link,
            recvr_link,
        )

        # Calcualte flow acc
        do_FA = False
        if hill_flow:
            if self._accumulate_flow_hill:
                do_FA = True
                a = self._hill_drainage_area
                q = self._hill_discharges
        else:
            if self._accumulate_flow:
                do_FA = True
                a = self._drainage_area
                q = self._discharges

        if do_FA:
            if any(self.grid.at_node["water__unit_flux_in"] != 1):
                wg_q = (
                    self.grid.at_node["water__unit_flux_in"]
                    * self.grid.dx
                    * self.grid.dx
                )
                # Only core nodes (status == 0) need to receive a weight
                wg_q[np.nonzero(self._grid.status_at_node)] = NodeStatus.CORE
                dis = wg_q

            else:
                dis = np.full(self.grid.number_of_nodes, self._node_cell_area)

            da = np.array(self._node_cell_area)
            stack_flip = np.flip(self._sort)
            # Filter out donors giving to receivers being -1
            stack_flip = stack_flip[receivers[stack_flip] != -1]

            _D8_FlowAcc(da, dis, stack_flip, receivers)

            a[:] = da
            q[:] = dis

        # Closed nodes flow to themselves
        val = np.arange(0, receivers.shape[0])
        receivers[receivers == -1] = val[receivers == -1]

        # Restore depression free DEM
        # self._depression_free_dem[self._closed == 1] = -1

        if hill_flow:
            self._hill_prps[:] = 1
            self._hill_rcvs[:] = receivers
            self._hill_slope[:] = steepest_slope
        else:
            self._prps[:] = 1
            self._rcvs[:] = receivers
            self._slope[:] = steepest_slope
            self._recvr_link[:] = recvr_link

    def remove_depressions(self, flow_metric="D8"):
        self._depression_free_dem = cp.deepcopy(
            self._richdem.rdarray(
                self._surface_values.reshape(self.grid.shape),
                no_data=-9999,
            )
        )
        self._depression_free_dem.geotransform = [0, 1, 0, 0, 0, -1]
        closed_boundary_values = self._depression_free_dem[self._closed == 1]
        self._depression_free_dem[self._closed == 1] = np.inf

        if flow_metric in ("D4", "Rho4"):
            topology = "D4"
        else:
            topology = "D8"
        with self._suppress_output():
            if self._depression_handler == "fill":
                self._richdem.FillDepressions(
                    self._depression_free_dem,
                    epsilon=self._epsilon,
                    in_place=True,
                    topology=topology,
                )

            elif self._depression_handler == "breach":
                self._richdem.BreachDepressions(
                    self._depression_free_dem, in_place=True, topology=topology
                )

        self._sort[:] = np.argsort(
            np.array(self._depression_free_dem.reshape(self.grid.number_of_nodes))
        )

        self._depression_free_dem[self._closed == 1] = closed_boundary_values
        self.grid.at_node["depression_free_elevation"] = self._depression_free_dem

        self.grid.at_node["flood_status_code"] = np.where(
            self.grid.at_node["depression_free_elevation"]
            == self.grid.at_node["topographic__elevation"],
            0,
            3,
        )

    def _accumulate_flow_RD(self, props_Pf, hill_flow=False):
        """
        Function to accumualte flow using the richdem package

        Parameters
        ----------
        props_Pf : float
            flow proportions calcualte with the RichDEM package using the
            FlowProportions function
        hill_flow : Boolean, optional
            Defines which instance of flow accumulation is updated.
            If FALSE, the first, default instance is updated.
            If TRUE, the second, hillslope, instance is updated.
            The default is False.

        Returns
        -------
        None.

        """
        if not hill_flow:
            a = self._drainage_area
            q = self._discharges
        else:
            a = self._hill_drainage_area
            q = self._hill_discharges

        # Create weight for flow accum: both open (status ==1) and closed
        # nodes (status ==4) will have zero weight
        wg = np.full(self.grid.number_of_nodes, self.grid.dx**2)

        # Only core nodes (status == 0) need to receive a weight
        wg[self._grid.status_at_node != NodeStatus.CORE] = 0
        wg = self._richdem.rdarray(
            wg.reshape(self.grid.shape),
            no_data=-9999,
        )
        wg.geotransform = [0, 1, 0, 0, 0, -1]

        with self._suppress_output():
            a[:] = np.array(
                self._richdem.FlowAccumFromProps(props=props_Pf, weights=wg).reshape(
                    self.grid.number_of_nodes
                )
            )

        if any(self.grid.at_node["water__unit_flux_in"] != 1):
            wg = self.grid.at_node["water__unit_flux_in"] * self.grid.dx * self.grid.dx
            # Only core nodes (status == 0) need to receive a weight
            wg[self._grid.status_at_node != NodeStatus.CORE] = 0
            wg = self._richdem.rdarray(wg.reshape(self.grid.shape), no_data=-9999)
            wg.geotransform = [0, 1, 0, 0, 0, -1]
            with self._suppress_output():
                q_pf = self._richdem.FlowAccumFromProps(props=props_Pf, weights=wg)
            q[:] = np.array(q_pf.reshape(self.grid.number_of_nodes))
        else:
            q[:] = self._drainage_area

    def update_hill_fdfa(self, update_depressions=False):
        if not self._separate_hill_flow:
            raise ValueError(
                "If hillslope properties are updated, the separate_hill_flow "
                "property of the PriorityFloodFlowRouter class should be "
                "True upon initialisation"
            )
        self.calc_flow_dir_acc(hill_flow=True, update_depressions=update_depressions)

    def run_one_step(self):
        self.calc_flow_dir_acc(
            hill_flow=False, update_depressions=self._update_flow_depressions
        )
        if self._separate_hill_flow and self._update_hill_flow_instantaneous:
            self.calc_flow_dir_acc(
                hill_flow=True, update_depressions=self._update_hill_depressions
            )



================================================
File: profiler/__init__.py
================================================
# ! /usr/env/python

from .channel_profiler import ChannelProfiler
from .profiler import Profiler
from .trickle_down_profiler import TrickleDownProfiler

__all__ = ["ChannelProfiler", "Profiler", "TrickleDownProfiler"]



================================================
File: profiler/base_profiler.py
================================================
# ! /usr/env/python
"""Base class for profile constructors."""

from abc import ABC
from abc import abstractmethod

import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection

from landlab import Component
from landlab.plot import imshow_grid
from landlab.utils.return_array import return_array_at_node


def _recursive_max(jagged):
    """
    Examples
    --------
    from landlab.components.profiler.base_profiler import _recursive_max
    >>> struct = [[1, 2, 3, 4], [[2, 3, 4, 5], [3, 4, 5, 6]], [4, 5, 6, 7]]
    >>> _recursive_max(struct)
    7
    >>> _recursive_max([100])
    100
    """
    return max(_recursive_max(j) if hasattr(j, "__iter__") else j for j in jagged)


def _recursive_min(jagged):
    """
    Examples
    --------
    from landlab.components.profiler.base_profiler import _recursive_min
    >>> struct = [[1, 2, 3, 4], [[2, 3, 4, 5], [3, 4, 5, 6]], [4, 5, 6, 7]]
    >>> _recursive_min(struct)
    1
    >>> _recursive_min([100])
    100
    """
    return min(_recursive_min(j) if hasattr(j, "__iter__") else j for j in jagged)


class _BaseProfiler(ABC, Component):
    """Base class to handle profilers.

    Primarily exists to handle plotting.
    """

    _name = "_BaseProfiler"

    _unit_agnostic = True

    _info = {}

    def __init__(self, grid):
        super().__init__(grid)

    def run_one_step(self):
        """Calculate the profile data structure and distances along it."""
        # calculate the profile IDs data structure.
        self._create_profile_structure()

    @abstractmethod
    def _create_profile_structure(self):
        """Private class for creating profile structure.

        Expectation is that this will be overridden to create the following
        three private attributes:

        self._nodes
        self._distance_along_profile

        are each lists of numpy arrays, one array per segment.

        self._colors

        is a list of RGBA tuples, one tuple per segment.

        The order of segments is expected to be consistent between each of the
        three data structures.
        """
        ...  # pragma: no cover

    @property
    def distance_along_profile(self):
        """List of distances along profile for each segment.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import (
        ...     FastscapeEroder,
        ...     FlowAccumulator,
        ...     ChannelProfiler,
        ... )
        >>> mg = RasterModelGrid((10, 10), xy_spacing=10)
        >>> np.random.seed(42)
        >>> z = mg.add_zeros("topographic__elevation", at="node")
        >>> z[mg.core_nodes] += np.random.randn(mg.core_nodes.size)
        >>> fa = FlowAccumulator(mg)
        >>> sp = FastscapeEroder(mg, K_sp=0.0001)
        >>> dt = 1000
        >>> for i in range(200):
        ...     fa.run_one_step()
        ...     sp.run_one_step(dt=dt)
        ...     z[mg.core_nodes] += 0.001 * dt
        ...
        >>> profiler = ChannelProfiler(mg)
        >>> profiler.run_one_step()
        >>> profiler.distance_along_profile
        [array([  0.,  10.,  20.,  30.,  40.,  50.])]
        """
        return self._distance_along_profile

    @property
    def nodes(self):
        """List of node ids for each segment.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import (
        ...     FastscapeEroder,
        ...     FlowAccumulator,
        ...     ChannelProfiler,
        ... )
        >>> mg = RasterModelGrid((10, 10), xy_spacing=10)
        >>> np.random.seed(42)
        >>> z = mg.add_zeros("topographic__elevation", at="node")
        >>> z[mg.core_nodes] += np.random.randn(mg.core_nodes.size)
        >>> fa = FlowAccumulator(mg)
        >>> sp = FastscapeEroder(mg, K_sp=0.0001)
        >>> dt = 1000
        >>> for i in range(200):
        ...     fa.run_one_step()
        ...     sp.run_one_step(dt=dt)
        ...     z[mg.core_nodes] += 0.001 * dt
        ...
        >>> profiler = ChannelProfiler(mg)
        >>> profiler.run_one_step()
        >>> profiler.nodes
        [array([59, 58, 57, 56, 46, 45])]
        """
        return self._nodes

    @property
    def colors(self):
        """List of colors for each segment.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import (
        ...     FastscapeEroder,
        ...     FlowAccumulator,
        ...     ChannelProfiler,
        ... )
        >>> mg = RasterModelGrid((10, 10), xy_spacing=10)
        >>> np.random.seed(42)
        >>> z = mg.add_zeros("topographic__elevation", at="node")
        >>> z[mg.core_nodes] += np.random.randn(mg.core_nodes.size)
        >>> fa = FlowAccumulator(mg)
        >>> sp = FastscapeEroder(mg, K_sp=0.0001)
        >>> dt = 1000
        >>> for i in range(200):
        ...     fa.run_one_step()
        ...     sp.run_one_step(dt=dt)
        ...     z[mg.core_nodes] += 0.001 * dt
        ...
        >>> profiler = ChannelProfiler(mg)
        >>> profiler.run_one_step()
        >>> np.round(profiler.colors, decimals=2)
        array([[0.27, 0.  , 0.33, 1.  ]])
        """
        return self._colors

    def plot_profiles(
        self,
        field="topographic__elevation",
        xlabel="Distance Along Profile",
        ylabel="Plotted Quantity",
        title="Extracted Profiles",
        color=None,
    ):
        """Plot distance-upstream vs at at-node or size (nnodes,) quantity.

        Parameters
        ----------
        field : field name or nnode array
            Array of the at-node-field to plot against distance upstream.
            Default value is the at-node field 'topographic__elevation'.
        xlabel : str, optional
            X-axis label, default is "Distance Along Profile".
        ylabel : str, optional
            Y-axis label, default value is "Plotted Quantity".
        title : str, optional
            Plot title, default value is "Extracted Profiles".
        color : RGBA tuple or color string
            Color to use in order to plot all profiles the same color. Default
            is None, and the colors assigned to each profile are used.
        """
        quantity = return_array_at_node(self._grid, field)

        # create segments the way that line collection likes them.
        segments = []
        qmin = []
        qmax = []
        for idx, nodes in enumerate(self._nodes):
            segments.append(
                list(zip(self._distance_along_profile[idx], quantity[nodes]))
            )
            qmin.append(min(quantity[nodes]))
            qmax.append(max(quantity[nodes]))

        # We need to set the plot limits.
        ax = plt.gca()
        ax.set_xlim(
            _recursive_min(self._distance_along_profile),
            _recursive_max(self._distance_along_profile),
        )
        ax.set_ylim(min(qmin), max(qmax))

        line_segments = LineCollection(segments)
        colors = color or self._colors
        line_segments.set_color(colors)
        ax.add_collection(line_segments)
        ax.set_xlabel(xlabel)
        ax.set_ylabel(ylabel)
        ax.set_title(title)

    def plot_profiles_in_map_view(
        self, field="topographic__elevation", endpoints_only=False, color=None, **kwds
    ):
        """Plot profile locations in map view.

        Parameters
        ----------
        field : field name or nnode array
            Array of the at-node-field to plot as the 2D map values.
            Default value is the at-node field 'topographic__elevation'.
        endpoints_only : boolean
            Boolean where False (default) indicates every node along the
            profile is plotted, or True indicating only segment endpoints are
            plotted.
        color : RGBA tuple or color string
            Color to use in order to plot all profiles the same color. Default
            is None, and the colors assigned to each profile are used.
        **kwds : dictionary
            Keyword arguments to pass to imshow_grid.
        """
        # make imshow_grid background
        imshow_grid(self._grid, field, **kwds)
        ax = plt.gca()

        # create segments the way that line collection likes them.
        segments = []
        for nodes in self._nodes:
            if endpoints_only:
                select_nodes = [nodes[0], nodes[-1]]
                segments.append(
                    list(
                        zip(
                            self._grid.x_of_node[select_nodes],
                            self._grid.y_of_node[select_nodes],
                        )
                    )
                )

            else:
                segments.append(
                    list(zip(self._grid.x_of_node[nodes], self._grid.y_of_node[nodes]))
                )

        line_segments = LineCollection(segments)
        colors = color or self._colors
        line_segments.set_color(colors)
        ax.add_collection(line_segments)



================================================
File: profiler/channel_profiler.py
================================================
# ! /usr/env/python
"""channel_profiler.py component to create channel profiles."""
from collections import OrderedDict

import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
from matplotlib import cm

from landlab.components.profiler.base_profiler import _BaseProfiler
from landlab.core.utils import as_id_array
from landlab.utils.flow__distance import calculate_flow__distance


class ChannelProfiler(_BaseProfiler):
    """Extract and plot the channel profiles in drainage networks.

    The ChannelProfiler extracts channel networks from a landlab grid.

    In order to extract channel networks, the flow connectivity across the grid
    must already be identified. This is typically done with the FlowAccumulator
    component. However, this component does not require that the
    FlowAccumulator was used. Instead it expects that the following at-node
    grid fields will be present:
    ::

        'flow__receiver_node'
        'flow__link_to_receiver_node'

    The ChannelProfiler can work on grids that have used route-to-one or
    route-to-multiple flow directing.

    To understand how this component works it is useful to define the following
    terms: *watershed*, *outlet*, *headwater node*, *segment*.

    A *watershed* is all model grid nodes that drain to a single node, called
    the *outlet*. Channels nodes are identified as nodes that have a
    ``channel_definition_field`` value greater than or equal to the
    ``minimum_channel_threshold``. This ``channel_definition_field`` is often
    the drainage area (this component's default). We use a flexible field
    rather than only drainage area to support alternative bases for channel
    extraction.

    The default behaviour of this component is to use an exclusive definition
    of a *watershed*. That is, the two largest watersheds are defined as the
    watersheds upstream of the two nodes on the model grid boundary with the
    largest values of the ``channel_definition_field`` rather than potentially
    nested watersheds. Nested watersheds are supported through the use of the
    ``outlet_nodes`` keyword argument.

    Consider the following grid with 10 columns and 7 rows. In this grid is one
    watershed with an outlet node indicated by ``o``. Here ``X`` indicates
    nodes that are not part of the channel network (based on the
    ``channel_definition_field``) and ``.`` indicates nodes that are part of
    the network.

    In this and the following examples, we will use only D4 connectivity. The
    ChannelProfiler, however, knows nothing of connectivity other than what is
    implied by the two required grid fields.
    ::

        X X X X X X X X X X
        X . X X X X X X X X
        X . . X X X . . . X
        X X . . X X . X X X
        X X X . . . . X X X
        X X X . X X X X X X
        X X X o X X X X X X

    This component can extract the channel network from one or more watersheds.
    This option is specified with the keyword argument
    ``number_of_watersheds``.

    The *headwater nodes*, shown as ``@`` are nodes that have no upstream
    nodes with sufficient area to be classified as a channel.
    ::

        X X X X X X X X X X
        X @ X X X X X X X X
        X . . X X X . . @ X
        X X . . X X . X X X
        X X X . . . . X X X
        X X X . X X X X X X
        X X X o X X X X X X

    For each watershed, the ChannelProfiler either extracts the largest channel
    (again, based on the ``channel_definition_field``) or all channel segments
    with sufficent values in the ``channel_definition_field``.

    Default behavior of this component is to extract only the largest channel
    in the single largest watershed. This would extract the following channel
    segment (indicated by the `*` s).
    ::

        X X X X X X X X X X
        X . X X X X X X X X
        X . . X X X * * * X
        X X . . X X * X X X
        X X X * * * * X X X
        X X X * X X X X X X
        X X X * X X X X X X

    This component verifies that all watershed outlets have a value in the
    ``channel_definition_field`` of at least ``minimum_outlet_threshold``
    (default is 0 units). If no watersheds exist that meet this criteria, an
    error is raised.

    If a user knows exactly which node or nodes they want to use as the outlet
    nodes, then this can be specified using the ``outlet_nodes`` keyword
    argument. Otherwise the ``number_of_watersheds`` (default 1) nodes with the
    largest value in the ``channel_definition_field`` will be selected as the
    outlet nodes from the model grid boundary nodes. Setting
    ``number_of_watersheds`` to ``None`` results in selecting all nodes at the
    model grid boundary that meet the criteria for an outlet based on the
    ``channel_definition_field`` and the ``minimum_outlet_threshold``.

    The node IDs and distances upstream of the channel network are stored in
    ``data_structure``. It is a dictionary with keys indicating the outlet
    node.

    For each watershed outlet, the value in the ``data_structure`` is itself
    a dictionary with keys that are a segment ID tuple of the
    ``(dowstream, upstream)`` nodes IDs of each channel segment.

    For our simple example, these are the node IDs:
    ::

            X  X  X  X  X  X  X  X  X  X
            X 51  X  X  X  X  X  X  X  X
            X 41 42  X  X  X 46 47 48  X
            X  X 32 33  X  X 36  X  X  X
            X  X  X 23 24 25 26  X  X  X
            X  X  X 13  X  X  X  X  X  X
            X  X  X  3  X  X  X  X  X  X

    So for our main channel only example, the outlet has an ID of 3, the
    downstream end of the channel segment is 3, and the upstream end is 48.

    The value associated with the segment ID tuple ``(3, 48)`` is itself a
    dictionary. It has three key-value pairs. First, ``"ids"`` contains a list
    of the segment node ids ordered from downstream to upstream. It includes
    the endpoints. Second, ``"distances"`` contains a list of distances
    upstream that mirrors the list in ``"ids"``. Finally, ``"color"`` is an
    RGBA tuple indicating the color for the segment.

    By default a unique color will be assigned to each watershed. To change the
    color, a user can change values stored in ``data_structure``.
    Additionally, a ``cmap`` keyword argument can provide some user control
    over the color at the instantiation of the component.

    In the main channel only example, the data structure will look as follows:

    .. code-block:: python

        {
            3: {
                (3, 48): {
                    "ids": [3, 13, 23, 24, 25, 26, 36, 46, 47, 48],
                    "distances": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
                    "color": (1, 0, 1, 1),
                }
            }
        }

    Three channel segments are idendified if ``main_channel_only=False``.
    ::

        X X X X X X X X X X     X X X X X X X X X X     X X X X X X X X X X
        X . X X X X X X X X     X . X X X X X X X X     X * X X X X X X X X
        X . . X X X . . . X     X . . X X X * * * X     X * * X X X . . . X
        X X . . X X . X X X     X X . . X X * X X X     X X * * X X . X X X
        X X X * . . . X X X     X X X * * * * X X X     X X X * . . . X X X
        X X X * X X X X X X     X X X . X X X X X X     X X X . X X X X X X
        X X X * X X X X X X     X X X . X X X X X X     X X X . X X X X X X

    The data structure associated with this set of three segments is

        .. code-block:: python

            {
                3: {
                    (3, 23): {
                        "ids": [3, 13, 23],
                        "distances": [0, 1, 2],
                        "color": (1, 0, 1, 1),
                    },
                    (23, 48): {
                        "ids": [23, 24, 25, 26, 36, 46, 47, 48],
                        "distances": [2, 3, 4, 5, 6, 7, 8, 9],
                        "color": (1, 0, 1, 1),
                    },
                    (23, 51): {
                        "ids": [23, 33, 32, 42, 41, 51],
                        "distances": [2, 3, 4, 5, 6, 7],
                        "color": (1, 0, 1, 1),
                    },
                }
            }

    Note that the distances upstream are relative to the outlet, not the
    downstream end of the stream segment.

    Next consider a model grid with two watersheds.
    ::

        X X X X X X X X X X
        X . . X X X . X X X
        X . X X . X . X X X
        o . . . . X . X X X
        X X X X X X . X X X
        X X X . . . . X X X
        X X X X X X . . . X
        X X X X X X X X o X

    And the following node IDs.
    ::

        X   X   X   X   X   X   X   X   X   X
        X  61  62   X   X   X  66   X   X   X
        X  51   X   X  54   X  56   X   X   X
       40  41  42  43  44   X  46   X   X   X
        X   X   X   X   X   X  36   X   X   X
        X   X   X  23  24  25  26   X   X   X
        X   X   X   X   X   X  16  17  18   X
        X   X   X   X   X   X   X   X   8   X


    The data structure for ``number_of_watersheds=2`` and
    ``main_channel_only=False`` will be as follows. Note that each watershed
    has been assigned a different color tuple value. Here the default viridis
    cmap is used.

    .. code-block:: python

        {
            8: {
                (8, 26): {
                    "ids": [8, 18, 17, 16, 26],
                    "distances": [0, 1, 2, 3, 4],
                    "color": [0.13, 0.57, 0.55, 1.0],
                },
                (26, 23): {
                    "ids": [26, 25, 24, 23],
                    "distances": [4, 5, 6, 7],
                    "color": [0.13, 0.57, 0.55, 1.0],
                },
                (26, 66): {
                    "ids": [26, 36, 46, 56, 66],
                    "distances": [4, 5, 6, 7, 8],
                    "color": [0.13, 0.57, 0.55, 1.0],
                },
            },
            40: {
                (40, 41): {
                    "ids": [40, 41],
                    "distances": [0, 1],
                    "color": [0.27, 0.0, 0.33, 1.0],
                },
                (41, 54): {
                    "ids": [41, 42, 43, 44, 54],
                    "distances": [2, 3, 4, 5, 6],
                    "color": [0.27, 0.0, 0.33, 1.0],
                },
                (41, 62): {
                    "ids": [41, 51, 61, 62],
                    "distances": [1, 2, 3, 4],
                    "color": [0.27, 0.0, 0.33, 1.0],
                },
            },
        }

    Examples
    --------

    Start by importing necessary modules

    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import FlowAccumulator, ChannelProfiler

    Create the second example grid we showed above. Note that in order to do
    this we need to enter the elevations starting from the lower left so the
    elevation order may seem upside-down. In addition, in this example,
    elevation is only provided along the profiles. The third line of code below
    sets all nodes with a value of zero to closed, such that these nodes are
    igored.

    >>> z = np.array(
    ...     [
    ...         [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
    ...         [0, 0, 0, 0, 0, 0, 4, 3, 2, 0],
    ...         [0, 0, 0, 8, 7, 6, 5, 0, 0, 0],
    ...         [0, 0, 0, 0, 0, 0, 6, 0, 0, 0],
    ...         [1, 3, 4, 5, 6, 0, 7, 0, 0, 0],
    ...         [0, 4, 0, 0, 7, 0, 8, 0, 0, 0],
    ...         [0, 5, 6, 0, 0, 0, 9, 0, 0, 0],
    ...         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    ...     ],
    ...     dtype=float,
    ... )
    >>> mg = RasterModelGrid((8, 10))
    >>> z = mg.add_field("topographic__elevation", z, at="node")
    >>> mg.set_nodata_nodes_to_closed(z, 0)
    >>> fa = FlowAccumulator(mg, flow_director="D4")
    >>> fa.run_one_step()
    >>> fa.node_drainage_area.reshape(mg.shape)
    array([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  11.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   9.,  10.,  11.,   0.],
           [  0.,   0.,   0.,   1.,   2.,   3.,   8.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   4.,   0.,   0.,   0.],
           [  8.,   8.,   4.,   3.,   2.,   0.,   3.,   0.,   0.,   0.],
           [  0.,   3.,   0.,   0.,   1.,   0.,   2.,   0.,   0.,   0.],
           [  0.,   2.,   1.,   0.,   0.,   0.,   1.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]])

    >>> profiler = ChannelProfiler(
    ...     mg,
    ...     number_of_watersheds=2,
    ...     minimum_channel_threshold=0,
    ...     main_channel_only=False,
    ... )
    >>> profiler.run_one_step()

    The keys of the property ``data_structure`` are the IDs  of the two
    outlet nodes.

    >>> profiler.data_structure.keys()
    odict_keys([40, 8])

    Within the data structure, the value at key 40, is a dictionary of the
    three segments, each specified by a ``(dowstream, upstream)`` tuple:

    >>> profiler.data_structure[40].keys()
    odict_keys([(40, 41), (41, 54), (41, 62)])

    The value of the segment between nodes 40 and 41 has the following
    components:

    >>> profiler.data_structure[40][(40, 41)]["ids"]
    array([40, 41])
    >>> profiler.data_structure[40][(40, 41)]["distances"]
    array([0., 1.])
    >>> np.round(profiler.data_structure[40][(40, 41)]["color"], decimals=2)
    array([0.27, 0.  , 0.33, 1.  ])

    A parallel structure exists for the segment between nodes 41 and 54:

    >>> profiler.data_structure[40][(41, 54)]["ids"]
    array([41, 42, 43, 44, 54])
    >>> profiler.data_structure[40][(41, 54)]["distances"]
    array([1., 2., 3., 4., 5.])
    >>> np.round(profiler.data_structure[40][(41, 54)]["color"], decimals=2)
    array([0.27, 0.  , 0.33, 1.  ])

    And the segment between nodes 41  and 62.

    >>> profiler.data_structure[40][(41, 62)]["ids"]
    array([41, 51, 61, 62])
    >>> profiler.data_structure[40][(41, 62)]["distances"]
    array([1., 2., 3., 4.])
    >>> np.round(profiler.data_structure[40][(41, 62)]["color"], decimals=2)
    array([0.27, 0.  , 0.33, 1.  ])

    The rest of the ``profile_structure`` encodes information about the second
    watershed, which drains to node 8.

    >>> profiler.data_structure[8].keys()
    odict_keys([(8, 26), (26, 23), (26, 66)])

    >>> profiler.data_structure[8][(8, 26)]["ids"]
    array([ 8, 18, 17, 16, 26])
    >>> profiler.data_structure[8][(8, 26)]["distances"]
    array([0., 1., 2., 3., 4.])
    >>> np.round(profiler.data_structure[8][(8, 26)]["color"], decimals=2)
    array([0.13, 0.57, 0.55, 1.  ])

    >>> profiler.data_structure[8][(26, 23)]["ids"]
    array([26, 25, 24, 23])
    >>> profiler.data_structure[8][(26, 23)]["distances"]
    array([4., 5., 6., 7.])
    >>> np.round(profiler.data_structure[8][(26, 23)]["color"], decimals=2)
    array([0.13, 0.57, 0.55, 1.  ])

    >>> profiler.data_structure[8][(26, 66)]["ids"]
    array([26, 36, 46, 56, 66])
    >>> profiler.data_structure[8][(26, 66)]["distances"]
    array([4., 5., 6., 7., 8.])
    >>> np.round(profiler.data_structure[8][(26, 66)]["color"], decimals=2)
    array([0.13, 0.57, 0.55, 1.  ])

    The ChannelProfiler is designed to be flexible, and by careful combination
    of its instantiation variables can be used to extract many useful forms of
    profile. In these examples, we will use the default
    ``channel_definition_field``, the drainage area.

    To illustrate, lets start by creating a landscape model.

    >>> from landlab.components import FastscapeEroder
    >>> mg = RasterModelGrid((100, 120), xy_spacing=2)
    >>> np.random.seed(42)
    >>> z = mg.add_zeros("topographic__elevation", at="node")
    >>> z[mg.core_nodes] += np.random.randn(mg.core_nodes.size)
    >>> fa = FlowAccumulator(mg)
    >>> sp = FastscapeEroder(mg, K_sp=0.0001)
    >>> dt = 1000
    >>> for i in range(200):
    ...     fa.run_one_step()
    ...     sp.run_one_step(dt=dt)
    ...     z[mg.core_nodes] += 0.001 * dt
    ...

    Some options:

    Default: Extract a the single biggest channel draining to the model grid
    boundary traced back all the way to the watershed divide.

    >>> profiler = ChannelProfiler(mg)

    Extract the largest channel draining to each of the four largest outlet
    nodes on the model grid boundary traced back all the way to the watershed
    divide.

    >>> profiler = ChannelProfiler(mg, number_of_watersheds=4)

    Extract the single largest channel draining to node 2933. Note that the
    keyword argument ``outlet_nodes`` must be an iterable.

    >>> profiler = ChannelProfiler(mg, outlet_nodes=[2933])

    Extract the largest channel draining to each of the four largest outlet
    nodes on the model grid boundary traced back to nodes with
    ``channel_definition_field`` values of 500.

    >>> profiler = ChannelProfiler(
    ...     mg, number_of_watersheds=4, minimum_channel_threshold=500
    ... )

    Extract a the single biggest channel draining to the model grid boundary
    based on the field ``surface_water__discharge`` traced back to discharge
    values of 500.

    >>> profiler = ChannelProfiler(
    ...     mg,
    ...     channel_definition_field="surface_water__discharge",
    ...     minimum_channel_threshold=500,
    ... )

    Extract the single largest channel within *all* watersheds with an outlet
    with ``channel_definition_field`` greater than 1e3. Trace the channels
    up to the point in each watershed in which the channels have values in the
    ``channel_definition_field`` of 500.

    >>> profiler = ChannelProfiler(
    ...     mg,
    ...     number_of_watersheds=None,
    ...     minimum_outlet_threshold=1e3,
    ...     minimum_channel_threshold=500,
    ... )

    Extract two trunk channels beginning at the given nodes, traced up to a
    a minimum ``channel_definition_field`` value of of 500. Note that
    ``number_of_watersheds`` must match the size of ``outlet_nodes``.

    >>> profiler = ChannelProfiler(
    ...     mg,
    ...     outlet_nodes=[6661, 6250],
    ...     number_of_watersheds=2,
    ...     minimum_channel_threshold=500,
    ... )

    Extract every possible channel (not just the largest one), leading from the
    four highest model grid boundary nodes traced back to a
    ``channel_definition_field`` threshold of 20.

    >>> profiler = ChannelProfiler(
    ...     mg,
    ...     number_of_watersheds=4,
    ...     main_channel_only=False,
    ...     minimum_channel_threshold=20,
    ... )

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    None Listed

    """

    _name = "ChannelProfiler"

    _unit_agnostic = True

    _info = {
        "drainage_area": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "m**2",
            "mapping": "node",
            "doc": "Upstream accumulated surface area contributing to the node's discharge",
        },
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
    }

    def __init__(
        self,
        grid,
        channel_definition_field="drainage_area",
        number_of_watersheds=1,
        minimum_outlet_threshold=0,
        main_channel_only=True,
        outlet_nodes=None,
        minimum_channel_threshold=0,
        cmap="viridis",
    ):
        """
        Parameters
        ----------
        grid : Landlab Model Grid instance
        channel_definition_field : field name as string, optional
            Name of field used to identify the outlet and headwater nodes of the
            channel network. Default is "drainage_area".
        minimum_outlet_threshold : float, optional
            Minimum value of the *channel_definition_field* to define a
            watershed outlet. Default is 0.
        minimum_channel_threshold : float, optional
            Value to use for the minimum drainage area associated with a
            plotted channel segment. Default values 0.
        number_of_watersheds : int, optional
            Total number of watersheds to plot. Default value is 1. If value is
            greater than 1 and outlet_nodes is not specified, then the
            number_of_watersheds largest watersheds is based on the drainage
            area at the model grid boundary. If given as None, then all grid
            cells on the domain boundary with a stopping field (typically
            drainage area) greater than the minimum_outlet_threshold in area are used.
        main_channel_only : Boolean, optional
            Flag to determine if only the main channel should be plotted, or if
            all stream segments with drainage area less than threshold should
            be plotted. Default value is True.
        outlet_nodes : length number_of_watersheds iterable, optional
            Length number_of_watersheds iterable containing the node IDs of
            nodes to start the channel profiles from. If not provided, the
            default is the number_of_watersheds node IDs on the model grid
            boundary with the largest terminal drainage area.
        cmap : str, optional
            A valid matplotlib cmap string. Default is "viridis".

        """
        super().__init__(grid)

        self._cmap = plt.colormaps[cmap]
        if channel_definition_field in grid.at_node:
            self._channel_definition_field = grid.at_node[channel_definition_field]
        else:
            raise ValueError(
                f"Required field {channel_definition_field!r} not present. "
                "This field is required by the ChannelProfiler to define "
                "the start and stop of channel networks."
            )

        self._flow_receiver = grid.at_node["flow__receiver_node"]

        self._link_to_flow_receiver = grid.at_node["flow__link_to_receiver_node"]

        self._main_channel_only = main_channel_only
        self._minimum_channel_threshold = minimum_channel_threshold

        # verify that the number of starting nodes is the specified number of channels
        if outlet_nodes is not None:
            if (number_of_watersheds is not None) and (
                len(outlet_nodes) is not number_of_watersheds
            ):
                raise ValueError(
                    "Length of outlet_nodes must equal the" "number_of_watersheds!"
                )
        else:
            large_outlet_ids = grid.boundary_nodes[
                np.argsort(self._channel_definition_field[grid.boundary_nodes])
            ]

            if number_of_watersheds is None:
                big_enough_watersheds = self._channel_definition_field[
                    large_outlet_ids
                ] > max(minimum_outlet_threshold, minimum_channel_threshold)
                outlet_nodes = large_outlet_ids[big_enough_watersheds]
            else:
                outlet_nodes = large_outlet_ids[-number_of_watersheds:]

        starting_da = self._channel_definition_field[outlet_nodes]
        outlet_nodes = np.asarray(outlet_nodes)

        bad_wshed = False
        if outlet_nodes.size == 0:
            bad_wshed = True  # not tested
        if np.any(starting_da <= minimum_outlet_threshold):
            bad_wshed = True
        if np.any(starting_da <= minimum_channel_threshold):
            bad_wshed = True

        if bad_wshed:
            raise ValueError(
                "The number of watersheds requested by the ChannelProfiler is "
                "greater than the number in the domain with channel_definition_field"
                f" area. {starting_da}"
            )

        self._outlet_nodes = outlet_nodes

    @property
    def data_structure(self):
        """OrderedDict defining the channel network.

        The IDs and upstream distance of the channel network nodes are stored
        in ``data_structure``. It is a dictionary with keys of the outlet node
        ID.

        For each watershed outlet, the value in the ``data_structure`` is
        itself a dictionary with keys that are a segment ID tuple of the
        ``(dowstream, upstream)`` nodes IDs of each channel segment.

        The value associated with the segment ID tuple
        ``(dowstream, upstream)`` is itself a dictionary. It has three
        key-value pairs. First, ``"ids"`` contains a list of the segment node
        IDs ordered from downstream to upstream. It includes the endpoints.
        Second, ``"distances"`` contains a list of distances upstream that
        mirrors the list in ``"ids"``. Finally, ``"color"`` is an RGBA tuple
        indicating the color for the segment.
        """
        return self._data_struct

    def _get_channel_segment(self, i):
        """Get channel segment and return additional nodes to process.

        Parameters
        ----------
        i : int, required
            Node id of start of channel segment.

        Returns
        ----------
        channel_segment : list
            Node IDs of the nodes in the current channel segment.
        nodes_to_process, list
            List of nodes to add to the processing queue. These nodes are those
            that drain to the upper end of this channel segment. If
            main_channel_only = False this will be an empty list.
        """
        j = i
        channel_segment = []
        channel_upstream = True

        # add the reciever of j to the channel segment if it is not j.
        # but only do this when j is not the watershed outlet.
        recieving_node = self._flow_receiver[j]
        if (recieving_node != j) and (j not in self._outlet_nodes):
            channel_segment.append(recieving_node)

        while channel_upstream:
            # add the new node to the channel segment
            channel_segment.append(j)

            # get supplying nodes
            supplying_nodes = np.where(self._flow_receiver == j)[0]

            # remove supplying nodes that are the outlet node
            supplying_nodes = supplying_nodes[np.where(supplying_nodes != i)]

            # if only adding the biggest channel, continue upstream choosing the
            # largest node until no more nodes remain.
            if (self._main_channel_only) and (len(supplying_nodes) > 0):
                max_drainage = np.argmax(
                    self._channel_definition_field[supplying_nodes]
                )

                if (
                    self._channel_definition_field[supplying_nodes[max_drainage]]
                    < self._minimum_channel_threshold
                ):
                    nodes_to_process = []
                    channel_upstream = False
                else:
                    j = supplying_nodes[max_drainage]

            # if considering multiple channel segments, continue upstream until
            # there are two or more donors with sufficient discharge, then
            # break, returning those nodes as starting points.
            else:
                # get all upstream drainage areas
                upstream_das = self._channel_definition_field[supplying_nodes]

                # if no nodes upstream exceed the threshold, exit
                if np.sum(upstream_das > self._minimum_channel_threshold) == 0:
                    nodes_to_process = []
                    channel_upstream = False

                # otherwise
                else:
                    # if only one upstream node exceeds the threshold, proceed
                    # up the channel.
                    if np.sum(upstream_das > self._minimum_channel_threshold) == 1:
                        max_drainage = np.argmax(
                            self._channel_definition_field[supplying_nodes]
                        )
                        j = supplying_nodes[max_drainage]
                    # otherwise provide the multiple upstream nodes to be
                    # processed into a new channel.
                    else:
                        nodes_to_process = supplying_nodes[
                            upstream_das > self._minimum_channel_threshold
                        ]
                        channel_upstream = False

        return (channel_segment, nodes_to_process)

    def _create_profile_structure(self):
        """Create the profile_IDs data structure for channel network.

        The bound attribute self._profile structure is the channel segment
        datastructure. profile structure is a list of length
        number_of_watersheds. Each element of profile_structure is itself a
        list of length number of stream segments that drain to each of the
        starting nodes. Each stream segment list contains the node ids of a
        stream segment from downstream to upstream.
        """
        self._data_struct = OrderedDict()

        if self._main_channel_only:
            for i in self._outlet_nodes:
                (channel_segment, nodes_to_process) = self._get_channel_segment(i)
                segment_tuple = (channel_segment[0], channel_segment[-1])
                self._data_struct[i] = {
                    segment_tuple: {"ids": as_id_array(channel_segment)}
                }

        else:
            for i in self._outlet_nodes:
                channel_network = OrderedDict()
                queue = [i]
                while len(queue) > 0:
                    node_to_process = queue.pop(0)
                    (channel_segment, nodes_to_process) = self._get_channel_segment(
                        node_to_process
                    )
                    segment_tuple = (channel_segment[0], channel_segment[-1])
                    channel_network[segment_tuple] = {
                        "ids": as_id_array(channel_segment)
                    }
                    queue.extend(nodes_to_process)
                self._data_struct[i] = channel_network

        self._calculate_distances()
        self.assign_colors()
        self._create_flat_structures()

    def _create_flat_structures(self):
        """Create expected flattened structures for ids, distances, and colors."""
        self._nodes = []

        self._distance_along_profile = []
        self._colors = []

        for outlet_id in self._data_struct:
            seg_tuples = self._data_struct[outlet_id].keys()
            self._nodes.extend(
                [self._data_struct[outlet_id][seg]["ids"] for seg in seg_tuples]
            )
            self._distance_along_profile.extend(
                [self._data_struct[outlet_id][seg]["distances"] for seg in seg_tuples]
            )
            self._colors.extend(
                [self._data_struct[outlet_id][seg]["color"] for seg in seg_tuples]
            )

    def assign_colors(self, color_mapping=None):
        """Assign a unique color for each watershed.

        Parameters
        ----------
        color_mapping : str
            Color map name.
        """

        if color_mapping is None:
            num_watersheds = len(self._data_struct)
            norm = mpl.colors.Normalize(vmin=0, vmax=num_watersheds)
            mappable = cm.ScalarMappable(norm=norm, cmap=self._cmap)
            color_mapping = {
                outlet_id: mappable.to_rgba(idx)
                for idx, outlet_id in enumerate(self._data_struct)
            }

        for outlet_id in self._data_struct:
            for segment_tuple in self._data_struct[outlet_id]:
                self._data_struct[outlet_id][segment_tuple]["color"] = color_mapping[
                    outlet_id
                ]

    def _calculate_distances(self):
        """Get distances along the network data structure."""
        distance_upstream = calculate_flow__distance(self._grid)
        for outlet_id in self._data_struct:
            offset = distance_upstream[outlet_id]

            for segment_tuple in self._data_struct[outlet_id]:
                ids = self._data_struct[outlet_id][segment_tuple]["ids"]
                d = distance_upstream[ids]
                self._data_struct[outlet_id][segment_tuple]["distances"] = d - offset



================================================
File: profiler/profiler.py
================================================
# ! /usr/env/python
"""profiler.py component to create profiles with user-defined endpoints."""
from collections import OrderedDict

import numpy as np
from matplotlib import cm
from matplotlib import colors
from matplotlib import pyplot as plt

from landlab.components.profiler.base_profiler import _BaseProfiler


class Profiler(_BaseProfiler):
    """Extract and plot profiles set up using points within a grid.

    The profile is constructed from the first to final point in ``endpoints``.
    Endpoints are located at grid nodes. Two successive endpoints bound a
    profile segment. A profile with one segment is a straight line. The
    segments of a profile with multiple segments meet at endpoints. The grid
    nodes along the profile are sampled, including the segment endpoints. The
    extracted quantity of the node is retained. No interpolation is conducted
    even for profile traces that reside between nodes.

    The structure of the profile in a model grid is diagrammed below. The grid
    contains nine columns and nine rows. The profile is constructed from three
    endpoints that bound two segments. Here, ``o`` indicates a segment
    endpoint, ``.`` and ``*`` are sample nodes of the first and second segment,
    respectively. ``X`` are nodes not included in the profile. The first
    segment begins in the lower-left and continues horizontally and almost
    reaches the right boundary. The second segment is joined to the first in
    the lower-right of the grid and it continues diagonally to the upper-left.
    Segments have seven sample points each (nodes at endpoints are also
    sampled). The segments share the second endpoint. Segment and sample
    ordering is dictated by the ordering of endpoints. If the horizontal
    segment is the first segment, the endpoints used to construct this profile
    must be ordered: lower-left, lower-right, and then upper-left.::

        X X X X X X X X X
        X o X X X X X X X
        X X * X X X X X X
        X X X * X X X X X
        X X X X * X X X X
        X X X X X * X X X
        X X X X X X * X X
        X o . . . . . o X
        X X X X X X X X X

    The node IDs and distances along the profile are stored in a data structure
    called ``data_structure``. It is a dictionary with keys indicating the
    segment IDs that are enumerated along the profile.

    By default, a unique color will be assigned to each segment. To change the
    color, a user can change values stored in ``data_structure``. Additionally,
    a ``cmap`` keyword argument can provide some user control over the color at
    the instantiation of the component.

    The data structure of the example above will look as follows:

    .. code-block:: python

        {
            0: {
                "ids": [10, 11, 12, 13, 14, 15, 16],
                "distances": [0, 1, 2, 3, 4, 5, 6],
                "color": (0.27, 0, 0.33, 1),
            },
            1: {
                "ids": [16, 24, 32, 40, 48, 56, 64],
                "distances": [6, 7.41, 8.83, 10.24, 11.66, 13.07, 14.49],
                "color": (0.13, 0.57, 0.55, 1),
            },
        }


    Examples
    --------
    Create a model grid with the same dimensions as the diagram above.

    >>> from landlab import RasterModelGrid
    >>> from landlab.components import Profiler
    >>> import numpy as np
    >>> mg = RasterModelGrid((10, 10), 10)
    >>> mg.at_node["topographic__elevation"] = mg.node_x * mg.node_y

    Create a profile with three endpoints. This profile is laid out the same as
    the diagram above.

    >>> endpoints = [10, 16, 64]
    >>> profiler = Profiler(mg, endpoints)
    >>> profiler.run_one_step()

    The keys of the data structure are the segment ids.

    >>> profiler.data_structure.keys()
    odict_keys([0, 1])

    The data structure contains data of segment samples. Below is the first
    segment.

    >>> profiler.data_structure[0]["ids"]
    array([10, 11, 12, 13, 14, 15, 16])
    >>> profiler.data_structure[0]["distances"]
    array([ 0., 10., 20., 30., 40., 50., 60.])
    >>> np.round(profiler.data_structure[0]["color"], decimals=2)
    array([0.27, 0.  , 0.33, 1.  ])

    Note that the first node of the second segment is the same as the final
    node of the first segment.

    >>> profiler.data_structure[1]["ids"]
    array([16, 26, 35, 45, 54, 64])

    Alternative to nodes, profiles can be instantiated with coordinates.

    >>> profiler = Profiler(mg, [(10, 10), (70, 10), (10, 70)])

    Endpoints can also be set with a combination of coordinates and nodes.

    >>> profiler = Profiler(mg, [(10, 10), 16, (10, 70)])

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    None Listed

    """

    _name = "Profiler"

    _unit_agnostic = True

    def __init__(self, grid, endpoints, cmap="viridis"):
        """Instantiate Profiler.

        Parameters
        ----------
        grid : RasterModelGrid
            A landlab RasterModelGrid.
        endpoints : list of node id integers or coordinate tuples
            The endpoints that bound segments of the profile. Endpoints can be
            node ids and/or tuples of coordinates (x, y, where these
            coordinates are the measurement from the grid lower-left). The list
            can be a mix of node ids and coordinate tuples. The profile begins
            with the first element of `endpoints` and continues in the order of
            this list.
        cmap : str
            A valid matplotlib cmap string. Default is "viridis".
        """
        super().__init__(grid)

        self._cmap = plt.colormaps[cmap]

        if not isinstance(endpoints, list) or len(endpoints) < 2:
            raise ValueError(
                "`endpoints` must be a list of at least 2 node IDs or a "
                "list of at least two tuples where each tuple contains the "
                "x, y coordinates of endpoints."
            )

        # Check if `endpoints` are within grid bounds while setting
        # `_end_nodes`.

        self._end_nodes = []
        for point in endpoints:
            node, _ = self._get_node_and_coords(point)
            self._end_nodes.append(node)

    @property
    def data_structure(self):
        """OrderedDict defining the profile.

        The node IDs and distances along the profile are stored in
        ``data_structure``. It is a dictionary with keys of the segment ID.
        The value of each key is itself a dictionary of the segment attributes.
        First, 'ids' contains a list of the node IDs of segment samples ordered
        from the start to the end of the segment. It includes the endpoints.
        Second, 'distances' contains a list of along-profile distances that
        mirrors the list in 'ids'. Finally, 'color' is an RGBA tuple indicating
        the color for the segment.
        """
        return self._data_struct

    def _create_profile_structure(self):
        """Create the data structure of the profile.

        The profile is processed by segment. Segments are bound by successive
        endpoints. The cumulative distance along the profile is accumulated by
        iteratively adding segment lengths.
        """
        self._data_struct = OrderedDict()
        grid = self._grid
        endnodes = self._end_nodes
        cum_dist = 0

        for i_endpt in range(len(endnodes) - 1):
            # Get the endpoints and samples of the segment.

            start_node, start_xy = self._get_node_and_coords(endnodes[i_endpt])
            end_node, end_xy = self._get_node_and_coords(endnodes[i_endpt + 1])

            sample_nodes = self._get_sample_nodes(start_node, end_node)

            # Calculate the along-profile distance of samples along the
            # segment.

            n_samples = len(sample_nodes)
            sample_distances = np.empty(n_samples, dtype=float)

            for i_sample, node in enumerate(sample_nodes):
                sample_xy = grid.xy_of_node[node]

                pt = self._project_point_onto_line(sample_xy, start_xy, end_xy)
                d = grid.calc_distances_of_nodes_to_point(pt, node_subset=start_node)
                sample_distances[i_sample] = d[0]

            # Store the segment data.

            self._data_struct[i_endpt] = {
                "ids": np.array(sample_nodes),
                "distances": sample_distances + cum_dist,
            }

            cum_dist += max(sample_distances)

        self._assign_colors()
        self._create_flat_structures()

    def _assign_colors(self, color_mapping=None):
        """Assign a unique color for each segment.

        Parameters
        ----------
        color_mapping : str
            Color map name.
        """
        if color_mapping is None:
            segment_count = len(self._data_struct)
            norm = colors.Normalize(vmin=0, vmax=segment_count)
            mappable = cm.ScalarMappable(norm=norm, cmap=self._cmap)
            color_mapping = {
                segment_id: mappable.to_rgba(idx)
                for idx, segment_id in enumerate(self._data_struct)
            }

        for segment_id in self._data_struct:
            self._data_struct[segment_id]["color"] = color_mapping[segment_id]

    def _create_flat_structures(self):
        """Create expected flattened structures for ids, distances, and colors."""
        self._nodes = []
        self._distance_along_profile = []
        self._colors = []

        for segment_id in self._data_struct:
            self._nodes.append(self._data_struct[segment_id]["ids"])
            self._distance_along_profile.append(
                self._data_struct[segment_id]["distances"]
            )
            self._colors.append(self._data_struct[segment_id]["color"])

    def _get_node_and_coords(self, point):
        """Get the node and coordinates for a point.

        This method handles the option that endpoints can be a node or tuple.
        The grid methods called here verify if the point is within the grid.
        """
        if isinstance(point, (float, int, np.integer)):
            return point, self._grid.xy_of_node[point]
        elif isinstance(point, (tuple, list, np.ndarray)) and len(point) == 2:
            return self._grid.find_nearest_node(point), point
        else:
            raise TypeError(
                "each element of `endpoints` must be a number "
                "representing a node id or a tuple of node x, y "
                "coordinates"
            )

    def _get_sample_nodes(self, start_node, end_node):
        """Get the profile sample nodes using Bresenham's line algorithm.

        Parameters
        ----------
        start_node, end_node : integer
            The node id of a profile endpoint.

        Returns
        -------
        list of integers
            The node ids of the profile samples.

        Notes
        -----
        See: https://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm
        """
        # Get node column and row numbers to act as the coordinates.

        y0, x0 = np.argwhere(self._grid.nodes == start_node)[0]
        y1, x1 = np.argwhere(self._grid.nodes == end_node)[0]

        dx = x1 - x0
        dy = y1 - y0

        trace_is_steep = abs(dy) > abs(dx)

        if trace_is_steep:
            x0, y0 = y0, x0
            x1, y1 = y1, x1

        flipped_nodes = x0 > x1

        if flipped_nodes:
            x0, x1 = x1, x0
            y0, y1 = y1, y0

        dx = x1 - x0
        dy = y1 - y0

        error = int(dx / 2.0)

        if y0 < y1:
            y_step = 1
        else:
            y_step = -1

        # Iterate within the bounding box to identify the profile sample nodes.

        samples = []
        y = y0

        for x in range(x0, x1 + 1):
            if trace_is_steep:
                coord = (x, y)
            else:
                coord = (y, x)

            samples.append(self._grid.grid_coords_to_node_id(*coord))

            error -= abs(dy)
            if error < 0:
                y += y_step
                error += dx

        if flipped_nodes:
            samples.reverse()

        return samples

    def _project_point_onto_line(self, p, ep0, ep1):
        """Get the coordinates along a line nearest to a point.

        Parameters
        ----------
        p : tuple of floats
            The x, y coordinates of the point to project onto the line.
        ep0, ep1 : tuple of floats
            The endpoints of the line. Each endpoint is a tuple of x, y
            coordinates.

        Returns
        -------
        tuple
            The x, y coordinates along a line (bounded by `ep1` and `ep2`) that
            is nearest to `p`.
        """
        dx, dy = ep1[0] - ep0[0], ep1[1] - ep0[1]
        determinant = dx * dx + dy * dy
        coeff = (dy * (p[1] - ep0[1]) + dx * (p[0] - ep0[0])) / determinant

        return ep0[0] + coeff * dx, ep0[1] + coeff * dy

    def plot_profiles_in_map_view(
        self, field="topographic__elevation", endpoints_only=True, **kwds
    ):
        """Plot profile locations in map view.

        This method overrides the method in ``_BaseProfiler`` to set the
        default of ``endpoints_only`` to True.

        Parameters
        ----------
        field : field name or nnode array
            Array of the at-node-field to plot as the 2D map values.
            Default value is the at-node field 'topographic__elevation'.
        endpoints_only : boolean
            Boolean where False indicates every node along the profile is
            plotted, or True (default) indicating only segment endpoints are
            plotted.
        **kwds : dictionary
            Keyword arguments to pass to imshow_grid.
        """
        super().plot_profiles_in_map_view(field, endpoints_only=endpoints_only, **kwds)



================================================
File: profiler/trickle_down_profiler.py
================================================
# ! /usr/env/python
"""trickle_down_profiler.py component to create channel profiles."""
from collections import OrderedDict

import matplotlib as mpl
import matplotlib.pyplot as plt
from matplotlib import cm

from landlab.components.profiler.base_profiler import _BaseProfiler
from landlab.core.utils import as_id_array
from landlab.utils.flow__distance import calculate_flow__distance


class TrickleDownProfiler(_BaseProfiler):
    """Extract and a profile from one or more node IDs to their downstream termini.

    The TrickleDownProfiler extracts channel networks from a landlab grid.
    Unlike the ChannelProfiler which starts at one or more watershed outlets
    and works upstream until it reaches the end of the channel (based on
    a specified threshold, such as drainage area) the TrickleDownProfiler
    starts at a *starting node* and works its way downhill until it reaches
    an outlet or sink.

    In order to follow the channel network, the flow connectivity across the
    grid must already be identified. This is typically done with the
    FlowAccumulator component. However, this component does not require that the
    FlowAccumulator was used. Instead it expects that the following at-node
    grid fields will be present:
    ::

        'flow__receiver_node'
        'flow__link_to_receiver_node'

    The TrickleDownProfiler can work on grids that have used route-to-one or
    route-to-multiple flow directing.

    To understand how this component works it is useful to define the following
    terms: *outlet*, *starting node*, and *segment*.

    Consider the following grid with 10 columns and 7 rows. ``@`` represents
    the *starting node*, ``.`` represents the nodes downstream, and the
    watershed outlet node is indicated by ``o``.

    In this and the following examples, we will use only D4 connectivity. The
    ChannelProfiler, however, knows nothing of connectivity other than what is
    implied by the two required grid fields.
    ::

        X X X X X X X X X X
        X X X X X X X X X X
        X X X X X X . . @ X
        X X X X X X . X X X
        X X X . . . . X X X
        X X X . X X X X X X
        X X X o X X X X X X

    For each starting node, the TrickleDownProfiler follows the network
    downstream until it reaches the outlet or sink. One or more starting nodes
    can be used, depending on a user's needs.

    The node IDs and distances upstream of the channel network are stored in
    ``data_structure``. It is a dictionary with keys indicating the starting
    node.

    For each starting node, the value in the ``data_structure`` is itself
    a dictionary with keys that are a segment ID tuple of the
    ``(dowstream, upstream)`` nodes IDs of each channel segment.

    For our simple example, these are the node IDs:
    ::

            X  X  X  X  X  X  X  X  X  X
            X  X  X  X  X  X  X  X  X  X
            X  X  X  X  X  X 46 47 48  X
            X  X  X  X  X  X 36  X  X  X
            X  X  X 23 24 25 26  X  X  X
            X  X  X 13  X  X  X  X  X  X
            X  X  X  3  X  X  X  X  X  X

    The starting node is 48 and the outlet node is 3.

    The value associated with the segment ID tuple ``(3, 48)`` is itself a
    dictionary. It has three key-value pairs. First, ``"ids"`` contains a list
    of the segment node ids ordered from downstream to upstream. It includes
    the endpoints. Second, ``"distances"`` contains a list of distances
    upstream that mirrors the list in ``"ids"``. Finally, ``"color"`` is an
    RGBA tuple indicating the color for the segment.

    By default a unique color will be assigned to each starting node. To change
    the color, a user can change values stored in ``data_structure``.
    Additionally, a ``cmap`` keyword argument can provide some user control
    over the color at the instantiation of the component.

    For example with a starting node of 48, the data structure will look as
    follows:

    .. code-block:: python

        {
            48: {
                (3, 48): {
                    "ids": [3, 13, 23, 24, 25, 26, 36, 46, 47, 48],
                    "distances": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
                    "color": (1, 0, 1, 1),
                }
            }
        }

    Note that the distances upstream are relative to the outlet.

    Next consider an example with two starting nodes, each noted with an ``@``.
    ::

        X X X X X X X X X X
        X X X X X X @ X X X
        X X X X @ X . X X X
        o . . . . X . X X X
        X X X X X X . X X X
        X X X X X X . X X X
        X X X X X X . . . X
        X X X X X X X X o X

    And the following node IDs.
    ::

        X   X   X   X   X   X   X   X   X   X
        X   X   X   X   X   X  66   X   X   X
        X   X   X   X  54   X  56   X   X   X
       40  41  42  43  44   X  46   X   X   X
        X   X   X   X   X   X  36   X   X   X
        X   X   X   X   X   X  26   X   X   X
        X   X   X   X   X   X  16  17  18   X
        X   X   X   X   X   X   X   X   8   X

    With our starting nodes of 54 and 66 our data structure will look like.

    .. code-block:: python

        {
            54: {
                (40, 54): {
                    "ids": [40, 41, 42, 43, 44, 54],
                    "distances": [0, 1, 3, 4, 5, 6],
                    "color": [0.27, 0.0, 0.33, 1.0],
                },
            },
            66: {
                (8, 66): {
                    "ids": [8, 18, 17, 16, 26, 36, 46, 56, 66],
                    "distances": [0, 1, 2, 3, 4, 5, 6, 7, 8],
                    "color": [0.13, 0.57, 0.55, 1.0],
                },
            },
        }

    Examples
    --------

    Start by importing necessary modules

    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import FlowAccumulator, TrickleDownProfiler

    Create the second example grid we showed above. Note that in order to do
    this we need to enter the elevations starting from the lower left so the
    elevation order may seem upside-down. In addition, in this example,
    elevation is only provided along the profiles. The third line of code below
    sets all nodes with a value of zero to closed, such that these nodes are
    igored.
    >>> z = np.array(
    ...     [
    ...         [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
    ...         [0, 0, 0, 0, 0, 0, 4, 3, 2, 0],
    ...         [0, 0, 0, 8, 7, 6, 5, 0, 0, 0],
    ...         [0, 0, 0, 0, 0, 0, 6, 0, 0, 0],
    ...         [1, 3, 4, 5, 6, 0, 7, 0, 0, 0],
    ...         [0, 4, 0, 0, 7, 0, 8, 0, 0, 0],
    ...         [0, 5, 6, 0, 0, 0, 9, 0, 0, 0],
    ...         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    ...     ],
    ...     dtype=float,
    ... )

    >>> mg = RasterModelGrid((8, 10))
    >>> z = mg.add_field("topographic__elevation", z, at="node")
    >>> mg.set_nodata_nodes_to_closed(z, 0)
    >>> fa = FlowAccumulator(mg, flow_director="D4")
    >>> fa.run_one_step()
    >>> fa.node_drainage_area.reshape(mg.shape)
    array([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  11.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   9.,  10.,  11.,   0.],
           [  0.,   0.,   0.,   1.,   2.,   3.,   8.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   4.,   0.,   0.,   0.],
           [  8.,   8.,   4.,   3.,   2.,   0.,   3.,   0.,   0.,   0.],
           [  0.,   3.,   0.,   0.,   1.,   0.,   2.,   0.,   0.,   0.],
           [  0.,   2.,   1.,   0.,   0.,   0.,   1.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]])

    >>> profiler = TrickleDownProfiler(mg, starting_nodes=[54, 66])
    >>> profiler.run_one_step()

    The keys of the property ``data_structure`` are the IDs of the two outlet
    nodes.

    >>> profiler.data_structure.keys()
    odict_keys([54, 66])

    Within the data structure, the value at key 54, is a dictionary of the
    one segment, each specified by a ``(dowstream, upstream)`` tuple:

    >>> profiler.data_structure[54].keys()
    dict_keys([(40, 54)])

    The value of the segment between nodes 40 and 54 has the following
    components:

    >>> profiler.data_structure[54][(40, 54)]["ids"]
    array([40, 41, 42, 43, 44, 54])
    >>> profiler.data_structure[54][(40, 54)]["distances"]
    array([0., 1., 2., 3., 4., 5.])
    >>> np.round(profiler.data_structure[54][(40, 54)]["color"], decimals=2)
    array([0.27, 0.  , 0.33, 1.  ])

    The rest of the ``profile_structure`` encodes information about the second
    profile which starts at node 66.

    >>> profiler.data_structure[66].keys()
    dict_keys([(8, 66)])

    >>> profiler.data_structure[66][(8, 66)]["ids"]
    array([ 8, 18, 17, 16, 26, 36, 46, 56, 66])
    >>> profiler.data_structure[66][(8, 66)]["distances"]
    array([0., 1., 2., 3., 4., 5., 6., 7., 8.])
    >>> np.round(profiler.data_structure[66][(8, 66)]["color"], decimals=2)
    array([0.13, 0.57, 0.55, 1.  ])


    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    None Listed

    """

    _name = "TrickleDownProfiler"

    _unit_agnostic = True

    _info = {
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
    }

    def __init__(
        self,
        grid,
        starting_nodes=None,
        cmap="viridis",
    ):
        """
        Parameters
        ----------
        grid : Landlab Model Grid instance
        starting_nodes : iterable
        cmap : str, optional
            A valid matplotlib cmap string. Default is "viridis".

        """
        super().__init__(grid)

        self._cmap = plt.colormaps[cmap]

        self._flow_receiver = grid.at_node["flow__receiver_node"]
        self._starting_nodes = starting_nodes

    @property
    def data_structure(self):
        """OrderedDict defining the trickle down network.

        The IDs and upstream distance of the channel network nodes are stored
        in ``data_structure``. It is a dictionary with keys of the outlet node
        ID.

        For each starting node, the value in the ``data_structure`` is
        itself a dictionary with keys that are a segment ID tuple of the
        ``(dowstream, upstream)`` nodes IDs of each channel segment.

        The value associated with the segment ID tuple
        ``(dowstream, upstream)`` is itself a dictionary. It has three
        key-value pairs. First, ``"ids"`` contains a list of the segment node
        IDs ordered from downstream to upstream. It includes the endpoints.
        Second, ``"distances"`` contains a list of distances upstream that
        mirrors the list in ``"ids"``. Finally, ``"color"`` is an RGBA tuple
        indicating the color for the segment.
        """
        return self._data_struct

    def _create_profile_structure(self):
        """Create the profile_IDs data structure for channel network.

        The bound attribute self._profile structure is the channel segment
        datastructure. Profile structure is a list of length
        starting_nodes. Each element of profile_structure is itself a
        list of length number of stream segments that drain to each of the
        starting nodes. Each stream segment list contains the node ids of a
        stream segment from downstream to upstream.
        """
        self._data_struct = OrderedDict()

        for i in self._starting_nodes:
            channel_segment = []
            current_node = i

            # march downstream
            while self._flow_receiver[current_node] != current_node:
                channel_segment.append(current_node)
                current_node = self._flow_receiver[current_node]
            channel_segment.append(current_node)

            channel_segment.reverse()
            segment_tuple = (current_node, i)
            self._data_struct[i] = {
                segment_tuple: {"ids": as_id_array(channel_segment)}
            }

        self._calculate_distances()
        self.assign_colors()
        self._create_flat_structures()

    def _create_flat_structures(self):
        """Create expected flattened structures for ids, distances, and colors."""
        self._nodes = []

        self._distance_along_profile = []
        self._colors = []

        for outlet_id in self._data_struct:
            seg_tuples = self._data_struct[outlet_id].keys()
            self._nodes.extend(
                [self._data_struct[outlet_id][seg]["ids"] for seg in seg_tuples]
            )
            self._distance_along_profile.extend(
                [self._data_struct[outlet_id][seg]["distances"] for seg in seg_tuples]
            )
            self._colors.extend(
                [self._data_struct[outlet_id][seg]["color"] for seg in seg_tuples]
            )

    def assign_colors(self, color_mapping=None):
        """Assign a unique color for each starting node.

        Parameters
        ----------
        color_mapping : str
            Color map name.
        """

        if color_mapping is None:
            num_watersheds = len(self._data_struct)
            norm = mpl.colors.Normalize(vmin=0, vmax=num_watersheds)
            mappable = cm.ScalarMappable(norm=norm, cmap=self._cmap)
            color_mapping = {
                outlet_id: mappable.to_rgba(idx)
                for idx, outlet_id in enumerate(self._data_struct)
            }

        for outlet_id in self._data_struct:
            for segment_tuple in self._data_struct[outlet_id]:
                self._data_struct[outlet_id][segment_tuple]["color"] = color_mapping[
                    outlet_id
                ]

    def _calculate_distances(self):
        """Get distances along the network data structure."""
        distance_upstream = calculate_flow__distance(self._grid)
        for outlet_id in self._data_struct:
            for segment_tuple in self._data_struct[outlet_id]:
                ids = self._data_struct[outlet_id][segment_tuple]["ids"]
                d = distance_upstream[ids]
                self._data_struct[outlet_id][segment_tuple]["distances"] = d



================================================
File: radiation/__init__.py
================================================
from .radiation import Radiation

__all__ = ["Radiation"]



================================================
File: radiation/radiation.py
================================================
import copy

import numpy as np

from landlab import Component
from landlab.grid.mappers import map_node_to_cell

_VALID_METHODS = {"Grid"}


def _assert_method_is_valid(method):
    if method not in _VALID_METHODS:
        raise ValueError("%s: Invalid method name" % method)


class Radiation(Component):
    """Compute 1D and 2D daily incident shortwave radiation.

    Landlab component that computes 1D and 2D daily extraterrestiral, clear-sky,
    incident shortwave, net shortwave, longwave, and net radiation. This code also
    computes relative incidence shortwave radiation compared to a flat surface
    calculated at noon.

    **References**

    Bras, R. L.: Hydrology: an introduction to hydrologic science, Addison
    Wesley Publishing Company, Boston, Mass., USA, 643 pp., 1990.

    ASCE-EWRI (2005) The ASCE standardized reference evapotranspiration equation.
    In: Allen RG, Walter IA, Elliot RL et al (eds) Environmental and Water Resources
    Institute (EWRI) of the American Society of Civil Engineers, ASCE,
    Standardization of Reference Evapotranspiration Task Committee final report.
    American Society of Civil Engineers (ASCE), Reston

    Allen, R.G., 1996. Assessing integrity of weather data for  reference
    evapotranspiration estimation. J. Irrig. Drain.  Eng., ASCE 122 (2), 97-106.

    Flores-Cervantes, J.H., E. Istanbulluoglu, E.R. Vivoni, and R.L. Bras (2014).
    A geomorphic perspective on terrain-modulate organization of vegetation productivity:
    Analysis in two semiarid grassland ecosystems in Southwestern United States.
    Ecohydrol., 7: 242-257. doi: 10.1002/eco.1333.

    .. codeauthor:: Sai Nudurupati & Erkan Istanbulluoglu & Berkan Mertan


    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import Radiation
    >>> import numpy as np

    >>> grid = RasterModelGrid((5, 4), xy_spacing=(0.2, 0.2))
    >>> z = grid.add_zeros("node", "topographic__elevation")
    >>> rad = Radiation(grid)

    >>> grid.at_node["topographic__elevation"] = [
    ...     [0.0, 0.0, 0.0, 0.0],
    ...     [1.0, 1.0, 1.0, 1.0],
    ...     [2.0, 2.0, 2.0, 2.0],
    ...     [3.0, 4.0, 4.0, 3.0],
    ...     [4.0, 4.0, 4.0, 4.0],
    ... ]
    >>> rad.current_time = 0.5
    >>> rad.update()

    >>> grid.at_cell["radiation__net_shortwave_flux"].reshape((3, 2))
    array([[251.63813643, 251.63813643],
           [251.62345462, 251.62345462],
           [251.59409258, 251.59409258]])
    >>> grid.at_node["topographic__elevation"] = [
    ...     [0.0, 0.0, 0.0, 0.0],
    ...     [100.0, 100.0, 100.0, 100.0],
    ...     [200.0, 200.0, 200.0, 200.0],
    ...     [300.0, 400.0, 400.0, 300.0],
    ...     [400.0, 400.0, 400.0, 400.0],
    ... ]
    >>> calc_rad = Radiation(grid, current_time=0.0, kt=0.2)
    >>> calc_rad.update()

    >>> grid.at_cell["radiation__net_shortwave_flux"].reshape((3, 2))
    array([[188.10745478, 188.10745478],
           [187.84329564, 187.69076199],
           [183.82445291, 183.41439585]])

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed


    """

    _name = "Radiation"

    _unit_agnostic = False

    _info = {
        "radiation__extraterrestrial_flux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "W/m^2",
            "mapping": "cell",
            "doc": "extraterrestrial radiation",
        },
        "radiation__clearsky_flux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "W/m^2",
            "mapping": "cell",
            "doc": "clearsky radiation",
        },
        "radiation__incoming_shortwave_flux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "W/m^2",
            "mapping": "cell",
            "doc": "incident shortwave radiation",
        },
        "radiation__net_flux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "W/m^2",
            "mapping": "cell",
            "doc": "net radiation",
        },
        "radiation__net_longwave_flux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "W/m^2",
            "mapping": "cell",
            "doc": "net incident longwave radiation",
        },
        "radiation__net_shortwave_flux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "W/m^2",
            "mapping": "cell",
            "doc": "net incident shortwave radiation",
        },
        "radiation__ratio_to_flat_surface": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "None",
            "mapping": "cell",
            "doc": (
                "ratio of incident shortwave radiation on sloped "
                "surface to flat surface"
            ),
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
    }

    def __init__(
        self,
        grid,
        method="Grid",
        cloudiness=0.2,
        latitude=34.0,
        albedo=0.2,
        kt=0.17,
        clearsky_turbidity=None,
        opt_airmass=None,
        current_time=0.0,
        max_daily_temp=25.0,
        min_daily_temp=10.0,
    ):
        """
        Parameters
        ----------
        grid: RasterModelGrid
            A grid.
        method: {'Grid'}, optional
            Currently, only default is available.
        cloudiness: float, optional
            Cloudiness.
        latitude: float, optional
            Latitude (radians).
        albedo: float, optional
            Albedo.
        kt: float, optional
            Regional coefficient applied to actual KT coefficient (0.15-0.2). Default is 0.15
        clearsky_turbidity: float, optional
            Clear sky turbidity.
        opt_airmass: float, optional
            Optical air mass.
        max_daily_temp: float, optional
            Maximum daily temperature (Celsius)
        min_daily_Temp: float, optional
            Minimum daily temperature (Celsius)
        current_time: float
              Current time (years).
        """
        super().__init__(grid)

        self.current_time = current_time
        self._hour = 12

        self._method = method
        self._N = cloudiness
        self._latitude = self._validate_latitude(latitude)
        self._A = self._validate_albedo(albedo)

        # note that kt provided by the user is just a
        # 0.15-0.2 range value meant to indicate the type of region
        # where the model is run, e.g 0.2 for coastal regions,
        # 0.17 for interior regions, where the default is 0.17.
        # this parameter can be used as a calibration coefficient
        self._kt = kt

        self._n = clearsky_turbidity
        self._m = opt_airmass

        # For computations requiring temperature
        self._Tmin, self._Tmax = self._validate_temperature_range(
            min_daily_temp, max_daily_temp
        )

        _assert_method_is_valid(self._method)

        self.initialize_output_fields()

        if "Slope" not in self._grid.at_cell:
            self._grid.add_zeros("Slope", at="cell", units="radians")

        if "Aspect" not in self._grid.at_cell:
            self._grid.add_zeros("Aspect", at="cell", units="radians")

        self._nodal_values = self._grid["node"]
        self._cell_values = self._grid["cell"]

        self._slope, self._aspect = grid.calculate_slope_aspect_at_nodes_burrough(
            vals="topographic__elevation"
        )

        self._cell_values["Slope"] = self._slope
        self._cell_values["Aspect"] = self._aspect

        # Create a 'status' nodal field to store the status_at_node values
        # then map it to a cell-based field the same statuses. Use this to
        # generate a "closed elevations" field of boolean indexing.self
        self._gridCopy = copy.deepcopy(self._grid)
        self._gridCopy.add_field(
            "radiation_status_at_node", self._grid.status_at_node, at="node"
        )
        self._cellular_status = map_node_to_cell(
            self._gridCopy, "radiation_status_at_node"
        )
        self._closed_elevations = (
            self._cellular_status == self._gridCopy.BC_NODE_IS_CLOSED
        )

    def run_one_step(self, dt=None):
        if dt is None:
            dt = 1.0 / 365.0
        self.current_time += dt
        self.update()

    def _validate_latitude(self, latitude):
        if latitude < -90.0 or latitude > 90.0:
            raise ValueError("latitude must be between -90 and 90 degrees")
        return latitude

    def _validate_albedo(self, albedo):
        if albedo < 0.0 or albedo > 1.0:
            raise ValueError("albedo must be between 0 and 1")
        return albedo

    def _validate_temperature_range(self, min_temp, max_temp):
        if min_temp > max_temp:
            raise ValueError(
                f"minimum temperature ({min_temp}) must be less than maximum ({max_temp})"
            )
        return min_temp, max_temp

    @property
    def day_of_year(self):
        return (self.current_time - np.floor(self.current_time)) * 365

    @property
    def solar_declination(self):
        return 0.409 * np.sin(2.0 * np.pi / 365.0 * self.day_of_year - 1.39)

    @property
    def relative_distance_factor(self):
        return 1 + (0.033 * np.cos(2.0 * np.pi / 365.0 * self.day_of_year))

    @property
    def actual_vapor_pressure(self):
        return 0.6108 * np.exp((17.27 * self._Tmin) / (237.7 + self._Tmin))

    def update(self):
        """Update fields with current loading conditions.

        This method looks to the properties ``current_time`` and
        ``hour`` and uses their values in updating fields.
        """
        self._t = self._hour

        # Julian Day - ASCE-EWRI Task Committee Report, Jan-2005 - Eqn 25, (52)
        self._julian = (self.current_time - np.floor(self.current_time)) * 365

        # Actual Vapor Pressure - ASCE-EWRI Task Committee Report,
        # Jan-2005 - Eqn 8, (38)
        self._ea = 0.6108 * np.exp((17.27 * self._Tmin) / (237.7 + self._Tmin))

        # Solar Declination Angle - ASCE-EWRI Task Committee Report,
        # Jan-2005 - Eqn 24,(51)
        self._sdecl = 0.409 * np.sin(((np.pi / 180.0) * self._julian) - 1.39)

        # Inverse Relative Distance Factor - ASCE-EWRI Task Committee Report,
        # Jan-2005 - Eqn 23,(50)
        self._dr = 1 + (0.033 * np.cos(np.pi / 180.0 * self._julian))

        # Generate spatially distributed field of flat surface to
        # sloped surface radiation incidence ratios
        self._ratio_flat_surface_calc()

        # Extraterrestrial radmodel.docx - ASCE-EWRI (2005), Eqn (21)
        self._Rext = (
            11.57
            * (24.0 / np.pi)
            * 4.92
            * self._dr
            * (
                (self._ws * np.sin(self._phi) * np.sin(self._sdecl))
                + (np.cos(self._phi) * np.cos(self._sdecl) * (np.sin(self._ws)))
            )
        )

        # Clear-sky Radiation, Rcs, ASCE-EWRI (2005) as default method
        # if (optionally) turbidity and optical air mass are both user defined,
        # an exponential decay model Bras (2.25) is used.
        # Optical airmass must be >0 or Rcs2 is disregarded (log of zero or / 0 error)
        self._rcs2valid = True
        if self._m is not None:
            self._Rcs2 = self._Rext * np.exp(
                -self._n * self._m * (0.128 - 0.054 * np.log10(self._m))
            )
        else:
            self._rcs2valid = False

        # Clear-sky Solar Radiation - ASCE-EWRI (2005), Eqn 19
        self._Rcs1 = self._Rext * (0.75 + 2 * (10**-5) * self._elevation)

        # Rcs1, set to Rcs2 for empirical method, Rcs for accurate method
        # Using optical air mass and turbidity is optional when watershed
        # is relatively flat. n and m are not required fields at all
        if self._n is not None and self._m is not None and self._rcs2valid:
            self._Rc = self._Rcs2
        else:
            self._Rc = self._Rcs1

        # KT adjustment factor for incoming short-wave calculations
        # this uses the ratio of local to sealevel barometric
        # pressure following Allen (1996)
        self._Po = 101.325

        # Local atmospheric pressure ASCE-EWRI (2005), Eqn (34)
        self._P = self._Po * ((293 - 0.0065 * self._elevation) / 293) ** 5.26

        # non-constant KT (adjustment coefficient) - Based on Allen (1995)
        self._KT = self._kt * (self._P / self._Po) ** 0.5

        # Incoming shortwave radiation cannot exceed clear-sky radiation
        # Shortwave radiation should ideally be below clearsky radiation
        # (Rc), if there is a case where the standard shortwave rad
        # formula yields a result greater than Rc, set shortwave
        # radiation to the clearsky radiation itself.
        self._Rs = np.minimum(
            self._KT * self._Rext * np.sqrt(self._Tmax - self._Tmin), self._Rc
        )

        # Net shortwave Radiation - ASCE-EWRI (2005), Eqn (43)
        self._Rns = self._Rs * (1 - self._A)

        # Relative Cloudiness - ASCE-EWRI (2005), Eqn (18)
        self._u = 1.35 * (self._Rs / self._Rc) - 0.35

        # Cloudiness should be within 0.05 and 1 so as to not
        # nullify or incorrectly calculate the net longwave radiation
        self._u = np.clip(self._u, 0.05, 1.0)

        # Net Longwave Radiation - ASCE-EWRI (2005), Eqn (17) in W/M^2
        self._Rnl = (
            5.67
            * (10**-8)
            * ((self._Tmax + 273.15) ** 4 + (self._Tmin + 273.15) ** 4)
            / 2
            * (0.34 - (0.14 * np.sqrt(self._ea)))
            * self._u
        )

        # Load spatially distributed ratio to flat surface function values
        self._cell_values["radiation__ratio_to_flat_surface"] = self._radf

        # If sun does not rise, accounted by an invalid trig function
        # input, set all NaN grid values to 0.0 instead.
        self._radf[np.isnan(self._radf)] = 0.0
        self._Rext = 0.0 if np.isnan(self._Rext) else self._Rext

        # Net Radiation - ASCE-EWRI (2005), Eqn 15
        # Apply the ratio to flat surface to net shortwave
        # radiation first, then use spatially distributed
        # shortwave radiation to calculate net radiation
        np.multiply(
            self._Rext,
            self._cell_values["radiation__ratio_to_flat_surface"],
            out=self._cell_values["radiation__extraterrestrial_flux"],
        )

        # Clearsky flux
        np.multiply(
            self._Rc,
            self._cell_values["radiation__ratio_to_flat_surface"],
            out=self._cell_values["radiation__clearsky_flux"],
        )

        # Incoming shortwave flux
        np.multiply(
            self._Rs,
            self._cell_values["radiation__ratio_to_flat_surface"],
            out=self._cell_values["radiation__incoming_shortwave_flux"],
        )

        # Net shortwave flux
        np.multiply(
            self._Rns,
            self._cell_values["radiation__ratio_to_flat_surface"],
            out=self._cell_values["radiation__net_shortwave_flux"],
        )

        # Net longwave flux
        np.multiply(
            self._Rnl,
            self._cell_values["radiation__ratio_to_flat_surface"],
            out=self._cell_values["radiation__net_longwave_flux"],
        )

        # Net radiation flux is net shortwave - net longwave
        np.subtract(
            self._cell_values["radiation__net_shortwave_flux"],
            self._cell_values["radiation__net_longwave_flux"],
            out=self._cell_values["radiation__net_flux"],
        )

    def _ratio_flat_surface_calc(self):
        """generate radiation__ratio_to_flat_surface field

        This method looks to the slope, aspect values across
        the grid provided to the component, then runs calculations
        with solar altitude, latitude, and elevation to create a
        spatially distributed field of flat surface to sloped surface
        ratios / factors.
        """

        # self._elevation, elevation of surface above sea level
        self._elevation = self._nodal_values["topographic__elevation"]

        # Handle invalid values (closed nodes are not invalid)
        if np.any(
            (self._elevation < 0.0)
            & (self._grid.status_at_node != self._grid.BC_NODE_IS_CLOSED)
        ):
            raise ValueError(
                "No negative (< 0.0) values allowed in an above sea level elevation field."
            )

        # Map nodal elevation values to cells to calculate clearsky incidence
        # across a spatially distributed field
        self._elevation = map_node_to_cell(self._grid, "topographic__elevation")

        # Convert latitude to radians and store trig calculations
        self._phi = np.radians(self._latitude)  # Latitude in Radians
        self._sinLat = np.sin(self._phi)
        self._cosLat = np.cos(self._phi)

        # Get the hour angle using time of day
        self._tau = (self._t + 12.0) * np.pi / 12.0  # Hour angle

        # Calculate solar altitude using declination angle, hour angle, and latitude
        self._alpha = np.arcsin(
            np.sin(self._sdecl) * self._sinLat
            + (np.cos(self._sdecl) * self._cosLat * np.cos(self._tau))
        )  # Solar Altitude/Angle

        if self._alpha <= 0.25:  # If altitude is -ve,
            self._alpha = 0.25  # sun is beyond the horizon

        # For less constant calculation, keeping solar altitude trig in fixed variables
        self._cosSA = np.cos(self._alpha)
        self._sinSA = np.sin(self._alpha)

        # Avoid div by zero
        if self._cosSA <= 0.0001:
            self._cosSA = 0.0001

        # Sunset Hour Angle - ASCE-EWRI (2005), Eqn (59)
        self._ws = np.arccos(-np.tan(self._sdecl) * np.tan(self._phi))

        # Sun's Azimuth calculation code
        F = np.tan(self._alpha) * np.tan(self._phi) - (
            np.sin(self._sdecl) / (self._cosSA * self._cosLat)
        )

        # Clip azimuth within these bounds
        F = np.clip(F, -0.99999, 0.99999)

        if self._t < 12.0:
            self._phisun = np.pi - np.arccos(F)
        else:
            self._phisun = np.pi + np.arccos(F)

        self._flat = np.sin(self._alpha)

        # solar angle of incidence, Multiplying this with incoming radiation
        # gives radiation on sloped surface see Flores-Cervantes, J.H. (2012)
        self._sloped = np.cos(self._slope) * self._sinSA + np.sin(
            self._slope
        ) * self._cosSA * np.cos(self._phisun - self._aspect)

        self._sloped[self._sloped <= 0.0] = 0.0

        # Ratio of cosine of solar incidence angle of sloped surface to that
        # of a flat surface
        self._radf = self._sloped / self._flat

        self._radf[self._radf <= 0.0] = 0.0
        self._radf[self._radf > 6.0] = 6.0

        # Closed nodes will be omitted from spatially distributed ratio calculations
        self._radf[self._closed_elevations] = 0.0



================================================
File: river_flow_dynamics/__init__.py
================================================
from .river_flow_dynamics import RiverFlowDynamics

__all__ = ["RiverFlowDynamics"]



================================================
File: river_flow_dynamics/river_flow_dynamics.py
================================================
"""Simulate surface fluid flow based on Casulli and Cheng (1992).

This component implements a semi-implicit, semi-Lagrangian finite-volume approximation of
the depth-averaged shallow water equations originally proposed by Casulli and Cheng in 1992,
and subsequent related work.

Written by Sebastian Bernal and Angel Monsalve.

Examples
--------

This example demonstrates basic usage of the RiverFlowDynamics component to simulate
a simple channel flow:

>>> import numpy as np
>>> from landlab import RasterModelGrid
>>> from landlab.components import RiverFlowDynamics

Create a small grid for demonstration purposes:

>>> grid = RasterModelGrid((8, 6), xy_spacing=0.1)

Set up a sloped channel with elevated sides (slope of 0.01).

>>> z = grid.add_zeros("topographic__elevation", at="node")
>>> z += 0.005 - 0.01 * grid.x_of_node
>>> z[grid.y_of_node > 0.5] = 1.0
>>> z[grid.y_of_node < 0.2] = 1.0

Instantiating the Component. To check the names of the required inputs, use
the 'input_var_names' class property.

>>> RiverFlowDynamics.input_var_names
('surface_water__depth',
 'surface_water__elevation',
 'surface_water__velocity',
 'topographic__elevation')

Initialize required fields:

>>> h = grid.add_zeros("surface_water__depth", at="node")
>>> vel = grid.add_zeros("surface_water__velocity", at="link")
>>> wse = grid.add_zeros("surface_water__elevation", at="node")
>>> wse += h + z

Set up inlet boundary conditions (left side of channel):
Water flows from left to right at a depth of 0.5 meters with a velocity of 0.45 m/s.

>>> fixed_entry_nodes = np.arange(12, 36, 6)
>>> fixed_entry_links = grid.links_at_node[fixed_entry_nodes][:, 0]
>>> entry_nodes_h_values = np.full(4, 0.5)
>>> entry_links_vel_values = np.full(4, 0.45)

Instantiate 'RiverFlowDynamics'

>>> rfd = RiverFlowDynamics(
...     grid,
...     dt=0.1,
...     mannings_n=0.012,
...     fixed_entry_nodes=fixed_entry_nodes,
...     fixed_entry_links=fixed_entry_links,
...     entry_nodes_h_values=entry_nodes_h_values,
...     entry_links_vel_values=entry_links_vel_values,
... )

Run the simulation for 100 timesteps (equivalent to 10 seconds).

>>> n_timesteps = 100
>>> for timestep in range(n_timesteps):
...     rfd.run_one_step()
...

Examine the flow depth at the center of the channel after 10 seconds.

>>> flow_depth = np.reshape(grid["node"]["surface_water__depth"], (8, 6))[3, :]
>>> np.round(flow_depth, 3)
array([0.5  , 0.5  , 0.5  , 0.501, 0.502, 0.502])

And the velocity at links along the center of the channel.

>>> linksAtCenter = grid.links_at_node[np.array(np.arange(24, 30))][:-1, 0]
>>> flow_velocity = grid["link"]["surface_water__velocity"][linksAtCenter]
>>> np.round(flow_velocity, 3)
array([0.45 , 0.457, 0.455, 0.452, 0.453])

"""

import numpy as np
import scipy as sp

from landlab import Component


class RiverFlowDynamics(Component):
    """Simulate surface fluid flow based on Casulli and Cheng (1992).

    This Landlab component simulates surface fluid flow using the approximations of the
    2D shallow water equations developed by Casulli and Cheng in 1992. It calculates water
    depth and velocity across the raster grid, given a specific input discharge.

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    Casulli, V., Cheng, R.T. (1992). “Semi-implicit finite difference methods for
    three-dimensional shallow water flow”. International Journal for Numerical Methods
    in Fluids. 15: 629-648.
    https://doi.org/10.1002/fld.1650150602
    """

    _name = "RiverFlowDynamics"

    _unit_agnostic = False

    _info = {
        "surface_water__depth": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of water on the surface",
        },
        "surface_water__velocity": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m/s",
            "mapping": "link",
            "doc": "Speed of water flow above the surface",
        },
        "surface_water__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Water surface elevation at time N",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
    }

    def __init__(
        self,
        grid,
        dt=0.01,  # Sets the time step (s)
        eddy_viscosity=1e-4,  # ddy viscosity coefficient
        mannings_n=0.012,  # Manning's n
        threshold_depth=0.01,  # Sets the wet/dry threshold
        theta=0.5,  # Degree of 'implicitness' of the solution
        fixed_entry_nodes=None,  # Node IDs where flow enters the domain
        fixed_entry_links=None,  # Link IDs where flow enters the domain
        entry_nodes_h_values=None,  # Water depth at nodes where flow enters the domain
        entry_links_vel_values=None,  # Water velocity at links where flow enters the domain
        pcg_tolerance=1e-05,  # Preconditioned Conjugate Gradient convergence tolerance
        pcg_max_iterations=None,  # Preconditioned Conjugate Gradient max iterations
        surface_water__elevation_at_N_1=0.0,  # Surf water elev at prev. time
        surface_water__elevation_at_N_2=0.0,  # Surf water elev at prev prev time
        surface_water__velocity_at_N_1=0.0,  # Speed of water at prev time
    ):
        """Simulate the vertical-averaged surface fluid flow

        Simulate vertical-averaged surface fluid flow using the Casulli and Cheng (1992)
        approximations of the 2D shallow water equations. This Landlab component calculates
        water depth and velocity across the raster grid based on a given input discharge.

        Parameters
        ----------
        grid : RasterModelGrid
            A grid.
        dt : float, optional
            Time step in seconds. If not provided, it is calculated from the CFL condition.
        eddy_viscosity : float, optional
            Eddy viscosity coefficient. Default = 1e-4 :math:`m^2 / s`
        mannings_n : float or array_like, optional
            Manning's roughness coefficient. Default = 0.012 :math:`s / m^1/3`
        threshold_depth : float, optional
            Threshold at which a cell is considered wet. Default = 0.01 m
        theta : float, optional
            Degree of 'implicitness' of the solution, ranging between 0.5 and 1.0.
            Default: 0.5. When set to 0.5, the approximation is centered in time;
            when set to 1.0, it is fully implicit.
        fixed_entry_nodes : array_like or None, optional
            Node IDs where flow enters the domain (Dirichlet boundary condition).
            If not provided, existing water in the domain is not renewed.
        fixed_entry_links : array_like or None, optional
            Link IDs where flow enters the domain (Dirichlet boundary condition).
            If not provided, existing water in the domain is not renewed.
        entry_nodes_h_values : array_like, optional
            Water depth values at nodes where flow enters the domain
            (Dirichlet boundary condition).
            If not provided, existing water in the domain is not renewed.
        entry_links_vel_values : array_like, optional
            Water velocity values at links where flow enters the domain
            (Dirichlet boundary condition).
            If not provided, existing water in the domain is not renewed.
        pcg_tolerance : float, optional
            Tolerance for convergence in the Preconditioned Conjugate Gradient
            method. Default: 1e-05.
        pcg_max_iterations : integer, optional
            Maximum number of iterations for the Preconditioned Conjugate Gradient
            method. Iteration stops after maxiter steps, even if the specified
            tolerance is not achieved. Default: None.
        surface_water__elevation_at_N_1: array_like of float, optional
            Water surface elevation at nodes at time N-1 [m].
        surface_water__elevation_at_N_2: array_like of float, optional
            Water surface elevation at nodes at time N-2 [m].
        surface_water__velocity_at_N_1: array_like of float, optional
            Speed of water flow at links above the surface at time N-1 [m/s].
        """
        super().__init__(grid)

        # User inputs
        self._dt = dt
        self._eddy_viscosity = eddy_viscosity
        self._g = sp.constants.g
        self._mannings_n = mannings_n
        self._threshold_depth = threshold_depth
        self._theta = theta
        self._pcg_tolerance = pcg_tolerance
        self._pcg_max_iterations = pcg_max_iterations

        # Getting topography for further calculations
        self._additional_z = 10  # To set the virtual reference elevation (z=0)
        self._max_elevation = self._grid.at_node["topographic__elevation"].max()
        self._z = (
            self._max_elevation
            + self._additional_z
            - self._grid.at_node["topographic__elevation"]
        )

        self._fixed_entry_nodes = [] if fixed_entry_nodes is None else fixed_entry_nodes
        self._fixed_entry_links = [] if fixed_entry_links is None else fixed_entry_links
        self._entry_nodes_h_values = (
            [] if entry_nodes_h_values is None else entry_nodes_h_values
        )
        self._entry_links_vel_values = (
            [] if entry_links_vel_values is None else entry_links_vel_values
        )

        # Creating fields if they don't exist
        if "surface_water__depth" not in self.grid.at_node:
            grid.add_zeros(
                "surface_water__depth",
                at="node",
                units=self._info["surface_water__depth"]["units"],
            )

        if "surface_water__velocity" not in self.grid.at_link:
            grid.add_zeros(
                "surface_water__velocity",
                at="link",
                units=self._info["surface_water__velocity"]["units"],
            )

        if "surface_water__elevation" not in self.grid.at_node:
            grid.add_field(
                "surface_water__elevation",
                self.grid.at_node["surface_water__depth"] - self._z,
                at="node",
                units=self._info["surface_water__elevation"]["units"],
            )

        self._surface_water__elevation_at_N_1 = np.broadcast_to(
            np.asarray(surface_water__elevation_at_N_1).flat, grid.number_of_nodes
        )

        self._surface_water__elevation_at_N_2 = np.broadcast_to(
            np.asarray(surface_water__elevation_at_N_2).flat, grid.number_of_nodes
        )

        self._surface_water__velocity_at_N_1 = np.broadcast_to(
            np.asarray(surface_water__velocity_at_N_1).flat, grid.number_of_links
        )

        # Assigning a class variable to the fields
        self._h = self._grid.at_node["surface_water__depth"]
        self._vel = self._grid.at_link["surface_water__velocity"]
        self._vel_at_N_1 = self._surface_water__velocity_at_N_1
        self._eta = self._grid.at_node["surface_water__elevation"] - (
            self._max_elevation + self._additional_z
        )
        self._eta_at_N_1 = self._surface_water__elevation_at_N_1 - (
            self._max_elevation + self._additional_z
        )
        self._eta_at_N_2 = self._surface_water__elevation_at_N_2 - (
            self._max_elevation + self._additional_z
        )

        # Open boundary conditions
        # water can leave the domain at everywhere, only limited by topography
        self.grid.status_at_node[self.grid.nodes_at_left_edge] = (
            self._grid.BC_NODE_IS_FIXED_VALUE
        )
        self.grid.status_at_node[self.grid.nodes_at_right_edge] = (
            self._grid.BC_NODE_IS_FIXED_VALUE
        )
        self.grid.status_at_node[self.grid.nodes_at_bottom_edge] = (
            self._grid.BC_NODE_IS_FIXED_VALUE
        )
        self.grid.status_at_node[self.grid.nodes_at_top_edge] = (
            self._grid.BC_NODE_IS_FIXED_VALUE
        )

        self._adjacent_nodes_at_corner_nodes = np.array(
            [
                # Top right
                [self.grid.nodes_at_top_edge[-2], self.grid.nodes_at_right_edge[-2]],
                # Top left
                [self.grid.nodes_at_top_edge[1], self.grid.nodes_at_left_edge[-2]],
                # Bottom left
                [self.grid.nodes_at_left_edge[1], self.grid.nodes_at_bottom_edge[1]],
                # Bottom right
                [self.grid.nodes_at_right_edge[1], self.grid.nodes_at_bottom_edge[-2]],
            ]
        )

        # Updating open boundary nodes/links
        self._open_boundary_nodes = self._grid.boundary_nodes
        self._open_boundary_links = np.unique(
            self._grid.links_at_node[self._open_boundary_nodes]
        )

        self._open_boundary_nodes = np.setdiff1d(
            self._open_boundary_nodes, self._fixed_entry_nodes
        )
        self._open_boundary_links = np.setdiff1d(
            self._open_boundary_links, self._fixed_entry_links
        )

        self._fixed_corner_nodes = np.setdiff1d(
            self.grid.corner_nodes, self._open_boundary_nodes
        )
        self._open_corner_nodes = np.setdiff1d(
            self.grid.corner_nodes, self._fixed_corner_nodes
        )

        self._open_boundary_nodes = np.setdiff1d(
            self._open_boundary_nodes, self._open_corner_nodes
        )

        # Using fixed entry nodes/links only when they exist
        self._fixed_nodes_exist = len(self._fixed_entry_nodes) > 0
        self._fixed_links_exist = len(self._fixed_entry_links) > 0

        # Updating grid fixed values according to the user input
        if self._fixed_nodes_exist:
            self._h[self._fixed_entry_nodes] = entry_nodes_h_values
            self._eta[self._fixed_entry_nodes] = (
                entry_nodes_h_values - self._z[self._fixed_entry_nodes]
            )
        if self._fixed_links_exist:
            self._vel[self._fixed_entry_links] = entry_links_vel_values

        # Mapping node values at links
        self._z_at_links = self._grid.map_mean_of_link_nodes_to_link(self._z)
        self._h_at_links = self._grid.map_mean_of_link_nodes_to_link(self._h)
        self._eta_at_links = self._h_at_links - self._z_at_links

        # Passing values to the time step N
        self._h_at_N = self._h.copy()
        self._h_at_N_at_links = self._grid.map_mean_of_link_nodes_to_link(self._h_at_N)

        self._vel_at_N = self._vel.copy()
        self._eta_at_N = self._eta.copy()

        # Boolean for wet nodes/links
        self._wet_nodes = np.where(self._h_at_N >= self._threshold_depth, True, False)
        self._wet_links = np.where(
            self._h_at_N_at_links >= self._threshold_depth, True, False
        )

    # Defining some functions
    def find_nearest_link(self, x_coordinates, y_coordinates, objective_links="all"):
        """Link nearest a point.

        Find the index to the link nearest the given x, y coordinates.
        Returns the indices of the links nearest the given coordinates.

        """
        # Defining the set of links that are going to be used
        if objective_links == "all":
            objective_links = np.arange(self._grid.number_of_links)
        elif objective_links == "horizontal":
            objective_links = self.grid.horizontal_links
        elif objective_links == "vertical":
            objective_links = self.grid.vertical_links
        # if (objective_links == "all") END

        # Coordinates of all the RasterModelGrid links
        x_of_objective_links = np.unique(self._grid.xy_of_link[objective_links][:, 0])
        y_of_objective_links = np.unique(self._grid.xy_of_link[objective_links][:, 1])

        # Getting the closest link-coordinate to the exit point
        tempCalc1 = np.repeat(x_coordinates, len(x_of_objective_links)).reshape(
            len(x_coordinates), len(x_of_objective_links)
        )
        tempCalc2 = np.tile(x_of_objective_links, len(x_coordinates)).reshape(
            len(x_coordinates), len(x_of_objective_links)
        )
        indices = abs(tempCalc2 - tempCalc1).argmin(axis=1)
        nearest_x = x_of_objective_links[indices]

        tempCalc1 = np.repeat(y_coordinates, len(y_of_objective_links)).reshape(
            len(y_coordinates), len(y_of_objective_links)
        )
        tempCalc2 = np.tile(y_of_objective_links, len(y_coordinates)).reshape(
            len(y_coordinates), len(y_of_objective_links)
        )
        indices = abs(tempCalc2 - tempCalc1).argmin(axis=1)
        nearest_y = y_of_objective_links[indices]

        # Getting the closest link to link
        tempCalc1 = np.repeat(
            nearest_x, len(self._grid.xy_of_link[objective_links][:, 0])
        ).reshape(len(nearest_x), len(self._grid.xy_of_link[objective_links][:, 0]))
        tempCalc2 = np.tile(
            self._grid.xy_of_link[objective_links][:, 0], len(x_coordinates)
        ).reshape(len(x_coordinates), len(self._grid.xy_of_link[objective_links][:, 0]))
        tempB1 = tempCalc1 == tempCalc2

        tempCalc1 = np.repeat(
            nearest_y, len(self._grid.xy_of_link[objective_links][:, 1])
        ).reshape(len(nearest_y), len(self._grid.xy_of_link[objective_links][:, 1]))
        tempCalc2 = np.tile(
            self._grid.xy_of_link[objective_links][:, 1], len(y_coordinates)
        ).reshape(len(y_coordinates), len(self._grid.xy_of_link[objective_links][:, 1]))
        tempB2 = tempCalc1 == tempCalc2

        tempCalc3 = (
            np.repeat(objective_links, len(x_coordinates))
            .reshape((len(objective_links), len(y_coordinates)))
            .T
        )
        nearest_link = tempCalc3[tempB1 * tempB2]

        return nearest_link.astype(int)

    def find_adjacent_links_at_link(self, current_link, objective_links="horizontal"):
        """Get adjacent links to the link.

        This function finds the links at right, above, left and below the given link.
        Similar purpose to the "adjacent_nodes_at_node" function.
        Return the adjacent links in as many rows as given links.
        Link IDs are returned as columns in clock-wise order starting from East (E, N, W, S).

        """

        # Defining the set of links that are going to be used
        if objective_links == "horizontal":
            objective_links = self.grid.horizontal_links
            reshape_pair = (self.grid.shape[0], self.grid.shape[1] - 1)
        elif objective_links == "vertical":
            objective_links = self.grid.vertical_links
            reshape_pair = (self.grid.shape[0] - 1, self.grid.shape[1])
        # if (objective_links == "horizontal") END

        # Coordinates of the current link
        x_of_current_links = self._grid.xy_of_link[current_link][:, 0]
        y_of_current_links = self._grid.xy_of_link[current_link][:, 1]

        # Coordinates of all the RasterModelGrid links
        x_of_objective_links = np.unique(self._grid.xy_of_link[objective_links][:, 0])
        y_of_objective_links = np.unique(self._grid.xy_of_link[objective_links][:, 1])

        # Getting links that share the same y-coordinates
        # The following matrices are built to be compared to each other.
        # tempCalc1 repeats "y_of_current_links" for every x-coordinate in
        # "objective_links": cols = "y_of_current_links"
        # tempCalc2 repeats "y_of_objective_links" for every x-coordinate in
        # the "current_link": rows = "y_of_objective_links"
        # tempCalc3 give us the index to extract all the objective links that
        # are located in the same row than the current link: rows = [0, 1, 2, ...]
        tempCalc1 = np.repeat(y_of_current_links, len(y_of_objective_links)).reshape(
            len(y_of_current_links), len(y_of_objective_links)
        )
        tempCalc2 = np.tile(y_of_objective_links, len(y_of_current_links)).reshape(
            len(y_of_current_links), len(y_of_objective_links)
        )
        tempCalc3 = (
            np.repeat(np.arange(len(y_of_objective_links)), len(y_of_current_links))
            .reshape(len(y_of_objective_links), len(y_of_current_links))
            .T
        )

        indices = tempCalc3[tempCalc1 == tempCalc2]
        links_at_same_rows = objective_links.reshape(reshape_pair)[indices, :]
        links_at_same_rows = np.append(
            np.array([-np.ones_like(current_link)]).T, links_at_same_rows, axis=1
        )
        links_at_same_rows = np.append(
            links_at_same_rows, np.array([-np.ones_like(current_link)]).T, axis=1
        )

        # Getting links that share the same x-coordinates
        # The following matrices are built to be compared to each other.
        # tempCalc1 repeats "x_of_current_links" for every x-coordinate in
        # "objective_links": cols = "x_of_current_links"
        # tempCalc2 repeats "x_of_objective_links" for every x-coordinate in
        # the "current_link": rows = "x_of_objective_links"
        # tempCalc3 give us the index to extract all the objective links that
        # are located in the same row than the current link: rows = [0, 1, 2, ...]
        tempCalc1 = np.repeat(x_of_current_links, len(x_of_objective_links)).reshape(
            len(x_of_current_links), len(x_of_objective_links)
        )
        tempCalc2 = np.tile(x_of_objective_links, len(x_of_current_links)).reshape(
            len(x_of_current_links), len(x_of_objective_links)
        )
        tempCalc3 = (
            np.repeat(np.arange(len(x_of_objective_links)), len(x_of_current_links))
            .reshape(len(x_of_objective_links), len(x_of_current_links))
            .T
        )

        indices = tempCalc3[tempCalc1 == tempCalc2]
        links_at_same_cols = objective_links.reshape(reshape_pair)[:, indices].T
        links_at_same_cols = np.append(
            np.array([-np.ones_like(current_link)]).T, links_at_same_cols, axis=1
        )
        links_at_same_cols = np.append(
            links_at_same_cols, np.array([-np.ones_like(current_link)]).T, axis=1
        )

        # Extracing the adjacent links to current link (E,N,W,S)
        adjacent_links_at_link = np.zeros((current_link.shape[0], 4))

        # Rows (E,W)
        tempCalc1 = np.repeat(current_link, links_at_same_rows.shape[1]).reshape(
            current_link.shape[0], links_at_same_rows.shape[1]
        )
        tempCalc2 = (
            np.repeat(np.arange(links_at_same_rows.shape[1]), current_link.shape[0])
            .reshape(links_at_same_rows.shape[1], current_link.shape[0])
            .T
        )
        tempCalc3 = tempCalc2[tempCalc1 == links_at_same_rows]

        adjacent_links_at_link[:, 0] = links_at_same_rows[
            (range(links_at_same_rows.shape[0])), (tempCalc3 + 1)
        ]
        adjacent_links_at_link[:, 2] = links_at_same_rows[
            (range(links_at_same_rows.shape[0])), (tempCalc3 - 1)
        ]

        # Cols (N,S)
        tempCalc1 = np.repeat(current_link, links_at_same_cols.shape[1]).reshape(
            current_link.shape[0], links_at_same_cols.shape[1]
        )
        tempCalc2 = (
            np.repeat(np.arange(links_at_same_cols.shape[1]), current_link.shape[0])
            .reshape(links_at_same_cols.shape[1], current_link.shape[0])
            .T
        )
        tempCalc3 = tempCalc2[tempCalc1 == links_at_same_cols]

        adjacent_links_at_link[:, 1] = links_at_same_cols[
            (range(links_at_same_cols.shape[0])), (tempCalc3 + 1)
        ]
        adjacent_links_at_link[:, 3] = links_at_same_cols[
            (range(links_at_same_cols.shape[0])), (tempCalc3 - 1)
        ]

        return adjacent_links_at_link.astype(int)

    def path_line_tracing(self):
        """ " Path line tracing algorithm.

        This function implements the semi-analytical path line tracing method
        of Pollock (1988).

        The semi-analytical path line tracing method was developed for particle
        tracking in ground water flow models. The assumption that each directional
        velocity component varies linearly in its coordinate directions within
        each computational volume or cell underlies the method.
        Linear variation allows the derivation of an analytical expression for
        the path line of a particle across a volume.

        Given an initial point located at each volume faces of the domain, particle
        trayectories are traced backwards on time. Then, this function returns
        the departure point of the particle at the beginning of the time step.
        """
        dx, dy = self.grid.dx, self.grid.dy

        # Calculating the partial time-step TAUx, TAUy, dt - sum_partial_times
        sum_partial_times = np.zeros_like(self._u_vel_of_particle)
        remaining_time = self._dt - sum_partial_times
        keep_tracing = np.where(remaining_time > 0, True, False)

        while np.any(remaining_time > 0):
            # Using the previous exit point as the new entry point
            self._x_of_particle = np.where(
                keep_tracing, self._x_at_exit_point, self._x_of_particle
            )
            self._y_of_particle = np.where(
                keep_tracing, self._y_at_exit_point, self._y_of_particle
            )

            # Checking if the particles departs (backwards) from a link position (True)
            tempBx = np.isin(
                self._x_of_particle, self._grid.xy_of_link[self.grid.active_links][:, 0]
            )  # Particles located on horizontal-links/vertical-faces
            tempBy = np.isin(
                self._y_of_particle, self._grid.xy_of_link[self.grid.active_links][:, 1]
            )  # Particles located on vertical-links/horizontal-faces

            # True, particles depart from link positions.
            # False, particles depart from random locations inside a cell
            tempBxy = tempBx + tempBy

            # Getting surrounding links for particles located inside a cell
            tempCalc1 = np.append(
                np.array([self._x_of_particle]), np.array([self._y_of_particle]), axis=0
            )
            tempCalc2 = self._grid.find_nearest_node(tempCalc1, mode="raise")
            temp_links_from_node = self._grid.links_at_node[tempCalc2]
            nodes_from_particle = tempCalc2

            # Getting surrounding links for particles located at link positions
            tempCalc1 = np.where(
                self._u_vel_of_particle >= 0,
                np.array([self._x_of_particle]) - dx / 10,
                np.array([self._x_of_particle]) + dx / 10,
            )
            tempCalc2 = np.where(
                self._v_vel_of_particle >= 0,
                np.array([self._y_of_particle]) - dy / 10,
                np.array([self._y_of_particle]) + dy / 10,
            )
            tempCalc3 = np.append(tempCalc1, tempCalc2, axis=0)
            tempCalc4 = self._grid.find_nearest_node(tempCalc3, mode="raise")
            temp_links_from_link = self._grid.links_at_node[tempCalc4]
            nodes_from_particle = np.where(tempBxy, tempCalc4, nodes_from_particle)

            # Getting links around particle
            tempBxy = np.tile(tempBxy, 4).reshape(4, len(tempBxy)).T
            links_at_particle = np.where(
                tempBxy, temp_links_from_link, temp_links_from_node
            )

            # Defining links based on velocity direction
            link_at_x2 = np.where(
                self._u_vel_of_particle >= 0,
                links_at_particle[:, 0],
                links_at_particle[:, 2],
            )
            link_at_x1 = np.where(
                self._u_vel_of_particle >= 0,
                links_at_particle[:, 2],
                links_at_particle[:, 0],
            )
            link_at_y2 = np.where(
                self._v_vel_of_particle >= 0,
                links_at_particle[:, 1],
                links_at_particle[:, 3],
            )
            link_at_y1 = np.where(
                self._v_vel_of_particle >= 0,
                links_at_particle[:, 3],
                links_at_particle[:, 1],
            )

            x_at_x2 = np.where(
                self._u_vel_of_particle >= 0,
                self._grid.x_of_node[nodes_from_particle] + dx / 2,
                self._grid.x_of_node[nodes_from_particle] - dx / 2,
            )
            x_at_x1 = np.where(
                self._u_vel_of_particle >= 0,
                self._grid.x_of_node[nodes_from_particle] - dx / 2,
                self._grid.x_of_node[nodes_from_particle] + dx / 2,
            )
            y_at_y2 = np.where(
                self._v_vel_of_particle >= 0,
                self._grid.y_of_node[nodes_from_particle] + dy / 2,
                self._grid.y_of_node[nodes_from_particle] - dy / 2,
            )
            y_at_y1 = np.where(
                self._v_vel_of_particle >= 0,
                self._grid.y_of_node[nodes_from_particle] - dy / 2,
                self._grid.y_of_node[nodes_from_particle] + dy / 2,
            )

            # Getting velocity around the particle
            u_vel_at_x2 = np.where(
                link_at_x2 >= 0, self._vel_at_N[link_at_x2], self._vel_at_N[link_at_x1]
            )
            u_vel_at_x1 = np.where(
                link_at_x1 >= 0, self._vel_at_N[link_at_x1], self._vel_at_N[link_at_x2]
            )
            v_vel_at_y2 = np.where(
                link_at_y2 >= 0, self._vel_at_N[link_at_y2], self._vel_at_N[link_at_y1]
            )
            v_vel_at_y1 = np.where(
                link_at_y1 >= 0, self._vel_at_N[link_at_y1], self._vel_at_N[link_at_y2]
            )

            # Calculating gradients for path line tracing
            gradient_x_direction = (u_vel_at_x2 - u_vel_at_x1) / dx
            gradient_y_direction = (v_vel_at_y2 - v_vel_at_y1) / dy

            # Calculating entry velocity for each particle
            self._u_vel_of_particle = u_vel_at_x2 - gradient_x_direction * (
                x_at_x2 - self._x_of_particle
            )
            self._v_vel_of_particle = v_vel_at_y2 - gradient_y_direction * (
                y_at_y2 - self._y_of_particle
            )
            self._u_vel_of_particle = np.where(
                self._u_vel_of_particle < 1e-10, 0, self._u_vel_of_particle
            )
            self._v_vel_of_particle = np.where(
                self._v_vel_of_particle < 1e-10, 0, self._v_vel_of_particle
            )

            ### Calculation accoss x-direction
            # Avoiding divisions by zero
            tempCalc1 = np.where(
                self._u_vel_of_particle == 0, 9999, self._u_vel_of_particle
            )
            tempCalc2 = np.where(u_vel_at_x1 == 0, 9999, u_vel_at_x1)
            tempCalc3 = np.where(gradient_x_direction == 0, 9999, gradient_x_direction)
            TAUx = (1 / tempCalc3) * np.log(abs(tempCalc1 / tempCalc2))

            # Calculation when gradient is equal to zero
            tempCalc4 = abs((self._x_of_particle - x_at_x1) / tempCalc2)
            TAUx = np.where(gradient_x_direction == 0, tempCalc4, TAUx)

            # Calculation when:
            # a) Uxp/Ux1 = 1,
            # b) Uxp,Vyp = 0,
            # c) Ux1,Vy1 = 0, and
            # d) Uxp/Ux1, Vxp/Vy1 = -1
            tempCalc5 = self._u_vel_of_particle / tempCalc2
            TAUx = np.where(tempCalc5 == 1, tempCalc4, TAUx)
            TAUx = np.where(self._u_vel_of_particle == 0, remaining_time, TAUx)
            TAUx = np.where(u_vel_at_x1 == 0, remaining_time, TAUx)
            TAUx = np.where(tempCalc5 < 0, remaining_time, TAUx)
            TAUx = np.where(TAUx > self._dt, self._dt, TAUx)
            TAUx = np.where(TAUx < 0, 0, TAUx)

            ### Calculation across y-direction
            # Avoiding divisions by zero
            tempCalc1 = np.where(
                self._v_vel_of_particle == 0, 9999, self._v_vel_of_particle
            )
            tempCalc2 = np.where(v_vel_at_y1 == 0, 9999, v_vel_at_y1)
            tempCalc3 = np.where(gradient_y_direction == 0, 9999, gradient_y_direction)
            TAUy = (1 / tempCalc3) * np.log(abs(tempCalc1 / tempCalc2))

            # Calculation when gradient is equal to zero
            tempCalc4 = abs((self._y_of_particle - y_at_y1) / tempCalc2)
            TAUy = np.where(gradient_y_direction == 0, tempCalc4, TAUy)

            # Calculation when
            # a) Vyp/Vy1 = 1,
            # b) Uxp,Vyp = 0,
            # c) Ux1,Vy1 = 0, and
            # d) Uxp/Ux1, Vxp/Vy1 = -1
            tempCalc5 = self._v_vel_of_particle / tempCalc2
            TAUy = np.where(tempCalc5 == 1, tempCalc4, TAUy)
            TAUy = np.where(self._v_vel_of_particle == 0, remaining_time, TAUy)
            TAUy = np.where(v_vel_at_y1 == 0, remaining_time, TAUy)
            TAUy = np.where(tempCalc5 < 0, remaining_time, TAUy)
            TAUy = np.where(TAUy > self._dt, self._dt, TAUy)
            TAUy = np.where(TAUy < 0, 0, TAUy)

            # Obtaining TAU = min(TAUx, TAUy, (dt - sum_partial_times))
            TAUx = abs(TAUx)
            TAUy = abs(TAUy)
            TAU = np.array((TAUx, TAUy, remaining_time)).min(axis=0)
            # TAU  = np.where(TAU < 1e-10, 0, TAU)

            # Calculating exit point Xe, Ye
            tempCalc1 = np.where(gradient_x_direction == 0, 9999, gradient_x_direction)
            tempCalc2 = np.where(gradient_y_direction == 0, 9999, gradient_y_direction)

            # Exit point Xe (tempCalc3) and Ye (tempCalc4)
            tempCalc3 = x_at_x2 - (1 / tempCalc1) * (
                u_vel_at_x2
                - self._u_vel_of_particle / np.exp(gradient_x_direction * TAU)
            )
            tempCalc4 = y_at_y2 - (1 / tempCalc2) * (
                v_vel_at_y2
                - self._v_vel_of_particle / np.exp(gradient_y_direction * TAU)
            )

            tempCalc3 = np.where(
                gradient_x_direction == 0,
                self._x_of_particle - u_vel_at_x2 * TAU,
                tempCalc3,
            )
            tempCalc4 = np.where(
                gradient_y_direction == 0,
                self._y_of_particle - v_vel_at_y2 * TAU,
                tempCalc4,
            )

            tempCalc3 = np.where(
                self._u_vel_of_particle == 0, self._x_of_particle, tempCalc3
            )
            tempCalc4 = np.where(
                self._v_vel_of_particle == 0, self._y_of_particle, tempCalc4
            )

            self._x_at_exit_point = np.where(
                keep_tracing, tempCalc3, self._x_at_exit_point
            )
            self._y_at_exit_point = np.where(
                keep_tracing, tempCalc4, self._y_at_exit_point
            )

            # Updating sum of partial time-steps, TAU
            sum_partial_times = np.where(
                keep_tracing, sum_partial_times + TAU, self._dt
            )

            # Checking remaining_time == 0 (dt = sum_partial_times)
            remaining_time = np.where(
                remaining_time == 0, 0, self._dt - sum_partial_times
            )

            # Correcting entry velocity
            tempCalc1 = np.where(self._u_vel_of_particle == 0, 1, 0)
            tempCalc2 = np.where(self._v_vel_of_particle == 0, 1, 0)
            remaining_time = np.where((tempCalc1 * tempCalc2) == 1, 0, remaining_time)

            # Correction for static particles
            remaining_time = np.where(
                abs(self._x_of_particle - self._x_at_exit_point) < 1e-7,
                0,
                remaining_time,
            )
            remaining_time = np.where(
                abs(self._y_of_particle - self._y_at_exit_point) < 1e-7,
                0,
                remaining_time,
            )

            # Stop tracing if a particle hits the boundary
            # Keep tracing for all particles
            tempCalc1 = np.repeat(True, len(keep_tracing))
            # Particle hits the left edge?
            tempCalc2 = np.isin(
                self._x_at_exit_point,
                self._grid.x_of_node[self.grid.nodes_at_left_edge],
            )
            # If above True, stop tracing for that particle
            tempCalc1 = np.where(tempCalc2, False, tempCalc1)
            # Particle hits the right edge?
            tempCalc2 = np.isin(
                self._x_at_exit_point,
                self._grid.x_of_node[self.grid.nodes_at_right_edge],
            )
            # If above True, stop tracing for that particle
            tempCalc1 = np.where(tempCalc2, False, tempCalc1)
            # Particle hits the top edge?
            tempCalc2 = np.isin(
                self._y_at_exit_point, self._grid.y_of_node[self.grid.nodes_at_top_edge]
            )
            # If above True, stop tracing for that particle
            tempCalc1 = np.where(tempCalc2, False, tempCalc1)
            # Particle hits the bottom edge?
            tempCalc2 = np.isin(
                self._y_at_exit_point,
                self._grid.y_of_node[self.grid.nodes_at_bottom_edge],
            )
            # If above True, stop tracing for that particle
            tempCalc1 = np.where(tempCalc2, False, tempCalc1)
            # Where particles reach the boundary, remaining time is equal to zero
            # remaining_time = np.where(not tempCalc1, 0, remaining_time)
            remaining_time = np.where(~tempCalc1, 0, remaining_time)

            # Updating on particles that need to traced backwards
            keep_tracing = np.where(remaining_time > 0, True, False)

            # WHILE "np.any(remaining_time > 0)" END
        # DEF "path_line_tracing(self)" END

    def run_one_step(self):
        """Calculate water depth and water velocity for a time period dt."""
        dx, dy = self.grid.dx, self.grid.dy

        # Getting velocity as U,V components
        self._u_vel = self._vel_at_N[self.grid.horizontal_links]
        self._v_vel = self._vel_at_N[self.grid.vertical_links]

        # Calculating Chezy coefficient
        self._chezy_at_nodes = self._h_at_N ** (1 / 6) / self._mannings_n
        self._chezy_at_links = self._h_at_N_at_links ** (1 / 6) / self._mannings_n

        # Computing V-velocity (vertical links) at U-velocity positions (horizontal links)
        tempCalc1 = self._grid.map_mean_of_horizontal_links_to_node(self._vel_at_N)
        self._u_vel_at_v_links = np.mean(
            tempCalc1[self._grid.nodes_at_link[self.grid.vertical_links]], axis=1
        )

        # Computing U-velocity (horizontal links) at V-velocity positions (vertical links)
        tempCalc1 = self._grid.map_mean_of_vertical_links_to_node(self._vel_at_N)
        self._v_vel_at_u_links = np.mean(
            tempCalc1[self._grid.nodes_at_link[self.grid.horizontal_links]], axis=1
        )

        # Setting A-faces
        self._a_links = np.zeros_like(self._vel_at_N)

        # Setting dry links equal to 1 to avoid divisions by zero
        tempCalc1 = np.where(self._wet_links, self._chezy_at_links, 1)

        # Computing A-faces
        self._a_links[self.grid.horizontal_links] = (
            self._h_at_N_at_links[self.grid.horizontal_links]
            + self._g
            * self._dt
            * (
                self._vel_at_N[self.grid.horizontal_links] ** 2
                + self._v_vel_at_u_links**2
            )
            ** (1 / 2)
            / tempCalc1[self.grid.horizontal_links]
        )
        self._a_links[self.grid.vertical_links] = (
            self._h_at_N_at_links[self.grid.vertical_links]
            + self._g
            * self._dt
            * (
                self._vel_at_N[self.grid.vertical_links] ** 2
                + self._u_vel_at_v_links**2
            )
            ** (1 / 2)
            / tempCalc1[self.grid.vertical_links]
        )

        # Using only wet-link values, and setting dry links equal to 1 to avoid
        # divisions by zero
        self._a_links = np.where(self._wet_links, self._a_links, 1)

        # Path line tracing
        # U-velocity, x-direction, horizontal links
        # Getting the initial particle location at each volume faces
        tempB1 = [i in self.grid.horizontal_links for i in self.grid.active_links]
        self._x_of_particle = self._grid.xy_of_link[:, 0][self.grid.active_links][
            tempB1
        ]
        self._y_of_particle = self._grid.xy_of_link[:, 1][self.grid.active_links][
            tempB1
        ]

        # Getting the initial particle velocity
        tempB2 = [i in self.grid.active_links for i in self.grid.horizontal_links]
        self._u_vel_of_particle = self._u_vel[tempB2]
        self._v_vel_of_particle = self._v_vel_at_u_links[tempB2]

        # Getting a first 'exit' point to begin the loop
        self._x_at_exit_point = self._x_of_particle
        self._y_at_exit_point = self._y_of_particle

        # Calculating path line backwards on time
        self.path_line_tracing()

        # Bicuatradic interpolation
        # U-velocity, x-direction, around (p,q) location
        self._UsL = np.zeros_like(self._u_vel)

        # Getting V-velocity at U-links
        temp_Vvel = np.zeros_like(self._vel_at_N)
        temp_Vvel[self.grid.horizontal_links] = self._v_vel_at_u_links

        # Getting links around the particle and defining downstream direction based on velocity
        nearest_link_to_particle = self.find_nearest_link(
            self._x_at_exit_point, self._y_at_exit_point, objective_links="horizontal"
        )
        adjacent_links_to_particle = self.find_adjacent_links_at_link(
            nearest_link_to_particle, objective_links="horizontal"
        )

        link_at_B2 = nearest_link_to_particle
        link_at_A2 = adjacent_links_to_particle[:, 1]  # 1: N, top
        link_at_C2 = adjacent_links_to_particle[:, 3]  # 3: S, bottom
        link_at_A2 = np.where(temp_Vvel[link_at_B2] >= 0, link_at_A2, link_at_C2)
        link_at_C2 = np.where(temp_Vvel[link_at_B2] >= 0, link_at_C2, link_at_A2)
        # We avoid "-1" links close to the boundary
        link_at_A2 = np.where(link_at_A2 >= 0, link_at_A2, link_at_B2)
        link_at_C2 = np.where(link_at_C2 >= 0, link_at_C2, link_at_B2)

        # Getting the surrounding links to every particle from closest link
        # 0: E, Right
        # 2: W, Left
        link_at_A1 = self.find_adjacent_links_at_link(
            link_at_A2, objective_links="horizontal"
        )[:, 0]
        link_at_A3 = self.find_adjacent_links_at_link(
            link_at_A2, objective_links="horizontal"
        )[:, 2]

        # Selecting downstream link based on velocity direction
        link_at_A1 = np.where(self._vel_at_N[link_at_B2] >= 0, link_at_A1, link_at_A3)
        link_at_A3 = np.where(self._vel_at_N[link_at_B2] >= 0, link_at_A3, link_at_A1)

        link_at_B1 = self.find_adjacent_links_at_link(
            link_at_B2, objective_links="horizontal"
        )[
            :, 0
        ]  # 0: E, Right
        link_at_B3 = self.find_adjacent_links_at_link(
            link_at_B2, objective_links="horizontal"
        )[
            :, 2
        ]  # 2: W, Left

        # Selecting downstream link based on velocity direction
        link_at_B1 = np.where(self._vel_at_N[link_at_B2] >= 0, link_at_B1, link_at_B3)
        link_at_B3 = np.where(self._vel_at_N[link_at_B2] >= 0, link_at_B3, link_at_B1)

        link_at_C1 = self.find_adjacent_links_at_link(
            link_at_C2, objective_links="horizontal"
        )[
            :, 0
        ]  # 0: E, Right
        link_at_C3 = self.find_adjacent_links_at_link(
            link_at_C2, objective_links="horizontal"
        )[
            :, 2
        ]  # 2: W, Left

        # Selecting downstream link based on velocity direction
        link_at_C1 = np.where(self._vel_at_N[link_at_B2] >= 0, link_at_C1, link_at_C3)
        link_at_C3 = np.where(self._vel_at_N[link_at_B2] >= 0, link_at_C3, link_at_C1)

        # Getting velocity around the particle
        vel_at_A1 = np.where(
            link_at_A1 >= 0, self._vel_at_N[link_at_A1], self._vel_at_N[link_at_A2]
        )
        vel_at_A2 = self._vel_at_N[link_at_A2]
        vel_at_A3 = np.where(
            link_at_A3 >= 0, self._vel_at_N[link_at_A3], self._vel_at_N[link_at_A2]
        )

        vel_at_B1 = np.where(
            link_at_B1 >= 0, self._vel_at_N[link_at_B1], self._vel_at_N[link_at_B2]
        )
        vel_at_B2 = self._vel_at_N[link_at_B2]
        vel_at_B3 = np.where(
            link_at_B3 >= 0, self._vel_at_N[link_at_B3], self._vel_at_N[link_at_B2]
        )

        vel_at_C1 = np.where(
            link_at_C1 >= 0, self._vel_at_N[link_at_C1], self._vel_at_N[link_at_C2]
        )
        vel_at_C2 = self._vel_at_N[link_at_C2]
        vel_at_C3 = np.where(
            link_at_C3 >= 0, self._vel_at_N[link_at_C3], self._vel_at_N[link_at_C2]
        )

        # Getting coordinates around the particle
        x_at_2 = self._grid.xy_of_link[link_at_B2][:, 0]
        x_at_1 = np.where(self._vel_at_N[link_at_B2] >= 0, x_at_2 + dx, x_at_2 - dx)
        x_at_3 = np.where(self._vel_at_N[link_at_B2] >= 0, x_at_2 - dx, x_at_2 + dx)

        y_at_B = self._grid.xy_of_link[link_at_B2][:, 1]
        y_at_A = np.where(temp_Vvel[link_at_B2] >= 0, y_at_B + dy, y_at_B - dy)
        y_at_C = np.where(temp_Vvel[link_at_B2] >= 0, y_at_B - dy, y_at_B + dy)

        # Calculating the weights W(i,j) for k around x-direction
        W1 = (
            (self._x_at_exit_point - x_at_2)
            * (self._x_at_exit_point - x_at_3)
            / ((x_at_1 - x_at_2) * (x_at_1 - x_at_3))
        )
        W2 = (
            (self._x_at_exit_point - x_at_1)
            * (self._x_at_exit_point - x_at_3)
            / ((x_at_2 - x_at_1) * (x_at_2 - x_at_3))
        )
        W3 = (
            (self._x_at_exit_point - x_at_1)
            * (self._x_at_exit_point - x_at_2)
            / ((x_at_3 - x_at_1) * (x_at_3 - x_at_2))
        )

        # Interpolation by row around 'x_at_exit_point'
        A = W1 * vel_at_A1 + W2 * vel_at_A2 + W3 * vel_at_A3
        B = W1 * vel_at_B1 + W2 * vel_at_B2 + W3 * vel_at_B3
        C = W1 * vel_at_C1 + W2 * vel_at_C2 + W3 * vel_at_C3

        # Calculating the weghts W(i,j) for l around y-direction
        W1 = (
            (self._y_at_exit_point - y_at_B)
            * (self._y_at_exit_point - y_at_C)
            / ((y_at_A - y_at_B) * (y_at_A - y_at_C))
        )
        W2 = (
            (self._y_at_exit_point - y_at_A)
            * (self._y_at_exit_point - y_at_C)
            / ((y_at_B - y_at_A) * (y_at_B - y_at_C))
        )
        W3 = (
            (self._y_at_exit_point - y_at_A)
            * (self._y_at_exit_point - y_at_B)
            / ((y_at_C - y_at_A) * (y_at_C - y_at_B))
        )

        # Calculating UsL by bicuadratic interpolation
        self._UsL[tempB2] = W1 * A + W2 * B + W3 * C

        # Computing viscous terms
        # U-located particles
        # Central difference scheme around x- and y- direction for U-located particles
        self._Uvis = np.zeros_like(self._u_vel)

        tempCalc1 = (
            self._eddy_viscosity
            * self._dt
            * (vel_at_B3 - 2 * vel_at_B2 + vel_at_B1)
            / (dx**2)
        )
        tempCalc2 = (
            self._eddy_viscosity
            * self._dt
            * (vel_at_C2 - 2 * vel_at_B2 + vel_at_A2)
            / (dy**2)
        )

        self._Uvis[tempB2] = tempCalc1 + tempCalc2

        # Path line tracing
        # V-velocity, y-direction, vertical links
        # Getting the initial particle location at each volume faces
        tempB1 = [j in self.grid.vertical_links for j in self.grid.active_links]
        self._x_of_particle = self._grid.xy_of_link[:, 0][self.grid.active_links][
            tempB1
        ]
        self._y_of_particle = self._grid.xy_of_link[:, 1][self.grid.active_links][
            tempB1
        ]

        # Getting the initial particle velocity
        tempB2 = [j in self.grid.active_links for j in self.grid.vertical_links]
        self._v_vel_of_particle = self._v_vel[tempB2]
        self._u_vel_of_particle = self._u_vel_at_v_links[tempB2]

        # Getting a first 'exit' point to begin the loop
        self._x_at_exit_point = self._x_of_particle
        self._y_at_exit_point = self._y_of_particle

        # Calculating path line backwards on time
        self.path_line_tracing()

        # Bicuatradic interpolation
        # V-velocity, y-direction, around (p,q) location
        self._VsL = np.zeros_like(self._v_vel)

        # Getting V-velocity at U-links
        temp_Uvel = np.zeros_like(self._vel_at_N)
        temp_Uvel[self.grid.vertical_links] = self._u_vel_at_v_links

        # Getting links around the particle and defining downstream direction based on velocity
        nearest_link_to_particle = self.find_nearest_link(
            self._x_at_exit_point, self._y_at_exit_point, objective_links="vertical"
        )
        adjacent_links_to_particle = self.find_adjacent_links_at_link(
            nearest_link_to_particle, objective_links="vertical"
        )

        link_at_B2 = nearest_link_to_particle
        link_at_A2 = adjacent_links_to_particle[:, 1]  # 1: N, top
        link_at_C2 = adjacent_links_to_particle[:, 3]  # 3: S, bottom
        link_at_A2 = np.where(self._vel_at_N[link_at_B2] >= 0, link_at_A2, link_at_C2)
        link_at_C2 = np.where(self._vel_at_N[link_at_B2] >= 0, link_at_C2, link_at_A2)
        link_at_A2 = np.where(link_at_A2 >= 0, link_at_A2, link_at_B2)
        link_at_C2 = np.where(link_at_C2 >= 0, link_at_C2, link_at_B2)
        # We avoid "-1" links close to the boundary

        # Getting the surrounding links to every particle from closest link
        link_at_A1 = self.find_adjacent_links_at_link(
            link_at_A2, objective_links="vertical"
        )[
            :, 0
        ]  # 0: E, Left
        link_at_A3 = self.find_adjacent_links_at_link(
            link_at_A2, objective_links="vertical"
        )[
            :, 2
        ]  # 2: W, Right

        # Selecting downstream link based on velocity direction
        link_at_A1 = np.where(temp_Uvel[link_at_B2] >= 0, link_at_A1, link_at_A3)
        link_at_A3 = np.where(temp_Uvel[link_at_B2] >= 0, link_at_A3, link_at_A1)

        link_at_B1 = self.find_adjacent_links_at_link(
            link_at_B2, objective_links="vertical"
        )[
            :, 0
        ]  # 0: E, Left
        link_at_B3 = self.find_adjacent_links_at_link(
            link_at_B2, objective_links="vertical"
        )[
            :, 2
        ]  # 2: W, Right

        # Selecting downstream link based on velocity direction
        link_at_B1 = np.where(temp_Uvel[link_at_B2] >= 0, link_at_B1, link_at_B3)
        link_at_B3 = np.where(temp_Uvel[link_at_B2] >= 0, link_at_B3, link_at_B1)

        link_at_C1 = self.find_adjacent_links_at_link(
            link_at_C2, objective_links="vertical"
        )[
            :, 0
        ]  # 0: E, Left
        link_at_C3 = self.find_adjacent_links_at_link(
            link_at_C2, objective_links="vertical"
        )[
            :, 2
        ]  # 2: W, Right

        # Selecting downstream link based on velocity direction
        link_at_C1 = np.where(temp_Uvel[link_at_B2] >= 0, link_at_C1, link_at_C3)
        link_at_C3 = np.where(temp_Uvel[link_at_B2] >= 0, link_at_C3, link_at_C1)

        # Getting velocity around the particle
        vel_at_A1 = np.where(
            link_at_A1 >= 0, self._vel_at_N[link_at_A1], self._vel_at_N[link_at_A2]
        )
        vel_at_A2 = self._vel_at_N[link_at_A2]
        vel_at_A3 = np.where(
            link_at_A3 >= 0, self._vel_at_N[link_at_A3], self._vel_at_N[link_at_A2]
        )

        vel_at_B1 = np.where(
            link_at_B1 >= 0, self._vel_at_N[link_at_B1], self._vel_at_N[link_at_B2]
        )
        vel_at_B2 = self._vel_at_N[link_at_B2]
        vel_at_B3 = np.where(
            link_at_B3 >= 0, self._vel_at_N[link_at_B3], self._vel_at_N[link_at_B2]
        )

        vel_at_C1 = np.where(
            link_at_C1 >= 0, self._vel_at_N[link_at_C1], self._vel_at_N[link_at_C2]
        )
        vel_at_C2 = self._vel_at_N[link_at_C2]
        vel_at_C3 = np.where(
            link_at_C3 >= 0, self._vel_at_N[link_at_C3], self._vel_at_N[link_at_C2]
        )

        # Getting coordinates around the particle
        x_at_2 = self._grid.xy_of_link[link_at_B2][:, 0]
        x_at_1 = np.where(temp_Uvel[link_at_B2] >= 0, x_at_2 + dx, x_at_2 - dx)
        x_at_3 = np.where(temp_Uvel[link_at_B2] >= 0, x_at_2 - dx, x_at_2 + dx)

        y_at_B = self._grid.xy_of_link[link_at_B2][:, 1]
        y_at_A = np.where(self._vel_at_N[link_at_B2] >= 0, y_at_B + dy, y_at_B - dy)
        y_at_C = np.where(self._vel_at_N[link_at_B2] >= 0, y_at_B - dy, y_at_B + dy)

        # Calculating the weights W(i,j) for k around x-direction
        W1 = (
            (self._x_at_exit_point - x_at_2)
            * (self._x_at_exit_point - x_at_3)
            / ((x_at_1 - x_at_2) * (x_at_1 - x_at_3))
        )
        W2 = (
            (self._x_at_exit_point - x_at_1)
            * (self._x_at_exit_point - x_at_3)
            / ((x_at_2 - x_at_1) * (x_at_2 - x_at_3))
        )
        W3 = (
            (self._x_at_exit_point - x_at_1)
            * (self._x_at_exit_point - x_at_2)
            / ((x_at_3 - x_at_1) * (x_at_3 - x_at_2))
        )

        # Interpolation by row around 'y_at_exit_point'
        A = W1 * vel_at_A1 + W2 * vel_at_A2 + W3 * vel_at_A3
        B = W1 * vel_at_B1 + W2 * vel_at_B2 + W3 * vel_at_B3
        C = W1 * vel_at_C1 + W2 * vel_at_C2 + W3 * vel_at_C3

        # Calculating the weghts W(i,j) for l around y-direction
        W1 = (
            (self._y_at_exit_point - y_at_B)
            * (self._y_at_exit_point - y_at_C)
            / ((y_at_A - y_at_B) * (y_at_A - y_at_C))
        )
        W2 = (
            (self._y_at_exit_point - y_at_A)
            * (self._y_at_exit_point - y_at_C)
            / ((y_at_B - y_at_A) * (y_at_B - y_at_C))
        )
        W3 = (
            (self._y_at_exit_point - y_at_A)
            * (self._y_at_exit_point - y_at_B)
            / ((y_at_C - y_at_A) * (y_at_C - y_at_B))
        )

        # Calculating VsL by bicuadratic interpolation
        self._VsL[tempB2] = W1 * A + W2 * B + W3 * C

        # Computing viscous terms
        # V-located particles
        # Central difference scheme around x- and y- direction for V-located particles
        self._Vvis = np.zeros_like(self._v_vel)

        tempCalc1 = (
            self._eddy_viscosity
            * self._dt
            * (vel_at_B3 - 2 * vel_at_B2 + vel_at_B1)
            / (dx**2)
        )
        tempCalc2 = (
            self._eddy_viscosity
            * self._dt
            * (vel_at_C2 - 2 * vel_at_B2 + vel_at_A2)
            / (dy**2)
        )

        self._Vvis[tempB2] = tempCalc1 + tempCalc2

        # Computing advective terms (FU, FV)
        self._f_vel = np.zeros_like(self._vel_at_N)

        # Adding semi-lagrangian and viscous terms
        tempCalc1 = self._UsL + self._Uvis
        tempCalc2 = self._VsL + self._Vvis

        # Including the results according with links directions
        self._f_vel[self.grid.horizontal_links] = tempCalc1
        self._f_vel[self.grid.vertical_links] = tempCalc2

        # Setting G-faces
        self._g_links = np.zeros_like(self._vel_at_N)

        # Computing G-faces
        self._g_links = self._h_at_N_at_links * self._f_vel - self._h_at_N_at_links * (
            1 - self._theta
        ) * self._g * self._dt / dx * self._grid.calc_diff_at_link(self._eta_at_N)

        # Using only wet-link values, and setting dry links equal to 0 to avoid
        # using wrong values
        self._g_links = np.where(self._wet_links, self._g_links, 0)
        self._g_links = np.where(
            self._grid.status_at_link == 4, 0, self._g_links
        )  # link is active (0), fixed (2), inactive (4)

        # Solving semi-implicit scheme with PCG method
        # Building the system of equations 'A*x=b'
        # Full 'A' matrix with all nodes on it
        A = np.zeros((self.grid.number_of_nodes, self.grid.number_of_nodes))
        b = self.grid.zeros(at="node")  # Full 'b' vector with all nodes on it

        # Getting surrounding locations for core nodes
        adjacent_nodes = self._grid.adjacent_nodes_at_node[
            self.grid.core_nodes
        ]  # East, North, West, South
        adjacent_links = self._grid.links_at_node[
            self.grid.core_nodes
        ]  # East, North, West, South
        nodes_location = np.append(
            adjacent_nodes, np.array([self.grid.core_nodes]).T, axis=1
        )  # East, North, West, South, Center

        # Boolean to differentiate between core and boundary nodes
        tempB1 = np.isin(nodes_location, self.grid.core_nodes)
        # Core node if tempB1 == True, boundary node if tempB1 == False
        tempB2 = ~tempB1
        # Boundary node if tempB2 == True, core node if tempB2 == False

        ## Building 'b' for the right-hand side of the system
        # Calculating 'delta' to build 'b'
        tempCalc1 = (
            self._h_at_N_at_links[adjacent_links] * self._vel_at_N[adjacent_links]
        )
        tempCalc2 = (
            self._eta_at_N[self.grid.core_nodes]
            - (1 - self._theta) * self._dt / dx * (tempCalc1[:, 0] - tempCalc1[:, 2])
            - (1 - self._theta) * self._dt / dy * (tempCalc1[:, 1] - tempCalc1[:, 3])
        )

        tempCalc1 = (
            self._h_at_N_at_links[adjacent_links]
            * self._g_links[adjacent_links]
            / self._a_links[adjacent_links]
        )
        b[self.grid.core_nodes] = (
            tempCalc2
            - self._theta * self._dt / dx * (tempCalc1[:, 0] - tempCalc1[:, 2])
            - self._theta * self._dt / dy * (tempCalc1[:, 1] - tempCalc1[:, 3])
        )

        ## Building 'A' for the left-hand side of the system
        # Calculating coefficients for the system of equations
        tempCalc1 = (
            self._h_at_N_at_links[adjacent_links] ** 2 / self._a_links[adjacent_links]
        )
        coefficients = [
            -tempCalc1[:, 0] * (self._g * self._theta * self._dt / dx) ** 2,
            -tempCalc1[:, 1] * (self._g * self._theta * self._dt / dy) ** 2,
            -tempCalc1[:, 2] * (self._g * self._theta * self._dt / dx) ** 2,
            -tempCalc1[:, 3] * (self._g * self._theta * self._dt / dy) ** 2,
            1
            + (tempCalc1[:, 0] + tempCalc1[:, 2])
            * (self._g * self._theta * self._dt / dx) ** 2
            + (tempCalc1[:, 1] + tempCalc1[:, 3])
            * (self._g * self._theta * self._dt / dy) ** 2,
        ]
        coefficients = np.array(coefficients).T

        ## For loop iterates through every row of 'nodes_location' to:
        # a) find the node number (row in A) and choose an equation, and
        # b) find the columns associated with adjacent nodes
        for row in range(nodes_location.shape[0]):
            # Getting the current node
            current_node = nodes_location[row, 4]
            # Getting the row associated with the current node
            current_rows_in_A = np.array([current_node])
            # Getting the columns associated with surrounding nodes
            current_cols_in_A = nodes_location[row, :]

            # Filling A matrix with coefficients associated with surrounding nodes
            A[np.ix_(current_rows_in_A, current_cols_in_A)] = (
                coefficients[row, :] * tempB1[row, :]
            )

            # Adding known terms (boundary nodes) to the right-hand side of the equation
            b[current_rows_in_A] = b[current_rows_in_A] - sum(
                self._eta_at_N[nodes_location[row, :]]
                * coefficients[row, :]
                * tempB2[row, :]
            )
        # for END

        # Extracting only core nodes to be solved
        left_hand_side = A[np.ix_(self.grid.core_nodes, self.grid.core_nodes)]
        right_hand_side = b[self.grid.core_nodes]

        # Applying PCG method to 'LHS*eta=RHS' using np.diag() as a preconditioner for 'LHS'
        # Preconditioned conjugated gradient output flag:
        # 0  : successful exit
        # >0 : convergence to tolerance not achieved, number of iterations
        # <0 : illegal input or breakdown
        # Alternative preconditiner: Li = np.linalg.cholesky(left_hand_side)
        Mi = np.diag(np.diag(left_hand_side))
        pcg_results = sp.sparse.linalg.cg(
            left_hand_side,
            right_hand_side,
            M=Mi,
            rtol=self._pcg_tolerance,
            maxiter=self._pcg_max_iterations,
            atol=0,
        )

        # Getting the new water surface elevation
        self._eta = np.zeros_like(self._eta_at_N)
        self._eta[self.grid.core_nodes] = pcg_results[0]

        # Boundary conditions
        # Radiation Boundary Conditions of Roed & Smedstad (1984) applied on open boundaries
        # Water surface elevation
        ## Updating the new WSE ('eta') with the fixed nodes values
        if self._fixed_nodes_exist is True:
            self._eta[self._fixed_entry_nodes] = (
                self._entry_nodes_h_values - self._z[self._fixed_entry_nodes]
            )

        ## Getting the 1-line-upstream nodes from boundary nodes
        tempCalc1 = self._grid.active_adjacent_nodes_at_node[self._open_boundary_nodes]
        open_boundary_nodes_1_backwards = np.extract(tempCalc1 >= 0, tempCalc1)

        ## Getting the 2-line-upstream nodes from boundary nodes
        # Looking for these nodes
        tempCalc1 = np.tile(self._open_boundary_nodes, (4, 1)).T
        tempCalc2 = self._grid.active_adjacent_nodes_at_node[
            open_boundary_nodes_1_backwards
        ]  # Surrounding nodes to 1-line-upstream

        # Getting the node positions to extract them from all the surrounding nodes
        tempB1 = np.tile([0, 1, 2, 3], (len(self._open_boundary_nodes), 1))
        tempB2 = tempB1[tempCalc1 == tempCalc2]

        # It gives me the node indices to extract
        # folowing the face direction
        tempB1 = np.where(tempB2 == 0, 2, tempB2)
        tempB1 = np.where(tempB2 == 1, 3, tempB1)
        tempB1 = np.where(tempB2 == 2, 0, tempB1)
        tempB1 = np.where(tempB2 == 3, 1, tempB1)

        open_boundary_nodes_2_backwards = tempCalc2[
            [range(tempCalc2.shape[0])], tempB1
        ][0]

        ## Getting WSE at different locations
        eta_at_N_at_B = self._eta_at_N[self._open_boundary_nodes]
        eta_at_N_at_B_1 = self._eta_at_N[open_boundary_nodes_1_backwards]
        eta_at_N_1_at_B_1 = self._eta_at_N_1[open_boundary_nodes_1_backwards]
        eta_at_N_1_at_B_2 = self._eta_at_N_1[open_boundary_nodes_2_backwards]

        ## Computing boundary condition
        tempCalc1 = eta_at_N_at_B_1 - eta_at_N_1_at_B_1
        tempCalc2 = eta_at_N_1_at_B_1 - eta_at_N_1_at_B_2
        tempCalc3 = np.where(tempCalc2 == 0, 1, tempCalc2)

        Ce = np.where(tempCalc2 == 0, 0, tempCalc1 / tempCalc3 * (-dx / self._dt))
        Ce = np.where(tempCalc2 == 0, 0, Ce)

        # eta[open_boundary_nodes] = tempCalc1/tempCalc2
        self._eta[self._open_boundary_nodes] = np.where(
            Ce >= 0, eta_at_N_at_B_1, eta_at_N_at_B
        )
        self._eta = np.where(
            abs(self._eta) > abs(self._z), -self._z, self._eta
        )  # Correcting WSE below topographic elevation

        self._eta_at_links = self._grid.map_mean_of_link_nodes_to_link(self._eta)

        # Corner nodes treatment
        self._eta[self.grid.corner_nodes] = np.mean(
            self._eta[self._adjacent_nodes_at_corner_nodes], axis=1
        )

        # Updating water velocity
        # tempB1 :  Considering only wet cells
        # tempB2 :  Cells with elevation below the water surface elevation
        tempB1 = np.where(
            abs(self._eta[self._grid.node_at_link_head])
            <= abs(self._z[self._grid.node_at_link_head] - self._threshold_depth),
            1,
            0,
        )
        tempB2 = np.where(
            abs(self._z[self._grid.node_at_link_head])
            > abs(self._eta[self._grid.node_at_link_tail]),
            1,
            0,
        )
        tempB1 = tempB1 + tempB2
        tempB1 = np.where(tempB1 > 1, 1, 0)

        # Updating water velocity
        tempCalc1 = (
            self._theta
            * self._g
            * self._dt
            / dx
            * (self._grid.calc_diff_at_link(self._eta))
            * self._h_at_N_at_links
            / self._a_links
        )
        self._vel = self._g_links / self._a_links - tempCalc1 * tempB1

        # Only updating velocity on wet cells
        self._vel = np.where(self._wet_links, self._vel, 0)

        # Boundary conditions
        # Radiation Boundary Conditions of Roed & Smedstad (1984) applied on open boundaries
        # Water velocity
        ## Updating the new Velocity with the fixed links values
        if self._fixed_links_exist is True:
            self._vel[self._fixed_entry_links] = self._entry_links_vel_values

        ## Getting the boundary links
        tempB1 = [i in self._open_boundary_links for i in self.grid.active_links]
        open_boundary_active_links = self._grid.active_links[tempB1]

        ## Getting the 1-line-upstream links from boundary links
        tempCalc1 = np.tile(open_boundary_active_links, (4, 1)).T
        tempCalc2 = self._grid.links_at_node[open_boundary_nodes_1_backwards]

        # Getting the link positions to extract them from all the surrounding nodes
        tempB1 = np.tile([0, 1, 2, 3], (len(self._open_boundary_nodes), 1))
        tempB2 = tempB1[tempCalc1 == tempCalc2]

        # It gives me the link indices to extract
        # folowing the face direction
        tempB1 = np.where(tempB2 == 0, 2, tempB2)
        tempB1 = np.where(tempB2 == 1, 3, tempB1)
        # tempB1 is where the target link is located
        tempB1 = np.where(tempB2 == 2, 0, tempB1)
        # tempB2 is where the upstream link is located
        tempB1 = np.where(tempB2 == 3, 1, tempB1)

        open_boundary_active_links_1_backwards = tempCalc2[
            [range(tempCalc2.shape[0])], tempB1
        ][0]

        ### Getting the 2-line-upstream links from boundary nodes
        tempCalc1 = np.tile(open_boundary_active_links_1_backwards, (4, 1)).T
        tempCalc2 = self._grid.links_at_node[open_boundary_nodes_2_backwards]

        # Getting the link positions to extract them from all the surrounding nodes
        tempB1 = np.tile([0, 1, 2, 3], (len(self._open_boundary_nodes), 1))
        tempB2 = tempB1[(tempCalc1 == tempCalc2)]

        # It gives me the link indices to extract
        # folowing the face direction
        tempB1 = np.where(tempB2 == 0, 2, tempB2)
        tempB1 = np.where(tempB2 == 1, 3, tempB1)
        # tempB1 is where the target link is located
        tempB1 = np.where(tempB2 == 2, 0, tempB1)
        # tempB2 is where the upstream link is located
        tempB1 = np.where(tempB2 == 3, 1, tempB1)

        open_boundary_active_links_2_backwards = tempCalc2[
            [range(tempCalc2.shape[0])], tempB1
        ][0]

        ## Getting water velocity at different locations
        vel_at_N_at_B = self._vel_at_N[open_boundary_active_links]
        vel_at_N_at_B_1 = self._vel_at_N[open_boundary_active_links_1_backwards]
        vel_at_N_1_at_B_1 = self._vel_at_N_1[open_boundary_active_links_1_backwards]
        vel_at_N_1_at_B_2 = self._vel_at_N_1[open_boundary_active_links_2_backwards]

        ## Computing boundary condition
        tempCalc1 = vel_at_N_at_B_1 - vel_at_N_1_at_B_1
        tempCalc2 = vel_at_N_1_at_B_1 - vel_at_N_1_at_B_2
        tempCalc3 = np.where(tempCalc2 == 0, 1, tempCalc2)

        Ce = np.where(tempCalc2 == 0, 0, tempCalc1 / tempCalc3 * (-dx / self._dt))
        Ce = np.where(tempCalc2 == 0, 0, Ce)

        self._vel[open_boundary_active_links] = np.where(
            Ce >= 0, vel_at_N_at_B_1, vel_at_N_at_B
        )

        # Updating water depth at links
        # Using only values where the WSE is above the topographic elevation
        tempB1 = np.where(abs(self._eta) <= abs(self._z - self._threshold_depth), 1, 0)

        # Updating water depth at links
        tempCalc1 = (
            self._z_at_links + self._eta[self._grid.node_at_link_head]
        ) * tempB1[self._grid.node_at_link_head]
        tempCalc2 = (
            self._z_at_links + self._eta[self._grid.node_at_link_tail]
        ) * tempB1[self._grid.node_at_link_tail]
        tempCalc3 = np.zeros_like(self._h_at_N_at_links)

        self._h_at_links = np.array((tempCalc1, tempCalc2, tempCalc3)).max(axis=0)

        # Applying boundary condition at links
        self._h_at_links[self._open_boundary_links] = (
            self._z_at_links[self._open_boundary_links]
            + self._eta_at_links[self._open_boundary_links]
        )

        # Wet cells threshold
        self._h_at_links = np.where(
            self._h_at_links < self._threshold_depth, 0, self._h_at_links
        )

        # Updating wet links
        self._wet_links = np.where(
            self._h_at_links >= self._threshold_depth, True, False
        )
        self._vel = self._vel * self._wet_links

        # Calculating average water depth at nodes
        # If a node is dry, using only surrounding links such that 'WSE' is above 'z'
        # If a node is wet, using all surrounding links even if 'WSE' is below 'z' (jumps)

        # Checking surrounding wet links
        surrounding_links = self._grid.links_at_node[self.grid.core_nodes]

        # Checking whether the core node is wet (T) or dry (F)
        tempB1 = abs(self._eta[self.grid.core_nodes]) < abs(
            self._z[self.grid.core_nodes] - self._threshold_depth
        )

        # Checking whether surrounding links are wet (T) or dry (F)
        tempB2 = self._wet_links[surrounding_links]

        # Checking whether surrounding 'WSE' links are above (T) or below (F) 'z' at nodes
        tempB3 = (
            abs(self._eta_at_links[surrounding_links])
            < abs(self._z[self.grid.core_nodes] - self._threshold_depth)[:, None]
        )

        # Getting the number of wet links around each core node, satisfying tempB2,
        # and avoiding divisions by zero
        tempCalc2 = np.sum(tempB2 * 1, axis=1)
        tempCalc2 = np.where(tempCalc2 == 0, -9999, tempCalc2)

        # Getting the number of wet links around each core node, satisfying tempB3,
        # and avoiding divisions by zero
        tempCalc3 = np.sum(tempB2 * tempB3 * 1, axis=1)
        tempCalc3 = np.where(tempCalc3 == 0, -9999, tempCalc3)

        # Updating water depth
        # h = h_at_N - rmg.calc_net_flux_at_node(h_at_links*vel) # (influx if negative)
        self._h[self.grid.core_nodes] = np.where(
            tempCalc3 > 0,
            np.sum(self._h_at_links[surrounding_links] * tempB2 * tempB3, axis=1)
            / tempCalc3,
            0,
        )  # Dry nodes, tempB1 == False

        ### Updating boundary nodes
        if self._fixed_nodes_exist is True:
            self._h[self._fixed_entry_nodes] = self._entry_nodes_h_values
        self._h[self._open_boundary_nodes] = (
            self._eta[self._open_boundary_nodes] + self._z[self._open_boundary_nodes]
        )
        self._h = np.where(self._h < self._threshold_depth, 0, self._h)

        # Corner nodes treatment
        self._h[self.grid.corner_nodes] = np.mean(
            self._h[self._adjacent_nodes_at_corner_nodes], axis=1
        )

        # Updating wet nodes
        self._wet_nodes = np.where(self._h >= self._threshold_depth, True, False)

        # Storing values in the grid
        self._grid.at_node["surface_water__depth"] = self._h
        self._grid.at_link["surface_water__velocity"] = self._vel
        self._grid.at_node["surface_water__elevation"] = self._eta + (
            self._max_elevation + self._additional_z
        )

        self._grid.at_link["surface_water__velocity_at_N-1"] = self._vel_at_N
        self._grid.at_node["surface_water__elevation_at_N-1"] = self._eta_at_N + (
            self._max_elevation + self._additional_z
        )
        self._grid.at_node["surface_water__elevation_at_N-2"] = self._eta_at_N_1 + (
            self._max_elevation + self._additional_z
        )

        # Storing values at previous time steps
        self._eta_at_N = self._eta.copy()
        self._eta_at_N_1 = self._eta_at_N.copy()
        self._eta_at_N_2 = self._eta_at_N_1.copy()

        self._h_at_N = self._h.copy()
        self._h_at_N_at_links = self._h_at_links.copy()

        self._vel_at_N = self._vel.copy()
        self._vel_at_N_1 = self._vel_at_N.copy()



================================================
File: sink_fill/__init__.py
================================================
from .fill_sinks import SinkFiller
from .sink_fill_barnes import SinkFillerBarnes

__all__ = ["SinkFiller", "SinkFillerBarnes"]



================================================
File: sink_fill/fill_sinks.py
================================================
"""Created on Mon Oct 19.

@author: dejh
"""

import contextlib

import numpy as np

from landlab import Component
from landlab import FieldError
from landlab import RasterModelGrid
from landlab.components import DepressionFinderAndRouter
from landlab.components import FlowAccumulator

# TODO: this should probably follow Barnes et al., 2014 for max efficiency


class SinkFiller(Component):
    """This component identifies depressions in a topographic surface, then
    fills them in in the topography.  No attempt is made to conserve sediment
    mass. User may specify whether the holes should be filled to flat, or with
    a gradient downwards towards the depression outlet. The gradient can be
    spatially variable, and is chosen to not reverse any drainage directions at
    the perimeter of each lake.

    The primary method of this class is 'run_one_step'. 'fill_pits' is a
    synonym.

    Constructor assigns a copy of the grid, and calls the initialize
    method.

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import FlowAccumulator, SinkFiller
    >>> import numpy as np
    >>> lake1 = np.array([34, 35, 36, 44, 45, 46, 54, 55, 56, 65, 74])
    >>> lake2 = np.array([78, 87, 88])
    >>> guard_nodes = np.array([23, 33, 53, 63, 73, 83])
    >>> lake = np.concatenate((lake1, lake2))
    >>> mg = RasterModelGrid((10, 10))
    >>> z = np.ones(100, dtype=float)
    >>> z += mg.node_x  # add a slope
    >>> z[guard_nodes] += 0.001  # forces the flow out of a particular node
    >>> z[lake] = 0.0
    >>> field = mg.add_field(
    ...     "topographic__elevation",
    ...     z,
    ...     at="node",
    ...     units="-",
    ...     copy=True,
    ... )
    >>> fr = FlowAccumulator(mg, flow_director="D8")
    >>> fr.run_one_step()
    >>> mg.at_node["flow__sink_flag"][mg.core_nodes].sum()
    14
    >>> hf = SinkFiller(mg, apply_slope=False)
    >>> hf.run_one_step()
    >>> np.allclose(mg.at_node["topographic__elevation"][lake1], 4.0)
    True
    >>> np.allclose(mg.at_node["topographic__elevation"][lake2], 7.0)
    True

    Now reset and demonstrate the adding of an inclined surface:

    >>> field[:] = z
    >>> hf = SinkFiller(mg, apply_slope=True)
    >>> hf.run_one_step()
    >>> hole1 = np.array(
    ...     [
    ...         4.00007692,
    ...         4.00015385,
    ...         4.00023077,
    ...         4.00030769,
    ...         4.00038462,
    ...         4.00046154,
    ...         4.00053846,
    ...         4.00061538,
    ...         4.00069231,
    ...         4.00076923,
    ...         4.00084615,
    ...     ]
    ... )
    >>> hole2 = np.array([7.4, 7.2, 7.6])
    >>> np.allclose(mg.at_node["topographic__elevation"][lake1], hole1)
    True
    >>> np.allclose(mg.at_node["topographic__elevation"][lake2], hole2)
    True
    >>> fr.run_one_step()
    >>> mg.at_node["flow__sink_flag"][mg.core_nodes].sum()
    0

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    Tucker, G., Lancaster, S., Gasparini, N., Bras, R., Rybarczyk, S. (2001).
    An object-oriented framework for distributed hydrologic and geomorphic
    modeling using triangulated irregular networks. Computers & Geosciences
    27(8), 959-973. https://dx.doi.org/10.1016/s0098-3004(00)00134-5

    """

    _name = "SinkFiller"

    _unit_agnostic = True

    _info = {
        "sediment_fill__depth": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of sediment added at eachnode",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
    }

    def __init__(self, grid, routing="D8", apply_slope=False, fill_slope=1.0e-5):
        """
        Parameters
        ----------
        grid : ModelGrid
            A landlab grid.
        routing : {'D8', 'D4'} (optional)
            If grid is a raster type, controls whether fill connectivity can
            occur on diagonals ('D8', default), or only orthogonally ('D4').
            Has no effect if grid is not a raster.
        apply_slope : bool
            If False (default), leave the top of the filled sink flat. If True,
            apply the slope fill_slope to the top surface to allow subsequent flow
            routing. A test is performed to ensure applying this slope will not
            alter the drainage structure at the edge of the filled region
            (i.e., that we are not accidentally reversing the flow direction
            far from the outlet.)
        fill_slope : float (m/m)
            The slope added to the top surface of filled pits to allow flow
            routing across them, if apply_slope.
        """
        super().__init__(grid)

        if "flow__receiver_node" in grid.at_node and grid.at_node[
            "flow__receiver_node"
        ].size != grid.size("node"):
            raise NotImplementedError(
                "A route-to-multiple flow director has been "
                "run on this grid. The landlab development team has not "
                "verified that SinkFiller is compatible with "
                "route-to-multiple methods. Please open a GitHub Issue "
                "to start this process."
            )

        if routing != "D8":
            assert routing == "D4"
        self._routing = routing
        if isinstance(self._grid, RasterModelGrid) and (routing == "D8"):
            self._D8 = True
            self._num_nbrs = 8
        else:
            self._D8 = False  # useful shorthand for thia test we do a lot
            if isinstance(self._grid, RasterModelGrid):
                self._num_nbrs = 4
        self._fill_slope = fill_slope
        self._apply_slope = apply_slope

        self._elev = self._grid.at_node["topographic__elevation"]
        self._topo_field_name = "topographic__elevation"

        # create the only new output field:
        self._sed_fill_depth = self._grid.add_zeros(
            "sediment_fill__depth", at="node", clobber=True
        )

        self._lf = DepressionFinderAndRouter(
            self._grid, routing=self._routing, reroute_flow=True
        )
        self._fr = FlowAccumulator(self._grid, flow_director=self._routing)

    def fill_pits(self):
        """This is a synonym for the main method :func:`run_one_step`."""
        self.run_one_step()

    def run_one_step(self):
        """This is the main method.

        Call it to fill depressions in a starting topography.
        """
        self._original_elev = self._elev.copy()
        # We need this, as we'll have to do ALL this again if we manage
        # to jack the elevs too high in one of the "subsidiary" lakes.
        # We're going to implement the lake_mapper component to do the heavy
        # lifting here, then delete its fields. This means we first need to
        # test if these fields already exist, in which case, we should *not*
        # delete them!
        existing_fields = {}
        spurious_fields = set()
        set_of_outputs = set(self._lf.output_var_names) | set(self._fr.output_var_names)
        with contextlib.suppress(KeyError):
            set_of_outputs.remove(self._topo_field_name)
        for field in set_of_outputs:
            try:
                existing_fields[field] = self._grid.at_node[field].copy()
            except FieldError:  # not there; good!
                spurious_fields.add(field)

        self._fr.run_one_step()
        self._lf.map_depressions()
        # add the depression depths to get up to flat:
        self._elev += self._grid.at_node["depression__depth"]
        # if apply_slope is none, we're now done! But if not...
        if self._apply_slope:
            # new way of doing this - use the upstream structure! Should be
            # both more general and more efficient
            for outlet_node, lake_code in zip(
                self._lf.lake_outlets, self._lf.lake_codes
            ):
                lake_nodes = np.where(self._lf.lake_map == lake_code)[0]
                lake_perim = self._get_lake_ext_margin(lake_nodes)
                perim_elevs = self._elev[lake_perim]
                out_elev = self._elev[outlet_node]
                lowest_elev_perim = perim_elevs[perim_elevs != out_elev].min()
                # note we exclude the outlet node
                elev_increment = (lowest_elev_perim - self._elev[outlet_node]) / (
                    lake_nodes.size + 2.0
                )
                assert elev_increment > 0.0
                all_ordering = self._grid.at_node["flow__upstream_node_order"]
                upstream_order_bool = np.isin(
                    all_ordering, lake_nodes, assume_unique=True
                )
                lake_upstream_order = all_ordering[upstream_order_bool]
                argsort_lake = np.argsort(lake_upstream_order)
                elevs_to_add = (
                    np.arange(lake_nodes.size, dtype=float) + 1.0
                ) * elev_increment
                sorted_elevs_to_add = elevs_to_add[argsort_lake]
                self._elev[lake_nodes] += sorted_elevs_to_add
        # now put back any fields that were present initially, and wipe the
        # rest:
        for delete_me in spurious_fields:
            if delete_me in self._grid.at_node:
                self._grid.delete_field("node", delete_me)
        for update_me in existing_fields.keys():
            self._grid.at_node[update_me][:] = existing_fields[update_me]
        # fill the output field
        self._sed_fill_depth[:] = self._elev - self._original_elev

    def _add_slopes(self, slope, outlet_node, lake_code):
        """Assuming you have already run the lake_mapper, adds an incline
        towards the outlet to the nodes in the lake."""
        new_elevs = self._elev.copy()
        outlet_coord = (self._grid.node_x[outlet_node], self._grid.node_y[outlet_node])
        lake_nodes = np.where(self._lf.lake_map == lake_code)[0]
        lake_nodes = np.setdiff1d(lake_nodes, self._lake_nodes_treated)
        # lake_ext_margin = self._get_lake_ext_margin(lake_nodes)
        d = self._grid.calc_distances_of_nodes_to_point(
            outlet_coord, node_subset=lake_nodes
        )
        add_vals = slope * d
        new_elevs[lake_nodes] += add_vals
        self._lake_nodes_treated = np.union1d(self._lake_nodes_treated, lake_nodes)
        return new_elevs, lake_nodes

    def _get_lake_ext_margin(self, lake_nodes):
        """Returns the nodes forming the external margin of the lake, honoring
        the *routing* method (D4/D8) if applicable."""
        if self._D8 is True:
            all_poss = np.union1d(
                self._grid.active_adjacent_nodes_at_node[lake_nodes],
                self._grid.diagonal_adjacent_nodes_at_node[lake_nodes],
            )
        else:
            all_poss = np.unique(self._grid.active_adjacent_nodes_at_node[lake_nodes])
        lake_ext_edge = np.setdiff1d(all_poss, lake_nodes)
        return lake_ext_edge[lake_ext_edge != self._grid.BAD_INDEX]

    def _get_lake_int_margin(self, lake_nodes, lake_ext_edge):
        """Returns the nodes forming the internal margin of the lake, honoring
        the *routing* method (D4/D8) if applicable."""
        lee = lake_ext_edge
        if self._D8 is True:
            all_poss_int = np.union1d(
                self._grid.active_adjacent_nodes_at_node[lee],
                self._grid.diagonal_adjacent_nodes_at_node[lee],
            )
        else:
            all_poss_int = np.unique(self._grid.active_adjacent_nodes_at_node[lee])
        lake_int_edge = np.intersect1d(all_poss_int, lake_nodes)
        return lake_int_edge[lake_int_edge != self._grid.BAD_INDEX]

    def _apply_slope_current_lake(self, apply_slope, outlet_node, lake_code, sublake):
        """Wraps the _add_slopes method to allow handling of conditions where
        the drainage structure would be changed or we're dealing with a
        sublake."""
        while 1:
            starting_elevs = self._elev.copy()
            self._elev[:], lake_nodes = self._add_slopes(
                apply_slope, outlet_node, lake_code
            )
            # ext_edge = self._get_lake_ext_margin(lake_nodes)
            if sublake:
                break
            else:
                if not self.drainage_directions_change(
                    lake_nodes, starting_elevs, self._elev
                ):
                    break
                else:
                    # put the elevs back...
                    self._elev[lake_nodes] = starting_elevs[lake_nodes]
                    # the slope was too big. Reduce it.
                    apply_slope *= 0.1
        # if we get here, either sublake, or drainage dirs are stable

    def drainage_directions_change(self, lake_nodes, old_elevs, new_elevs):
        """True if the drainage structure at lake margin changes, False
        otherwise."""
        ext_edge = self._get_lake_ext_margin(lake_nodes)
        if self._D8:
            edge_neighbors = np.hstack(
                (
                    self._grid.active_adjacent_nodes_at_node[ext_edge],
                    self._grid.diagonal_adjacent_nodes_at_node[ext_edge],
                )
            )
        else:
            edge_neighbors = self._grid.active_adjacent_nodes_at_node[ext_edge].copy()
        edge_neighbors[edge_neighbors == self._grid.BAD_INDEX] = -1
        # ^value irrelevant
        old_neighbor_elevs = old_elevs[edge_neighbors]
        new_neighbor_elevs = new_elevs[edge_neighbors]
        # enforce the "don't change drainage direction" condition:
        edge_elevs = old_elevs[ext_edge].reshape((ext_edge.size, 1))
        cond = np.allclose(
            (edge_elevs >= old_neighbor_elevs), (edge_elevs >= new_neighbor_elevs)
        )
        # if True, we're good, the tilting didn't mess with the fr
        return not cond



================================================
File: sink_fill/sink_fill_barnes.py
================================================
#!/usr/env/python

"""fill_sinks_barnes.py.

Fill sinks in a landscape to the brim, following the Barnes et al.
(2014) algorithms.
"""


import numpy as np

from landlab.components import LakeMapperBarnes
from landlab.utils.return_array import return_array_at_node


class SinkFillerBarnes(LakeMapperBarnes):
    """Uses the Barnes et al (2014) algorithms to replace pits in a topography
    with flats, or optionally with very shallow gradient surfaces to allow
    continued draining.

    This component is NOT intended for use iteratively as a model runs;
    rather, it is to fill in an initial topography. If you want to repeatedly
    fill pits as a landscape develops, you are after the LakeMapperBarnes
    component. If you want flow paths on your filled landscape, manually run a
    FlowDirector and FlowAccumulator for yourself.

    The locations and depths etc. of the fills will be tracked, and properties
    are provided to access this information.

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Barnes, R., Lehman, C., Mulla, D. (2014). Priority-flood: An optimal
    depression-filling and watershed-labeling algorithm for digital elevation
    models. Computers and Geosciences  62(C), 117 - 127.
    https://dx.doi.org/10.1016/j.cageo.2013.04.024

    **Additional References**

    None Listed

    """

    _name = "SinkFillerBarnes"

    _unit_agnostic = True

    _cite_as = """
    @article{BARNES2014117,
        title = {Priority-flood: An optimal depression-filling and
                 watershed-labeling algorithm for digital elevation models},
        journal = "Computers & Geosciences",
        volume = "62",
        pages = "117 - 127",
        year = "2014",
        issn = "0098-3004",
        doi = "https://doi.org/10.1016/j.cageo.2013.04.024",
        url = "http://www.sciencedirect.com/science/article/pii/S0098300413001337",
        author = "Richard Barnes and Clarence Lehman and David Mulla",
        keywords = "Pit filling, Terrain analysis, Hydrology, Drainage network, Modeling, GIS"
        }"""

    _info = {
        "sediment_fill__depth": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of sediment added at eachnode",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
    }

    def __init__(
        self,
        grid,
        surface="topographic__elevation",
        method="D8",
        fill_flat=False,
        ignore_overfill=False,
    ):
        """Initialise the component.

        Parameters
        ----------
        grid : ModelGrid
            A grid.
        surface : field name at node or array of length node
            The surface to fill.
        method : {'Steepest', 'D8'}
            Whether or not to recognise diagonals as valid flow paths, if a raster.
            Otherwise, no effect.
        fill_flat : bool
            If True, pits will be filled to perfectly horizontal. If False, the new
            surface will be slightly inclined (at machine precision) to give
            steepest descent flow paths to the outlet, once they are calculated.
        ignore_overfill : bool
            If True, suppresses the Error that would normally be raised during
            creation of a gentle incline on a fill surface (i.e., if not
            fill_flat). Typically this would happen on a synthetic DEM where more
            than one outlet is possible at the same elevation. If True, the
            was_there_overfill property can still be used to see if this has
            occurred.

        """
        if "flow__receiver_node" in grid.at_node and grid.at_node[
            "flow__receiver_node"
        ].size != grid.size("node"):
            raise NotImplementedError(
                "A route-to-multiple flow director has been "
                "run on this grid. The landlab development team has not "
                "verified that SinkFillerBarnes is compatible with "
                "route-to-multiple methods. Please open a GitHub Issue "
                "to start this process."
            )

        # Most of the functionality of this component is directly inherited
        # from SinkFillerBarnes, so
        super().__init__(
            grid,
            surface=surface,
            method=method,
            fill_flat=fill_flat,
            fill_surface=surface,
            redirect_flow_steepest_descent=False,
            reaccumulate_flow=False,
            ignore_overfill=ignore_overfill,
            track_lakes=True,
        )
        # note we will always track the fills, since we're only doing this
        # once... Likewise, no need for flow routing; this is not going to
        # get used dynamically.
        self._supplied_surface = return_array_at_node(grid, surface).copy()
        # create the only new output field:
        self._sed_fill_depth = self._grid.add_zeros(
            "sediment_fill__depth", at="node", clobber=True
        )

    def run_one_step(self):
        """Fills the surface to remove all pits.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import SinkFillerBarnes, FlowAccumulator
        >>> mg = RasterModelGrid((5, 6))
        >>> for edge in ("left", "top", "bottom"):
        ...     mg.status_at_node[mg.nodes_at_edge(edge)] = mg.BC_NODE_IS_CLOSED
        ...
        >>> mg.at_node["topographic__elevation"] = [
        ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ...     [0.0, 2.1, 1.1, 0.6, 1.6, 0.0],
        ...     [0.0, 2.0, 1.0, 0.5, 1.5, 0.0],
        ...     [0.0, 2.2, 1.2, 0.7, 1.7, 0.0],
        ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ... ]
        >>> z = mg.at_node["topographic__elevation"]
        >>> z_init = z.copy()
        >>> sfb = SinkFillerBarnes(mg, method="Steepest")  # , surface=z

        TODO: once return_array_at_node is fixed, this example should also
        take surface... GIVE IT surface=z  !!

        >>> sfb.run_one_step()
        >>> z_out = [
        ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ...     [0.0, 2.1, 1.5, 1.5, 1.6, 0.0],
        ...     [0.0, 2.0, 1.5, 1.5, 1.5, 0.0],
        ...     [0.0, 2.2, 1.5, 1.5, 1.7, 0.0],
        ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ... ]
        >>> np.allclose(z.reshape(mg.shape), z_out)
        True
        >>> fill_map = [
        ...     [-1, -1, -1, -1, -1, -1],
        ...     [-1, -1, 16, 16, -1, -1],
        ...     [-1, -1, 16, 16, -1, -1],
        ...     [-1, -1, 16, 16, -1, -1],
        ...     [-1, -1, -1, -1, -1, -1],
        ... ]
        >>> np.all(sfb.fill_map.reshape(mg.shape) == fill_map)
        True
        >>> np.all(sfb.fill_at_node == (sfb.fill_map > -1))
        True
        >>> sfb.was_there_overfill  # everything fine with slope adding
        False

        >>> fr = FlowAccumulator(mg, flow_director="D4")  # routing now works
        >>> fr.run_one_step()
        >>> np.all(mg.at_node["flow__sink_flag"][mg.core_nodes] == 0)
        True
        >>> drainage_area = [
        ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ...     [0.0, 1.0, 2.0, 3.0, 1.0, 1.0],
        ...     [0.0, 1.0, 4.0, 9.0, 10.0, 10.0],
        ...     [0.0, 1.0, 2.0, 1.0, 1.0, 1.0],
        ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ... ]
        >>> np.allclose(mg.at_node["drainage_area"].reshape(mg.shape), drainage_area)
        True

        Test two pits and a flat fill:

        >>> from collections import deque
        >>> z[:] = mg.node_x.max() - mg.node_x
        >>> z[[10, 23]] = 1.1  # raise "guard" exit nodes
        >>> z[7] = 2.0  # is a lake on its own
        >>> z[9] = 0.5
        >>> z[15] = 0.3
        >>> z[14] = 0.6  # [9, 14, 15] is a lake
        >>> z[22] = 0.9  # a non-contiguous lake node also draining to 16
        >>> z_init = z.copy()
        >>> sfb = SinkFillerBarnes(mg, method="Steepest", fill_flat=True)
        >>> sfb.run_one_step()
        >>> sfb.fill_dict == {8: deque([7]), 16: deque([15, 9, 14, 22])}
        True
        >>> sfb.number_of_fills
        2
        >>> sfb.fill_outlets == [16, 8]
        True
        >>> np.allclose(sfb.fill_areas, np.array([4.0, 1.0]))  # same order
        True

        Unlike the LakeMapperBarnes equivalents, fill_depths and fill_volumes
        are always available through this component:

        >>> fill_depths = [
        ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ...     [0.0, 1.0, 0.0, 0.5, 0.0, 0.0],
        ...     [0.0, 0.0, 0.4, 0.7, 0.0, 0.0],
        ...     [0.0, 0.0, 0.0, 0.0, 0.1, 0.0],
        ...     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        ... ]
        >>> np.allclose(sfb.fill_depths.reshape(mg.shape), fill_depths)
        True
        >>> np.allclose(sfb.fill_volumes, np.array([1.7, 1.0]))
        True

        Note that with a flat fill, we can't drain the surface. The surface
        is completely flat, so each and every core node within the fill
        becomes a sink.

        >>> fr.run_one_step()
        >>> where_is_filled = np.where(sfb.fill_map > -1, 1, 0)
        >>> np.all(
        ...     mg.at_node["flow__sink_flag"][mg.core_nodes]
        ...     == where_is_filled[mg.core_nodes]
        ... )
        True

        (Note that the fill_map does not think that the perimeter nodes are
        sinks, since they haven't changed elevation. In contrast, the
        FlowAccumulator *does* think they are, because these nodes are where
        flow terminates.)
        """
        if "flow__receiver_node" in self._grid.at_node and self._grid.at_node[
            "flow__receiver_node"
        ].size != self._grid.size("node"):
            raise NotImplementedError(
                "A route-to-multiple flow director has been "
                "run on this grid. The landlab development team has "
                "not verified that SinkFillerBarnes is compatible with "
                "route-to-multiple methods. Please open a GitHub Issue "
                "to start this process."
            )

        super().run_one_step()

        self._sed_fill_depth[:] = self._surface - self._supplied_surface

    @property
    def fill_dict(self):
        """Return a dictionary where the keys are the outlet nodes of each
        filled area, and the values are deques of nodes within each.

        Items are not returned in ID order.
        """
        return super().lake_dict

    @property
    def fill_outlets(self):
        """Returns the outlet for each filled area, not necessarily in ID
        order."""
        return super().lake_outlets

    @property
    def number_of_fills(self):
        """Return the number of individual filled areas."""
        return super().number_of_lakes

    @property
    def fill_map(self):
        """Return an array of ints, where each filled node is labelled with its
        outlet node ID.

        Nodes not in a filled area are labelled with
        BAD_INDEX_VALUE (default -1).
        """
        return super().lake_map

    @property
    def fill_at_node(self):
        """Return a boolean array, True if the node is filled, False
        otherwise."""
        return super().lake_at_node

    @property
    def fill_depths(self):
        """Return the change in surface elevation at each node this step."""
        return self._sed_fill_depth

    @property
    def fill_areas(self):
        """A nlakes-long array of the area of each fill.

        The order is the same as that of the keys in fill_dict, and of
        fill_outlets.
        """
        return super().lake_areas

    @property
    def fill_volumes(self):
        """A nlakes-long array of the volume of each fill.

        The order is the same as that of the keys in fill_dict, and of
        fill_outlets.
        """
        fill_vols = np.empty(self.number_of_fills, dtype=float)
        col_vols = self._grid.cell_area_at_node * self._sed_fill_depth
        for i, fillnodes in enumerate(self.fill_dict.values()):
            fill_vols[i] = col_vols[fillnodes].sum()
        return fill_vols



================================================
File: soil_moisture/__init__.py
================================================
from .infiltrate_soil_green_ampt import SoilInfiltrationGreenAmpt
from .soil_moisture_dynamics import SoilMoisture

__all__ = ["SoilMoisture", "SoilInfiltrationGreenAmpt"]



================================================
File: soil_moisture/infiltrate_soil_green_ampt.py
================================================
#!/usr/bin/env python

import numpy as np

from landlab import Component


class SoilInfiltrationGreenAmpt(Component):
    """Infiltrate surface water into a soil following the Green-Ampt method.

    This component calculates the infiltation of surface water into the soil,
    using the Green-Ampt method. The component tracks the depth of infiltrated
    water over time, in the field soil_water_infiltration__depth. It also
    modifies the depth of surface water (surface_water__depth) as surface water
    progressively infiltrates into the soil below.

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import SoilInfiltrationGreenAmpt
    >>> mg = RasterModelGrid((4, 3), xy_spacing=10.0)
    >>> hydraulic_conductivity = mg.ones("node") * 1.0e-6
    >>> hydraulic_conductivity.reshape((4, 3))[0:2, :] *= 10000.0
    >>> h = mg.add_ones("surface_water__depth", at="node")
    >>> h *= 0.01
    >>> d = mg.add_ones("soil_water_infiltration__depth", at="node", dtype=float)
    >>> d *= 0.2
    >>> SI = SoilInfiltrationGreenAmpt(
    ...     mg, hydraulic_conductivity=hydraulic_conductivity
    ... )
    >>> for i in range(10):  # 100s total
    ...     SI.run_one_step(10.0)
    ...
    >>> mg.at_node["surface_water__depth"]
    array([1.00000000e-08, 1.00000000e-08, 1.00000000e-08,
           1.00000000e-08, 1.00000000e-08, 1.00000000e-08,
           9.88530416e-03, 9.88530416e-03, 9.88530416e-03,
           9.88530416e-03, 9.88530416e-03, 9.88530416e-03])
    >>> mg.at_node["soil_water_infiltration__depth"]
    array([0.20999999, 0.20999999, 0.20999999, 0.20999999, 0.20999999,
           0.20999999, 0.2001147 , 0.2001147 , 0.2001147 , 0.2001147 ,
           0.2001147 , 0.2001147 ])

    Notes
    -----
    This code is based on an overland flow model by Francis Rengers and
    colleagues, after Julien et al., 1995. The infiltration scheme follows the
    Green and Ampt equation. It was implemented in Landlab by DEJH, March 2016.

    **Where to learn more**

    A description of the Green-Ampt infiltration equation can be found in many
    hydrology texts, as well as online resources. The original theory was
    published by Green and Ampt (1911).

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Rengers, F. K., McGuire, L. A., Kean, J. W., Staley, D. M., and Hobley, D.:
    Model simulations of flood and debris flow timing in steep catchments after
    wildfire, Water Resour. Res., 52, 6041–6061, doi:10.1002/2015WR018176, 2016.

    **Additional References**

    Julien, P. Y., Saghaﬁan, B., and Ogden, F. L.: Raster-based hydrologic
    modeling of spatially-varied surface runoff, J. Am. Water Resour. As., 31,
    523–536, doi:10.1111/j.17521688.1995.tb04039.x, 1995.

    Green, W. H., & Ampt, G. A. (1911). Studies on Soil Phyics. The Journal of
    Agricultural Science, 4(1), 1-24.
    """

    _name = "SoilInfiltrationGreenAmpt"

    _unit_agnostic = False

    _cite_as = """
    @article{rengers2016model,
      author = {Rengers, F K and McGuire, L A and Kean, J W and Staley, D M
                and Hobley, D E J},
      title = {{Model simulations of flood and debris flow timing in steep
                catchments after wildfire}},
      doi = {10.1002/2015wr018176},
      pages = {6041 -- 6061},
      number = {8},
      volume = {52},
      journal = {Water Resources Research},
      year = {2016},
    }
    """

    _info = {
        "soil_water_infiltration__depth": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": (
                "Water column height above the surface previously absorbed "
                "into the soil. Note that this is NOT the actual depth of "
                "the wetted front, which also depends on the porosity."
            ),
        },
        "surface_water__depth": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of water on the surface",
        },
    }

    # This follows mean values from Rawls et al., 1992; lambda then h_b
    SOIL_PROPS = {
        "sand": (0.694, 0.0726),
        "loamy sand": (0.553, 0.0869),
        "sandy loam": (0.378, 0.1466),
        "loam": (0.252, 0.1115),
        "silt loam": (0.234, 0.2076),
        "sandy clay loam": (0.319, 0.2808),
        "clay loam": (0.242, 0.2589),
        "silty clay loam": (0.177, 0.3256),
        "sandy clay": (0.223, 0.2917),
        "silty clay": (0.150, 0.3419),
        "clay": (0.165, 0.3730),
    }

    def __init__(
        self,
        grid,
        hydraulic_conductivity=0.005,
        soil_bulk_density=1590.0,
        rock_density=2650.0,
        initial_soil_moisture_content=0.15,
        soil_type="sandy loam",
        volume_fraction_coarse_fragments=0.2,
        coarse_sed_flag=False,
        surface_water_minimum_depth=1.0e-8,
        soil_pore_size_distribution_index=None,
        soil_bubbling_pressure=None,
        wetting_front_capillary_pressure_head=None,
    ):
        """
        Parameters
        ----------
        grid : RasterModelGrid
            A grid.
        hydraulic_conductivity : float, array, or field name (m/s)
            The soil effective hydraulic conductivity.
        soil_bulk_density : float (kg/m**3)
            The dry bulk density of the soil.
        rock_density : float (kg/m**3)
            The density of the soil constituent material (i.e., lacking porosity).
        initial_soil_moisture_content : float (m**3/m**3, 0. to 1.)
            The fraction of the initial pore space filled with water.
        soil_type : str
            A soil type to automatically set soil_pore_size_distribution_index
            and soil_bubbling_pressure, using mean values from Rawls et al.,
            1992. The following options are supported: 'sand', loamy sand',
            'sandy loam', 'loam', 'silt loam', 'sandy clay loam', 'clay loam',
            'silty clay loam', 'sandy clay', 'silty clay', or 'clay'.
        volume_fraction_coarse_fragments : float (m**3/m**3, 0. to 1.)
            The fraction of the soil made up of rocky fragments with very
            little porosity, with diameter > 2 mm.
        coarse_sed_flag : boolean, optional
            If this flag is set to true, the fraction of coarse material in the
            soil column with be used as a correction for phi, the porosity factor.
        surface_water_minimum_depth : float (m), optional
            A minimum water depth to stabilize the solutions for surface flood
            modelling. Leave as the default in most normal use cases.
        soil_pore_size_distribution_index : float, optional
            An index describing the distribution of pore sizes in the soil,
            and controlling effective hydraulic conductivity at varying water
            contents, following Brooks and Corey (1964). Can be set by
            soil_type. Typically denoted "lambda".
        soil_bubbling_pressure : float (m), optional
            The bubbling capillary pressure of the soil, controlling effective
            hydraulic conductivity at varying water contents, following Brooks
            and Corey (1964). Can be set by soil_type. Typically denoted "h_b".
        wetting_front_capillary_pressure_head : float (m), optional
            The effective head at the wetting front in the soil driven by
            capillary pressure in the soil pores. If not set, will be
            calculated by the component from the pore size distribution and
            bubbling pressure, following Brooks and Corey.

        """
        super().__init__(grid)

        self._min_water = surface_water_minimum_depth
        self._hydraulic_conductivity = hydraulic_conductivity

        if not coarse_sed_flag:
            volume_fraction_coarse_fragments = 0.0

        self._moisture_deficit = self.calc_moisture_deficit(
            soil_bulk_density=soil_bulk_density,
            rock_density=rock_density,
            volume_fraction_coarse_fragments=volume_fraction_coarse_fragments,
            soil_moisture_content=initial_soil_moisture_content,
        )

        if wetting_front_capillary_pressure_head is None:
            self._capillary_pressure = self.calc_soil_pressure(
                soil_type=soil_type,
                soil_pore_size_distribution_index=soil_pore_size_distribution_index,
                soil_bubbling_pressure=soil_bubbling_pressure,
            )
        else:
            self._capillary_pressure = wetting_front_capillary_pressure_head

    @staticmethod
    def calc_soil_pressure(
        soil_type=None,
        soil_pore_size_distribution_index=1.0,
        soil_bubbling_pressure=0.0,
    ):
        """Calculate capillary pressure in a soil type.

        Parameters
        ----------
        soil_type : str, optional
            The name of a soil type.
        soil_pore_size_distribution_index : float
            Pore-size distribution index [-].
        soil_bubbling_pressure : float
            Bubbling pressure [m].
        """
        if soil_type is None:
            soil_props = (soil_pore_size_distribution_index, soil_bubbling_pressure)
        else:
            try:
                soil_props = SoilInfiltrationGreenAmpt.SOIL_PROPS[soil_type]
            except KeyError as exc:
                raise ValueError(f"{soil_type}: unknown soil type") from exc

        return SoilInfiltrationGreenAmpt.calc_pressure_head(*soil_props)

    @staticmethod
    def calc_pressure_head(lam, h_b):
        """Calculate pressure head.

        Pressure head is set using *lambda* and *h_b*, using an
        equation after Brooks-Corey (1964), following Rawls et al., 1992.

        Parameters
        ----------
        lam : float, optional
            Pore-size distribution index. Exponent that describes the
            distribution of pore sizes in the soil, and controls
            effective hydraulic conductivity at varying water
            contents, following Brooks and Corey (1964) [-].
        h_b : float (m), optional
            Bubbling pressure. Capillary pressure of the soil,
            controlling effective hydraulic conductivity at varying
            water contents, following Brooks and Corey (1964) [m]
        """
        return (2.0 + 3.0 * lam) / (1.0 + 3.0 * lam) * h_b * 0.5

    @staticmethod
    def calc_moisture_deficit(
        soil_bulk_density=1590.0,
        rock_density=2650.0,
        volume_fraction_coarse_fragments=0.0,
        soil_moisture_content=0.0,
    ):
        """Calculate the moisture deficit in a soil.

        Parameters
        ----------
        soil_bulk_density : float or array of float
            Bulk density of the soil [kg / m3].
        rock_density : float or array of float
            Density of rock [kg / m3].
        volume_fraction_coarse_fragments : float or array of float
            Volume fraction of sediment made up of coarse grains [-].
        soil_moisture_content : float or array of float
            Fraction of soil filled with water [-].

        Returns
        -------
        float or array of float
            Moisture deficit.
        """
        if np.any(soil_bulk_density <= 0.0):
            raise ValueError("non-positive soil bulk density")
        if np.any(rock_density < soil_bulk_density):
            raise ValueError("soil bulk density greater than rock density")
        if np.any(volume_fraction_coarse_fragments < 0.0):
            raise ValueError("negative volume fraction of coarse grains")
        if np.any(volume_fraction_coarse_fragments > 1.0):
            raise ValueError("volume fraction of coarse grains")

        soil_porosity = 1.0 - np.true_divide(soil_bulk_density, rock_density)
        soil_porosity *= 1.0 - volume_fraction_coarse_fragments

        if np.any(soil_moisture_content > soil_porosity):
            raise ValueError("soil moisture greater than porosity")

        return soil_porosity - soil_moisture_content

    @property
    def min_water(self):
        """Minimum surface water depth."""
        return self._min_water

    @min_water.setter
    def min_water(self, new_value):
        if np.any(new_value <= 0.0):
            raise ValueError("minimum water depth must be positive")
        self._min_water = new_value

    @property
    def hydraulic_conductivity(self):
        """Hydraulic conductivity of soil."""
        return self._hydraulic_conductivity

    @hydraulic_conductivity.setter
    def hydraulic_conductivity(self, new_value):
        if isinstance(new_value, str):
            new_value = self._grid.at_node[new_value]
        if np.any(new_value < 0.0):
            raise ValueError("hydraulic conductivity must be positive")
        self._hydraulic_conductivity = new_value

    @property
    def moisture_deficit(self):
        """Moisture deficit of soil."""
        return self._moisture_deficit

    @moisture_deficit.setter
    def moisture_deficit(self, new_value):
        if np.any(new_value < 0.0):
            raise ValueError("negative moisture deficit")
        self._moisture_deficit = new_value

    @property
    def capillary_pressure(self):
        """Capillary pressure of soil."""
        return self._capillary_pressure

    @capillary_pressure.setter
    def capillary_pressure(self, new_value):
        if np.any(new_value < 0.0):
            raise ValueError("negative capillary pressure")
        self._capillary_pressure = new_value

    def run_one_step(self, dt):
        """Update fields with current hydrologic conditions.

        Parameters
        ----------
        dt : float (s)
            The imposed timestep for the model.
        """
        water_depth = self._grid.at_node["surface_water__depth"]
        infiltration_depth = self._grid.at_node["soil_water_infiltration__depth"]

        assert np.all(infiltration_depth >= 0.0)

        wettingfront_depth = infiltration_depth / self._moisture_deficit

        potential_infilt = (
            dt
            * self._hydraulic_conductivity
            * (
                (wettingfront_depth + self._capillary_pressure + water_depth)
                / wettingfront_depth
            )
        )
        np.clip(potential_infilt, 0.0, None, out=potential_infilt)

        available_water = water_depth - self._min_water
        np.clip(available_water, 0.0, None, out=available_water)

        actual_infiltration = np.choose(
            potential_infilt > available_water, (potential_infilt, available_water)
        )

        water_depth -= actual_infiltration
        infiltration_depth += actual_infiltration



================================================
File: soil_moisture/soil_moisture_dynamics.py
================================================
import numpy as np

from landlab import Component

_VALID_METHODS = {"Grid", "Multi"}


def assert_method_is_valid(method):
    if method not in _VALID_METHODS:
        raise ValueError("%s: Invalid method name" % method)


class SoilMoisture(Component):
    """Landlab component that simulates root-zone average soil moisture at each
    cell using inputs of potential evapotranspiration, live leaf area index,
    and vegetation cover.

    This component uses a single soil moisture layer and models soil moisture
    loss through transpiration by plants, evaporation by bare soil, and
    leakage. The solution of water balance is based on Laio et. al 2001. The
    component requires fields of initial soil moisture, rainfall input (if
    any), time to the next storm and potential transpiration.

    Ref: Laio, F., Porporato, A., Ridolfi, L., & Rodriguez-Iturbe, I. (2001).
    Plants in water-controlled ecosystems: active role in hydrologic processes
    and response to water stress: II. Probabilistic soil moisture dynamics.
    Advances in Water Resources, 24(7), 707-723.

    .. codeauthor:: Sai Nudurupati and Erkan Istanbulluoglu

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> from landlab.components.soil_moisture import SoilMoisture
    >>> grid = RasterModelGrid((5, 4), xy_spacing=(0.2, 0.2))
    >>> SoilMoisture.name
    'Soil Moisture'
    >>> sorted(SoilMoisture.output_var_names)
    ['soil_moisture__root_zone_leakage',
     'soil_moisture__saturation_fraction',
     'surface__evapotranspiration',
     'surface__runoff',
     'vegetation__water_stress']
    >>> sorted(SoilMoisture.units)
    [('rainfall__daily_depth', 'mm'),
     ('soil_moisture__initial_saturation_fraction', 'None'),
     ('soil_moisture__root_zone_leakage', 'mm'),
     ('soil_moisture__saturation_fraction', 'None'),
     ('surface__evapotranspiration', 'mm'),
     ('surface__potential_evapotranspiration_rate', 'mm'),
     ('surface__runoff', 'mm'),
     ('vegetation__cover_fraction', 'None'),
     ('vegetation__live_leaf_area_index', 'None'),
     ('vegetation__plant_functional_type', 'None'),
     ('vegetation__water_stress', 'None')]
    >>> grid["cell"]["vegetation__plant_functional_type"] = np.zeros(
    ...     grid.number_of_cells, dtype=int
    ... )
    >>> _ = grid.add_zeros("vegetation__cover_fraction", at="cell")
    >>> _ = grid.add_zeros("vegetation__live_leaf_area_index", at="cell")
    >>> _ = grid.add_zeros("surface__potential_evapotranspiration_rate", at="cell")
    >>> _ = grid.add_zeros("soil_moisture__initial_saturation_fraction", at="cell")
    >>> _ = grid.add_zeros("rainfall__daily_depth", at="cell")
    >>> SM = SoilMoisture(grid)
    >>> SM.grid.number_of_cell_rows
    3
    >>> SM.grid.number_of_cell_columns
    2
    >>> SM.grid is grid
    True
    >>> import numpy as np
    >>> np.allclose(grid.at_cell["soil_moisture__saturation_fraction"], 0.0)
    True
    >>> grid["cell"]["surface__potential_evapotranspiration_rate"] = np.array(
    ...     [0.2554777, 0.2554777, 0.22110221, 0.22110221, 0.24813062, 0.24813062]
    ... )
    >>> grid["cell"]["soil_moisture__initial_saturation_fraction"] = 0.75 * np.ones(
    ...     grid.number_of_cells
    ... )
    >>> grid["cell"]["vegetation__live_leaf_area_index"] = 2.0 * np.ones(
    ...     grid.number_of_cells
    ... )
    >>> grid["cell"]["vegetation__cover_fraction"] = np.ones(grid.number_of_cells)
    >>> grid["cell"]["rainfall__daily_depth"] = 25.0 * np.ones(grid.number_of_cells)
    >>> SM.current_time = 0.5
    >>> current_time = SM.update()
    >>> np.allclose(grid.at_cell["soil_moisture__saturation_fraction"], 0.0)
    False

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    Laio, F., Porporato, A., Ridolfi, L., Rodriguez-Iturbe, I. (2001). Plants
    in water-controlled ecosystems: active role in hydrologic processes and
    response to water stress II. Probabilistic soil moisture dynamics. Advances
    in Water Resources  24(7), 707-723.
    https://dx.doi.org/10.1016/s0309-1708(01)00005-7

    """

    _name = "Soil Moisture"

    _unit_agnostic = False

    _info = {
        "rainfall__daily_depth": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "mm",
            "mapping": "cell",
            "doc": (
                "Rain in (mm) as a field, allowing spatio-temporal soil "
                "moisture saturation analysis."
            ),
        },
        "soil_moisture__initial_saturation_fraction": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "None",
            "mapping": "cell",
            "doc": "initial soil_moisture__saturation_fraction",
        },
        "soil_moisture__root_zone_leakage": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "mm",
            "mapping": "cell",
            "doc": (
                "leakage of water into deeper portions of the soil not "
                "accessible to the plant"
            ),
        },
        "soil_moisture__saturation_fraction": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "None",
            "mapping": "cell",
            "doc": "relative volumetric water content (theta) - limits=[0,1]",
        },
        "surface__evapotranspiration": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "mm",
            "mapping": "cell",
            "doc": "actual sum of evaporation and plant transpiration",
        },
        "surface__potential_evapotranspiration_rate": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "mm",
            "mapping": "cell",
            "doc": "potential sum of evaporation and potential transpiration",
        },
        "surface__runoff": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "mm",
            "mapping": "cell",
            "doc": "runoff from ground surface",
        },
        "vegetation__cover_fraction": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "None",
            "mapping": "cell",
            "doc": "fraction of land covered by vegetation",
        },
        "vegetation__live_leaf_area_index": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "None",
            "mapping": "cell",
            "doc": "one-sided green leaf area per unit ground surface area",
        },
        "vegetation__plant_functional_type": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "None",
            "mapping": "cell",
            "doc": (
                "classification of plants (int), grass=0, shrub=1, tree=2, "
                "bare=3, shrub_seedling=4, tree_seedling=5"
            ),
        },
        "vegetation__water_stress": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "None",
            "mapping": "cell",
            "doc": "parameter that represents nonlinear effects of water deficit on plants",
        },
    }

    def __init__(
        self,
        grid,
        runon=0.0,
        f_bare=0.7,
        soil_ew=0.1,
        intercept_cap_grass=1.0,
        zr_grass=0.3,
        I_B_grass=20.0,
        I_V_grass=24.0,
        pc_grass=0.43,
        fc_grass=0.56,
        sc_grass=0.33,
        wp_grass=0.13,
        hgw_grass=0.1,
        beta_grass=13.8,
        LAI_max_grass=2.0,
        LAIR_max_grass=2.88,
        intercept_cap_shrub=1.5,
        zr_shrub=0.5,
        I_B_shrub=20.0,
        I_V_shrub=40.0,
        pc_shrub=0.43,
        fc_shrub=0.56,
        sc_shrub=0.24,
        wp_shrub=0.13,
        hgw_shrub=0.1,
        beta_shrub=13.8,
        LAI_max_shrub=2.0,
        LAIR_max_shrub=2.0,
        intercept_cap_tree=2.0,
        zr_tree=1.3,
        I_B_tree=20.0,
        I_V_tree=40.0,
        pc_tree=0.43,
        fc_tree=0.56,
        sc_tree=0.22,
        wp_tree=0.15,
        hgw_tree=0.1,
        beta_tree=13.8,
        LAI_max_tree=4.0,
        LAIR_max_tree=4.0,
        intercept_cap_bare=1.0,
        zr_bare=0.15,
        I_B_bare=20.0,
        I_V_bare=20.0,
        pc_bare=0.43,
        fc_bare=0.56,
        sc_bare=0.33,
        wp_bare=0.13,
        hgw_bare=0.1,
        beta_bare=13.8,
        LAI_max_bare=0.01,
        LAIR_max_bare=0.01,
        method="Grid",
        Tb=24.0,
        Tr=0.0,
        current_time=0,
    ):
        """
        Parameters
        ----------
        grid: RasterModelGrid
            A grid.
        runon: float, optional
            Runon from higher elevation (mm).
        f_bare: float, optional
            Fraction to partition PET for bare soil (None).
        soil_ew: float, optional
            Residual Evaporation after wilting (mm/day).
        intercept_cap: float, optional
            Plant Functional Type (PFT) specific full canopy interception
            capacity.
        zr: float, optional
            Root depth (m).
        I_B: float, optional
            Infiltration capacity of bare soil (mm/h).
        I_V: float, optional
            Infiltration capacity of vegetated soil (mm/h).
        pc: float, optional
            Soil porosity (None).
        fc: float, optional
            Soil saturation degree at field capacity (None).
        sc: float, optional
            Soil saturation degree at stomatal closure (None).
        wp: float, optional
            Soil saturation degree at wilting point (None).
        hgw: float, optional
            Soil saturation degree at hygroscopic point (None).
        beta: float, optional
            Deep percolation constant = 2*b+3 where b is
            water retention (None).
        LAI_max: float, optional
            Maximum leaf area index (m^2/m^2).
        LAIR_max: float, optional
            Reference leaf area index (m^2/m^2).
        method: str
            Method used
        Tr: float, optional
            Storm duration (hours).
        Tb: float, optional
            Inter-storm duration (hours).
        current_time: float
              Current time (years).
        """
        super().__init__(grid)

        self.current_time = 0
        self._method = method
        self.Tr = Tr
        self.Tb = Tb
        assert_method_is_valid(self._method)

        self.initialize(
            runon=runon,
            f_bare=f_bare,
            soil_ew=soil_ew,
            intercept_cap_grass=intercept_cap_grass,
            zr_grass=zr_grass,
            I_B_grass=I_B_grass,
            I_V_grass=I_V_grass,
            pc_grass=pc_grass,
            fc_grass=fc_grass,
            sc_grass=sc_grass,
            wp_grass=wp_grass,
            hgw_grass=hgw_grass,
            beta_grass=beta_grass,
            LAI_max_grass=LAI_max_grass,
            LAIR_max_grass=LAIR_max_grass,
            intercept_cap_shrub=intercept_cap_shrub,
            zr_shrub=zr_shrub,
            I_B_shrub=I_B_shrub,
            I_V_shrub=I_V_shrub,
            pc_shrub=pc_shrub,
            fc_shrub=fc_shrub,
            sc_shrub=sc_shrub,
            wp_shrub=wp_shrub,
            hgw_shrub=hgw_shrub,
            beta_shrub=beta_shrub,
            LAI_max_shrub=LAI_max_shrub,
            LAIR_max_shrub=LAIR_max_shrub,
            intercept_cap_tree=intercept_cap_tree,
            zr_tree=zr_tree,
            I_B_tree=I_B_tree,
            I_V_tree=I_V_tree,
            pc_tree=pc_tree,
            fc_tree=fc_tree,
            sc_tree=sc_tree,
            wp_tree=wp_tree,
            hgw_tree=hgw_tree,
            beta_tree=beta_tree,
            LAI_max_tree=LAI_max_tree,
            LAIR_max_tree=LAIR_max_tree,
            intercept_cap_bare=intercept_cap_bare,
            zr_bare=zr_bare,
            I_B_bare=I_B_bare,
            I_V_bare=I_V_bare,
            pc_bare=pc_bare,
            fc_bare=fc_bare,
            sc_bare=sc_bare,
            wp_bare=wp_bare,
            hgw_bare=hgw_bare,
            beta_bare=beta_bare,
            LAI_max_bare=LAI_max_bare,
            LAIR_max_bare=LAIR_max_bare,
        )

        self.initialize_output_fields()

        self._nodal_values = self._grid["node"]

        self._cell_values = self._grid["cell"]

    @property
    def Tb(self):
        """Storm duration (hours)."""
        return self._Tb

    @Tb.setter
    def Tb(self, Tb):
        assert Tb >= 0
        self._Tb = Tb

    @property
    def Tr(self):
        """Inter-storm duration (hours)."""
        return self._Tr

    @Tr.setter
    def Tr(self, Tr):
        assert Tr >= 0
        self._Tr = Tr

    def initialize(
        self,
        runon=0.0,
        f_bare=0.7,
        soil_ew=0.1,
        intercept_cap_grass=1.0,
        zr_grass=0.3,
        I_B_grass=20.0,
        I_V_grass=24.0,
        pc_grass=0.43,
        fc_grass=0.56,
        sc_grass=0.33,
        wp_grass=0.13,
        hgw_grass=0.1,
        beta_grass=13.8,
        LAI_max_grass=2.0,
        LAIR_max_grass=2.88,
        intercept_cap_shrub=1.5,
        zr_shrub=0.5,
        I_B_shrub=20.0,
        I_V_shrub=40.0,
        pc_shrub=0.43,
        fc_shrub=0.56,
        sc_shrub=0.24,
        wp_shrub=0.13,
        hgw_shrub=0.1,
        beta_shrub=13.8,
        LAI_max_shrub=2.0,
        LAIR_max_shrub=2.0,
        intercept_cap_tree=2.0,
        zr_tree=1.3,
        I_B_tree=20.0,
        I_V_tree=40.0,
        pc_tree=0.43,
        fc_tree=0.56,
        sc_tree=0.22,
        wp_tree=0.15,
        hgw_tree=0.1,
        beta_tree=13.8,
        LAI_max_tree=4.0,
        LAIR_max_tree=4.0,
        intercept_cap_bare=1.0,
        zr_bare=0.15,
        I_B_bare=20.0,
        I_V_bare=20.0,
        pc_bare=0.43,
        fc_bare=0.56,
        sc_bare=0.33,
        wp_bare=0.13,
        hgw_bare=0.1,
        beta_bare=13.8,
        LAI_max_bare=0.01,
        LAIR_max_bare=0.01,
    ):
        # GRASS = 0; SHRUB = 1; TREE = 2; BARE = 3;
        # SHRUBSEEDLING = 4; TREESEEDLING = 5
        """
        Parameters
        ----------
        grid: RasterModelGrid
            A grid.
        runon: float, optional
            Runon from higher elevation (mm).
        f_bare: float, optional
            Fraction to partition PET for bare soil (None).
        soil_ew: float, optional
            Residual Evaporation after wilting (mm/day).
        intercept_cap: float, optional
            Plant Functional Type (PFT) specific full canopy interception
            capacity.
        zr: float, optional
            Root depth (m).
        I_B: float, optional
            Infiltration capacity of bare soil (mm/h).
        I_V: float, optional
            Infiltration capacity of vegetated soil (mm/h).
        pc: float, optional
            Soil porosity (None).
        fc: float, optional
            Soil saturation degree at field capacity (None).
        sc: float, optional
            Soil saturation degree at stomatal closure (None).
        wp: float, optional
            Soil saturation degree at wilting point (None).
        hgw: float, optional
            Soil saturation degree at hygroscopic point (None).
        beta: float, optional
            Deep percolation constant = 2*b+3 where b is
            water retention (None).
        parameter (None)
        LAI_max: float, optional
            Maximum leaf area index (m^2/m^2).
        LAIR_max: float, optional
            Reference leaf area index (m^2/m^2).
        """

        self._vegtype = self._grid["cell"]["vegetation__plant_functional_type"]
        self._runon = runon
        self._fbare = f_bare
        self._interception_cap = np.choose(
            self._vegtype,
            [
                intercept_cap_grass,
                intercept_cap_shrub,
                intercept_cap_tree,
                intercept_cap_bare,
                intercept_cap_shrub,
                intercept_cap_tree,
            ],
        )

        self._zr = np.choose(
            self._vegtype, [zr_grass, zr_shrub, zr_tree, zr_bare, zr_shrub, zr_tree]
        )

        self._soil_Ib = np.choose(
            self._vegtype,
            [I_B_grass, I_B_shrub, I_B_tree, I_B_bare, I_B_shrub, I_B_tree],
        )

        self._soil_Iv = np.choose(
            self._vegtype,
            [I_V_grass, I_V_shrub, I_V_tree, I_V_bare, I_V_shrub, I_V_tree],
        )

        self._soil_Ew = soil_ew
        self._soil_pc = np.choose(
            self._vegtype, [pc_grass, pc_shrub, pc_tree, pc_bare, pc_shrub, pc_tree]
        )

        self._soil_fc = np.choose(
            self._vegtype, [fc_grass, fc_shrub, fc_tree, fc_bare, fc_shrub, fc_tree]
        )

        self._soil_sc = np.choose(
            self._vegtype, [sc_grass, sc_shrub, sc_tree, sc_bare, sc_shrub, sc_tree]
        )

        self._soil_wp = np.choose(
            self._vegtype, [wp_grass, wp_shrub, wp_tree, wp_bare, wp_shrub, wp_tree]
        )

        self._soil_hgw = np.choose(
            self._vegtype,
            [hgw_grass, hgw_shrub, hgw_tree, hgw_bare, hgw_shrub, hgw_tree],
        )

        self._soil_beta = np.choose(
            self._vegtype,
            [beta_grass, beta_shrub, beta_tree, beta_bare, beta_shrub, beta_tree],
        )

        self._LAI_max = np.choose(
            self._vegtype,
            [
                LAI_max_grass,
                LAI_max_shrub,
                LAI_max_tree,
                LAI_max_bare,
                LAI_max_shrub,
                LAI_max_tree,
            ],
        )

        self._LAIR_max = np.choose(
            self._vegtype,
            [
                LAIR_max_grass,
                LAIR_max_shrub,
                LAIR_max_tree,
                LAIR_max_bare,
                LAIR_max_shrub,
                LAIR_max_tree,
            ],
        )

    def update(self):
        """Update fields with current loading conditions.

        This method looks to the properties ``current_time``, ``Tb``,
        and ``Tr``, and uses their values in updating fields.
        """
        Tb = self._Tb
        Tr = self._Tr
        current_time = self._current_time

        P_ = self._cell_values["rainfall__daily_depth"]
        self._PET = self._cell_values["surface__potential_evapotranspiration_rate"]
        self._SO = self._cell_values["soil_moisture__initial_saturation_fraction"]
        self._vegcover = self._cell_values["vegetation__cover_fraction"]
        self._water_stress = self._cell_values["vegetation__water_stress"]
        self._S = self._cell_values["soil_moisture__saturation_fraction"]
        self._D = self._cell_values["soil_moisture__root_zone_leakage"]
        self._ETA = self._cell_values["surface__evapotranspiration"]
        self._fr = (
            self._cell_values["vegetation__live_leaf_area_index"] / self._LAIR_max
        )
        self._runoff = self._cell_values["surface__runoff"]
        # LAIl = self._cell_values['vegetation__live_leaf_area_index']
        # LAIt = LAIl+self._cell_values['DeadLeafAreaIndex']
        # if LAIt.all() == 0.:
        #     self._fr = np.zeros(self._grid.number_of_cells)
        # else:
        #     self._fr = (self._vegcover[0]*LAIl/LAIt)
        self._fr[self._fr > 1.0] = 1.0
        self._Sini = np.zeros(self._SO.shape)
        self._ETmax = np.zeros(self._SO.shape)

        for cell in range(0, self._grid.number_of_cells):
            P = P_[cell]
            # print cell
            s = self._SO[cell]
            fbare = self._fbare
            ZR = self._zr[cell]
            pc = self._soil_pc[cell]
            fc = self._soil_fc[cell]
            scc = self._soil_sc[cell]
            wp = self._soil_wp[cell]
            hgw = self._soil_hgw[cell]
            beta = self._soil_beta[cell]
            if self._vegtype[cell] == 0:  # 0 - GRASS
                sc = scc * self._fr[cell] + (1 - self._fr[cell]) * fc
            else:
                sc = scc

            Inf_cap = (
                self._soil_Ib[cell] * (1 - self._vegcover[cell])
                + self._soil_Iv[cell] * self._vegcover[cell]
            )
            # Infiltration capacity
            Int_cap = min(self._vegcover[cell] * self._interception_cap[cell], P)
            # Interception capacity
            Peff = max(P - Int_cap, 0.0)  # Effective precipitation depth
            mu = (Inf_cap / 1000.0) / (pc * ZR * (np.exp(beta * (1.0 - fc)) - 1.0))
            Ep = max(
                (
                    self._PET[cell] * self._fr[cell]
                    + fbare * self._PET[cell] * (1.0 - self._fr[cell])
                )
                - Int_cap,
                0.0001,
            )  # mm/d
            self._ETmax[cell] = Ep
            nu = ((Ep / 24.0) / 1000.0) / (pc * ZR)  # Loss function parameter
            nuw = ((self._soil_Ew / 24.0) / 1000.0) / (pc * ZR)
            # Loss function parameter
            sini = self._SO[cell] + ((Peff + self._runon) / (pc * ZR * 1000.0))

            if sini > 1.0:
                self._runoff[cell] = (sini - 1.0) * pc * ZR * 1000.0
                # print 'Runoff =', self._runoff
                sini = 1.0
            else:
                self._runoff[cell] = 0.0

            if sini >= fc:
                tfc = (1.0 / (beta * (mu - nu))) * (
                    beta * (fc - sini)
                    + np.log((nu - mu + mu * np.exp(beta * (sini - fc))) / nu)
                )
                tsc = ((fc - sc) / nu) + tfc
                twp = ((sc - wp) / (nu - nuw)) * np.log(nu / nuw) + tsc

                if Tb < tfc:
                    s = abs(
                        sini
                        - (1.0 / beta)
                        * np.log(
                            (
                                (nu - mu + mu * np.exp(beta * (sini - fc)))
                                * np.exp(beta * (nu - mu) * Tb)
                                - mu * np.exp(beta * (sini - fc))
                            )
                            / (nu - mu)
                        )
                    )

                    self._D[cell] = ((pc * ZR * 1000.0) * (sini - s)) - (
                        Tb * (Ep / 24.0)
                    )
                    self._ETA[cell] = Tb * (Ep / 24.0)

                elif Tb >= tfc and Tb < tsc:
                    s = fc - (nu * (Tb - tfc))
                    self._D[cell] = ((pc * ZR * 1000.0) * (sini - fc)) - (
                        (tfc) * (Ep / 24.0)
                    )
                    self._ETA[cell] = Tb * (Ep / 24.0)

                elif Tb >= tsc and Tb < twp:
                    s = wp + (sc - wp) * (
                        (nu / (nu - nuw))
                        * np.exp((-1) * ((nu - nuw) / (sc - wp)) * (Tb - tsc))
                        - (nuw / (nu - nuw))
                    )
                    self._D[cell] = ((pc * ZR * 1000.0) * (sini - fc)) - (
                        tfc * Ep / 24.0
                    )
                    self._ETA[cell] = (1000.0 * ZR * pc * (sini - s)) - self._D[cell]

                else:
                    s = hgw + (wp - hgw) * np.exp(
                        (-1) * (nuw / (wp - hgw)) * max(Tb - twp, 0.0)
                    )
                    self._D[cell] = ((pc * ZR * 1000.0) * (sini - fc)) - (
                        tfc * Ep / 24.0
                    )
                    self._ETA[cell] = (1000.0 * ZR * pc * (sini - s)) - self._D[cell]

            elif sini < fc and sini >= sc:
                tfc = 0.0
                tsc = (sini - sc) / nu
                twp = ((sc - wp) / (nu - nuw)) * np.log(nu / nuw) + tsc

                if Tb < tsc:
                    s = sini - nu * Tb
                    self._D[cell] = 0.0
                    self._ETA[cell] = 1000.0 * ZR * pc * (sini - s)

                elif Tb >= tsc and Tb < twp:
                    s = wp + (sc - wp) * (
                        (nu / (nu - nuw))
                        * np.exp((-1) * ((nu - nuw) / (sc - wp)) * (Tb - tsc))
                        - (nuw / (nu - nuw))
                    )
                    self._D[cell] = 0
                    self._ETA[cell] = 1000.0 * ZR * pc * (sini - s)

                else:
                    s = hgw + (wp - hgw) * np.exp(
                        (-1) * (nuw / (wp - hgw)) * (Tb - twp)
                    )
                    self._D[cell] = 0.0
                    self._ETA[cell] = 1000.0 * ZR * pc * (sini - s)

            elif sini < sc and sini >= wp:
                tfc = 0
                tsc = 0
                twp = ((sc - wp) / (nu - nuw)) * np.log(
                    1 + (nu - nuw) * (sini - wp) / (nuw * (sc - wp))
                )

                if Tb < twp:
                    s = wp + ((sc - wp) / (nu - nuw)) * (
                        (np.exp((-1) * ((nu - nuw) / (sc - wp)) * Tb))
                        * (nuw + ((nu - nuw) / (sc - wp)) * (sini - wp))
                        - nuw
                    )
                    self._D[cell] = 0.0
                    self._ETA[cell] = 1000.0 * ZR * pc * (sini - s)

                else:
                    s = hgw + (wp - hgw) * np.exp(
                        (-1) * (nuw / (wp - hgw)) * (Tb - twp)
                    )
                    self._D[cell] = 0.0
                    self._ETA[cell] = 1000.0 * ZR * pc * (sini - s)

            else:
                tfc = 0.0
                tsc = 0.0
                twp = 0.0

                s = hgw + (sini - hgw) * np.exp((-1) * (nuw / (wp - hgw)) * Tb)
                self._D[cell] = 0.0
                self._ETA[cell] = 1000.0 * ZR * pc * (sini - s)

            self._water_stress[cell] = min(
                ((max(((sc - (s + sini) / 2.0) / (sc - wp)), 0.0)) ** 4.0), 1.0
            )
            self._S[cell] = s
            self._SO[cell] = s
            self._Sini[cell] = sini

        self.current_time += (Tb + Tr) / (24.0 * 365.25)
        return current_time



================================================
File: space/__init__.py
================================================
from .space import Space
from .space_large_scale_eroder import SpaceLargeScaleEroder

__all__ = ["Space", "SpaceLargeScaleEroder"]



================================================
File: space/space.py
================================================
import numpy as np
from scipy.integrate import quad

from landlab.components.erosion_deposition.generalized_erosion_deposition import (
    DEFAULT_MINIMUM_TIME_STEP,
)
from landlab.components.erosion_deposition.generalized_erosion_deposition import (
    _GeneralizedErosionDeposition,
)
from landlab.utils.return_array import return_array_at_node

from .ext.calc_qs import calculate_qs_in

ROOT2 = np.sqrt(2.0)  # syntactic sugar for precalculated square root of 2
TIME_STEP_FACTOR = 0.5  # factor used in simple subdivision solver


class Space(_GeneralizedErosionDeposition):
    """Stream Power with Alluvium Conservation and Entrainment (SPACE)

    See the publication:

    Shobe, C. M., Tucker, G. E., and Barnhart, K. R.: The SPACE 1.0 model: a
    Landlab component for 2-D calculation of sediment transport, bedrock
    erosion, and landscape evolution, Geosci. Model Dev., 10, 4577-4604,
    `https://doi.org/10.5194/gmd-10-4577-2017 <https://www.geosci-model-dev.net/10/4577/2017/>`_, 2017.

    Unlike other some other fluvial erosion componets in Landlab, in this
    component (and :py:class:`~landlab.components.ErosionDeposition`) no
    erosion occurs in depressions or in areas with adverse slopes. There is no
    ability to pass a keyword argument ``erode_flooded_nodes``.

    If a depressions are handled (as indicated by the presence of the field
    "flood_status_code" at nodes), then deposition occurs throughout the
    depression and sediment is passed out of the depression. Where pits are
    encountered, then all sediment is deposited at that node only.

    Note: If timesteps are large enough that Es*dt (sediment erosion)
    exceeds sediment thickness H, the 'adaptive' solver is necessary to
    subdivide timesteps. Compare Es and H arrays to determine whether
    timesteps are appropriate or too large for the 'basic' solver.

    Examples
    ---------
    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import (
    ...     FlowAccumulator,
    ...     DepressionFinderAndRouter,
    ...     Space,
    ...     FastscapeEroder,
    ... )
    >>> np.random.seed(seed=5000)

    Define grid and initial topography:

    *  5x5 grid with base level in the lower left corner
    *  All other boundary nodes closed
    *  Initial topography is plane tilted up to the upper right with
       noise

    >>> mg = RasterModelGrid((5, 5), xy_spacing=10.0)
    >>> _ = mg.add_zeros("topographic__elevation", at="node")
    >>> mg.at_node["topographic__elevation"] += (
    ...     mg.node_y / 10.0 + mg.node_x / 10.0 + np.random.rand(len(mg.node_y)) / 10.0
    ... )
    >>> mg.set_closed_boundaries_at_grid_edges(
    ...     bottom_is_closed=True,
    ...     left_is_closed=True,
    ...     right_is_closed=True,
    ...     top_is_closed=True,
    ... )
    >>> mg.set_watershed_boundary_condition_outlet_id(
    ...     0, mg.at_node["topographic__elevation"], -9999.0
    ... )
    >>> fsc_dt = 100.0
    >>> space_dt = 100.0

    Instantiate Fastscape eroder, flow router, and depression finder

    >>> fr = FlowAccumulator(mg, flow_director="D8")
    >>> df = DepressionFinderAndRouter(mg)
    >>> fsc = FastscapeEroder(mg, K_sp=0.001, m_sp=0.5, n_sp=1)

    Burn in an initial drainage network using the Fastscape eroder:

    >>> for _ in range(100):
    ...     fr.run_one_step()
    ...     df.map_depressions()
    ...     fsc.run_one_step(dt=fsc_dt)
    ...     mg.at_node["topographic__elevation"][0] -= 0.001  # Uplift
    ...

    Add some soil to the drainage network:

    >>> _ = mg.add_zeros("soil__depth", at="node", dtype=float)
    >>> mg.at_node["soil__depth"] += 0.5
    >>> mg.at_node["topographic__elevation"] += mg.at_node["soil__depth"]

    Instantiate the Space component:

    >>> ha = Space(
    ...     mg,
    ...     K_sed=0.00001,
    ...     K_br=0.00000000001,
    ...     F_f=0.5,
    ...     phi=0.1,
    ...     H_star=1.0,
    ...     v_s=0.001,
    ...     m_sp=0.5,
    ...     n_sp=1.0,
    ...     sp_crit_sed=0,
    ...     sp_crit_br=0,
    ... )

    Now run the Space component for 2000 short timesteps:

    >>> for _ in range(2000):  # Space component loop
    ...     fr.run_one_step()
    ...     df.map_depressions()
    ...     ha.run_one_step(dt=space_dt)
    ...     mg.at_node["bedrock__elevation"][0] -= 2e-6 * space_dt
    ...

    Now we test to see if soil depth and topography are right:

    >>> np.around(mg.at_node["soil__depth"], decimals=3)
    array([0.5  , 0.5  , 0.5  , 0.5  , 0.5  , 0.5  , 0.495, 0.492,
           0.491, 0.5  , 0.5  , 0.492, 0.492, 0.49 , 0.5  , 0.5  ,
           0.491, 0.49 , 0.484, 0.5  , 0.5  , 0.5  , 0.5  , 0.5  ,
           0.5  ])

    >>> np.around(mg.at_node["topographic__elevation"], decimals=3)
    array([0.423, 1.536, 2.573, 3.511, 4.561, 1.582, 0.424, 0.428,
           0.438, 5.51 , 2.54 , 0.428, 0.428, 0.438, 6.526, 3.559,
           0.438, 0.438, 0.45 , 7.553, 4.559, 5.541, 6.57 , 7.504,
           8.51 ])

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Shobe, C., Tucker, G., Barnhart, K. (2017). The SPACE 1.0 model: a Landlab
    component for 2-D calculation of sediment transport, bedrock erosion, and
    landscape evolution. Geoscientific Model Development  10(12), 4577 - 4604.
    https://dx.doi.org/10.5194/gmd-10-4577-2017

    **Additional References**

    None Listed

    """  # noqa: B950

    _name = "Space"

    _unit_agnostic = True

    _info = {
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__upstream_node_order": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array containing downstream-to-upstream ordered list of node IDs",
        },
        "sediment__influx": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m3/s",
            "mapping": "node",
            "doc": "Sediment flux (volume per unit time of sediment entering each node)",
        },
        "sediment__outflux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m3/s",
            "mapping": "node",
            "doc": "Sediment flux (volume per unit time of sediment leaving each node)",
        },
        "soil__depth": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of soil or weathered bedrock",
        },
        "surface_water__discharge": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m**3/s",
            "mapping": "node",
            "doc": "Volumetric discharge of surface water",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__steepest_slope": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "The steepest *downhill* slope",
        },
    }

    _cite_as = """
    @Article{gmd-10-4577-2017,
        AUTHOR = {Shobe, C. M. and Tucker, G. E. and Barnhart, K. R.},
        TITLE = {The SPACE~1.0 model: a~Landlab component for 2-D calculation
                 of sediment transport, bedrock erosion, and landscape evolution},
        JOURNAL = {Geoscientific Model Development},
        VOLUME = {10},
        YEAR = {2017},
        NUMBER = {12},
        PAGES = {4577--4604},
        URL = {https://www.geosci-model-dev.net/10/4577/2017/},
        DOI = {10.5194/gmd-10-4577-2017}
    }"""

    def __init__(
        self,
        grid,
        K_sed=0.002,
        K_br=0.002,
        F_f=0.0,
        phi=0.3,
        H_star=0.1,
        v_s=1.0,
        m_sp=0.5,
        n_sp=1.0,
        sp_crit_sed=0.0,
        sp_crit_br=0.0,
        discharge_field="surface_water__discharge",
        solver="basic",
        dt_min=DEFAULT_MINIMUM_TIME_STEP,
    ):
        """Initialize the Space model.

        Parameters
        ----------
        grid : ModelGrid
            Landlab ModelGrid object
        K_sed : float, field name, or array
            Erodibility for sediment (units vary).
        K_br : float, field name, or array
            Erodibility for bedrock (units vary).
        F_f : float
            Fraction of permanently suspendable fines in bedrock [-].
        phi : float
            Sediment porosity [-].
        H_star : float
            Sediment thickness required for full entrainment [L].
        v_s : float
            Effective settling velocity for chosen grain size metric [L/T].
        m_sp : float
            Drainage area exponent (units vary)
        n_sp : float
            Slope exponent (units vary)
        sp_crit_sed : float, field name, or array
            Critical stream power to erode sediment [E/(TL^2)]
        sp_crit_br : float, field name, or array
            Critical stream power to erode rock [E/(TL^2)]
        discharge_field : float, field name, or array
            Discharge [L^2/T]. The default is to use the grid field
            'surface_water__discharge', which is simply drainage area
            multiplied by the default rainfall rate (1 m/yr). To use custom
            spatially/temporally varying rainfall, use 'water__unit_flux_in'
            to specify water input to the FlowAccumulator.
        solver : string
            Solver to use. Options at present include:
                (1) 'basic' (default): explicit forward-time extrapolation.
                    Simple but will become unstable if time step is too large.
                (2) 'adaptive': subdivides global time step as needed to
                    prevent slopes from reversing and alluvium from going
                    negative.

        """
        if grid.at_node["flow__receiver_node"].size != grid.size("node"):
            raise NotImplementedError(
                "A route-to-multiple flow director has been "
                "run on this grid. The landlab development team has not "
                "verified that SPACE is compatible with "
                "route-to-multiple methods. Please open a GitHub Issue "
                "to start this process."
            )

        super().__init__(
            grid,
            m_sp=m_sp,
            n_sp=n_sp,
            F_f=F_f,
            v_s=v_s,
            dt_min=dt_min,
            discharge_field=discharge_field,
        )

        if phi >= 1.0:
            raise ValueError("Porosity must be < 1.0")
        if phi < 0.0:
            raise ValueError("Porosity must be > 0.0")

        self._phi = float(phi)
        self._porosity_factor = 1.0 / (1.0 - self._phi)

        # space specific inits
        self._H_star = H_star
        self._soil__depth = grid.at_node["soil__depth"]

        if "bedrock__elevation" in grid.at_node:
            self._bedrock__elevation = grid.at_node["bedrock__elevation"]
        else:
            self._bedrock__elevation = grid.add_zeros(
                "bedrock__elevation", at="node", dtype=float
            )

            self._bedrock__elevation[:] = (
                self._topographic__elevation - self._soil__depth
            )

        self._sed_erosion_term = np.zeros(grid.number_of_nodes)
        self._br_erosion_term = np.zeros(grid.number_of_nodes)
        self._Es = np.zeros(grid.number_of_nodes)
        self._Er = np.zeros(grid.number_of_nodes)

        # K's and critical values can be floats, grid fields, or arrays
        # use setters defined below
        self.K_sed = K_sed
        self.K_br = K_br

        self._sp_crit_sed = return_array_at_node(grid, sp_crit_sed)
        self._sp_crit_br = return_array_at_node(grid, sp_crit_br)

        # Handle option for solver
        if solver == "basic":
            self.run_one_step = self.run_one_step_basic
        elif solver == "adaptive":
            self.run_one_step = self.run_with_adaptive_time_step_solver
            self._time_to_flat = np.zeros(grid.number_of_nodes)
            self._time_to_zero_alluv = np.zeros(grid.number_of_nodes)
            self._dzdt = np.zeros(grid.number_of_nodes)
        else:
            raise ValueError(
                "Parameter 'solver' must be one of: " + "'basic', 'adaptive'"
            )

    @property
    def K_br(self):
        """Erodibility of bedrock(units depend on m_sp)."""
        return self._K_br

    @K_br.setter
    def K_br(self, new_val):
        self._K_br = return_array_at_node(self._grid, new_val)

    @property
    def K_sed(self):
        """Erodibility of sediment(units depend on m_sp)."""
        return self._K_sed

    @K_sed.setter
    def K_sed(self, new_val):
        self._K_sed = return_array_at_node(self._grid, new_val)

    def _calc_erosion_rates(self):
        """Calculate erosion rates."""
        # if sp_crits are zero, then this colapses to correct all the time.
        if self._n_sp == 1.0:
            S_to_the_n = self._slope
        else:
            S_to_the_n = np.power(self._slope, self._n_sp)
        omega_sed = self._K_sed * self._Q_to_the_m * S_to_the_n
        omega_br = self._K_br * self._Q_to_the_m * S_to_the_n

        omega_sed_over_sp_crit = np.divide(
            omega_sed,
            self._sp_crit_sed,
            out=np.zeros_like(omega_sed),
            where=self._sp_crit_sed != 0,
        )

        omega_br_over_sp_crit = np.divide(
            omega_br,
            self._sp_crit_br,
            out=np.zeros_like(omega_br),
            where=self._sp_crit_br != 0,
        )

        self._sed_erosion_term = omega_sed - self._sp_crit_sed * (
            1.0 - np.exp(-omega_sed_over_sp_crit)
        ) / (
            1 - self._phi
        )  # convert from a volume to a mass flux.

        self._br_erosion_term = omega_br - self._sp_crit_br * (
            1.0 - np.exp(-omega_br_over_sp_crit)
        )

        H_over_Hstar = self._soil__depth / self._H_star
        self._Es = self._sed_erosion_term * (1.0 - np.exp(-H_over_Hstar))

        self._Er = self._br_erosion_term * np.exp(-H_over_Hstar)

    @property
    def Es(self):
        """Sediment erosion term."""
        return self._Es

    @property
    def Er(self):
        """Bedrock erosion term."""
        return self._Er

    @property
    def H(self):
        """Sediment thickness."""
        return self._H

    def _calc_qs_in_and_depo_rate(self):
        # Choose a method for calculating erosion:
        self._calc_hydrology()
        self._calc_erosion_rates()

        is_flooded_core_node = self._get_flooded_core_nodes()

        self._Es[is_flooded_core_node] = 0.0
        self._Er[is_flooded_core_node] = 0.0

        self._sed_erosion_term[is_flooded_core_node] = 0.0
        self._br_erosion_term[is_flooded_core_node] = 0.0

        self.sediment_influx[:] = 0
        self._depo_rate[:] = 0.0

        # iterate top to bottom through the stack, calculate qs
        # cythonized version of calculating qs_in
        calculate_qs_in(
            np.flipud(self._stack),
            self._flow_receivers,
            self._cell_area_at_node,
            self._q,
            self._qs,
            self.sediment_influx,
            self._Es,
            self._Er,
            self._v_s,
            self._F_f,
        )

        self._depo_rate[self._q > 0] = self._qs[self._q > 0] * (
            self._v_s / self._q[self._q > 0]
        )

        if not self._depressions_are_handled():  # all sed dropped here
            self._depo_rate[is_flooded_core_node] = (
                self.sediment_influx[is_flooded_core_node]
                / self._cell_area_at_node[is_flooded_core_node]
            )
        return is_flooded_core_node

    def run_one_step_basic(self, dt=1.0):
        """Calculate change in rock and alluvium thickness for a time period
        'dt'.

        Parameters
        ----------
        dt : float
            Model timestep [T]
        """
        self._calc_qs_in_and_depo_rate()
        cores = self._grid.core_nodes

        H0 = self._soil__depth.copy()

        # now, the analytical solution to soil thickness in time:
        # need to distinguish D=kqS from all other cases to save from blowup!

        # distinguish cases:

        # there is a numerical blow up as well, which is not discussed in the
        # paper. when H>>H* the analytical solution (Eq 34) has a term that is
        # exp (H/H*) This can becom infinite. We will consider the case of
        # H/H*>50 as distinct.
        H_over_H_star = self._soil__depth / self._H_star
        too_thick = H_over_H_star > 100

        H_over_H_star[too_thick] = 100

        no_entrainment = self._sed_erosion_term <= 0.0  # this will include pits.
        blowup = (
            (self._depo_rate == self._sed_erosion_term)
            & (self._sed_erosion_term > 0.0)
            & (~no_entrainment)
        )
        full_equation = (~blowup) & (~no_entrainment)

        # First do blowup.
        # this is Space paper Eq 34. This example has the same issues with
        # very thick sed. If sed is very thick AND ero=depo, there is no change.
        self._soil__depth[blowup * (~too_thick)] = self._H_star * np.log(
            ((self._sed_erosion_term[blowup]) / self._H_star) * dt
            + np.exp(H_over_H_star[blowup])
        )

        # Second, do no entrainment of sediment. This is equation 35.
        self._soil__depth[no_entrainment] += (
            self._depo_rate[no_entrainment] / (1 - self._phi)
        ) * dt

        # Treat the case of very thick sediments and ero != depo.
        self._soil__depth[full_equation * too_thick] += (
            (
                self._depo_rate[full_equation * (full_equation * too_thick)]
                / (1 - self._phi)
            )
            - (self._sed_erosion_term[full_equation * too_thick] / (1 - self._phi))
        ) * dt

        # Finally do the full equation (Eq 32) when not too thick.
        self._soil__depth[full_equation * (~too_thick)] = self._H_star * np.log(
            (
                1
                / (
                    (self._depo_rate[full_equation * (~too_thick)] / (1 - self._phi))
                    / (
                        self._sed_erosion_term[full_equation * (~too_thick)]
                        / (1 - self._phi)
                    )
                    - 1
                )
            )
            * (
                np.exp(
                    (
                        self._depo_rate[full_equation * (~too_thick)] / (1 - self._phi)
                        - (
                            self._sed_erosion_term[full_equation * (~too_thick)]
                            / (1 - self._phi)
                        )
                    )
                    * (dt / self._H_star)
                )
                * (
                    (
                        (
                            self._depo_rate[full_equation * (~too_thick)]
                            / (1 - self._phi)
                            / (
                                self._sed_erosion_term[full_equation * (~too_thick)]
                                / (1 - self._phi)
                            )
                        )
                        - 1
                    )
                    * np.exp(H_over_H_star[full_equation * (~too_thick)])
                    + 1
                )
                - 1
            )
        )

        # Equation 12 gives dRdt, and Equation 36 gives R. However, we don't
        # include dH/dt within timestep in integrating for R.
        # This matters when we are starting with very little soil and increasing.

        # to do this right I think we need a numerical integral for
        # R(t). Unfortunately we have three cases for H(t). These are now
        # implemented in the function _dRdt. This loop and the solver can be
        # cythonized. But not today.

        dR = self._grid.zeros(at="node")
        for idx in self.grid.core_nodes:
            args = (
                self._br_erosion_term[idx],
                1.0 / self._H_star,
                self._depo_rate[idx] / (1.0 - self._phi),
                self._sed_erosion_term[idx] / (1.0 - self._phi),
                H0[idx],
            )

            dR[idx] = quad(_dRdt, 0, dt, args)[0]

        self._bedrock__elevation += dR

        # finally, determine topography by summing bedrock and soil
        self._topographic__elevation[cores] = (
            self._bedrock__elevation[cores] + self._soil__depth[cores]
        )

    def run_with_adaptive_time_step_solver(self, dt=1.0):
        """Run step with CHILD-like solver that adjusts time steps to prevent
        slope flattening.

        Parameters
        ----------
        dt : float
            Model timestep [T]

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> import numpy as np

        >>> rg = RasterModelGrid((3, 4))
        >>> z = rg.add_zeros("topographic__elevation", at="node")
        >>> z[:] = 0.1 * rg.x_of_node
        >>> H = rg.add_zeros("soil__depth", at="node")
        >>> H += 0.1
        >>> br = rg.add_zeros("bedrock__elevation", at="node")
        >>> br[:] = z - H

        >>> fa = FlowAccumulator(rg, flow_director="FlowDirectorSteepest")
        >>> fa.run_one_step()
        >>> sp = Space(
        ...     rg,
        ...     K_sed=1.0,
        ...     K_br=0.1,
        ...     F_f=0.5,
        ...     phi=0.0,
        ...     H_star=1.0,
        ...     v_s=1.0,
        ...     m_sp=0.5,
        ...     n_sp=1.0,
        ...     sp_crit_sed=0,
        ...     sp_crit_br=0,
        ...     solver="adaptive",
        ... )
        >>> sp.run_one_step(dt=10.0)

        >>> np.round(sp.Es[5:7], 4)
        array([0.0029, 0.0074])
        >>> np.round(sp.Er[5:7], 4)
        array([0.0032, 0.0085])
        >>> np.round(H[5:7], 3)
        array([0.088, 0.078])
        """

        # Initialize remaining_time, which records how much of the global time
        # step we have yet to use up.
        remaining_time = dt

        z = self._grid.at_node["topographic__elevation"]
        br = self._grid.at_node["bedrock__elevation"]
        H = self._grid.at_node["soil__depth"]
        r = self._flow_receivers
        cores = self._grid.core_nodes

        first_iteration = True

        is_flooded_core_node = self._get_flooded_core_nodes()

        # Outer WHILE loop: keep going until time is used up
        while remaining_time > 0.0:
            # Update all the flow-link slopes.
            #
            # For the first iteration, we assume this has already been done
            # outside the component (e.g., by flow router), but we need to do
            # it ourselves on subsequent iterations.
            if not first_iteration:
                # update the link slopes
                self._update_flow_link_slopes()
                # update where nodes are flooded. This shouuldn't happen because
                # of the dynamic timestepper, but just in case, we update here.
                is_flooded_core_node[self._slope < 0] = True
            else:
                first_iteration = False

            is_flooded_core_node = (
                self._calc_qs_in_and_depo_rate()
            )  # THIS IS THE SPEED BOTTLENECK

            # Now look at upstream-downstream node pairs, and recording the
            # time it would take for each pair to flatten. Take the minimum.
            self._dzdt[cores] = self._depo_rate[cores] * self._porosity_factor - (
                self._Es[cores] + self._Er[cores]
            )
            rocdif = self._dzdt - self._dzdt[r]
            zdif = z - z[r]
            self._time_to_flat[:] = remaining_time

            converging = np.where(rocdif < 0.0)[0]
            self._time_to_flat[converging] = -(
                TIME_STEP_FACTOR * zdif[converging] / rocdif[converging]
            )
            self._time_to_flat[np.where(zdif <= 0.0)[0]] = remaining_time

            # From this, find the maximum stable time step with regard to slope
            # evolution.
            dt_max1 = np.amin(self._time_to_flat)

            # Next we consider time to exhaust regolith
            self._time_to_zero_alluv[:] = remaining_time

            # poof deposition by phi
            dHdt = self._porosity_factor * (self._depo_rate - self._Es)
            decreasing_H = np.where(dHdt < 0.0)[0]
            self._time_to_zero_alluv[decreasing_H] = -(
                TIME_STEP_FACTOR * H[decreasing_H] / dHdt[decreasing_H]
            )

            # Now find the smallest time that would lead to near-empty alluv
            dt_max2 = np.amin(self._time_to_zero_alluv)

            # Take the smaller of the limits
            dt_max = max(self._dt_min, min(dt_max1, dt_max2))

            # Now a vector operation: apply dzdt and dhdt to all nodes
            br[cores] -= self._Er[cores] * dt_max
            H[cores] += dHdt[cores] * dt_max
            z[cores] = br[cores] + H[cores]

            # Update remaining time and continue
            remaining_time -= dt_max


def _dRdt(t, a, b, c, d, H0):
    """
    dRdt = a * exp(-b * H (t))

    a =  Kr q S*n
    b = 1/H*

    c = VQs/(Q * (1-phi))
    d = Ks q Sn

    if d <= 0:
        H (t) = (1/b) ln [ d*b*t + exp(H0/H*) ]

    """
    # truncate H/H*
    if b * H0 > 100:
        too_thick = True
    else:
        too_thick = False
    bH0 = min(b * H0, 100)

    # set cases for H depending on values of constants.

    # deposition only case.
    if d <= 0:
        H = H0 + (c * t)
    else:
        # sediment deposition = sediment entrainment.
        if c == d:
            if too_thick:
                H = H0
            else:
                H = (1 / b) * np.log((d * b * t) + np.exp(bH0))
        # full equation 32
        else:
            if too_thick:
                H = H0 + (c - d) * t
            else:
                term1 = 1.0 / ((c / d) - 1)
                term2 = np.exp((c - d) * t * b)
                term3 = (c / d - 1) * np.exp(bH0) + 1
                interior = term1 * (term2 * (term3) - 1)
                H = (1 / b) * np.log(interior)

    # calculate dRdt
    dRdt = -a * np.exp(-b * H)
    return dRdt



================================================
File: space/space_large_scale_eroder.py
================================================
"""Grid-based simulation of lateral erosion by channels in a drainage network.

Benjamin Campforts
"""

import numpy as np

from landlab import Component
from landlab import RasterModelGrid
from landlab.components.depression_finder.lake_mapper import _FLOODED
from landlab.components.space.ext.calc_sequential_ero_depo import _sequential_ero_depo
from landlab.grid.nodestatus import NodeStatus
from landlab.utils.return_array import return_array_at_node

ROOT2 = np.sqrt(2.0)
TIME_STEP_FACTOR = 0.5  # factor used in simple subdivision solver


class SpaceLargeScaleEroder(Component):
    """Stream Power with Alluvium Conservation and Entrainment large scale eroder.

    The :class:`~.SpaceLargeScaleEroder` is based on the SPACE component
    and is designed to be more robust against large time steps and coded in
    such a way that mass conservation is explicitly conserved during calculation.

    See the publication:

        Shobe, C. M., Tucker, G. E., and Barnhart, K. R.: The SPACE 1.0 model: a
        Landlab component for 2-D calculation of sediment transport, bedrock
        erosion, and landscape evolution, Geosci. Model Dev., 10, 4577-4604,
        `https://doi.org/10.5194/gmd-10-4577-2017
        <https://www.geosci-model-dev.net/10/4577/2017/>`_, 2017.

    Unlike other some other fluvial erosion componets in Landlab, in this
    component (and class:`~landlab.components.ErosionDeposition`) no
    erosion occurs in depressions or in areas with adverse slopes. There is no
    ability to pass a keyword argument ``erode_flooded_nodes``.

    If depressions are handled (as indicated by the presence of the field
    ``"flood_status_code"`` at nodes), then deposition occurs throughout the
    depression and sediment is passed out of the depression. Where pits are
    encountered, then all sediment is deposited at that node only.

    .. note::

        In the current version, we do not provide an adaptive time stepper.
        This will be addded in future versions of this component.

    For more explanation and examples, check out the correponding notebook of
    this component.

    Examples
    ---------
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import PriorityFloodFlowRouter
    >>> from landlab.components import SpaceLargeScaleEroder

    >>> mg = RasterModelGrid((5, 4), xy_spacing=100.0)

    >>> mg.at_node["soil__depth"] = [
    ...     [0.0, 0.0, 0.0, 0.0],
    ...     [0.0, 2.0, 2.0, 0.0],
    ...     [0.0, 2.0, 2.0, 0.0],
    ...     [0.0, 2.0, 2.0, 0.0],
    ...     [0.0, 0.0, 0.0, 0.0],
    ... ]
    >>> mg.at_node["bedrock__elevation"] = mg.y_of_node / 10.0 + mg.x_of_node / 10.0
    >>> mg.at_node["topographic__elevation"] = (
    ...     mg.at_node["bedrock__elevation"] + mg.at_node["soil__depth"]
    ... )

    >>> mg.set_closed_boundaries_at_grid_edges(
    ...     bottom_is_closed=True,
    ...     left_is_closed=True,
    ...     right_is_closed=True,
    ...     top_is_closed=True,
    ... )
    >>> mg.set_watershed_boundary_condition_outlet_id(
    ...     0, mg.at_node["topographic__elevation"], -9999.0
    ... )

    >>> fr = PriorityFloodFlowRouter(mg, flow_metric="D8", suppress_out=True)
    >>> sp = SpaceLargeScaleEroder(
    ...     mg,
    ...     K_sed=0.01,
    ...     K_br=0.001,
    ...     F_f=0.0,
    ...     phi=0.0,
    ...     H_star=1.0,
    ...     v_s=5.0,
    ...     m_sp=0.5,
    ...     n_sp=1.0,
    ...     sp_crit_sed=0,
    ...     sp_crit_br=0,
    ... )

    >>> node_next_to_outlet = mg.shape[1] + 1
    >>> sed_flux = []
    >>> for _ in range(500):
    ...     fr.run_one_step()
    ...     _ = sp.run_one_step(dt=10.0)
    ...     sed_flux.append(mg.at_node["sediment__flux"][node_next_to_outlet])
    ...

    Look at the results.

    >>> mg.at_node["sediment__flux"].reshape(mg.shape)  # doctest: +SKIP
    array([[0.        , 0.        , 0.        , 0.        ],
           [0.        , 0.26889843, 0.02719885, 0.        ],
           [0.        , 0.09130624, 0.13962599, 0.        ],
           [0.        , 0.06421368, 0.09527108, 0.        ],
           [0.        , 0.        , 0.        , 0.        ]])
    >>> sed_flux[::100]  # doctest: +SKIP
    [2053.1526698811363,
     601.0591808186842,
     405.95978528106235,
     272.8232194793999,
     176.63159183013272]

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Shobe, C., Tucker, G., Barnhart, K. (2017). The SPACE 1.0 model: a Landlab
    component for 2-D calculation of sediment transport, bedrock erosion, and
    landscape evolution. Geoscientific Model Development  10(12), 4577 - 4604.
    https://dx.doi.org/10.5194/gmd-10-4577-2017

    **Additional References**

    None Listed

    """

    _name = "SpaceLargeScaleEroder"

    _unit_agnostic = True

    _info = {
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": True,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__upstream_node_order": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array containing downstream-to-upstream ordered list of node IDs",
        },
        "sediment__influx": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m3/s",
            "mapping": "node",
            "doc": "Sediment flux (volume per unit time of sediment entering each node)",
        },
        "sediment__outflux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m3/s",
            "mapping": "node",
            "doc": "Sediment flux (volume per unit time of sediment leaving each node)",
        },
        "sediment__erosion_flux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/s",
            "mapping": "node",
            "doc": (
                "Sediment erosion flux from bed to water column (depth eroded per"
                " unit time)"
            ),
        },
        "sediment__deposition_flux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/s",
            "mapping": "node",
            "doc": (
                "Sediment deposition flux from water column to bed (depth deposited"
                " per unit time)"
            ),
        },
        "bedrock__erosion_flux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/s",
            "mapping": "node",
            "doc": (
                "Bedrock erosion flux from bedrock to water column (depth eroded per"
                " unit time)"
            ),
        },
        "soil__depth": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of soil or weathered bedrock",
        },
        "surface_water__discharge": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m**3/s",
            "mapping": "node",
            "doc": "Volumetric discharge of surface water",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__steepest_slope": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "-",
            "mapping": "node",
            "doc": "The steepest *downhill* slope",
        },
    }

    _cite_as = """
    @Article{gmd-10-4577-2017,
        AUTHOR = {Shobe, C. M. and Tucker, G. E. and Barnhart, K. R.},
        TITLE = {The SPACE~1.0 model: a~Landlab component for 2-D calculation
                 of sediment transport, bedrock erosion, and landscape evolution},
        JOURNAL = {Geoscientific Model Development},
        VOLUME = {10},
        YEAR = {2017},
        NUMBER = {12},
        PAGES = {4577--4604},
        URL = {https://www.geosci-model-dev.net/10/4577/2017/},
        DOI = {10.5194/gmd-10-4577-2017}
    }"""

    def __init__(
        self,
        grid,
        K_sed=0.02,
        K_br=0.02,
        F_f=0.0,
        phi=0.3,
        H_star=0.1,
        v_s=1.0,
        v_s_lake=None,
        m_sp=0.5,
        n_sp=1.0,
        sp_crit_sed=0.0,
        sp_crit_br=0.0,
        discharge_field="surface_water__discharge",
        erode_flooded_nodes=False,
        thickness_lim=100.0,
    ):
        """Initialize the SpaceLargeScaleEroder model.

        Parameters
        ----------
        grid : ModelGrid
            Landlab ModelGrid object
        K_sed : float, array of float, or str, optional
            Erodibility for sediment (units vary) as either a number or a field name.
        K_br : float, array of float, or str, optional
            Erodibility for bedrock (units vary) as either a number or a field name.
        F_f : float, optional
            Fraction of permanently suspendable fines in bedrock [-].
        phi : float, optional
            Sediment porosity [-].
        H_star : float, optional
            Sediment thickness required for full entrainment [L].
        v_s : float, optional
            Effective settling velocity for chosen grain size metric [L/T].
        v_s_lake : float, optional
            Effective settling velocity in lakes for chosen grain size metric [L/T].
        m_sp : float, optional
            Drainage area exponent (units vary).
        n_sp : float, optional
            Slope exponent (units vary).
        sp_crit_sed : float, array of float, or str, optional
            Critical stream power to erode sediment [E/(TL^2)].
        sp_crit_br : float, array of float, or str, optional
            Critical stream power to erode rock [E/(TL^2)]
        discharge_field : float, array of float, or str, optional
            Discharge [L^2/T]. The default is to use the grid field
            'surface_water__discharge', which is simply drainage area
            multiplied by the default rainfall rate (1 m/yr). To use custom
            spatially/temporally varying rainfall, use 'water__unit_flux_in'
            to specify water input to the FlowAccumulator.
        erode_flooded_nodes : bool, optional
            Whether erosion occurs in flooded nodes identified by a
            depression/lake mapper (e.g., DepressionFinderAndRouter). When set
            to false, the field *flood_status_code* must be present on the grid
            (this is created by the DepressionFinderAndRouter). Default True.
        """
        if grid.at_node["flow__receiver_node"].size != grid.size("node"):
            raise NotImplementedError(
                "A route-to-multiple flow director has been "
                "run on this grid. The landlab development team has not "
                "verified that SpaceLargeScaleEroder is compatible with "
                "route-to-multiple methods. Please open a GitHub Issue "
                "to start this process."
            )

        super().__init__(grid)

        self._soil__depth = grid.at_node["soil__depth"]
        self._topographic__elevation = grid.at_node["topographic__elevation"]

        if "bedrock__elevation" in grid.at_node:
            self._bedrock__elevation = grid.at_node["bedrock__elevation"]
        else:
            self._bedrock__elevation = grid.add_zeros(
                "bedrock__elevation", at="node", dtype=float
            )

            self._bedrock__elevation[:] = (
                self._topographic__elevation - self._soil__depth
            )

        # Check consistency of bedrock, soil and topogarphic elevation fields
        np.testing.assert_almost_equal(
            grid.at_node["bedrock__elevation"] + grid.at_node["soil__depth"],
            grid.at_node["topographic__elevation"],
            decimal=5,
            err_msg=(
                "The sum of bedrock elevation and topographic elevation should "
                "be equal"
            ),
        )

        # specific inits
        self._thickness_lim = float(thickness_lim)
        self._H_star = H_star

        self._sed_erosion_term = np.zeros(grid.number_of_nodes)
        self._br_erosion_term = np.zeros(grid.number_of_nodes)
        self._Es = np.zeros(grid.number_of_nodes)
        self._Er = np.zeros(grid.number_of_nodes)

        # K's and critical values can be floats, grid fields, or arrays
        # use setters defined below
        self._K_sed = K_sed
        self._K_br = K_br

        self._sp_crit_sed = return_array_at_node(grid, sp_crit_sed)
        self._sp_crit_br = return_array_at_node(grid, sp_crit_br)

        self._erode_flooded_nodes = erode_flooded_nodes

        self._flow_receivers = grid.at_node["flow__receiver_node"]
        self._stack = grid.at_node["flow__upstream_node_order"]
        self._slope = grid.at_node["topographic__steepest_slope"]

        self.initialize_output_fields()

        self._qs = grid.at_node["sediment__outflux"]
        self._q = return_array_at_node(grid, discharge_field)

        # for backward compatibility (remove in 3.0.0+)
        grid.at_node["sediment__flux"] = grid.at_node["sediment__outflux"]

        self._Q_to_the_m = np.zeros(grid.number_of_nodes)
        self._S_to_the_n = np.zeros(grid.number_of_nodes)

        # store other constants
        self._m_sp = np.float64(m_sp)
        self._n_sp = np.float64(n_sp)
        self._phi = np.float64(phi)
        self._v_s = np.float64(v_s)

        if isinstance(grid, RasterModelGrid):
            self._link_lengths = grid.length_of_d8
        else:
            self._link_lengths = grid.length_of_link

        if v_s_lake is None:
            self._v_s_lake = np.float64(v_s)
        else:
            self._v_s_lake = np.float64(v_s_lake)
        self._F_f = np.float64(F_f)

        if phi >= 1.0:
            raise ValueError("Porosity must be < 1.0")

        if F_f > 1.0:
            raise ValueError("Fraction of fines must be <= 1.0")

        if phi < 0.0:
            raise ValueError("Porosity must be > 0.0")

        if F_f < 0.0:
            raise ValueError("Fraction of fines must be > 0.0")

    @property
    def K_br(self):
        """Erodibility of bedrock(units depend on m_sp)."""
        return self._K_br

    @K_br.setter
    def K_br(self, new_val):
        self._K_br = return_array_at_node(self._grid, new_val)

    @property
    def K_sed(self):
        """Erodibility of sediment(units depend on m_sp)."""
        return self._K_sed

    @K_sed.setter
    def K_sed(self, new_val):
        self._K_sed = return_array_at_node(self._grid, new_val)

    @property
    def fraction_fines(self):
        """Fraction of permanently suspendable fines in bedrock [-]."""
        return self._F_f

    @property
    def sediment_porosity(self):
        """Sediment porosity [-]."""
        return self._phi

    @property
    def settling_velocity(self):
        """Effective settling velocity for chosen grain size metric [L/T]."""
        return self._v_s

    @property
    def drainage_area_exp(self):
        """Drainage area exponent (units vary)."""
        return self._m_sp

    @property
    def slope_exp(self):
        """Slope exponent (units vary)."""
        return self._n_sp

    @property
    def Es(self):
        """Sediment erosion term."""
        return self._Es

    @property
    def Er(self):
        """Bedrock erosion term."""
        return self._Er

    @property
    def sediment_influx(self):
        """Volumetric sediment influx to each node."""
        return self.grid.at_node["sediment__influx"]

    def _calc_erosion_rates(self):
        """Calculate erosion rates."""

        H = self.grid.at_node["soil__depth"]

        # if sp_crits are zero, then this colapses to correct all the time.
        if np.isclose(self._n_sp, 1.0):
            S_to_the_n = self._slope
        else:
            S_to_the_n = np.power(self._slope, self._n_sp)
        omega_sed = self._K_sed * self._Q_to_the_m * S_to_the_n
        omega_br = self._K_br * self._Q_to_the_m * S_to_the_n

        omega_sed_over_sp_crit = np.divide(
            omega_sed,
            self._sp_crit_sed,
            out=np.zeros_like(omega_sed),
            where=self._sp_crit_sed != 0,
        )

        omega_br_over_sp_crit = np.divide(
            omega_br,
            self._sp_crit_br,
            out=np.zeros_like(omega_br),
            where=self._sp_crit_br != 0,
        )

        self._sed_erosion_term = omega_sed - self._sp_crit_sed * (
            1.0 - np.exp(-omega_sed_over_sp_crit)
        ) / (
            1 - self._phi
        )  # convert from a volume to a mass flux.
        self._br_erosion_term = omega_br - self._sp_crit_br * (
            1.0 - np.exp(-omega_br_over_sp_crit)
        )

        self._Es = self._sed_erosion_term * (1.0 - np.exp(-H / self._H_star))
        self._Er = self._br_erosion_term * np.exp(-H / self._H_star)

        # if the soil layer becomes exceptionally thick (e.g. because of
        # landslide derived sediment deposition(,) the algorithm will become
        # unstable because np.exp(x) with x > 709 yeilds inf values.
        # Therefore soil depth is temporqlly topped of at 200m and the remaining
        # values are added back after the space component has run

        self._Es[H > self._thickness_lim] = self._sed_erosion_term[
            H > self._thickness_lim
        ]
        self._Er[H > self._thickness_lim] = 0

    def run_one_step_basic(self, dt=10):
        node_status = self.grid.status_at_node

        z = self.grid.at_node["topographic__elevation"]
        br = self.grid.at_node["bedrock__elevation"]
        H = self.grid.at_node["soil__depth"]
        link_to_rcvr = self.grid.at_node["flow__link_to_receiver_node"]
        area = self.grid.cell_area_at_node

        r = self.grid.at_node["flow__receiver_node"]
        stack_flip_ud = np.flipud(self.grid.at_node["flow__upstream_node_order"])
        # Select core nodes where qs >0
        stack_flip_ud_sel = stack_flip_ud[
            (node_status[stack_flip_ud] == NodeStatus.CORE)
            & (self._q[stack_flip_ud] > 0.0)
        ]
        slope = (z - z[r]) / self._link_lengths[link_to_rcvr]

        # Choose a method for calculating erosion:
        self._Q_to_the_m[:] = np.power(self._q, self._m_sp)
        self._calc_erosion_rates()

        if "flood_status_code" in self.grid.at_node:
            flood_status = self.grid.at_node["flood_status_code"]
            flooded_nodes = np.nonzero(flood_status == _FLOODED)[0]
        else:
            flooded_nodes = np.nonzero([slope < 0])[1]

        self._Es[flooded_nodes] = 0.0
        self._Er[flooded_nodes] = 0.0
        self._sed_erosion_term[flooded_nodes] = 0.0
        self._br_erosion_term[flooded_nodes] = 0.0

        self.sediment_influx[:] = 0

        K_sed_vector = np.broadcast_to(self._K_sed, self._q.shape)

        ero_sed_effective = np.zeros_like(K_sed_vector)
        depo_effective = np.zeros_like(K_sed_vector)

        vol_SSY_riv = _sequential_ero_depo(
            stack_flip_ud_sel,
            r,
            area,
            self._q,
            self._qs,
            self.sediment_influx,
            self._Es,
            self._Er,
            self._Q_to_the_m,
            slope,
            H,
            br,
            self._sed_erosion_term,
            self._br_erosion_term,
            K_sed_vector,
            ero_sed_effective,
            depo_effective,
            self._v_s,
            self._phi,
            self._F_f,
            self._H_star,
            dt,
            self._thickness_lim,
        )

        V_leaving_riv = np.sum(self.sediment_influx[self.grid.boundary_nodes]) * dt
        # Update topography
        cores = self._grid.core_nodes
        z[cores] = br[cores] + H[cores]

        self.grid.at_node["sediment__erosion_flux"][:] = ero_sed_effective
        self.grid.at_node["sediment__deposition_flux"][:] = depo_effective
        self.grid.at_node["bedrock__erosion_flux"][:] = self._Er

        return vol_SSY_riv, V_leaving_riv

    def run_one_step(self, dt):
        """
        Returns:
        - vol_SSY_riv (float): Suspended sediment yield leaving the domain as wash load
        - V_leaving_riv (float): Volume of bedload sediment leaving the domain.
        """

        (vol_SSY_riv, V_leaving_riv) = self.run_one_step_basic(dt)
        return vol_SSY_riv, V_leaving_riv



================================================
File: space/ext/calc_qs.pyx
================================================
cimport cython

# https://cython.readthedocs.io/en/stable/src/userguide/fusedtypes.html
ctypedef fused id_t:
    cython.integral
    long long


cdef extern from "math.h":
    double exp(double x) nogil


def calculate_qs_in(
    const id_t [:] stack_flip_ud,
    const id_t [:] flow_receivers,
    const cython.floating [:] cell_area_at_node,
    cython.floating [:] q,
    cython.floating [:] qs,
    cython.floating [:] qs_in,
    cython.floating [:] Es,
    cython.floating [:] Er,
    double v_s,
    double F_f,
):
    """Calculate and qs and qs_in."""
    # define internal variables
    cdef unsigned int n_nodes = stack_flip_ud.shape[0]
    cdef unsigned int node_id
    cdef unsigned int i

    # iterate top to bottom through the stack, calculate qs and adjust  qs_in
    for i in range(n_nodes):

        # choose the node id
        node_id = stack_flip_ud[i]

        # If q at current node is greather than zero, calculate qs based on a
        # local analytical solution. This local analytical solution depends on
        # qs_in, the sediment flux coming into the node from upstream (hence
        # the upstream to downstream node ordering).

        # Because calculation of qs requires qs_in, this operation must be done
        # in an upstream to downstream loop, and cannot be vectorized.

        # there is water flux (q) and this node is not a pit then calculate qs.
        if q[node_id] > 0 and (flow_receivers[node_id] != node_id):
            qs[node_id] = (
                qs_in[node_id]
                + ((Es[node_id]) + ((1.0 - F_f) * (Er[node_id])))
                * cell_area_at_node[node_id]
            ) / (1.0 + (v_s * cell_area_at_node[node_id] / (q[node_id])))

            # finally, add this nodes qs to recieiving nodes qs_in.
            # if qs[node_id] == 0, then there is no need for this line to be
            # evaluated.
            qs_in[flow_receivers[node_id]] += qs[node_id]

        else:
            # if q at the current node is zero, set qs at that node is zero.
            qs[node_id] = 0



================================================
File: space/ext/calc_sequential_ero_depo.pyx
================================================
cimport cython
from libc.math cimport exp
from libc.math cimport isinf
from libc.math cimport log

# https://cython.readthedocs.io/en/stable/src/userguide/fusedtypes.html
ctypedef fused id_t:
    cython.integral
    long long


def _sequential_ero_depo(
    const id_t [:] stack_flip_ud_sel,
    const id_t [:] flow_receivers,
    const cython.floating [:] cell_area,
    const cython.floating [:] q,
    cython.floating [:] qs,
    cython.floating [:] qs_in,
    const cython.floating [:] Es,
    const cython.floating [:] Er,
    const cython.floating [:] Q_to_the_m,
    const cython.floating [:] slope,
    cython.floating [:] H,
    cython.floating [:] br,
    const cython.floating [:] sed_erosion_term,
    const cython.floating [:] bed_erosion_term,
    const cython.floating [:] K_sed,
    cython.floating [:] ero_sed_effective,
    cython.floating [:] depo_effective,
    const double v,
    const double phi,
    const double F_f,
    const double H_star,
    const double dt,
    const double thickness_lim,
):
    """Calculate and qs and qs_in."""
    # define internal variables
    cdef unsigned int node_id
    cdef double H_Before
    cdef double vol_SSY_riv
    vol_SSY_riv =0.0

    for node_id in stack_flip_ud_sel:
        qs_out = (
            qs_in[node_id]
            + Es[node_id] * cell_area[node_id]
            + (1.0 - F_f) * Er[node_id] * cell_area[node_id]
        ) / (1.0 + (v * cell_area[node_id] / q[node_id]))

        depo_rate = v*qs_out/q[node_id]
        H_loc = H[node_id]
        H_Before = H[node_id]
        slope_loc = slope[node_id]
        sed_erosion_loc = sed_erosion_term[node_id]
        bed_erosion_loc = bed_erosion_term[node_id]

        # Correct for thick soils where soil thickness can grow to inf
        if (H_loc > thickness_lim or slope_loc <= 0 or   sed_erosion_loc==0):
            H_loc += (depo_rate / (1 - phi) - sed_erosion_loc/ (1 - phi)) * dt
        else:
            # Blowup
            if (depo_rate == (K_sed[node_id] * Q_to_the_m[node_id] * slope_loc)) :
                H_loc = H_loc * log(
                    ((sed_erosion_loc / (1 - phi)) / H_star) * dt + exp(H_loc / H_star)
                )
            # No blowup
            else:
                H_loc = H_star * log(
                    (1 / ((depo_rate / (1 - phi)) / (sed_erosion_loc / (1 - phi)) - 1))
                    * (
                        exp(
                            (depo_rate / (1 - phi) - (sed_erosion_loc / (1 - phi)))
                            * (dt / H_star)
                        )
                        * (
                            (
                                (depo_rate / (1 - phi) / (sed_erosion_loc / (1 - phi)))
                                - 1
                            )
                            * exp(H_loc / H_star)
                            + 1
                        )
                        - 1
                    )
                )
            # In case soil depth evolves to infinity, fall back to no entrainment
            if isinf(H_loc):
                H_loc = (
                    H[node_id]
                    + (depo_rate / (1 - phi) - sed_erosion_loc/ (1 - phi)) * dt
                )

        H_loc = max(0, H_loc)
        ero_bed = bed_erosion_loc* (exp(-H_loc / H_star))

        # should always be bigger than 0
        qs_out_adj = (
            qs_in[node_id]
            - ((H_loc - H_Before) * (1 - phi) * cell_area[node_id] / dt)
            + (1.0 - F_f) * ero_bed * cell_area[node_id]
        )

        qs[node_id] = qs_out_adj
        qs_in[flow_receivers[node_id]] += qs[node_id]

        H[node_id] = H_loc
        br[node_id] += -dt * ero_bed
        vol_SSY_riv += F_f*ero_bed* cell_area[node_id]

        # Update deposition rate based on adjusted fluxes
        Hd = H_loc - H_Before
        depo_effective[node_id] = (v*qs_out_adj/q[node_id])/(1 - phi)
        # Deposition should be larger or equal to increase in soil depth
        depo_effective[node_id] = max(depo_effective[node_id], Hd/dt)
        ero_sed_effective[node_id] = depo_effective[node_id] - Hd/dt
    return vol_SSY_riv



================================================
File: spatial_precip/__init__.py
================================================
from .generate_spatial_precip import SpatialPrecipitationDistribution

__all__ = ["SpatialPrecipitationDistribution"]



================================================
File: spatial_precip/generate_spatial_precip.py
================================================
import contextlib

import numpy as np
from scipy.stats import fisk
from scipy.stats import genextreme

from landlab import Component
from landlab import RasterModelGrid


class SpatialPrecipitationDistribution(Component):
    """Generate spatially resolved precipitation events.

    A component to generate a sequence of spatially resolved storms over a
    grid, following a lightly modified version (see below) of the
    stochastic methods of Singer & Michaelides, Env Res Lett 12, 104011,
    2017, & Singer et al., Geosci. Model Dev., accepted, 10.5194/gmd-2018-86.

    The method is heavily stochastic, and at the present time is intimately
    calibrated against the conditions at Walnut Gulch, described in those
    papers. In particular, assumptions around intensity-duration
    calibration and orographic rainfall are "burned in" for now, and are
    not accessible to the user. The various probability distributions
    supplied to the various run methods default to WG values, but are
    easily modified.  This calibration reflects a US desert southwest
    "monsoonal" climate, and the component distinguishes (optionally)
    between two seasons, "monsoonal" and "winter". The intensity-duration
    relationship is shared between the seasons, and so may prove useful in
    a variety of storm-dominated contexts.

    The default is to disable the orographic rainfall functionality of the
    component. However, if orographic_scenario == 'Singer', the component
    requires a 'topographic__elevation' field to already exist on the grid
    at the time of instantiation.

    The component has two ways of simulating a "year". This choice is
    controlled by the 'limit' parameter of the yield methods. If limit==
    'total_rainfall', the component will continue to run until the total
    rainfall for the season and/or year exceeds a stochastically generated
    value. This method is directly comparable to the Singer & Michaelides
    method, but will almost always result in years which are not one
    calendar year long, unless the input distributions are very carefully
    recalibrated for each use case. If limit=='total_time', the component
    will terminate a season and/or year once the elapsed time exceeds one
    year. In this case, the total rainfall will not correspond to the
    stochastically generated total. You can access the actual total for the
    last season using the property `(median_)total_rainfall_last_season`.

    Note that this component cannot simulate the occurrence of more than one
    storm at the same time. Storms that should be synchronous will instead
    occur sequentially, with no interstorm time. This limitation means that
    if enough storms occur in a year that numstorms*mean_storm_duration
    exceeds one year, the number of simulated storms will saturate. This
    limitation may be relaxed in the future.

    The component offers the option to modify the maximum number of storms
    simulated per year. If you find simulations encountering this limit too
    often, you may need to raise this limit. Conversely, it could be lowered
    to reduce memory usage over small grids. However, in increasing the value,
    beware - the component maintains two limit*nnodes arrays, which will chew
    through memory if the limit gets too high. The default will happily
    simulate grids up to around 50 km * 50 km using the default probability
    distributions.

    Key methods are:

    yield_storms
        Generate a timeseries of storm:interstorm duration pairs, alongside
        a field that describes the spatial distribution of rain during that
        storm.
    yield_years
        Generate a timeseries of ints giving number of storms per year,
        alongside a field that describes the spatial distribution of total
        rainfall across that year.
    yield_seasons
        Generate a timeseries of ints giving number of storms per season,
        alongside a field that describes the spatial distribution of total
        rainfall across that season.
    calc_annual_rainfall
        Produce a timeseries of tuples giving total rainfall each season,
        without resolving the storms spatially (i.e., fast!).

    A large number of properties are available to access storm properties
    during generation:

        - current_year
        - current_season
        - storm_depth_last_storm
        - storm_recession_value_last_storm
        - storm_duration_last_storm
        - storm_area_last_storm
        - storm_intensity_last_storm
        - total_rainfall_this_season
        - total_rainfall_this_year
        - total_rainfall_last_season
        - total_rainfall_last_year
        - median_total_rainfall_this_season
        - median_total_rainfall_this_year
        - median_total_rainfall_last_season
        - median_total_rainfall_last_year
        - number_of_nodes_under_storm
        - nodes_under_storm
        - target_median_total_rainfall_this_season

    Note that becuase these are medians not means,
    median_total_rainfall_last_season + median_total_rainfall_this_season
    != median_total_rainfall_this_year.

    Significant differences between this component and the Singer code are:

        - The component does not model evapotranspiration. Use a separate
            Landlab component for this.
        - The component runs only over a LL grid; there is no such thing as a
            validation or simulation run.
        - It produces "fuzz" around intensity values using a continuous
            distribution; Singer does this with integer steps.
        - Step changes mid-run cannot be explicitly modelled. Instead, run the
            component for a fixed duration, make the change to the
            distribution input parameter, then run it again.
        - Storms can be centred at any spatial coordinate, not just over nodes.
        - Edge buffering is now dynamic; i.e., big storms have a bigger edge
            buffer than smaller storms. Storms can be centered off the grid
            edges.
        - Storms are never discarded - once a storm is drawn, it must hit the
            catchment, and positions are repeatedly selected until this can
            happen. Singer's method would discard such a storm and draw a new
            one.
        - Durations are not rescaled to ensure both total duration and total
            precip are both satisfied at the same time, as in Singer's method.
            Instead, the component either matches a year's duration, *or*
            exactly a year's worth of rain. This choice is dictated by the
            `limit` parameter in the yield methods.

    Examples
    --------

    >>> import numpy as np
    >>> from landlab import RasterModelGrid, VoronoiDelaunayGrid
    >>> mg = RasterModelGrid((10, 10), xy_spacing=1000.0)
    >>> rain = SpatialPrecipitationDistribution(mg)

    Calling yield_storms will produce storm-interstorm duration (hr) pairs
    until the model runtime has elapsed.

    >>> np.random.seed(1)
    >>> total_t_each_step = [
    ...     (storm + interstorm) for (storm, interstorm) in rain.yield_storms()
    ... ]
    >>> len(total_t_each_step)
    41
    >>> np.isclose(sum(total_t_each_step) / 24.0, 365.0)
    True

    The actual rainfall intensities during that interval are accessible in the
    'rainfall__flux' field (mm/hr). The storm centre does not have to be over
    the grid, but in this case, it was for the last simulated storm:

    >>> mg.at_node["rainfall__flux"].argmax()
    80

    We can also run the component for only one season (i.e., only using one
    of the pdf sets describing the storm properties):

    >>> for field in ("rainfall__flux", "rainfall__total_depth_per_year"):
    ...     _ = mg.at_node.pop(field)  # clear out the existing fields
    ...
    >>> rain = SpatialPrecipitationDistribution(mg, number_of_years=2)
    >>> np.random.seed(5)
    >>> total_t_each_step = [
    ...     (storm + interstorm)
    ...     for (storm, interstorm) in rain.yield_storms(
    ...         style="monsoonal", monsoon_fraction_of_year=0.35
    ...     )
    ... ]
    >>> np.isclose(sum(total_t_each_step) / 24.0 / 365.0 / 2.0, 0.35)
    True

    Note this behaviour can be stopped by upping monsoon_fraction_of_year:

    >>> np.random.seed(5)
    >>> total_t_each_step = [
    ...     (storm + interstorm)
    ...     for (storm, interstorm) in rain.yield_storms(
    ...         style="monsoonal", monsoon_fraction_of_year=1.0
    ...     )
    ... ]
    >>> np.isclose(round(sum(total_t_each_step) / 24.0 / 365.0 / 2.0, 2), 1.0)
    True

    yield_years yields the number of storms in the last whole year.
    Use 'rainfall__total_depth_per_year' to access the rainfall map for the
    last fully elapsed year, or equivalently, the total_rainfall_last_year
    property. Note the component seamlessly handles non-raster grid types:

    >>> vdg = VoronoiDelaunayGrid(
    ...     np.random.rand(100) * 1000.0, np.random.rand(100) * 1000.0
    ... )
    >>> np.random.seed(3)
    >>> rain = SpatialPrecipitationDistribution(vdg, number_of_years=3)
    >>> storms_each_year = []
    >>> for total_storms in rain.yield_years(
    ...     style="monsoonal", total_rf_trend=0.05, storminess_trend=-0.02
    ... ):
    ...     storms_each_year.append(total_storms)
    ...     assert np.all(
    ...         np.equal(
    ...             vdg.at_node["rainfall__total_depth_per_year"],
    ...             rain.total_rainfall_last_year,
    ...         )
    ...     )
    >>> sum(storms_each_year)
    11

    yield_seasons yields rainfall statistics for individual seasons. Access
    these using the various provided component properties. Note that we can
    get the component to yield a total rainfall that is calibrated to the
    supplied total_rf_gaussians if we set limit to 'total__rainfall' rather
    than 'total_time' (at the cost of exactly matching the season length):

    >>> for field in ("rainfall__flux", "rainfall__total_depth_per_year"):
    ...     _ = mg.at_node.pop(field)  # clear out the existing fields
    ...
    >>> rain = SpatialPrecipitationDistribution(mg, number_of_years=2)
    >>> np.random.seed(5)
    >>> season_list = []
    >>> theoretical_median_rf_season = []
    >>> median_rf_season = []
    >>> median_rf_last_year = []
    >>> mean_rf_season = []
    >>> mean_rf_last_year = []
    >>> for storm_number in rain.yield_seasons(limit="total_rainfall"):
    ...     season_list.append(rain.current_season)
    ...     theoretical_median_rf_season.append(
    ...         rain.target_median_total_rainfall_this_season
    ...     )
    ...     median_rf_season.append(rain.median_total_rainfall_this_season)
    ...     median_rf_last_year.append(rain.median_total_rainfall_last_year)
    ...     mean_rf_season.append(rain.total_rainfall_this_season.mean())
    ...     mean_rf_last_year.append(rain.total_rainfall_last_year.mean())
    ...
    >>> season_list == ["M", "W", "M", "W"]
    True
    >>> [
    ...     meas > sim
    ...     for (meas, sim) in zip(median_rf_season, theoretical_median_rf_season)
    ... ]  # must exceed
    [True, True, True, True]
    >>> np.isclose(median_rf_last_year[0], 0.0)
    True
    >>> for season in (0, 2):  # this property must be the same in both seasons
    ...     np.isclose(median_rf_last_year[season], median_rf_last_year[season + 1])
    ...
    True
    True

    Note that because we work here with medians, the seasonal medians don't sum
    to the year median, but the means do:

    >>> np.isclose(median_rf_last_year[2], median_rf_season[0] + median_rf_season[1])
    False
    >>> np.isclose(mean_rf_last_year[2], mean_rf_season[0] + mean_rf_season[1])
    True

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Singer, M., Michaelides, K., Hobley, D. (2018). STORM 1.0: a simple,
    flexible, and parsimonious stochastic rainfall generator for simulating
    climate and climate change. Geoscientific Model Development  11(9),
    3713-3726. https://dx.doi.org/10.5194/gmd-11-3713-2018

    **Additional References**

    None Listed

    """

    _name = "SpatialPrecipitationDistribution"

    _unit_agnostic = False

    _cite_as = """@Article{gmd-2018-86,
        title={STORM: A simple, flexible, and parsimonious stochastic rainfall
               generator for simulating climate and climate change},
        author={Singer, M. B. and Michaelides, K. and Hobley, D. E. J.},
        journal={Geoscientific Model Development Discussions},
        volume={2018},
        pages={1--25},
        year={2018},
        url={https://www.geosci-model-dev-discuss.net/gmd-2018-86/},
        doi={10.5194/gmd-2018-86}
        }"""

    _info = {
        "rainfall__flux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "mm/hr",
            "mapping": "node",
            "doc": "Depth of water delivered per unit time in each storm",
        },
        "rainfall__total_depth_per_year": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "mm/yr",
            "mapping": "node",
            "doc": "Depth of water delivered in total in each model year",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
    }

    def __init__(
        self, grid, number_of_years=1, orographic_scenario=None, max_numstorms=5000
    ):
        """Create the SpatialPrecipitationDistribution generator component.

        Parameters
        ----------
        grid : ModelGrid
            A Landlab model grid of any type.
        number_of_years : int
            The number of years over which to generate storms.
        orographic_scenario : {None, 'Singer', func}
            Whether to use no orographic rule, or to adopt S&M's 2017
            calibration for Walnut Gulch. Alternatively, provide a function
            here that turns the provided elevation of the storm center into
            a length-11 curve weighting to select which orographic scenario
            to apply.
        """
        super().__init__(grid)

        gaugecount = (grid.status_at_node != grid.BC_NODE_IS_CLOSED).sum()
        self._gauge_dist_km = np.zeros(gaugecount, dtype="float")
        self._temp_dataslots1 = np.zeros(gaugecount, dtype="float")
        self._temp_dataslots2 = np.zeros(gaugecount, dtype="float")
        self._numyrs = number_of_years

        self._max_numstorms = max_numstorms
        # This is for initializing matrices. Trailing zeros are deleted from
        # matrixes at the end of the code.

        assert orographic_scenario in (None, "Singer")
        self._orographic_scenario = orographic_scenario

        # build LL fields:
        self.initialize_output_fields()
        # bind the field to the internal variable:
        self._rain_int_gauge = self._grid.at_node["rainfall__flux"]
        self._total_rf_year = self._grid.at_node["rainfall__total_depth_per_year"]

        # store some info on the open node grid extent:
        open_nodes = self._grid.status_at_node != self._grid.BC_NODE_IS_CLOSED
        self._minx = self._grid.node_x[open_nodes].min()
        self._maxx = self._grid.node_x[open_nodes].max()
        self._miny = self._grid.node_y[open_nodes].min()
        self._maxy = self._grid.node_y[open_nodes].max()
        self._widthx = self._maxx - self._minx
        self._widthy = self._maxy - self._miny
        self._running_total_rainfall_this_year = self._grid.zeros(at="node")
        self._running_total_rainfall_this_season = self._grid.zeros(at="node")

        self._open_area = self._grid.cell_area_at_node[open_nodes].sum()
        self._scaling_to_WG = self._open_area / 275710702.0
        # ^ this is the relative size of the catchment compared to WG

    def yield_storms(
        self,
        limit="total_time",
        style="whole_year",
        total_rf_trend=0.0,
        storminess_trend=0.0,
        monsoon_fraction_of_year=0.42,
        monsoon_total_rf_gaussian=(("sigma", 64.0), ("mu", 207.0)),
        monsoon_storm_duration_GEV=(
            ("shape", -0.570252),
            ("sigma", 35.7389),
            ("mu", 34.1409),
            ("trunc_interval", (0.0, 1040.0)),
        ),
        monsoon_storm_area_GEV=(
            ("shape", 0.0),
            ("sigma", 2.83876e07),
            ("mu", 1.22419e08),
            ("trunc_interval", (5.0e06, 3.0e08)),
        ),
        monsoon_storm_interarrival_GEV=(
            ("shape", -0.807971),
            ("sigma", 9.4957),
            ("mu", 10.6108),
            ("trunc_interval", (0.0, 720.0)),
        ),
        monsoon_storm_radial_weakening_gaussian=(("sigma", 0.08), ("mu", 0.25)),
        winter_total_rf_gaussian=(("sigma", 52.0), ("mu", 1.65)),
        winter_storm_duration_fisk=(
            ("c", 1.0821),
            ("scale", 68.4703),
            ("trunc_interval", (0.0, 5000.0)),
        ),
        winter_storm_area_GEV=(
            ("shape", 0.0),
            ("sigma", 2.83876e07),
            ("mu", 1.22419e08),
            ("trunc_interval", (5.0e06, 3.0e08)),
        ),
        winter_storm_interarrival_GEV=(
            ("shape", 1.1131),
            ("sigma", 53.2671),
            ("mu", 47.4944),
            ("trunc_interval", (0.0, 720.0)),
        ),
        winter_storm_radial_weakening_gaussian=(
            ("sigma", 0.08),
            ("mu", 0.25),
            ("trunc_interval", (0.15, 0.67)),
        ),
    ):
        """Yield a timeseries giving the number of storms occurring each year
        in a rainfall simulation.

        All default distributions specified as parameters reflect values for
        Walnut Gulch, see Singer & Michaelides, 2017 & Singer et al, submitted.

        Parameters
        ----------
        limit : str
            Controls whether a season is defined based on its total rainfall
            (and can be any length), or by its duration (and can have any
            amount of rainfall). One of 'total_time' or 'total_rainfall'.
            If 'total_time', monsoon_fraction_of_year
            sets the fraction of a year occupied by the monsoon.
        style : str
            Controls whether the component seeks to simulate a western US-
            style "monsoonal" climate, a western US-style winter climate,
            or a full year combining both. One of 'whole_year', 'monsoonal',
            or 'winter' These distributions are by default
            based on Singer et al.'s calibrations. Note if 'monsoonal',
            the total duration of a "year" will appear to be only
            `monsoon_fraction_of_year`, and the opposite for `winter`.
        total_rf_trend : float
            Controls if a drift is applied to the total rainfall distribution
            through time. If 0., no trend. If positive, rainfall totals
            increase gradually through time. If negative, they fall through
            time. S&M recommend +/- 0.07 for a realistic climate change driven
            drift at Walnut Gulch.
        storminess_trend : float
            Controls if a drift is applied to the expected intensity of
            individual storms through time. If 0., no trend. If positive,
            storms get more intense through time, if negative, less so. S&M
            recommend +/- 0.01 for a realistic climate change driven drift at
            Walnut Gulch.
        monsoon_fraction_of_year : float
            If limit == 'total_time', sets the fraction of one year occupied
            by the monsoon season. If not, ignored. Singer's monsoon runs from
            May to September, inclusive, and the default reflects this.
        monsoon_total_rf_gaussian : dict
            Parameters defining the normal distribution controlling the total
            rainfall expected in each year. S&M use 'mu' in {143., 271.} for
            step changes up/down in rainfall totals.
        monsoon_storm_duration_GEV : dict
            Parameters defining a generalised extreme value distribution
            controlling the duration of each storm. In minutes.
        monsoon_storm_area_GEV : dict
            Parameters defining a generalised extreme value distribution
            controlling the plan view area of each storm. S&M use 'shape': 0.,
            which collapses the distribution to a plain extreme value
            distribution.
        monsoon_storm_interarrival_GEV : dict
            Parameters defining a generalised extreme value
            distribution controlling the interarrival time between each storm.
            In HRS. Note that this calibration is specifically to Walnut Gulch,
            which has an area of 275 km**2. The generator directly scales this
            resulting distribution to the area ratio of Walnut Gulch to the
            open cells of the grid. This crudely accounts for the fact that
            bigger catchments will have more storms, but note that the heavy
            tail on this distribution means the default distribution shape
            will not be trustworthy for catchments with big differences in
            size from Walnut Gulch.
        monsoon_storm_radial_weakening_gaussian : dict
            Parameters defining a normal distribution
            controlling the rate of intensity decline with distance from storm
            center. For more detail see Rodriguez-Iturbe et al., 1986; Morin
            et al., 2005.
        winter_total_rf_gaussian : dict
            Parameters defining a normal distribution controlling the total
            rainfall expected in each year. S&M use 'mu' in {143., 271.} for
            step changes up/down in rainfall totals.
        winter_storm_duration_fisk : dict
            Parameters defining a Fisk (i.e., log-logistic) distribution
            controlling the duration of each storm. Note this differs from the
            summer scaling. In Minutes.
        winter_storm_area_GEV is a generalised extreme value distribution
            controlling the plan view area of each storm. S&M use 'shape': 0.,
            which collapses the distribution to a plain extreme value
            distribution.
        winter_storm_interarrival_GEV : dict
            Parameters defining a generalised extreme value
            distribution controlling the interarrival time between each storm.
            In HRS. The same considerations apply here as for the monsoonal
            interstorm equivalent.
        winter_storm_radial_weakening_gaussian : dict
            Parameters defining a normal distribution
            controlling the rate of intensity decline with distance from storm
            center. For more detail see Rodriguez-Iturbe et al., 1986; Morin
            et al., 2005.

        Yields
        ------
        (storm_t, interval_t) : (float, float)
            Tuple pair of duration of a single storm, then the interstorm
            interval that follows it. In hrs. The rainfall__flux field
            describes the rainfall rate during the interval storm_t as the
            tuple is yielded. In HRS.
            Note that the rainfall__total_depth_per_year field gives the total
            accumulated rainfall depth during the *last completed* model year,
            not the year to the point of yield. For the latter, use the
            property `total_rainfall_this_year`.
        """
        return self._run_the_process(
            yield_storms=True,
            yield_years=False,
            yield_seasons=False,
            limit=limit,
            style=style,
            monsoon_fraction_of_year=monsoon_fraction_of_year,
            total_rf_trend=total_rf_trend,
            storminess_trend=storminess_trend,
            monsoon_total_rf_gaussian=monsoon_total_rf_gaussian,
            monsoon_storm_duration_GEV=monsoon_storm_duration_GEV,
            monsoon_storm_area_GEV=monsoon_storm_area_GEV,
            monsoon_storm_interarrival_GEV=monsoon_storm_interarrival_GEV,
            monsoon_storm_radial_weakening_gaussian=monsoon_storm_radial_weakening_gaussian,
            winter_total_rf_gaussian=winter_total_rf_gaussian,
            winter_storm_duration_fisk=winter_storm_duration_fisk,
            winter_storm_area_GEV=winter_storm_area_GEV,
            winter_storm_interarrival_GEV=winter_storm_interarrival_GEV,
            winter_storm_radial_weakening_gaussian=winter_storm_radial_weakening_gaussian,
        )

    def yield_years(
        self,
        limit="total_time",
        style="whole_year",
        total_rf_trend=0.0,
        storminess_trend=0.0,
        monsoon_fraction_of_year=0.42,
        monsoon_total_rf_gaussian=(("sigma", 64.0), ("mu", 207.0)),
        monsoon_storm_duration_GEV=(
            ("shape", -0.570252),
            ("sigma", 35.7389),
            ("mu", 34.1409),
            ("trunc_interval", (1.0, 1040.0)),
        ),
        monsoon_storm_area_GEV=(
            ("shape", 0.0),
            ("sigma", 2.83876e07),
            ("mu", 1.22419e08),
            ("trunc_interval", (5.0e06, 3.0e08)),
        ),
        monsoon_storm_interarrival_GEV=(
            ("shape", -0.807971),
            ("sigma", 9.4957),
            ("mu", 10.6108),
            ("trunc_interval", (0.0, 720.0)),
        ),
        monsoon_storm_radial_weakening_gaussian=(
            ("sigma", 0.08),
            ("mu", 0.25),
            ("trunc_interval", (0.15, 0.67)),
        ),
        winter_total_rf_gaussian=(("sigma", 52.0), ("mu", 1.65)),
        winter_storm_duration_fisk=(
            ("c", 1.0821),
            ("scale", 68.4703),
            ("trunc_interval", (1.0, 5000.0)),
        ),
        winter_storm_area_GEV=(
            ("shape", 0.0),
            ("sigma", 2.83876e07),
            ("mu", 1.22419e08),
            ("trunc_interval", (5.0e06, 3.0e08)),
        ),
        winter_storm_interarrival_GEV=(
            ("shape", 1.1131),
            ("sigma", 53.2671),
            ("mu", 47.4944),
            ("trunc_interval", (0.0, 720.0)),
        ),
        winter_storm_radial_weakening_gaussian=(
            ("sigma", 0.08),
            ("mu", 0.25),
            ("trunc_interval", (0.15, 0.67)),
        ),
    ):
        """Yield a timeseries giving the number if storms occurring each year
        in a rainfall simulation.

        All default distributions specified as parameters reflect values for
        Walnut Gulch, see Singer & Michaelides, 2017 & Singer et al, submitted.

        Parameters
        ----------
        limit : ('total_time', 'total_rainfall')
            Controls whether a season is defined based on its total rainfall
            (and can be any length), or by its duration (and can have any
            amount of rainfall). If 'total_time', monsoon_fraction_of_year
            sets the fraction of a year occupied by the monsoon.
        style : ('whole_year', 'monsoonal', 'winter')
            Controls whether the component seeks to simulate a western US-
            style "monsoonal" climate, a western US-style winter climate,
            or a full year combining both. These distributions are by default
            based on Singer et al.'s calibrations. Note if 'monsoonal',
            the total duration of a "year" will appear to be only
            `monsoon_fraction_of_year`, and the opposite for `winter`.
        total_rf_trend : float
            Controls if a drift is applied to the total rainfall distribution
            through time. If 0., no trend. If positive, rainfall totals
            increase gradually through time. If negative, they fall through
            time. S&M recommend +/- 0.07 for a realistic climate chage driven
            drift at Walnut Gulch.
        storminess_trend : float
            Controls if a drift is applied to the expected intensity of
            individual storms through time. If 0., no trend. If positive,
            storms get more intense through time, if negative, less so. S&M
            recommend +/- 0.01 for a realistic climate change driven drift at
            Walnut Gulch.
        monsoon_fraction_of_year : float
            If limit == 'total_time', sets the fraction of one year occupied
            by the monsoon season. If not, ignored. Singer's monsoon runs from
            May to September, inclusive, and the default reflects this.

        monsoon_total_rf_gaussian is a normal distribution controlling the
            total rainfall expected in each year. S&M use 'mu' in {143., 271.}
            for step changes up/down in rainfall totals. In mm.
        monsoon_storm_duration_GEV is a generalised extreme value distribution
            controlling the duration of each storm. In MIN.
        monsoon_storm_area_GEV is a generalised extreme value distribution
            controlling the plan view area of each storm. S&M use 'shape': 0.,
            which collapses the distribution to a plain extreme value
            distribution.
        monsoon_storm_interarrival_GEV is a generalised extreme value
            distribution controlling the interarrival time between each storm.
            In HRS. Note that this calibration is specifically to Walnut Gulch,
            which has an area of 275 km**2. The generator directly scales this
            resulting distribution to the area ratio of Walnut Gulch to the
            open cells of the grid. This crudely accounts for the fact that
            bigger catchments will have more storms, but note that the heavy
            tail on this distribution means the default distribution shape
            will not be trustworthy for catchments with big differences in
            size from Walnut Gulch.
        monsoon_storm_radial_weakening_gaussian is a normal distribution
            controlling the rate of intensity decline with distance from storm
            center. For more detail see Rodriguez-Iturbe et al., 1986; Morin
            et al., 2005.

        winter_total_rf_gaussian is a normal distribution controlling the total
            rainfall expected in each year. S&M use 'mu' in {143., 271.} for
            step changes up/down in rainfall totals.
        winter_storm_duration_fisk is a Fisk (i.e., log-logistic) distribution
            controlling the duration of each storm. Note this differs from the
            summer scaling. In MIN.
        winter_storm_area_GEV is a generalised extreme value distribution
            controlling the plan view area of each storm. S&M use 'shape': 0.,
            which collapses the distribution to a plain extreme value
            distribution.
        winter_storm_interarrival_GEV is a generalised extreme value
            distribution controlling the interarrival time between each storm.
            In HRS. The same considerations apply here as for the monsoonal
            interstorm equivalent.
        winter_storm_radial_weakening_gaussian is a normal distribution
            controlling the rate of intensity decline with distance from storm
            center. For more detail see Rodriguez-Iturbe et al., 1986; Morin
            et al., 2005.

        Yields
        ------
        number_of_storms_per_year : float
            Float that gives the number of storms simulated in the year that
            elapsed since the last yield. The rainfall__total_depth_per_year
            field gives the total accumulated rainfall depth during the year
            preceding the yield. rainfall__flux gives the rainfall intensity of
            the last storm in that year.
        """
        return self._run_the_process(
            yield_storms=False,
            yield_years=True,
            yield_seasons=False,
            limit=limit,
            style=style,
            total_rf_trend=total_rf_trend,
            storminess_trend=storminess_trend,
            monsoon_fraction_of_year=monsoon_fraction_of_year,
            monsoon_total_rf_gaussian=monsoon_total_rf_gaussian,
            monsoon_storm_duration_GEV=monsoon_storm_duration_GEV,
            monsoon_storm_area_GEV=monsoon_storm_area_GEV,
            monsoon_storm_interarrival_GEV=monsoon_storm_interarrival_GEV,
            monsoon_storm_radial_weakening_gaussian=monsoon_storm_radial_weakening_gaussian,
            winter_total_rf_gaussian=winter_total_rf_gaussian,
            winter_storm_duration_fisk=winter_storm_duration_fisk,
            winter_storm_area_GEV=winter_storm_area_GEV,
            winter_storm_interarrival_GEV=winter_storm_interarrival_GEV,
            winter_storm_radial_weakening_gaussian=winter_storm_radial_weakening_gaussian,
        )

    def yield_seasons(
        self,
        limit="total_time",
        style="whole_year",
        total_rf_trend=0.0,
        storminess_trend=0.0,
        monsoon_fraction_of_year=0.42,
        monsoon_total_rf_gaussian=(("sigma", 64.0), ("mu", 207.0)),
        monsoon_storm_duration_GEV=(
            ("shape", -0.570252),
            ("sigma", 35.7389),
            ("mu", 34.1409),
            ("trunc_interval", (1.0, 1040.0)),
        ),
        monsoon_storm_area_GEV=(
            ("shape", 0.0),
            ("sigma", 2.83876e07),
            ("mu", 1.22419e08),
            ("trunc_interval", (5.0e06, 3.0e08)),
        ),
        monsoon_storm_interarrival_GEV=(
            ("shape", -0.807971),
            ("sigma", 9.4957),
            ("mu", 10.6108),
            ("trunc_interval", (0.0, 720.0)),
        ),
        monsoon_storm_radial_weakening_gaussian=(
            ("sigma", 0.08),
            ("mu", 0.25),
            ("trunc_interval", (0.15, 0.67)),
        ),
        winter_total_rf_gaussian=(("sigma", 52.0), ("mu", 1.65)),
        winter_storm_duration_fisk=(
            ("c", 1.0821),
            ("scale", 68.4703),
            ("trunc_interval", (1.0, 5000.0)),
        ),
        winter_storm_area_GEV=(
            ("shape", 0.0),
            ("sigma", 2.83876e07),
            ("mu", 1.22419e08),
            ("trunc_interval", (5.0e06, 3.0e08)),
        ),
        winter_storm_interarrival_GEV=(
            ("shape", 1.1131),
            ("sigma", 53.2671),
            ("mu", 47.4944),
            ("trunc_interval", (0.0, 720.0)),
        ),
        winter_storm_radial_weakening_gaussian=(
            ("sigma", 0.08),
            ("mu", 0.25),
            ("trunc_interval", (0.15, 0.67)),
        ),
    ):
        """Yield a timeseries giving the number if storms occurring each season
        in a rainfall simulation. Only meaningfully different from yield_years
        if style=='whole_year'.

        All default distributions specified as parameters reflect values for
        Walnut Gulch, see Singer & Michaelides, 2017 & Singer et al, submitted.

        Parameters
        ----------
        limit : ('total_time', 'total_rainfall')
            Controls whether a season is defined based on its total rainfall
            (and can be any length), or by its duration (and can have any
            amount of rainfall). If 'total_time', monsoon_fraction_of_year
            sets the fraction of a year occupied by the monsoon.
        style : ('whole_year', 'monsoonal', 'winter')
            Controls whether the component seeks to simulate a western US-
            style "monsoonal" climate, a western US-style winter climate,
            or a full year combining both. These distributions are by default
            based on Singer et al.'s calibrations. Note if 'monsoonal',
            the total duration of a "year" will appear to be only
            `monsoon_fraction_of_year`, and the opposite for `winter`.
        total_rf_trend : float
            Controls if a drift is applied to the total rainfall distribution
            through time. If 0., no trend. If positive, rainfall totals
            increase gradually through time. If negative, they fall through
            time. S&M recommend +/- 0.07 for a realistic climate chage driven
            drift at Walnut Gulch.
        storminess_trend : float
            Controls if a drift is applied to the expected intensity of
            individual storms through time. If 0., no trend. If positive,
            storms get more intense through time, if negative, less so. S&M
            recommend +/- 0.01 for a realistic climate change driven drift at
            Walnut Gulch.
        monsoon_fraction_of_year : float
            If limit == 'total_time', sets the fraction of one year occupied
            by the monsoon season. If not, ignored. Singer's monsoon runs from
            May to September, inclusive, and the default reflects this.

        monsoon_total_rf_gaussian is a normal distribution controlling the
            total rainfall expected in each year. S&M use 'mu' in {143., 271.}
            for step changes up/down in rainfall totals. In mm.
        monsoon_storm_duration_GEV is a generalised extreme value distribution
            controlling the duration of each storm. In MIN.
        monsoon_storm_area_GEV is a generalised extreme value distribution
            controlling the plan view area of each storm. S&M use 'shape': 0.,
            which collapses the distribution to a plain extreme value
            distribution.
        monsoon_storm_interarrival_GEV is a generalised extreme value
            distribution controlling the interarrival time between each storm.
            In HRS. Note that this calibration is specifically to Walnut Gulch,
            which has an area of 275 km**2. The generator directly scales this
            resulting distribution to the area ratio of Walnut Gulch to the
            open cells of the grid. This crudely accounts for the fact that
            bigger catchments will have more storms, but note that the heavy
            tail on this distribution means the default distribution shape
            will not be trustworthy for catchments with big differences in
            size from Walnut Gulch.
        monsoon_storm_radial_weakening_gaussian is a normal distribution
            controlling the rate of intensity decline with distance from storm
            center. For more detail see Rodriguez-Iturbe et al., 1986; Morin
            et al., 2005.

        winter_total_rf_gaussian is a normal distribution controlling the total
            rainfall expected in each year. S&M use 'mu' in {143., 271.} for
            step changes up/down in rainfall totals.
        winter_storm_duration_fisk is a Fisk (i.e., log-logistic) distribution
            controlling the duration of each storm. Note this differs from the
            summer scaling. In MIN.
        winter_storm_area_GEV is a generalised extreme value distribution
            controlling the plan view area of each storm. S&M use 'shape': 0.,
            which collapses the distribution to a plain extreme value
            distribution.
        winter_storm_interarrival_GEV is a generalised extreme value
            distribution controlling the interarrival time between each storm.
            In HRS. The same considerations apply here as for the monsoonal
            interstorm equivalent.
        winter_storm_radial_weakening_gaussian is a normal distribution
            controlling the rate of intensity decline with distance from storm
            center. For more detail see Rodriguez-Iturbe et al., 1986; Morin
            et al., 2005.

        Yields
        ------
        number_of_storms_per_season : float
            Float that gives the number of storms simulated in the season that
            elapsed since the last yield. The rainfall__total_depth_per_year
            field gives the total accumulated rainfall depth during the *year*
            preceding the yield, *so far*. rainfall__flux gives the rainfall
            intensity of the last storm in that year.
        NB: Use the component property total_rainfall_last_season to access
            the *actual* amount of rainfall in the season that has the number
            of storms that the method generates.
        """
        return self._run_the_process(
            yield_storms=False,
            yield_years=False,
            yield_seasons=True,
            limit=limit,
            style=style,
            total_rf_trend=total_rf_trend,
            storminess_trend=storminess_trend,
            monsoon_fraction_of_year=monsoon_fraction_of_year,
            monsoon_total_rf_gaussian=monsoon_total_rf_gaussian,
            monsoon_storm_duration_GEV=monsoon_storm_duration_GEV,
            monsoon_storm_area_GEV=monsoon_storm_area_GEV,
            monsoon_storm_interarrival_GEV=monsoon_storm_interarrival_GEV,
            monsoon_storm_radial_weakening_gaussian=monsoon_storm_radial_weakening_gaussian,
            winter_total_rf_gaussian=winter_total_rf_gaussian,
            winter_storm_duration_fisk=winter_storm_duration_fisk,
            winter_storm_area_GEV=winter_storm_area_GEV,
            winter_storm_interarrival_GEV=winter_storm_interarrival_GEV,
            winter_storm_radial_weakening_gaussian=winter_storm_radial_weakening_gaussian,
        )

    def _run_the_process(
        self,
        yield_storms=True,
        yield_years=False,
        yield_seasons=False,
        limit="total_time",
        style="whole_year",
        monsoon_fraction_of_year=0.42,
        total_rf_trend=0.0,
        storminess_trend=0.0,
        monsoon_total_rf_gaussian=(("sigma", 64.0), ("mu", 207.0)),
        monsoon_storm_duration_GEV=(
            ("shape", -0.570252),
            ("sigma", 35.7389),
            ("mu", 34.1409),
            ("trunc_interval", (1.0, 1040.0)),
        ),
        monsoon_storm_area_GEV=(
            ("shape", 0.0),
            ("sigma", 2.83876e07),
            ("mu", 1.22419e08),
            ("trunc_interval", (5.0e06, 3.0e08)),
        ),
        monsoon_storm_interarrival_GEV=(
            ("shape", -0.807971),
            ("sigma", 9.4957),
            ("mu", 10.6108),
            ("trunc_interval", (0.0, 720.0)),
        ),
        monsoon_storm_radial_weakening_gaussian=(
            ("sigma", 0.08),
            ("mu", 0.25),
            ("trunc_interval", (0.15, 0.67)),
        ),
        winter_total_rf_gaussian=(("sigma", 52.0), ("mu", 1.65)),
        winter_storm_duration_fisk=(
            ("c", 1.0821),
            ("scale", 68.4703),
            ("trunc_interval", (1.0, 5000.0)),
        ),
        winter_storm_area_GEV=(
            ("shape", 0.0),
            ("sigma", 2.83876e07),
            ("mu", 1.22419e08),
            ("trunc_interval", (5.0e06, 3.0e08)),
        ),
        winter_storm_interarrival_GEV=(
            ("shape", 1.1131),
            ("sigma", 53.2671),
            ("mu", 47.4944),
            ("trunc_interval", (0.0, 720.0)),
        ),
        winter_storm_radial_weakening_gaussian=(
            ("sigma", 0.08),
            ("mu", 0.25),
            ("trunc_interval", (0.15, 0.67)),
        ),
    ):
        """This is the underlying process that runs the component, but it
        should be run by a user through the yield_storms and yield_years
        methods.

        Fuzz to the chosen values is now selected from a continuous
        distribution, not from integer values.

        total_rf_trend controls if a drift is applied to the total rainfall
        distribution through time. If 0., no trend. If positive, rainfall
        totals increase gradually through time. If negative, they fall through
        time. S&M recommend +/- 0.07 for a realistic climate chage driven drift
        at Walnut Gulch.

        storminess_trend controls if a drift is applied to the expected
        intensity of individual storms through time. If 0., no trend. If
        positive, storms get more intense through time, if negative, less so.
        S&M recommend +/- 0.01 for a realistic climate change driven drift at
        Walnut Gulch.

        All default distributions reflect values for Walnut Gulch, see Singer &
        Michaelides, submitted:

        monsoon_total_rf_gaussian is a normal distribution controlling the
            total rainfall expected in each year. S&M use 'mu' in {143., 271.}
            for step changes up/down in rainfall totals. In mm.
        monsoon_storm_duration_GEV is a generalised extreme value distribution
            controlling the duration of each storm. In MIN.
        monsoon_storm_area_GEV is a generalised extreme value distribution
            controlling the plan view area of each storm. S&M use 'shape': 0.,
            which collapses the distribution to a plain extreme value
            distribution.
        monsoon_storm_interarrival_GEV is a generalised extreme value
            distribution controlling the interarrival time between each storm.
            In HRS. Note that this calibration is specifically to Walnut Gulch,
            which has an area of 275 km**2. The generator directly scales this
            resulting distribution to the area ratio of Walnut Gulch to the
            open cells of the grid. This crudely accounts for the fact that
            bigger catchments will have more storms, but note that the heavy
            tail on this distribution means the default distribution shape
            will not be trustworthy for catchments with big differences in
            size from Walnut Gulch.
        monsoon_storm_radial_weakening_gaussian is a normal distribution
            controlling the rate of intensity decline with distance from storm
            center. For more detail see Rodriguez-Iturbe et al., 1986; Morin
            et al., 2005.
        winter_total_rf_gaussian is a normal distribution controlling the total
            rainfall expected in each year. S&M use 'mu' in {143., 271.} for
            step changes up/down in rainfall totals.
        winter_storm_duration_fisk is a Fisk (i.e., log-logistic) distribution
            controlling the duration of each storm. Note this differs from the
            summer scaling. In MIN.
        winter_storm_area_GEV is a generalised extreme value distribution
            controlling the plan view area of each storm. S&M use 'shape': 0.,
            which collapses the distribution to a plain extreme value
            distribution.
        winter_storm_interarrival_GEV is a generalised extreme value
            distribution controlling the interarrival time between each storm.
            In HRS. The same considerations apply here as for the monsoonal
            interstorm equivalent.
        winter_storm_radial_weakening_gaussian is a normal distribution
            controlling the rate of intensity decline with distance from storm
            center. For more detail see Rodriguez-Iturbe et al., 1986; Morin
            et al., 2005.
        """
        monsoon_total_rf_gaussian = dict(monsoon_total_rf_gaussian)
        monsoon_storm_duration_GEV = dict(monsoon_storm_duration_GEV)
        monsoon_storm_area_GEV = dict(monsoon_storm_area_GEV)
        monsoon_storm_interarrival_GEV = dict(monsoon_storm_interarrival_GEV)
        monsoon_storm_radial_weakening_gaussian = dict(
            monsoon_storm_radial_weakening_gaussian
        )
        winter_total_rf_gaussian = dict(winter_total_rf_gaussian)
        winter_storm_duration_fisk = dict(winter_storm_duration_fisk)
        winter_storm_area_GEV = dict(winter_storm_area_GEV)
        winter_storm_interarrival_GEV = dict(winter_storm_interarrival_GEV)
        winter_storm_radial_weakening_gaussian = dict(
            winter_storm_radial_weakening_gaussian
        )

        FUZZMETHOD = "DEJH"
        FUZZWIDTH = 5.0  # if DEJH
        self._phantom_storm_count = 0
        # ^this property tracks the number of storms in the run that received
        # zero intensity (and thus didn't really exist)
        self._opennodes = self._grid.status_at_node != self._grid.BC_NODE_IS_CLOSED
        self._total_rainfall_last_season = self._grid.zeros(at="node")

        # safety check for init conds:
        if yield_storms:
            assert yield_years is False
            assert yield_seasons is False
        if yield_years:
            assert yield_storms is False
            assert yield_seasons is False
        if yield_seasons:
            assert yield_storms is False
            assert yield_years is False

        # add variable for number of simulations of simyears
        simyears = self._numyrs  # number of years to simulate
        numcurves = 11  # number of intensity-duration curves (see below for
        # curve equations)
        hrsinyr = 24.0 * 365.0
        hrsinmonsoon = monsoon_fraction_of_year * hrsinyr
        hrsinwinter = (1.0 - monsoon_fraction_of_year) * hrsinyr

        assert limit in ("total_rainfall", "total_time")

        assert style in ("whole_year", "monsoonal", "winter")
        if style == "whole_year":
            reps = 2
        else:
            reps = 1

        opennodes = self._opennodes
        num_opennodes = np.sum(opennodes)
        IDs_open = np.where(opennodes)[0]  # need this later
        X1 = self._grid.node_x
        Y1 = self._grid.node_y
        Xin = X1[opennodes]
        Yin = Y1[opennodes]
        try:
            Zz = self._grid.at_node["topographic__elevation"][opennodes]
        except KeyError:
            assert self._orographic_scenario is None
        numgauges = Xin.size  # number of rain gauges in the basin.
        # NOTE: In this version this produces output on a grid, rather than at
        # real gauge locations.

        assert FUZZMETHOD == "DEJH", "The Singer method for fuzz is no longer supported"

        # lambda_, kappa, and C are parameters of the intensity-duration curves
        # of the form: intensity =
        # lambda*exp(-0.508*duration)+kappa*exp(-0.008*duration)+C
        lambda_ = [
            642.2,
            578.0,
            513.8,
            449.5,
            385.3,
            321.1,
            256.9,
            192.7,
            128.4,
            64.1,
            21.0,
        ]
        kappa = [93.1, 83.8, 74.5, 65.2, 55.9, 46.6, 37.2, 27.9, 18.6, 9.3, 0.9]
        C = [4.5, 4.0, 3.5, 3.0, 2.5, 2.0, 1.5, 1.0, 0.5, 0.25, 0.05]

        # Unlike MS's original implementation, we no longer pull ET values, as
        # this should be a different component.

        self._Ptot_ann_global = np.zeros(simyears)
        self._Ptot_monsoon_global = np.zeros(simyears)

        master_storm_count = 0
        storm_trend = 0

        for syear in range(simyears):
            self._year = syear
            year_time = 0.0  # tracks simulation time per year in hours
            storm_trend += storminess_trend
            year_storm_count = 0
            breaker = False
            Storm_total_local_year = np.zeros((self._max_numstorms, num_opennodes))
            self._storm_running_sum_of_seasons = np.zeros(num_opennodes)
            self._storm_running_sum_1st_seas = np.zeros(num_opennodes)

            storms_yr_so_far = 0
            for seas in range(reps):
                seas_time = 0.0  # tracks elapsed season time in hours
                Storm_running_sum_seas = np.zeros((2, num_opennodes))
                # ^ 1st col is running total, 2nd is data to add to it
                if seas == 0 and style != "winter":
                    self._current_season = "M"
                    # This is the pdf fitted to all available station precip
                    # data (normal dist). It will be sampled below.
                    Ptot_pdf_norm = monsoon_total_rf_gaussian

                    # This is the pdf fitted to all available station duration
                    # data (GEV dist). It will be sampled below.
                    # #### matlab's GEV is (shape_param, scale(sigma), pos(mu))
                    # note that in Scipy, we must add a minus to the shape
                    # param for a GEV to match Matlab's implementation
                    Duration_pdf = monsoon_storm_duration_GEV
                    # This is the pdf fitted to all available station area
                    # data (EV dist). It will be sampled below.
                    # #### matlab's EV is (mu, sigma)
                    Area_pdf_EV = monsoon_storm_area_GEV
                    # This is the pdf fitted to all available station area
                    # data (GEV dist). It will be sampled below.
                    Int_arr_pdf_GEV = monsoon_storm_interarrival_GEV
                    # This is the pdf of storm gradient recession coefficients
                    # from Morin et al, 2005 (normal dist). It will be sampled
                    # below.
                    Recess_pdf_norm = monsoon_storm_radial_weakening_gaussian
                    seas_total = hrsinmonsoon
                else:
                    self._current_season = "W"
                    Ptot_pdf_norm = winter_total_rf_gaussian
                    Duration_pdf = winter_storm_duration_fisk
                    Area_pdf_EV = winter_storm_area_GEV
                    Int_arr_pdf_GEV = winter_storm_interarrival_GEV
                    Recess_pdf_norm = winter_storm_radial_weakening_gaussian
                    seas_total = hrsinwinter

                if not np.isclose(total_rf_trend, 0.0):
                    mu = Ptot_pdf_norm.pop("mu")
                    mu += mu * total_rf_trend
                    Ptot_pdf_norm["mu"] = mu
                # sample from normal distribution and saves global value of
                # Ptot (that must be equalled or exceeded) for each year
                season_rf_limit = self.calc_annual_rainfall(
                    style=style, monsoon_total_rf_gaussian=Ptot_pdf_norm
                )[seas]
                self._season_rf_limit = season_rf_limit
                self._Ptot_ann_global[syear] += season_rf_limit
                if seas == 0 and style != "winter":
                    self._Ptot_monsoon_global[syear] = season_rf_limit
                Storm_total_local_seas = np.zeros((self._max_numstorms, num_opennodes))
                seas_cum_Ptot_gauge = np.zeros(numgauges)
                self._entries = 0

                for seas_storm_count, storm in enumerate(range(self._max_numstorms)):
                    self._rain_int_gauge.fill(0.0)
                    int_arr_val = genextreme.rvs(
                        c=Int_arr_pdf_GEV["shape"],
                        loc=Int_arr_pdf_GEV["mu"],
                        scale=Int_arr_pdf_GEV["sigma"],
                    )
                    try:
                        int_arr_val = np.clip(
                            int_arr_val,
                            Int_arr_pdf_GEV["trunc_interval"][0],
                            Int_arr_pdf_GEV["trunc_interval"][1],
                        )
                    except KeyError:
                        # ...just in case
                        if int_arr_val < 0.0:
                            int_arr_val = 0.0
                    # now, correct the scaling relative to WG
                    int_arr_val /= self._scaling_to_WG
                    self._int_arr_val = int_arr_val
                    # ^Samples from distribution of interarrival times (hr).
                    # This can be used to develop STORM output for use in
                    # rainfall-runoff models or any water balance application.
                    # sample uniformly from storm center matrix from grid w
                    # 10 m spacings covering basin:

                    area_val = genextreme.rvs(
                        c=Area_pdf_EV["shape"],
                        loc=Area_pdf_EV["mu"],
                        scale=Area_pdf_EV["sigma"],
                    )
                    try:
                        area_val = np.clip(
                            area_val,
                            Area_pdf_EV["trunc_interval"][0],
                            Area_pdf_EV["trunc_interval"][1],
                        )
                    except KeyError:
                        # ...just in case
                        if area_val < 0.0:
                            area_val = 0.0
                    self._area_val = area_val
                    # ^Samples from distribution of storm areas

                    r = np.sqrt(area_val / np.pi)  # value here shd be selected
                    rsq = r**2
                    # based on area above in meters to match the UTM values

                    # This way of handling storm locations is really quite
                    # different to MS's. He uses a fixed buffer width, and
                    # throws away any storm that doesn't intersect. We
                    # instead retain all storms, and *make sure* the storm
                    # intersects using a dynamic buffer. MS's method will
                    # preferentially sample larger storms, though unclear
                    # what that would mean in practice.
                    # MS also snaps his storms onto the grid. This seems
                    # unnecessary, and we don't do it here.
                    while 1:
                        cx, cy = self._locate_storm(r)
                        # Determine which gauges are hit by Euclidean geometry:
                        gdist = (Xin - cx) ** 2 + (Yin - cy) ** 2
                        mask_name = gdist <= rsq  # this is defacto MS's aa
                        # this short circuits the storm loop in the case that
                        # the storm does not affect any 'gauging' location
                        if np.any(np.equal(mask_name, True)):
                            break

                    self._x = cx
                    self._y = cy
                    year_storm_count += 1
                    seas_storm_count += 1
                    master_storm_count += 1

                    # This routine below determines to which orographic group
                    # the closest gauge to the storm center belongs to, and
                    # censors the number of curves accordingly
                    # missing top curve in GR1, top and bottom curves for GR2,
                    # and bottom curve for GR3
                    # NOTE again, DEJH thinks this could be generalised a lot

                    # original curve# probs for 30%-20%-10%: [0.0636, 0.0727,
                    # 0.0819, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.1001,
                    # 0.1090, 0.1182]
                    # original curve# probs are modified as below
                    # add weights to reflect reasonable probabilities that
                    # favor lower curves:
                    if self._orographic_scenario is not None:
                        # this routine below allows for orography in precip by
                        # first determining the closest gauge and then
                        # determining its orographic grouping
                        cc = np.argmin(gdist)
                        closest_gauge_z = Zz[cc]  # this will be
                        # compared against orographic gauge groupings to
                        # determine the appropriate set of intensity-duration
                        # curves
                        if self._orographic_scenario == "Singer":
                            wgts = Singer_orographic_rainfall(closest_gauge_z)
                        else:
                            wgts = self._orographic_scenario(closest_gauge_z)
                    elif self._orographic_scenario is None:
                        wgts = [
                            0.0636,
                            0.0727,
                            0.0819,
                            0.0909,
                            0.0909,
                            0.0909,
                            0.0909,
                            0.0909,
                            0.1001,
                            0.1090,
                            0.1182,
                        ]
                    if seas == 0 and style != "winter":
                        duration_val = genextreme.rvs(
                            c=Duration_pdf["shape"],
                            loc=Duration_pdf["mu"],
                            scale=Duration_pdf["sigma"],
                        )
                    else:
                        duration_val = fisk.rvs(
                            c=Duration_pdf["c"], scale=Duration_pdf["scale"]
                        )
                    # hacky fix to prevent occasional < 0 values:
                    # (I think because Matlab is able to set limits manually)
                    try:
                        duration_val = np.clip(
                            duration_val,
                            Duration_pdf["trunc_interval"][0],
                            Duration_pdf["trunc_interval"][1],
                        )
                    except KeyError:
                        # ...just in case
                        if duration_val < 0.0:
                            duration_val = 0.0
                    durationhrs = duration_val / 60.0
                    self._durationhrs = durationhrs
                    year_time += durationhrs
                    seas_time += durationhrs
                    # we will always do the next storm, even if it exceeds the
                    # specified "total" time

                    # which curve did we pick?:
                    int_dur_curve_val = np.random.choice(numcurves, p=wgts)

                    intensity_val = (
                        lambda_[int_dur_curve_val] * np.exp(-0.508 * duration_val)
                        + kappa[int_dur_curve_val] * np.exp(-0.008 * duration_val)
                        + C[int_dur_curve_val]
                    )
                    # ...these curves are based on empirical data from WG

                    # this dist should look identical, w/o discretisation
                    fuzz_int_val = FUZZWIDTH * 2.0 * (np.random.rand() - 0.5)

                    intensity_val += fuzz_int_val
                    # ^this allows for specified fuzzy tolerance around
                    # selected intensity (but it can go -ve)
                    # formerly, here MS used a rounding and threshold to
                    # prevent storms with a value < 1. We're going to remove
                    # the rounding and threshold at zero instead. (below)

                    # This scales the storm center intensity upward, so the
                    # values at each gauge are realistic once the gradient is
                    # applied.
                    intensity_val += intensity_val * storm_trend
                    # storminess trend is applied and its effect rises each
                    # year of simulation
                    # DEJH has removed the rounding
                    # Note that is is now possible for intensity_val to go
                    # negative, so:
                    if intensity_val < 0.0:
                        intensity_val = 0.0
                        self._phantom_storm_count += 1
                    # note storms of zero intensity are now permitted (though
                    # should hopefully remain pretty rare.)
                    self._intensity_val = intensity_val

                    # area to determine which gauges are hit:
                    recess_val = np.random.normal(
                        loc=Recess_pdf_norm["mu"], scale=Recess_pdf_norm["sigma"]
                    )
                    with contextlib.suppress(KeyError):
                        recess_val = np.clip(
                            recess_val,
                            Recess_pdf_norm["trunc_interval"][0],
                            Recess_pdf_norm["trunc_interval"][1],
                        )
                    self._recess_val = recess_val
                    # this pdf of recession coefficients determines how
                    # intensity declines with distance from storm center (see
                    # below)
                    # determine cartesian distances to all hit gauges and
                    # associated intensity values at each gauge hit by the
                    # storm
                    # This is a data storage solution to avoid issues that can
                    # arise with slicing grid areas with heavy tailed sizes
                    self._entries = np.sum(mask_name)  # only open nodes
                    entries = self._entries
                    # NOTE _gauge_dist_km only contains nodes under the storm!
                    # The remaining entries are garbage
                    # Xin -> only the open nodes, note
                    self._gauge_dist_km[:entries] = np.sqrt(gdist[mask_name]) / 1000.0
                    self._temp_dataslots2[:entries] = gdist[mask_name] / 1.0e6
                    self._temp_dataslots2[:entries] *= -2.0 * recess_val**2
                    np.exp(
                        self._temp_dataslots2[:entries],
                        out=self._temp_dataslots2[:entries],
                    )
                    self._temp_dataslots2[:entries] *= intensity_val
                    mask_incl_closed = IDs_open[mask_name]
                    self._nodes_hit = mask_incl_closed
                    # ^note this is by ID, not bool
                    self._rain_int_gauge[mask_incl_closed] = self._temp_dataslots2[
                        :entries
                    ]
                    # calc of _rain_int_gauge follows Rodriguez-Iturbe et al.,
                    # 1986; Morin et al., 2005 but sampled from a distribution
                    # only need to add the bit that got rained on, so:
                    self._temp_dataslots2[:entries] *= duration_val / 60.0
                    seas_cum_Ptot_gauge[mask_name] += self._temp_dataslots2[:entries]
                    # collect storm totals for all gauges into rows by storm
                    Storm_total_local_seas[storm, :] = (
                        self._rain_int_gauge[opennodes] * duration_val / 60.0
                    )
                    Storm_total_local_year[(storm + storms_yr_so_far), :] = (
                        Storm_total_local_seas[storm, :]
                    )
                    self._max_storm_depth = Storm_total_local_seas[storm, :].max()

                    self._Storm_total_local_seas = Storm_total_local_seas
                    self._Storm_total_local_year = Storm_total_local_year
                    Storm_running_sum_seas[1, :] = Storm_total_local_seas[storm, :]
                    np.nansum(
                        Storm_running_sum_seas, axis=0, out=Storm_running_sum_seas[0, :]
                    )
                    if np.any(Storm_total_local_seas < 0.0):
                        raise ValueError(syear, storm)
                    self._median_seas_rf_total = np.nanmedian(
                        Storm_running_sum_seas[0, :]
                    )
                    self._Storm_running_sum_seas = Storm_running_sum_seas[0, :]

                    if limit == "total_time":
                        if seas_time + int_arr_val > seas_total:
                            int_arr_val = (seas_total - seas_time).clip(0.0)
                            breaker = True
                    else:
                        if self._median_seas_rf_total > season_rf_limit:
                            breaker = True
                    if yield_storms is True:
                        yield (durationhrs, int_arr_val)
                    seas_time += int_arr_val
                    year_time += int_arr_val
                    if breaker:
                        # Don't create Ptotal_local per MS... just
                        breaker = False
                        break
                    if storm + 1 == self._max_numstorms:
                        raise ValueError("_max_numstorms set too low for this run")
                storms_yr_so_far = seas_storm_count
                self._storm_running_sum_of_seasons += Storm_running_sum_seas[0, :]
                self._total_rainfall_last_season[self._opennodes] = (
                    Storm_running_sum_seas[0, :]
                )
                self._storm_running_sum_1st_seas += Storm_running_sum_seas[0, :]
                if yield_seasons:
                    yield seas_storm_count

            self._total_rf_year[opennodes] = self._storm_running_sum_of_seasons
            if yield_years is True and yield_seasons is False:
                yield year_storm_count

    def calc_annual_rainfall(
        self,
        style="whole_year",
        monsoon_total_rf_gaussian=(("sigma", 64.0), ("mu", 207.0)),
        winter_total_rf_gaussian=(("sigma", 52.0), ("mu", 1.65)),
    ):
        """Return a tuple of rainfall totals (mm) for the year, with entries
        subdividing the yearly total into seasons as appropriate.

        Parameters
        ----------
        style : ('whole_year', 'monsoonal', 'winter')
            Whether to simulate 2 seasons, or a single season.
        monsoon_total_rf_gaussian : dict of sigma and mu for the summer
            distribution (if used). Defaults to Walnut Gulch.
        winter_total_rf_gaussian : dict of sigma and mu for the summer
            distribution (if used). Defaults to Walnut Gulch.

        Returns
        -------
        tuple : (first_season_total, [second_season_total])
            If style=='monsoonal' or 'winter', a len(1) tuple of the total rf.
            If style=='whole_year', a len(2) tuple of (monsoon, winter) totals.

        Examples
        --------
        >>> mg = RasterModelGrid((10, 10), xy_spacing=500.0)
        >>> z = mg.add_zeros("topographic__elevation", at="node")
        >>> rain = SpatialPrecipitationDistribution(mg)
        >>> mytotals = []
        >>> for yr in range(5):
        ...     mytotals.append(rain.calc_annual_rainfall(style="whole_year"))
        ...
        >>> [len(x) == 2 for x in mytotals]
        [True, True, True, True, True]
        >>> mytotals = []
        >>> for yr in range(3):
        ...     mytotals.append(rain.calc_annual_rainfall(style="monsoonal"))
        ...
        >>> [len(x) == 1 for x in mytotals]
        [True, True, True]
        """
        monsoon_total_rf_gaussian = dict(monsoon_total_rf_gaussian)
        winter_total_rf_gaussian = dict(winter_total_rf_gaussian)

        assert style in ("whole_year", "monsoonal", "winter")
        if style in ("whole_year", "monsoonal"):
            # sample from normal distribution and saves global value of Ptot
            # (that must be equalled or exceeded) for each year
            summer_rf_limit = np.random.normal(
                loc=monsoon_total_rf_gaussian["mu"],
                scale=monsoon_total_rf_gaussian["sigma"],
            )
            try:
                summer_rf_limit = np.clip(
                    summer_rf_limit,
                    monsoon_total_rf_gaussian["trunc_interval"][0],
                    monsoon_total_rf_gaussian["trunc_interval"][1],
                )
            except KeyError:
                # ...just in case
                if summer_rf_limit < 0.0:
                    summer_rf_limit = 0.0
        if style in ("whole_year", "winter"):
            # sample from normal distribution and saves global value of Ptot
            # (that must be equalled or exceeded) for each year
            winter_rf_limit = np.random.normal(
                loc=winter_total_rf_gaussian["mu"],
                scale=winter_total_rf_gaussian["sigma"],
            )
            try:
                winter_rf_limit = np.clip(
                    winter_rf_limit,
                    winter_total_rf_gaussian["trunc_interval"][0],
                    winter_total_rf_gaussian["trunc_interval"][1],
                )
            except KeyError:
                # ...just in case
                if winter_rf_limit < 0.0:
                    winter_rf_limit = 0.0
        if style == "monsoonal":
            return (summer_rf_limit,)
        elif style == "winter":
            return (winter_rf_limit,)
        else:
            return (summer_rf_limit, winter_rf_limit)

    def _locate_storm(self, storm_radius):
        """Because of the way the stats fall out, any simulated storm from the
        distribution must intersect the catchment somewhere.

        Note written in a grid-agnostic fashion.
        """
        stormposx = np.random.rand() * (self._widthx + 2.0 * storm_radius)
        stormposy = np.random.rand() * (self._widthy + 2.0 * storm_radius)
        stormx = self._minx - storm_radius + stormposx
        stormy = self._miny - storm_radius + stormposy
        return stormx, stormy

    @property
    def current_year(self):
        """Get the current year as an int."""
        return self._year

    @property
    def current_season(self):
        """Get the current season.

        'M' is monsoon, 'W' is winter.
        """
        return self._current_season

    @property
    def storm_depth_last_storm(self):
        """Get the maximum storm depth during the last storm (mm)."""
        return self._max_storm_depth

    @property
    def storm_recession_value_last_storm(self):
        """Get the recession parameter (radial die-off) for the last storm."""
        return self._recess_val

    @property
    def storm_duration_last_storm(self):
        """Get the duration (in hrs) of the last storm."""
        return self._durationhrs

    @property
    def storm_area_last_storm(self):
        """Get the area (in m**2) of the last storm."""
        return self._area_val

    @property
    def storm_intensity_last_storm(self):
        """Get the intensity (mm/hr) of the last storm, averaged under the
        storm.

        footprint. Note that duration * intensity != storm max depth.
        """
        return self._intensity_val

    @property
    def total_rainfall_last_season(self):
        """Get the total recorded rainfall over the last (completed) simulated
        season, spatially resolved (mm)."""
        return self._total_rainfall_last_season

    @property
    def total_rainfall_last_year(self):
        """Get the total recorded rainfall over the last (completed) simulated
        year, spatially resolved (mm).

        Equivalent to the field 'rainfall__total_depth_per_year'.
        """
        return self._total_rf_year

    @property
    def total_rainfall_this_season(self):
        """Get the accumulated, spatially resolved total rainfall over the grid
        for the season so far (mm)."""
        self._running_total_rainfall_this_season[self._opennodes] = (
            self._Storm_running_sum_seas
        )
        return self._running_total_rainfall_this_season

    @property
    def total_rainfall_this_year(self):
        """Get the accumulated, spatially resolved total rainfall over the grid
        for the year so far (mm)."""
        self._running_total_rainfall_this_year[self._opennodes] = (
            self._storm_running_sum_1st_seas + self._Storm_running_sum_seas
        )
        return self._running_total_rainfall_this_year

    @property
    def median_total_rainfall_last_season(self):
        """Get the median total rainfall recorded over the open nodes of the
        grid during the last (completed) simulated season (mm)."""
        return np.nanmedian(self._total_rainfall_last_season[self._opennodes])

    @property
    def median_total_rainfall_last_year(self):
        """Get the median total rainfall recorded over the open nodes of the
        grid during the last (completed) simulated year (mm)."""
        return np.nanmedian(self.total_rainfall_last_year[self._opennodes])

    @property
    def median_total_rainfall_this_season(self):
        """Get the accumulated median total rainfall over the open nodes of the
        grid so far this season (mm)."""
        return self._median_seas_rf_total

    @property
    def median_total_rainfall_this_year(self):
        """Get the accumulated median total rainfall over the open nodes of the
        grid so far this year (mm)."""
        return np.nanmedian(self.total_rainfall_this_year[self._opennodes])

    @property
    def number_of_nodes_under_storm(self):
        """Get the number of nodes under the last storm."""
        return self._entries

    @property
    def nodes_under_storm(self):
        """Get the IDs of the nodes under the last storm."""
        return self._nodes_hit

    @property
    def coordinates_of_last_storm_center(self):
        """Get the coordinates of the center of the last storm as (x, y)."""
        return (self._x, self._y)

    @property
    def target_median_total_rainfall_this_season(self):
        """Get the stochastically generated "target" average total rainfall
        amount over the catchment for the current season.

        If limit == 'total_rainfall', this will be very close to
        median_total_rainfall_last_season. If 'total_time', it will
        diverge from this value.
        """
        return self._season_rf_limit


def Singer_orographic_rainfall(z_closest_node_to_center):
    """Return a set of curve weights for a provided z, assuming an orographic
    rule following that presented in Singer & Michaelides 2017 & Singer et al.
    2018 and applicable specifically to Walnut Gulch. i.e., there are three
    orographic divisions, divided at 1350 m and 1500 m.

    Parameters
    ----------
    z_closest_node_to_center : float
        The elevation of the node closest to the storm center.

    Returns
    -------
    wgts : length 11 list
        The weighting parameters to use in selecting a storm distribution
        curve.
    """
    if z_closest_node_to_center <= 1350:
        wgts = [
            0.0318,
            0.0759,
            0.0851,
            0.0941,
            0.0941,
            0.0941,
            0.0941,
            0.0941,
            0.1033,
            0.1121,
            0.1213,
        ]
    elif 1350 < z_closest_node_to_center <= 1500:
        wgts = [
            0.0478,
            0.0778,
            0.0869,
            0.0959,
            0.0959,
            0.0959,
            0.0959,
            0.0959,
            0.1051,
            0.1141,
            0.0888,
        ]
    elif z_closest_node_to_center > 1500:
        wgts = [
            0.0696,
            0.0786,
            0.0878,
            0.0968,
            0.0968,
            0.0968,
            0.0968,
            0.0968,
            0.1060,
            0.1149,
            0.0591,
        ]
    return wgts


if __name__ == "__main__":
    from matplotlib.pyplot import show

    nx = 17
    ny = 8
    dx = 1000.0
    mg = RasterModelGrid((nx, ny), xy_spacing=dx)

    z = mg.add_zeros("topographic__elevation", at="node")
    z += 1400.0
    rain = SpatialPrecipitationDistribution(mg, number_of_years=1)
    total_t = 0.0
    for count, dt, interval_t in enumerate(
        rain.yield_storms(style="whole_year", limit="total_time")
    ):
        total_t += dt + interval_t
        print(dt, interval_t)
        if count % 100 == 0:
            print("Season:", rain.current_season, "of yr", rain.current_year)
            print("Current storm:", count)
            show()
    print("Effective total years:")
    print(total_t / 24.0 / 365.0)
    print("Storms simulated:")
    print(count)



================================================
File: species_evolution/README.md
================================================
# SpeciesEvolver: evolve life in simulated landscapes

[![status](https://joss.theoj.org/papers/446f3d17d642682b234ffed2b53198f6/status.svg)](https://joss.theoj.org/papers/446f3d17d642682b234ffed2b53198f6)

Life evolves alongside landscapes by biotic and abiotic processes under complex
dynamics at Earth's surface. Researchers who wish to explore these dynamics can
use this component as a tool for them to build landscape-life evolution models.
Landlab components, including SpeciesEvolver are designed to work with a shared
model grid. Researchers can build novel models using plug-and-play surface
process components to evolve the grid's landscape alongside the life tracked by
SpeciesEvolver. The simulated life evolves following customizable processes.

SpeciesEvolver is introduced [in this paper](https://doi.org/10.21105/joss.02066)
published February 2020 by the Journal of Open Source Software.

## Documentation and installation

Landlab documentation is hosted on [csdms.io](https://landlab.csdms.io/),
including instructions to install Landlab. SpeciesEvolver is installed with
Landlab.

SpeciesEvolver documentation is located [here](https://landlab.csdms.io/generated/api/landlab.components.species_evolution.html).

## SpeciesEvolver tutorial

A SpeciesEvolver tutorial exists in the form of a Jupyter Notebook accessible
through the following links:
- [Launch the tutorial](https://mybinder.org/v2/gh/landlab/landlab/release?filepath=notebooks/tutorials/species_evolution/Introduction_to_SpeciesEvolver.ipynb)
as interactive notebook in your browser, with no need to install software,
launched using Binder.
- [A static version of the same tutorial](https://landlab.csdms.io/tutorials/species_evolution/Introduction_to_SpeciesEvolver.html)
- All Landlab tutorials can be launched from [this directory](https://mybinder.org/v2/gh/landlab/landlab/release?filepath=notebooks/welcome.ipynb) using Binder.

## Get or give help

[Open an Issue here](https://github.com/landlab/landlab/issues) where we can
respond to your questions, comments, issues, ideas, or any identified bugs
related to Landlab including SpeciesEvolver.



================================================
File: species_evolution/__init__.py
================================================
#!/usr/bin/env python
"""
.. codeauthor:: Nathan Lyons <nlyons@tulane.edu>

.. sectionauthor:: Nathan Lyons <nlyons@tulane.edu>
"""
from .species_evolver import SpeciesEvolver
from .zone import Zone
from .zone_controller import ZoneController
from .zone_taxon import ZoneTaxon

__all__ = ["SpeciesEvolver", "Zone", "ZoneTaxon", "ZoneController"]



================================================
File: species_evolution/base_taxon.py
================================================
#!/usr/bin/env python
"""Base Taxon of SpeciesEvolver."""
from abc import ABC
from abc import abstractmethod


class Taxon(ABC):
    """Base Taxon of SpeciesEvolver.

    A SpeciesEvolver Taxon represents a group of organisms. Examples of groups
    represented by this object include an analogue group (e.g., seed plants in
    general, a specific trout species), a taxonomic level (e.g., phylum,
    species, population). More generally, Taxon subclasses act as approaches to
    simulate evolution of biologic groups. Taxon was designed to represent
    biologic groups, although they could be individual organisms.

    This class is intended to be subclassed. Subclasses must implement the
    attributes and methods of this base class that are designated as abstract.
    The methods must take the same parameters and both the attributes and
    methods must return the values described in their docstrings.
    """

    def __init__(self):
        """Instantiate a Taxon object.

        A base taxon object cannot be instantiated. This initialization method
        can be called by `super` in the initialization method of a subclass to
        set initial values of required private properties.
        """
        self._extant = True
        self._tid = None
        self._parent = None

    def __repr__(self):
        return f"<{self.__class__.__name__}, tid={self._tid}>"

    @property
    def tid(self):
        """The identifier of the taxon.

        The identifier is an integer automatically and uniquely assigned by
        SpeciesEvolver once the component begins tracking the taxon. It is
        read-only as it should not be changed once this parameter is set.
        """
        return self._tid

    @property
    def extant(self):
        """The living state of the taxon.

        The taxon lives at the current model time if ``True``. The taxon is
        extinct as of the current model time if ``False``.
        """
        return self._extant

    @extant.setter
    def extant(self, state):
        """Set the living state of the taxon."""
        self._extant = state

    @property
    def parent(self):
        """The parent taxon.

        The parent is the taxon object that produced this object. A value of
        ``None`` indicates no parent taxon.
        """
        return self._parent

    @parent.setter
    def parent(self, taxon):
        """Set the parent taxon."""
        self._parent = taxon

    @property
    @abstractmethod
    def range_mask(self):
        """A mask of the taxon geographic extent.

        The range mask is a boolean numpy array where True values indicate
        where the taxon is located in the model grid associated with a
        SpeciesEvolver instance.

        This property must be implemented in a subclass.
        """
        # pragma: no cover

    @abstractmethod
    def _evolve(self, dt, stage, record):
        """Run the evolutionary processes of the taxon.

        SpeciesEvolver loops through the evolution processes of extant taxa in
        stages during the ``run_one_step`` method of the component. Therefore
        if a taxon type requires all other taxa to undergo some processing
        before an evolution process, then the taxon can evolve at a later
        stage using the ``stage`` parameter. Taxon subclasses should be
        designed with as few stages in this method as possible.

        This method at each stage must return both a boolean indicating if the
        taxon has additional stages to run and a list of child taxa produced
        during that evolution stage. The ``evolve`` method of child taxon will
        be called in stages subsequent to the stage the child taxon was
        produced. An empty list indicates no child taxon.

        See this method in ``ZoneTaxon`` for an example implementation.

        This method must be implemented in a subclass.

        Parameters
        ----------
        dt : float
            The model time step duration.
        stage : int
            The evolution stage of the time step.
        record : defaultdict
            The SpeciesEvolver record.

        Returns
        -------
        boolean
           Indicates if the taxon is still evolving. When `False` is returned,
           this method will not be called for the taxon in subsequent stages in
           the current model time step.
        list of Taxon
            The children produced by the taxon at a given stage. The ``evolve``
            method of child taxon will be called in stages following the stage
            the child taxon was produced. An empty list indicates no child
            taxon.
        """
        # pragma: no cover



================================================
File: species_evolution/record.py
================================================
#!/usr/bin/env python
"""Structure to store data over time for SpeciesEvolver."""
from collections import OrderedDict

import numpy as np
from pandas import DataFrame


class Record:
    """Structure to store data over time for SpeciesEvolver.

    This object is intended to be used internally by the SpeciesEvolver
    component and its objects.

    The record attribute, ``_dict`` is a dictionary that stores the data of the
    record. The dictionary key, 'time' is created when the record is
    initialized. All other dictionary keys are variables of the record.

    The values of variables correspond to the time values elementwise. For
    example, if the time values are [0, 1, 2] and the values of var1 are
    [5, 6, 7] then the values of var 1 at time 0, 1, and 2 are 5, 6, and 7,
    respectively.

    A new record entry is created by advancing the record time with the method,
    ``advance_time``. Variables can be added to the record with the method,
    ``set_value``. Variable values not set at a time have a value of `nan`.
    """

    def __init__(self, initial_time=0):
        """Instantiate Record.

        Parameters
        ----------
        initial_time : float or int
            The initial time in the record.
        """
        self._dict = OrderedDict([("time", [initial_time])])

    @property
    def times(self):
        """The times stored in the record."""
        return self._dict["time"]

    @property
    def prior_time(self):
        """The penultimate time in the record.

        nan is returned if the record stores only one time step.
        """
        if len(self.times) < 2:
            return np.nan
        else:
            return sorted(self.times)[-2]

    @property
    def earliest_time(self):
        """The earliest time in the record."""
        return min(self.times)

    @property
    def latest_time(self):
        """The latest time in the record."""
        return max(self.times)

    @property
    def count_of_time_steps(self):
        """The count of record time steps."""
        return len(self._dict["time"])

    @property
    def variables(self):
        """The variables in the record."""
        variables = list(self._dict.keys())
        variables.remove("time")
        return variables

    @property
    def data_frame(self):
        """A Pandas DataFrame of the record."""
        return DataFrame(self._dict)

    def __len__(self):
        """The count of record time steps."""
        return self.count_of_time_steps

    def advance_time(self, dt):
        """Advance the time in the record by a time step duration.

        This method creates a new record entry. The new entry includes the time
        and all variables have a value of nan for this time. The time of the
        entry is the sum of the latest time of the record and the time step
        duration, ``dt``.

        Parameters
        ----------
        dt : float or int
            The time step duration to advance time in the record.
        """
        self._dict["time"].append(self.latest_time + dt)

        for var in self.variables:
            self._dict[var].append(np.nan)

    def get_value(self, var_name, time=np.nan):
        """Get the value of a variable.

        Parameters
        ----------
        var_name : string
            The name of the variable to get.
        time : float or int, optional
            The time of the variable to get. The latest time in the record is
            the default.

        Returns
        -------
        T
            The value of `var_name` that has the type, T. `nan` is returned if
            `var_name` does not exist in the record.
        """
        if var_name not in self.variables:
            return np.nan

        time = self._check_time(time)

        idx = self._get_time_index(time)
        return self._dict[var_name][idx]

    def set_value(self, var_name, value, time=np.nan):
        """Set the value of a variable.

        Parameters
        ----------
        var_name : string
            The name of the variable to set. A new variable is created if
            `var_name` does not exist in the record.
        value : T
            The value of `var_name`. The type, T should be an appropriate type
            for `var_name`.
        time : float or int, optional
            The time in the record to change a variable. The latest time in the
            record is the default.
        """
        time = self._check_time(time)

        if var_name not in self.variables:
            self._dict[var_name] = [np.nan] * (self.count_of_time_steps)

        idx = self._get_time_index(time)
        self._dict[var_name][idx] = value

    def increment_value(self, var_name, increase, time=np.nan):
        """Increment the value of a variable.

        Parameters
        ----------
        var_name : string
            The name of the variable to set. A new variable is created if
            `var_name` does not exist in the record.
        increase : T
            The value to increase `var_name`. The type, T should be an
            appropriate type for `var_name`.
        time : float or int, optional
            The time in the record to change a variable. The latest time in the
            record is the default.
        """
        time = self._check_time(time)
        idx = self._get_time_index(time)

        if var_name in self.variables:
            vals = self._dict[var_name]

            if np.isnan(vals[idx]):
                vals[idx] = increase
            else:
                vals[idx] += increase
        else:
            self._dict[var_name] = [np.nan] * self.count_of_time_steps
            self._dict[var_name][idx] = increase

    def _get_time_index(self, time):
        return np.where(np.array(self.times) == time)[0][0]

    def _check_time(self, time):
        if np.isnan(time):
            time = self.latest_time
        elif time not in self.times:
            raise ValueError(f"the time, {time} not in record")

        return time



================================================
File: species_evolution/species_evolver.py
================================================
#!/usr/bin/env python
"""Evolve life in a landscape.

Life evolves alongside landscapes by biotic and abiotic processes under complex
dynamics at Earth's surface. Researchers who wish to explore these dynamics can
use this component as a tool for them to build landscape-life evolution models.
Landlab components, including SpeciesEvolver are designed to work with a shared
model grid. Researchers can build novel models using plug-and-play surface
process components to evolve the grid's landscape alongside the life tracked by
SpeciesEvolver. The simulated life evolves following customizable processes.

Component written by Nathan Lyons beginning August 2017.
"""
from collections import OrderedDict

import numpy as np
from pandas import DataFrame

from landlab import Component

from .record import Record


class SpeciesEvolver(Component):
    """Evolve life in a landscape.

    This component tracks ``Taxon`` objects as they evolve in a landscape. The
    component calls the evolutionary process methods of tracked ``Taxon``
    objects. ``Taxon`` are intended to be subclassed for unique behavior,
    attributes, and model approaches, including different implementations of
    evolutionary processes.

    The general workflow to use this component in a model is

    1. Instantiate the component.
    2. Instantiate taxa.
    3. Introduce taxa to SpeciesEvolver using the ``track_taxon`` method.
    4. Advance the component instance in time using ``run_one_step`` method.

    Taxa can be introduced at model onset and later time steps. Multiple types
    can be tracked by the same SpeciesEvolver instance.

    The taxon type, ``ZoneTaxon`` is distributed with SpeciesEvolver. The
    spatial aspect of ``ZoneTaxon`` macroevolutionary processes is determined
    using ``Zone`` objects. A ``ZoneController`` is used to create and manage
    zones as well as efficiently create multiple ZoneTaxon objects. See the
    documentation of ``ZoneController`` and ``ZoneTaxon`` for more information.
    SpeciesEvolver knows nothing about zones and their controller, meaning the
    concept of zones are not required for other taxon types.

    Model time and other variables can be viewed with the class attribute,
    ``record_data_frame``. Time is recorded to track the history of taxa
    lineages. The unit of time is not considered within the component other
    than the record, and can be thought of as in years or whatever unit is
    needed. Time is advanced with the ``dt`` parameter of the ``run_one_step``
    method.

    The geographic ranges of the taxa at the current model time are evaluated
    during the ``run_one_step`` method. Each taxon object determines if it
    persists or becomes extinct, and if it creates child ``Taxon`` objects.
    Metadata of all taxa introduced to the component can be viewed with the
    attribute, ``taxa_data_frame``.

    Taxa are automatically assigned unique taxon identifiers, ``tid``.
    Identifiers are used to reference and retrieve taxon objects. Identifiers
    are assigned in the order taxa are introduced to SpeciesEvolver.

    Examples
    --------
    The evolution of a lowland taxa lineage in response to mountain range
    formation is simulated using ZoneTaxon managed by ZoneController. Mountain
    range formation is forced without processes for simplicity in this example.

    Import modules used in the following examples.

    >>> from landlab import RasterModelGrid
    >>> from landlab.components import SpeciesEvolver
    >>> from landlab.components.species_evolution import ZoneController

    Create a model grid with mountain scale resolution. The elevation is
    equally low throughout the grid at model onset.

    >>> mg = RasterModelGrid((3, 7), 1000)
    >>> z = mg.add_ones("topographic__elevation", at="node")
    >>> z.reshape(mg.shape)
    array([[1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1.]])

    Instantiate the component with the grid as the first parameter.

    >>> se = SpeciesEvolver(mg)

    ZoneController requires a function that returns a mask of the total extent
    of taxa habitat. The mask is a boolean array where `True` values represent
    nodes that satisfy habitat conditions. Zone objects are not created here.
    The mask only maps the extent where taxa can exist. This function returns
    `True` where elevation is below 100, which is where the simulated lowland
    taxa of this model can inhabit.

    >>> def zone_func(grid):
    ...     return grid.at_node["topographic__elevation"] < 100
    ...

    Instantiate ZoneController with the grid and zone function. The initial
    zones are created at controller instantiation. In this example, one zone is
    created because all nodes of the zone mask are adjacent to each other.

    >>> zc = ZoneController(mg, zone_func)
    >>> len(zc.zones) == 1
    True

    Additional examples of controller usage are provided in ``ZoneController``
    documentation.

    The ``mask`` of the zone is True where the conditions of the zone function
    are met. All nodes of the grid are included because the elevation of each
    node is below 100. The ``zones`` attribute of ``ZoneController`` returns a
    list of the zones that currently exist in the model. Below we return the
    mask of the single zone by indexing this list.

    >>> zc.zones[0].mask
    array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
            True,  True,  True,  True,  True,  True,  True,  True,  True,
            True,  True,  True])

    Populate a taxon to the zone.

    >>> taxon = zc.populate_zones_uniformly(1)
    >>> se.track_taxa(taxon)

    The attribute, ``taxa_data_frame`` indicates only the one taxon exists
    because we populated each zone with one taxon, and only the one zone
    exists.

    >>> se.taxa_data_frame
    pid  type    t_first  t_final      tid
    0    <NA>  ZoneTaxon        0     <NA>

    The identifier of the taxon, ``tid`` is 0. The identifier of the taxon's
    parent, ``pid``, is '<NA>' because it does not have a parent taxon given
    that it was manually introduced using the ``track_taxa`` method. The taxon
    was introduced at time, ``t_first`` and time, ``t_final`` is '<NA>'
    because the taxon remains extant. See the documentation of this attribute
    for further explanation of data frame columns.

    Force a change in the zone mask to demonstrate component functionality.
    Here we begin a new time step where topography is uplifted by 200 that
    forms a ridge trending north-south in the center of the grid.

    >>> z[[3, 10, 17]] = 200
    >>> z.reshape(mg.shape)
    array([[  1.,   1.,   1., 200.,   1.,   1.,   1.],
           [  1.,   1.,   1., 200.,   1.,   1.,   1.],
           [  1.,   1.,   1., 200.,   1.,   1.,   1.]])

    The current elevation, the elevation following uplift, is represented here.
    ::

        - - - ^ - - -       elevation:  - 1
        - - - ^ - - -                   ^ 200
        - - - ^ - - -

    The updated zone mask is below.
    ::

        . . . x . . .       key:    . node in zone mask
        . . . x . . .               x node outside of zone mask
        . . . x . . .

    Run a step of both the ZoneController and SpeciesEvolver. Both are run to
    keep time in sync between the ``ZoneController``and ``SpeciesEvolver``
    instances.
    >>> delta_time = 1000
    >>> zc.run_one_step(delta_time)
    >>> se.run_one_step(delta_time)

    Two zones exist following this time step.

    >>> len(zc.zones) == 2
    True

    An additional zone was created because the zone mask was not continuous.
    ::

        . . . ^ * * *       key:    . a zone
        . . . ^ * * *               * another zone
        . . . ^ * * *               ^ mountain range

    The split of the initial zone triggered speciation of taxon 1 by taxon 0.

    >>> se.taxa_data_frame
    pid  type    t_first  t_final      tid
    0    <NA>  ZoneTaxon        0     <NA>
    1       0  ZoneTaxon     1000     <NA>

    The phylogenetic tree of the simulated taxa is represented below. The
    number at the line tips are the taxa identifiers.
    ::

          0 ──────┬── 0
                  │
                  └── 1
            _________
            0    1000
              time

    The split of the initial zone into two zones at time 1000 triggered taxon 0
    to speciate. Taxon 0 occupies a zone on one side of the mountain range, and
    the child, taxon 1 occupies a zone on the other side. This outcome is the
    result of the evolutionary processes programmed within ``ZoneTaxon`` as
    well as the parameters used in this example (default values were used
    as optional parameters were not set). Different behavior can be achieved by
    subclassing ``ZoneTaxon`` or ``Taxon``.

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Lyons, N.J., Albert, J.S., Gasparini, N.M. (2020). SpeciesEvolver: A
    Landlab component to evolve life in simulated landscapes. Journal of Open
    Source Software 5(46), 2066, https://doi.org/10.21105/joss.02066

    **Additional References**

    Albert, J.S., Schoolmaster Jr, D.R., Tagliacollo, V., Duke-Sylvester, S.M.
    (2016). Barrier displacement on a neutral landscape: Toward a theory of
    continental biogeography. Systematic Biology 66(2), 167–182.

    Lyons, N.J., Val, P., Albert, J.S., Willenbring, J.K., Gasparini, N.M., in
    review. Topographic controls on divide migration, stream capture, and
    diversification in riverine life. Earth Surface Dynamics.

    """

    _name = "SpeciesEvolver"

    _unit_agnostic = True

    _info = {
        "taxa__richness": {
            "dtype": int,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "The number of taxa at each node",
        }
    }

    _cite_as = """@article{lyons2020species,
        author = {Lyons, N.J. and Albert, J.S. and Gasparini, N.M.},
        title = {SpeciesEvolver: A Landlab component to evolve life in simulated landscapes},
        year = {2020},
        journal = {Journal of Open Source Software},
        volume = {5},
        number = {46},
        doi = {10.21105/joss.02066},
        url = {https://doi.org/10.21105/joss.02066}
        }"""

    def __init__(self, grid, initial_time=0):
        """Instantiate SpeciesEvolver.

        Parameters
        ----------
        grid : ModelGrid
            A Landlab ModelGrid.
        initial_time : float, int, optional
            The initial time. The unit of time is not considered within the
            component, with the exception that time is logged in the record.
            The default value of this parameter is 0.
        """
        super().__init__(grid)

        # Create data structures.

        self._record = Record(initial_time)
        self._record.set_value("taxa", 0)

        self._taxa_data = OrderedDict(
            [("tid", []), ("pid", []), ("type", []), ("t_first", []), ("t_final", [])]
        )

        self._taxon_objs = []

        # Create a taxa richness field.

        _ = grid.add_zeros("taxa__richness", at="node", dtype=int, clobber=True)

    @property
    def record_data_frame(self):
        """A Pandas DataFrame of SpeciesEvolver variables over time.

        Each row is data of a model time step. The time of the step is recorded
        in the `time` column. `taxa` is the count of taxa extant at a time.
        Additional columns can be added and updated by SpeciesEvolver objects
        during the component ``run_one_step`` method. See documention of Taxon
        objects for an explanation of these columns.

        The DataFrame is created from a dictionary associated with a
        SpeciesEvolver ``Record`` object. nan values in Pandas DataFrame force
        the column to become float values even when data are integers. The
        original value type is retained in the ``Record`` object.
        """
        return self._record.data_frame

    @property
    def taxa_data_frame(self):
        """A Pandas DataFrame of taxa metadata.

        Each row is the metadata of a taxon. The column, ``tid`` is the taxon
        identifier assigned when SpeciesEvolver begins tracking the taxon. The
        column, ``pid`` is the tid of the parent of the taxon. A pid of `<NA>`
        indicates no parent taxon. ``type`` is the type of ``Taxon`` object.
        ``t_first`` is the initial model time the taxon was added to
        SpeciesEvolver. ``t_final`` is the model time the taxon was recognized
        as extinct. A t_final of `<NA>` indicates the taxon is extant.

        Additional columns may be added by some taxon types. See the
        documentation of these taxa for column description.

        The DataFrame is created from a data structure within the component.
        """
        data = self._taxa_data
        cols = list(data.keys())
        cols.remove("tid")
        df = DataFrame(data, columns=cols, index=data["tid"])
        df.index.name = "tid"

        # Change column number type because pandas makes a column float if it
        # includes nan values.
        df["pid"] = df["pid"].astype("Int64")
        if all(isinstance(item, int) for item in data["t_final"] if not np.isnan(item)):
            df["t_final"] = df["t_final"].astype("Int64")

        return df

    def run_one_step(self, dt):
        """Update the taxa for a single time step.

        This method advances the model time in the component record, calls the
        evolve method of taxa extant at the current time, and updates the
        variables in the record and taxa dataframes.

        Parameters
        ----------
        dt : float
            The model time step duration. Time in the record is advanced by the
            value of this parameter.
        """
        record = self._record
        record.advance_time(dt)

        # Create a dictionary of the taxa to update at the current model time.
        # Keys are objects of extant taxa. Values are booleans indicating if
        # stages remain for respective taxa.

        time_dict = OrderedDict.fromkeys(self._taxon_objs, True)

        # Iteratively call taxa ``_evolve`` method until all stages of all taxa
        # have run.

        stage = 0

        while any(time_dict.values()):
            # Run evolution stage.

            stage_dict = OrderedDict([])
            evolving_taxa = filter(time_dict.get, time_dict)

            for taxon in evolving_taxa:
                # Run evolution stage of taxon with remaining stages.
                stages_remain, taxon_children = taxon._evolve(dt, stage, record)

                if taxon_children:
                    stage_dict.update(
                        OrderedDict.fromkeys(taxon_children, stages_remain)
                    )

                stage_dict[taxon] = stages_remain and taxon.extant

            time_dict.update(stage_dict)

            stage += 1

        self._update_taxa_data(time_dict.keys())

    def track_taxa(self, taxa):
        """Add taxa to be tracked over time by SpeciesEvolver.

        The taxon/taxa are introduced at the latest time in the record and
        also tracked during following model times. Each taxon is assigned an
        identifier and then can be viewed in ``taxa_data_frame``.

        Parameters
        ----------
        taxa : Taxon or list of Taxon
            The taxa to introduce.

        Examples
        --------
        ZoneTaxon are used to demonstrate this method.

        Import modules used in the following examples.

        >>> from landlab import RasterModelGrid
        >>> from landlab.components import SpeciesEvolver
        >>> from landlab.components.species_evolution import ZoneController

        Create a model grid with flat topography.

        >>> mg = RasterModelGrid((3, 7), 1000)
        >>> z = mg.add_ones("topographic__elevation", at="node")

        Instantiate SpeciesEvolver and a ZoneController. Instantiate the
        latter with a function that masks the low elevation zone extent. Only
        one zone is created.

        >>> se = SpeciesEvolver(mg)
        >>> def zone_func(grid):
        ...     return grid.at_node["topographic__elevation"] < 100
        ...
        >>> zc = ZoneController(mg, zone_func)
        >>> len(zc.zones) == 1
        True

        Track the taxon of the one zone.

        >>> taxon = zc.populate_zones_uniformly(1)
        >>> se.track_taxa(taxon)

        The one taxon is now tracked by SpeciesEvolver as indicated by the taxa
        DataFrame.

        >>> se.taxa_data_frame
        pid  type    t_first  t_final      tid
        0    <NA>  ZoneTaxon        0     <NA>
        """
        if not isinstance(taxa, list):
            taxa = [taxa]

        self._update_taxa_data(taxa)

    def _update_taxa_data(self, taxa_at_time):
        """Update the taxa data structure, set identifiers, and taxa statistics.

        This method sets identifiers and metadata for the newly introduced
        taxa. For the previously introduced, this method updates the
        'latest_time` value of the taxa metadata.

        Parameters
        ----------
        taxa_at_time : list of Taxon
            The taxa at the current model time.
        """
        time = self._record.latest_time
        data = self._taxa_data
        objs = self._taxon_objs

        t_recorded = self._taxon_objs
        t_introduced = [taxon for taxon in taxa_at_time if taxon in t_recorded]
        t_new = [taxon for taxon in taxa_at_time if taxon not in t_recorded]

        # Update previously introduced taxa.

        for taxon in t_introduced:
            if not taxon.extant:
                idx = data["tid"].index(taxon.tid)
                data["t_final"][idx] = time
                objs.remove(taxon)

        # Set the data of new taxa.

        for taxon in t_new:
            # Set identifier.

            if data["tid"]:
                taxon._tid = max(data["tid"]) + 1
            else:
                taxon._tid = 0

            # Append taxon data.

            data["tid"].append(taxon.tid)
            if taxon.parent is not None:
                data["pid"].append(taxon.parent.tid)
            else:
                data["pid"].append(np.nan)
            data["type"].append(type(taxon).__name__)
            data["t_first"].append(time)
            if taxon.extant:
                data["t_final"].append(np.nan)
                objs.append(taxon)
            else:
                data["t_final"].append(time)

        # Update taxa stats.

        self._record.set_value("taxa", len(objs))

        self._grid.at_node["taxa__richness"] = self._get_taxa_richness_map()

    def get_extant_taxon_objects(self, tids=np.nan, ancestor=np.nan, time=np.nan):
        """Get extant taxon objects filtered by parameters.

        This method returns all taxon objects tracked by the component when no
        optional parameters are included. The objects returned can be limited
        using one or more parameters.

        Parameters
        ----------
        tids : list of int, optional
            The taxa with these identifiers will be returned. A list is
            returned even if only one object is contained within the list. By
            default, when `tids` is not specified, extant taxa with any
            identifier can be returned.
        ancestor : int, optional
            Limit the taxa returned to those descending from the taxon
            designated as the ancestor. The ancestor is designated using its
            ``tid``. By default, taxa with any or no ancestors are returned.
        time : float, int, optional
            Limit the taxa returned to those that were extant at the time
            designated by this parameter as well as extant at the current model
            time. By default, extant taxa at all of the times listed in the
            component record can be returned.

        Returns
        -------
        taxa : a list of Taxon
            The Taxon objects that pass through the filter. The list is sorted
            by ``tid``. An empty list is returned if no taxa pass through the
            filter.

        Examples
        --------
        ZoneTaxon are used to demonstrate this method.

        Import modules used in the following examples.

        >>> from landlab import RasterModelGrid
        >>> from landlab.components import SpeciesEvolver
        >>> from landlab.components.species_evolution import ZoneController

        Create a model grid.

        >>> mg = RasterModelGrid((3, 7), 1000)
        >>> z = mg.add_ones("topographic__elevation", at="node")

        Instantiate SpeciesEvolver and a ZoneController. Instantiate the latter
        with a function that masks the low elevation zone extent. Only one zone
        is created.

        >>> se = SpeciesEvolver(mg)
        >>> def zone_func(grid):
        ...     return grid.at_node["topographic__elevation"] < 100
        ...
        >>> zc = ZoneController(mg, zone_func)
        >>> len(zc.zones) == 1
        True

        Introduce two taxa to the zone.

        >>> taxa = zc.populate_zones_uniformly(2)
        >>> se.track_taxa(taxa)

        Force north-south mountain ranges over two time steps that drives taxa
        evolution.

        >>> z[mg.x_of_node == 2000] = 200
        >>> zc.run_one_step(1000)
        >>> se.run_one_step(1000)
        >>> z[mg.x_of_node == 4000] = 200
        >>> zc.run_one_step(1000)
        >>> se.run_one_step(1000)

        Display taxa metadata.

        >>> se.taxa_data_frame
        pid  type  t_first    t_final      tid
        0    <NA>  ZoneTaxon        0     <NA>
        1    <NA>  ZoneTaxon        0     <NA>
        2       0  ZoneTaxon     1000     <NA>
        3       1  ZoneTaxon     1000     <NA>
        4       0  ZoneTaxon     2000     <NA>
        5       1  ZoneTaxon     2000     <NA>

        Objects of all extant taxon are returned when no parameters are
        inputted.

        >>> se.get_extant_taxon_objects()
        [<ZoneTaxon, tid=0>,
         <ZoneTaxon, tid=1>,
         <ZoneTaxon, tid=2>,
         <ZoneTaxon, tid=3>,
         <ZoneTaxon, tid=4>,
         <ZoneTaxon, tid=5>]

        The returned objects of extant species can be limited using parameters.
        Here, get the taxon objects with identifiers, 4 and 5.

        >>> se.get_extant_taxon_objects(tids=[4, 5])
        [<ZoneTaxon, tid=4>, <ZoneTaxon, tid=5>]

        Extant taxon objects descending from a taxon can be obtained using the
        ``ancestor`` property. Here, get the taxa that descended from taxon 0.

        >>> se.get_extant_taxon_objects(ancestor=0)
        [<ZoneTaxon, tid=2>, <ZoneTaxon, tid=4>]

        Taxa can be limited to those that were extant ``time``.

        >>> se.get_extant_taxon_objects(time=1000)
        [<ZoneTaxon, tid=0>,
         <ZoneTaxon, tid=1>,
         <ZoneTaxon, tid=2>,
         <ZoneTaxon, tid=3>]

        The returned taxa can be further limited by including multiple
        method properties.

        >>> se.get_extant_taxon_objects(ancestor=0, time=1000)
        [<ZoneTaxon, tid=2>]

        An empty list is returned when no extant taxa match parameter criteria.

        >>> se.get_extant_taxon_objects(tids=[11])
        []
        """
        # Create `results` that contains tids of the taxa matching parameter
        # criteria.

        extant_tids = [taxon.tid for taxon in self._taxon_objs]
        results = set(extant_tids)

        data = self._taxa_data

        # Query by identifiers.

        if isinstance(tids, list):
            results = results.intersection(tids)

        # Query by ancestor.

        if not np.isnan(ancestor) and ancestor in data["tid"]:
            df = self.taxa_data_frame
            df["pid"] = df["pid"].fillna(-1)

            taxon = ancestor

            descendants = []
            stack = [taxon]

            while stack:
                children = df.index[df["pid"] == taxon].tolist()

                if children:
                    descendants.extend(children)
                    stack.extend(children)

                stack.remove(taxon)

                if stack:
                    taxon = stack[0]

            results = results.intersection(descendants)
        elif not np.isnan(ancestor):
            results = []

        # Query by time.

        if not np.isnan(time):
            t_first = np.array(data["t_first"])
            t_latest = np.nan_to_num(data["t_final"], nan=self._record.latest_time)
            mask = np.all([time >= t_first, time <= t_latest], 0)
            results = results.intersection(np.array(data["tid"])[mask].tolist())

        # Get the Taxon objects that match all parameter query results.

        taxa = [taxon for taxon in self._taxon_objs if taxon.tid in results]
        taxa.sort(key=lambda taxon: taxon.tid)

        return taxa

    def _get_taxa_richness_map(self):
        """Get a map of the number of taxa."""
        objs = self._taxon_objs

        if objs:
            masks = np.stack([taxon.range_mask for taxon in objs])
            richness_mask = masks.sum(axis=0).astype(int)
        else:
            richness_mask = np.zeros(self._grid.number_of_nodes, dtype=int)

        return richness_mask



================================================
File: species_evolution/zone.py
================================================
#!/usr/bin/env python
"""Zone functions and class of SpeciesEvolver."""
from collections import OrderedDict
from enum import IntEnum
from enum import unique

import numpy as np
from pandas import Series


@unique
class Connection(IntEnum):
    """Zone connection type.

    Connection type represents the connectivity of zones in two time steps. It
    is described with the pattern, x-to-y where x and y are the descriptive
    counts (none, one, or many) of zone(s) at the earlier and later time step,
    respectively. For example, a connection type of one-to-many is a zone in
    the earlier time step that spatially overlaps multiple zones in the later
    time step.
    """

    NONE_TO_NONE = 0
    NONE_TO_ONE = 1
    ONE_TO_NONE = 2
    ONE_TO_ONE = 3
    ONE_TO_MANY = 4
    MANY_TO_ONE = 5
    MANY_TO_MANY = 6


def _update_zones(grid, prior_zones, new_zones, record):
    """Resolve the spatial connectivity of zones across two time steps.

    This method iterates over each zone of the prior time step to identify the
    zones of the current time step it spatially intersects. This method updates
    the zone attribute, ``successors``. Successor zones are the new zones
    existing at the current time step that are the continuation of prior time
    step zones referred to as the predecessor zones.

    The type of connection between predecessor and successor zones is described
    ``Connection``. The `_conn_type` property of zones are set by this method.

    In the `many` connections, a rule determines which of the prior zones
    persist as the zone in the current time step. The zone with the greatest
    area of intersection between the prior and current time steps persists to
    the current time step along with the others in `new_zones`.

    Parameters
    ----------
    grid : ModelGrid
        A Landlab ModelGrid.
    prior_zones : Zone list
        The zones of the prior timestep.
    new_zones : Zone list
        The zones of the current timestep.
    record : Record
        The ZoneController record.

    Returns
    -------
    list of Zones
        The successor zones. The zones determined to exist at the current time
        step with an updated ``successors`` attribute.
    """
    # Stats are calculated for `record`.

    fragment_ct = 0
    capture_ct = 0
    area_captured = [0]

    # Get `successors`.

    if len(prior_zones) == 0 and len(new_zones) == 0:
        successors = []

    elif len(prior_zones) == 0 and len(new_zones) > 0:
        successors = new_zones
        conn_type = _determine_connection_type(0, 1)
        for n in new_zones:
            n._conn_type = conn_type
            n._successors = [n]

    elif len(prior_zones) > 0 and len(new_zones) == 0:
        successors = []
        conn_type = _determine_connection_type(1, 0)
        for p in prior_zones:
            p._conn_type = conn_type
            p._successors = []

    elif len(prior_zones) > 0 and len(new_zones) > 0:
        successors = []

        # Get index maps for the prior zones (`ps`) and new zones (`ns`).
        ps_index_map = _create_index_map(grid, prior_zones)
        ns_index_map = _create_index_map(grid, new_zones)

        replacements = OrderedDict()

        for i_p, p in enumerate(prior_zones):
            # Retain a copy of the prior zone mask to compare with the new zone
            # mask after the zone masks are updated.
            p_mask_copy = p.mask.copy()

            # Get the new zones that intersect (`i`) the prior zone.
            ns_i_p = _intersecting_zones(ps_index_map == i_p, ns_index_map, new_zones)
            ns_i_p_ct = len(ns_i_p)

            # Get the other prior zones that intersect the new zones.
            ns_mask = np.any([n.mask for n in ns_i_p], axis=0)
            ps_i_ns = _intersecting_zones(ns_mask, ps_index_map, prior_zones)
            ps_i_ns_ct = len(ps_i_ns)

            if ps_i_ns_ct == 0:
                ps_i_ns_ct = 1
                ps_i_ns = p

            # Get successors depending on connection type.

            conn_type = _determine_connection_type(ps_i_ns_ct, ns_i_p_ct)

            p_successors = _get_successors(
                p,
                conn_type,
                ps_i_ns,
                ns_i_p,
                prior_zones,
                ps_index_map,
                replacements,
                successors,
            )

            # Update statistics.

            if conn_type in [Connection.MANY_TO_ONE, Connection.MANY_TO_MANY]:
                # Copy successors to be `captured_zones`.
                captured_zones = list(p_successors)
                if p in captured_zones:
                    captured_zones.remove(p)
                capture_ct += len(captured_zones)

                for z in captured_zones:
                    captured_mask = np.all([~p_mask_copy, z.mask], 0)
                    area = grid.cell_area_at_node[captured_mask.flatten()].sum()
                    area_captured.append(area)

            elif conn_type in [Connection.ONE_TO_MANY, Connection.MANY_TO_MANY]:
                fragment_ct += ns_i_p_ct

            # Set connection.

            p._conn_type = conn_type
            p._successors = p_successors

            successors.extend(p_successors)

        for key, value in replacements.items():
            key._mask = value.mask

        # Get unique list of successors, preserving order.

        successors = Series(successors).drop_duplicates().tolist()

    # Update the record.

    record.increment_value("fragmentations", fragment_ct)
    record.increment_value("captures", capture_ct)
    record.increment_value("area_captured_sum", sum(area_captured))

    old_value = record.get_value("area_captured_max")
    old_value_is_nan = np.isnan(old_value)
    new_value_is_greater = max(area_captured) > old_value

    if old_value_is_nan or new_value_is_greater:
        record.set_value("area_captured_max", max(area_captured))

    return successors


def _create_index_map(grid, zones):
    i = np.where([z.mask for z in zones])

    index_map = np.zeros(grid.number_of_nodes, dtype=int)
    index_map[:] = -1
    index_map[i[1]] = i[0]

    return index_map


def _intersecting_zones(condition, zone_index_map, zone_list):
    mask = np.all([condition, zone_index_map > -1], 0)
    indices = np.unique(zone_index_map[np.argwhere(mask)])
    zones = [zone_list[i] for i in indices]
    return zones


def _determine_connection_type(prior_zone_count, new_zone_count):
    """Get the connection type based on the count of prior and new zones."""
    if prior_zone_count == 0:
        if new_zone_count == 1:
            return Connection.NONE_TO_ONE

    elif prior_zone_count == 1:
        if new_zone_count == 0:
            return Connection.ONE_TO_NONE
        elif new_zone_count == 1:
            return Connection.ONE_TO_ONE
        elif new_zone_count > 1:
            return Connection.ONE_TO_MANY

    elif prior_zone_count > 1:
        if new_zone_count == 1:
            return Connection.MANY_TO_ONE
        elif new_zone_count > 1:
            return Connection.MANY_TO_MANY


def _get_successors(
    p,
    conn_type,
    ps_i_ns,
    ns_i_p,
    prior_zones,
    ps_index_map,
    replacements,
    all_successors,
):
    if conn_type == Connection.ONE_TO_NONE:
        successors = []

    elif conn_type == Connection.ONE_TO_ONE:
        # The prior zone is set as the new zone because only the one new
        # and the one prior overlap.
        n = ns_i_p[0]
        replacements[p] = n
        successors = [p]

    elif conn_type in [Connection.ONE_TO_MANY, Connection.MANY_TO_MANY]:
        # Set the successors to the new zones that overlap p.
        # Although, replace the dominant n with p.

        dn = p._get_largest_intersection(ns_i_p, exclusions=list(replacements.values()))

        successors = []

        for n in ns_i_p:
            dp = n._get_largest_intersection(
                ps_i_ns, exclusions=list(replacements.keys())
            )

            if n == dn and n in replacements.values():
                d = _get_replacement(replacements, n)
                successors.append(d)
            elif n == dn:
                replacements[dp] = n
                successors.append(dp)
            elif n in replacements.values():
                d = _get_replacement(replacements, n)
                successors.append(d)
            else:
                successors.append(dp)
                replacements[dp] = n

    elif conn_type == Connection.MANY_TO_ONE:
        # Set the successor to the prior zone that intersects n the most.
        n = ns_i_p[0]
        dp = n._get_largest_intersection(ps_i_ns)

        if p == dp and n in replacements.values():
            successors = [_get_replacement(replacements, n)]
        elif p == dp:
            successors = [p]
            replacements[p] = n
        elif n in replacements.values():
            successors = [_get_replacement(replacements, n)]
        else:
            successors = [dp]
            replacements[dp] = n

    return successors


def _get_replacement(replacements, new_zone):
    for key, value in replacements.items():
        if value == new_zone:
            return key


class Zone:
    """Zone object of SpeciesEvolver.

    The nodes and attributes of the spatial entities that taxa populate. This
    class is not intended to be managed directly.
    """

    def __init__(self, controller, mask):
        """Initialize a zone.

        Parameters
        ----------
        controller : ZoneController
            A SpeciesEvolver ZoneController.
        mask : ndarray
            The mask of the zone. True elements of this array correspond to the
            grid nodes of the zone.
        """
        self._controller = controller
        self._mask = mask.flatten()
        self._conn_type = None
        self._successors = []

    @property
    def mask(self):
        """The mask of the zone."""
        return self._mask

    @property
    def successors(self):
        """A list of zones connected to zone at the current time step."""
        return self._successors

    @property
    def area(self):
        """Zone area calculated as the sum of cell area at grid nodes."""
        area = self._controller._grid.cell_area_at_node[self._mask].sum()
        return area

    def _get_largest_intersection(self, zones, exclusions=None):
        exclusions = set() if exclusions is None else set(exclusions)
        node_intersection_count = []
        for z in zones:
            if z in exclusions:
                node_intersection_count.append(-1)
            else:
                n = len(np.where(np.all([z.mask, self.mask], 0))[0])
                node_intersection_count.append(n)

        if all(x == -1 for x in node_intersection_count):
            return self

        return zones[np.argmax(node_intersection_count)]



================================================
File: species_evolution/zone_controller.py
================================================
#!/usr/bin/env python
"""ZoneController of SpeciesEvolver."""
import numpy as np
from scipy.ndimage import label

from .record import Record
from .zone import Zone
from .zone import _update_zones
from .zone_taxon import ZoneTaxon


class ZoneController:
    """Controls zones and populates them with taxa.

    This object manages 'zones' that are used to evaluate the spatial aspect of
    taxa. A zone represents a portion of a model grid. It is made up of
    spatially continuous grid nodes.

    This controller creates zones using the initialization parameter,
    ``zone_function``. This function identifies all of the grid nodes where
    zones are to be created. A zone is created for each cluster of spatially
    continuous nodes. Zones are updated also using this function when the
    ``run_one_step`` method of this controller is called.

    The structure of an example model grid is diagrammed below to demonstrate
    how zones are created. The grid contains six columns and six rows. In this
    example, the function returns True where node values are greater than 0.
    Nodes marked with an ``*`` are nodes that will belong to a zone because the
    mask is True at these nodes. All other nodes are marked with a ``·``. A
    zone is created for each cluster of continuous nodes where the mask is True.
    ::

        values         mask returned
        evaluated      by zone function
        0 0 0 0 0 0    · · · · · ·
        0 0 0 5 4 2    · · · * * *
        0 6 0 4 6 0    · * · * * ·
        0 2 0 0 0 0    · * · · · ·
        0 0 4 0 4 0    · · * · * ·
        0 0 0 0 0 0    · · · · · ·

    The above example is continued in the following four diagrams that
    demonstrate how individual zones are identified. Each zone is marked with a
    ``x``, ``o``, ``+``, or ``@``. Clusters can be identified using ``D8``
    where diagonal neighbors are included or ``D4`` where diagonal neighbors
    are excluded. A minimum zone area can be enforced with the ``minimum_area``
    initialization parameter.
    ::

        D8             D4             D8             D4
        min area = 0   min area = 0   min area = 2   min area = 2
        · · · · · ·    · · · · · ·    · · · · · ·    · · · · · ·
        · · · + + +    · · · + + +    · · · + + +    · · · + + +
        · x · + + ·    · x · + + ·    · x · + + ·    · x · + + ·
        · x · · · ·    · x · · · ·    · x · · · ·    · x · · · ·
        · · x · o ·    · · @ · o ·    · · x · · ·    · · · · · ·
        · · · · · ·    · · · · · ·    · · · · · ·    · · · · · ·

    The grid perimeter affects zone creation. Zones can include perimeter nodes
    (the nodes along grid edges), although because perimeter nodes are not
    associated with cells that have area, perimeter nodes do not contribute to
    the area summation of clusters. The area summation only takes into account
    the cells associated with core nodes.

    Creation of zones along boundaries is illustrated below. A zone extent mask
    different from the above example was produced by the hypothetical zone
    function in this example. Again ``*`` indicates where a zone can exist.
    Distinct zones include the symbols, ``$`` and ``#`` in addition to the
    symbols defined above. Individual zone masks and the count of zones are
    affected by the use of ``D8`` or ``D4`` along with the minimum area
    parameter, especially when zone clusters are along the grid parameter.
    ::

        zone function  D8             D4             D8             D4
        returned mask  min area = 0   min area = 0   min_area = 2   min_area = 2
        * · · * * ·    + · · x x ·    + · · x x ·    + · · · · ·    · · · · · ·
        · * · · * ·    · + · · x ·    · # · · x ·    · + · · · ·    · # · · · ·
        · * · · · ·    · + · · · ·    · # · · · ·    · + · · · ·    · # · · · ·
        * · · · * *    + · · · o o    $ · · · o o    + · · · o o    · · · · o o
        · · · · * ·    · · · · o ·    · · · · o ·    · · · · o ·    · · · · o ·
        · * * · · ·    · @ @ · · ·    · @ @ · · ·    · · · · · ·    · · · · · ·

    By default, ``ZoneTaxon`` are used with this controller, and the
    following paragraphs make that assumption. See the documentation of the
    populate methods to learn how to use other types. Speciation of
    ``ZoneTaxon`` objects occurs when the taxon exists in more than one zone
    once the allopatric wait time has been exceeded in that zone. See
    ``ZoneTaxon`` documentation for more about allopatric wait time.

    A different example grid demonstrates here the temporal connectivity of
    zones. The grid represents the time, ``T0`` with the nodes of a zone
    marked with ``x``. The following examples will use D8 neighborhoods and a
    minimum zone area of 0.
    ::

        T0
        · · · · · ·
        · · · · · ·
        · x x x x ·
        · x x x x ·
        · · · · · ·
        · · · · · ·

    Below are variations of the grid at a later time, ``T1`` in four variations
    where each contains one zone. In ``T1a``, ``T1b``, and ``T1c`` the zone
    stayed the same, moved, and changed size, respectively. Taxa migrate with
    the zone when at least one zone node overlaps between the two time steps.
    However, in ``T1d``, no nodes overlaps, therefore taxa do not disperse from
    the zone in T0 to the zone in T1d.
    ::

        T1a            T1b            T1c            T1d
        · · · · · ·    · · · · · ·    · · · · · ·    · · · · · ·
        · + + + + ·    · · · · · ·    · · + + · ·    · · · · · ·
        · + + + + ·    · + + + + ·    · + + + + ·    · · · · · ·
        · · · · · ·    · + + + + ·    · · · · + ·    · + + + + ·
        · · · · · ·    · · · · · ·    · · · · · ·    · + + + + ·
        · · · · · ·    · · · · · ·    · · · · · ·    · · · · · ·

    Another ``T1`` variation, now demonstrating two zones, ``+`` and ``x``.
    Multiple zones overlapping a zone in the prior time step can be interpreted
    as a zone that fragmented, which may affect resident taxa. The number of
    zone fragmentations can be viewed in the ``record_data_frame`` attribute.
    In the T1e example, the fragmentation count for time 1 would be 2 because
    2 zones that fragmented from a prior zone were recognized at this time.
    ::

        T1e
        · · · · · ·
        · · · · + ·
        · x · · + ·
        · x x · · ·
        · x x · · ·
        · · · · · ·

    The controller had to decide which of the two clusters of continuous nodes
    in T1 should be designated as the same zone in T0. This decision must be
    made in general when multiple clusters overlap the same zone in the prior
    time step. The zone in the current time step that overlaps the prior time
    step zone the most becomes the same zone in the earlier time step. In this
    example, the cluster to the right overlapped four nodes and the left
    cluster overlapped only one node, therefore the right cluster became the
    star zone. This is merely for creating new zones objects.

    The grid diagrammed below continues from T1e. The continuous nodes
    overlapped two zones in T1e. When multiple zones overlap, one zone is
    assumed to be the prior zone and the others are considered captured zones.
    The number of zone captures can be viewed in the ``record_data_frame``
    attribute.
    ::

        T2
        · · · · · ·
        · · · · · ·
        · x x x x ·
        · x x x · ·
        · · · · · ·
        · · · · · ·

    The controller had to again decide which of the two clusters of continuous
    nodes in T1e should be designated as the same zone in T2. This decision
    must be made in general when multiple clusters in the prior time step
    overlap a zone in the current time step. The zone in the prior time step
    that overlaps the current time step zone the most becomes the zone in the
    earlier time step. In this example, the cluster to the left overlapped two
    nodes and the right cluster overlapped only one node, therefore the new
    zone keeps the designation of the left cluster. However, this is merely for
    creating new zone objects.

    ZoneController is currently designed for use with only the grid type,
    ``RasterModelGrid``.

    Examples
    --------
    Import modules used in the following examples.

    >>> from landlab import RasterModelGrid
    >>> from landlab.components.species_evolution import ZoneController

    The first example uses the default parameters of ZoneController.

    Create a model grid and an elevation field for this grid.

    >>> mg = RasterModelGrid((3, 7))
    >>> z = mg.add_zeros("topographic__elevation", at="node")

    Set elevation to 1 for some nodes.

    >>> z[[9, 10, 11, 12]] = 1

    Define a zone function that returns a boolean array where `True` values
    indicate the nodes where zones can be created.

    >>> def zone_func(grid):
    ...     z = grid.at_node["topographic__elevation"]
    ...     return z == 1
    ...

    Instantiate ZoneController. Only one zone exists because the nodes that
    were set to one are adjacent to each other in the grid.

    >>> zc = ZoneController(mg, zone_func)
    >>> zc.record_data_frame[["time", "zones"]]
       time  zones
    0     0      1

    Populate each zone with a taxon.

    >>> taxon = zc.populate_zones_uniformly(1)
    >>> len(taxon)
    1

    A change in elevation is forced to demonstrate a zone fragmentation, and
    then the zones are updated by advancing the record time by 1000.

    >>> z[10] = 0
    >>> zc.run_one_step(1000)

    Two zones now exist because the zone in time 0 fragmented into two zones.

    >>> zc.record_data_frame[["time", "zones", "fragmentations"]]
       time  zones  fragmentations
    0     0      1             NaN
    1  1000      2             2.0

    A change in elevation is forced again, this time to demonstrate zone
    capture where multiple zones are overlapped by a zone in the later time
    step. Statistics of the capture can be attained with ``record_data_frame``.

    >>> z[10] = 1
    >>> zc.run_one_step(1000)
    >>> zc.record_data_frame[
    ...     ["time", "zones", "captures", "area_captured_sum", "area_captured_max"]
    ... ]
       time  zones  captures  area_captured_sum  area_captured_max
    0     0      1       NaN                NaN                NaN
    1  1000      2       0.0                0.0                0.0
    2  2000      1       1.0                2.0                2.0

    The follow example demonstrates non-default ZoneController parameters.

    >>> mg = RasterModelGrid((3, 7))
    >>> z = mg.add_zeros("topographic__elevation", at="node")

    Similar to the prior example, define a zone function that returns a boolean
    array where `True` values indicate the nodes where zones can be created.

    >>> def zone_func(grid):
    ...     z = grid.at_node["topographic__elevation"]
    ...     return z == 1
    ...

    Set elevation to 1 for nodes so that two clusters of nodes within the zone
    mask exist.

    >>> z[[9, 10, 12]] = 1

    Instantiate ZoneController with options.

    >>> zc = ZoneController(mg, zone_func, minimum_area=2, initial_time=100)

    Only one zone exist, despite two clusters of nodes meeting the zone
    definition, because the ``minimum_area`` was set to 2. Also, the first
    time in the record was set by the ``initial_time`` parameter.

    >>> zc.record_data_frame[["time", "zones"]]
       time  zones
    0   100      1
    """

    def __init__(
        self,
        grid,
        zone_function,
        minimum_area=0,
        neighborhood_structure="D8",
        initial_time=0,
        **kwargs,
    ):
        """Initialize the controller.

        Parameters
        ----------
        grid : RasterModelGrid
            A Landlab RasterModelGrid.
        zone_function : function
            A function that return a mask of the total zone extent. The first
            input parameter of this function must be `grid`.
        minimum_area : float, optional
            The minimum area of the zones that will be created.
        neighborhood_structure : {'D8', 'D4'}, optional
            The structure describes how zones are identified. The default,
            'D8' evaluates the eight neighboring nodes. The diagonal
            neighboring nodes are excluded when 'D4' is selected.
        initial_time : float, int, optional
            The initial time. The unit of time is unspecified within the
            controller. The default is 0.
        kwargs
            Keyword arguments for ``zone_function``. Do not include ``grid``
            in kwargs because ``grid``, the first parameter of this method, is
            automatically added to ``kwargs``.
        """
        # Set parameters.

        self._grid = grid
        self._zone_func = zone_function
        self._zone_params = kwargs
        self._min_area = minimum_area
        self._record = Record(initial_time)
        if neighborhood_structure in ["D8", "D4"]:
            self._neighborhood_struct = neighborhood_structure
        else:
            raise ValueError("`neighborhood_structure` must be 'D8' or 'D4'")

        # Set record initial values.

        self._record.set_value("zones", np.nan)
        self._record.set_value("fragmentations", np.nan)
        self._record.set_value("captures", np.nan)
        self._record.set_value("area_captured_sum", np.nan)
        self._record.set_value("area_captured_max", np.nan)

        # Include `grid` in the zone params dictionary.

        self._zone_params["grid"] = self._grid

        # Set initial zones.

        initial_zone_extent = self._zone_func(**self._zone_params)
        self._zones = self._get_zones_with_mask(initial_zone_extent)

        self._record.set_value("zones", len(self._zones))

    @property
    def zones(self):
        """The zones of the ZoneController."""
        return self._zones

    @property
    def record_data_frame(self):
        """A DataFrame of ZoneController variables over time.

        Each row is data of a model time step. The step time is recorded in the
        `time` column. The columns, `zones`, `fragmentations`, and `captures`
        are the count of these variables at a given time. `area_captured_sum`
        is the summation of captures over a time. `area_captured_max` is the
        maximum area captured of a single capture during a time.
        """
        return self._record.data_frame

    def populate_zones_uniformly(self, count, taxon_type=ZoneTaxon, **kwargs):
        """Populate each zone with the same type and count of taxa.

        Parameters
        ----------
        count : int
            The count of taxon to populate to each zone.
        taxon_type : type of Taxon
            A Taxon type that takes a Zone as its first parameter.
        kwargs : dictionary
            Keyword arguments of ``taxon_type``.
        """
        taxa = []

        for z in self._zones:
            taxa.extend([taxon_type([z], **kwargs) for _ in range(count)])

        return taxa

    def run_one_step(self, dt):
        """Update the zones for a single timestep.

        This method advances time in the record and determines the connectivity
        of zones between the current and prior time steps.

        Parameters
        ----------
        dt : float
            The model time step duration.
        """
        self._record.advance_time(dt)

        # Resolve the spatiotemporal connectivity of the prior time step zones
        # to the new zones.

        prior_zones = self._zones

        zone_mask = self._zone_func(**self._zone_params)
        new_zones = self._get_zones_with_mask(zone_mask)

        self._zones = _update_zones(self._grid, prior_zones, new_zones, self._record)

        self._record.set_value("zones", len(self._zones))

    def _get_zones_with_mask(self, mask):
        """Get zones using a mask.

        Parameters
        ----------
        mask : ndarray
            A boolean array with the grid number of nodes where `True` values
            are nodes within the extent of all the zones to be created.

        Returns
        -------
        list of Zones
            The discrete zones identified in the mask.
        """
        # Label clusters of `True` values in `mask`.

        if self._neighborhood_struct == "D8":
            s = 3 * [[1, 1, 1]]
        elif self._neighborhood_struct == "D4":
            s = [[0, 1, 0], [1, 1, 1], [0, 1, 0]]

        cluster_arr, cluster_ct = label(mask.reshape(self._grid.shape), structure=s)

        # Create zones for clusters.

        zones = []

        for i in range(1, cluster_ct + 1):
            mask = (cluster_arr == i).flatten()
            cluster_area = self._grid.cell_area_at_node[mask].sum()

            if cluster_area >= self._min_area:
                zones.append(Zone(self, mask))

        return zones



================================================
File: species_evolution/zone_taxon.py
================================================
#!/usr/bin/env python
"""ZoneTaxon object of SpeciesEvolver."""
import numpy as np
from pandas import Series

from .base_taxon import Taxon


class ZoneTaxon(Taxon):
    """A taxon based in zones.

    A ``ZoneTaxon`` is composed of members of a lower taxonomic level that each
    exists within a ``Zone`` object. Taxonomic rank is not considered by this
    class despite the use of the term, 'speciation', which is used herein to
    generally describe creation of a child taxon object.

    All zones of the taxon can be obtained with the attribute, ``zones`` that
    are the objects that manage the geographic aspect of taxon member
    populations. The total geographic extent of all populations is depicted by
    the ``range_mask`` attribute. The zones of a ZoneTaxon instance are created
    and updated using a ``ZoneController``. At model time steps, the
    connectivity of zones over time is obtained using attributes of the
    ``Zone`` object.

    The evolution of this taxon type is carried out in two stages during a
    model time step. In the first stage, the zones of the taxon are updated
    as the result of zone connectivity between the prior and current step in
    the method, ``_update_zones``. This method is the primary implementation of
    taxon dispersal and it is called in a stage prior to other evolutionary
    processes so that all taxa are positioned in their landscape locations
    prior to the other processes.

    In the second stage, processes are carried out in methods that are readily
    expanded or overridden when needed. The primary methods of second stage
    macroevolution are ``_evaluate_dispersal``, ``_evaluate_speciation``, and
    ``_evaluate_extinction``. The evaluate dispersal method is intended to
    modify dispersal conducted in the first stage and it has no effect unless
    it is expanded or overridden to have an effect. Processes other than those
    listed above can be called by expanding or overridding the ``_evolve``
    method.

    The taxon is allopatric when it is associated with/exists within multiple
    zones (signifying multiple member populations). A timer is started when a
    taxon becomes allopatric. Allopatric speciation occurs once the timer
    reaches or exceeds the ``time_to_allopatric_speciation`` initialization
    parameter. If the initialization parameter, ``persists_post_speciation``
    is True (default), a child taxon is created in each zone except one zone
    (the largest by area) that becomes the sole zone of the taxon. If
    ``persists_post_speciation`` is set to False, a child taxon is created in
    each and every zone, and the parent no longer occupies any zones, and
    therefore the parent taxon is no longer extant.

    Extinction occurs when the taxon is no longer associated with any zones.
    This occurs when zones in the prior time step do not overlap zones in the
    current time step, signifying the geographic range of the taxon is no more.
    A taxon can become no longer extant also when the taxon speciates and
    ``persists_post_speciation`` is False signifying that the parent taxon
    has evolved into multiple taxon distinct from the original taxon.

    The following columns will be added to the ``record_data_frame`` of the
    SpeciesEvolver instance that tracks objects of this Taxon: 'speciations'
    and 'extinctions', which are the counts of these variables at time steps.
    Another column, 'pseudoextinctions' will be included when
    ``persists_post_speciation`` is False. This variable is the count of
    occurrences when a parent taxon became non-extant due to speciation and not
    because of an absence of zones.
    """

    def __init__(
        self,
        zones,
        parent=None,
        time_to_allopatric_speciation=0,
        persists_post_speciation=True,
    ):
        """Initialize a taxon.

        Parameters
        ----------
        zones : list of Zones
            The initial SpeciesEvolver Zones where the taxon is located.
        parent : Taxon, optional
            A SpeciesEvolver taxon that is the parent taxon. The default value,
            'None' indicates no parent.
        time_to_allopatric_speciation : float, int, optional
            The delay in model time to speciate following taxon geographic
            fragmentation, indicated by multiple objects in the attribute,
            ``zones``. Speciation occurs at the time step when the delay is
            reached or exceeded. The default value of 0 indicates speciation
            occurs at the same time step as geographic fragmentation.
        persists_post_speciation : boolean, optional
            When 'True', the default, taxon persists despite speciation. When
            'False' and following speciation, the taxon is no longer extant.
        """
        super().__init__()

        self.parent = parent
        self._tas = time_to_allopatric_speciation
        self._pps = persists_post_speciation

        # Store zones that each represent an instance of a narrower taxonomic
        # level, e.g. a population.

        self._zones = zones

        # Set taxon time in allopatry.

        self._time_in_allopatry = None
        self._update_allopatry_state()

    @Taxon.extant.setter
    def extant(self, state):
        """Set the living state of the taxon."""
        self._extant = state

        if not state:
            # Ensure the taxon is not associated with zones when it is no
            # longer extant.
            self._zones = []

    @property
    def range_mask(self):
        """A mask representing the geographic extent of the taxon.

        The mask is an array with a length of grid number of nodes. The taxon
        exists at nodes where mask elements are ``True``. The mask of a
        ZoneTaxon object is the union of all of its zone masks.
        """
        masks = [zone.mask for zone in self.zones]
        mask = np.any(masks, 0)

        return mask

    @property
    def zones(self):
        """The zones of the taxon."""
        return self._zones

    def _evolve(self, dt, stage, record):
        """Run a step of evolutionary processes.

        Dispersal resolved during stage 1 can be modified by extending or
        overriding the dispersal evaluation method, as can speciation and
        extinction that are also evaluated in this stage.

        The attribute, ``extant`` is updated by this method.

        Parameters
        ----------
        dt : float
            The model time step duration.
        stage : int
            The evolution stage of the time step.
        record : defaultdict
            The SpeciesEvolver record.

        Returns
        -------
        boolean
           Indicates if the taxon is still evolving. When `False` is returned,
           this method will not be called for the taxon in subsequent stages in
           the current model time step.
        list of Taxon
            The children produced by the taxon at a given stage. The ``evolve``
            method of child taxon will be called in stages following the stage
            the child taxon was produced. An empty list indicates no child
            taxon.
        """
        if stage == 0:
            self._update_zones()
            child_taxa = []

        elif stage == 1:
            # Evaluate macroevolutionary processes now that zones of all taxon
            # objects were updated in stage 1.

            self._evaluate_dispersal(dt)
            self._update_allopatry_state(dt)

            child_taxa = self._evaluate_speciation(dt)
            child_count = len(child_taxa)

            extinct = self._evaluate_extinction(dt)
            pseudoextinct = child_count > 1 and not self._pps
            self.extant = not extinct and not pseudoextinct

            # Update the record.

            record.increment_value("speciations", child_count)
            record.increment_value("extinctions", int(extinct and not pseudoextinct))

            if not self._pps:
                record.increment_value("pseudoextinctions", int(pseudoextinct))

        return stage < 1, child_taxa

    def _update_zones(self):
        """Update the zones of the taxon.

        Dispersal is represented by setting taxon zones to the zones of the
        current time step that overlap the taxon zones of the prior time step
        (`successors of a zone`).
        """
        successors = []

        for zone in self._zones:
            successors.extend(zone.successors)

        self._zones = Series(successors).drop_duplicates().tolist()

    def _update_allopatry_state(self, dt=None):
        """Update taxon time in allopatry.

        Parameter, ``dt`` can optionally be set to increment time in allopatry
        given that the taxon is already allopatric.

        Parameters
        ----------
        dt : float, int, optional
            The model time step duration.
        """
        if len(self.zones) < 2:
            self._time_in_allopatry = None
        elif self._time_in_allopatry is None:
            self._time_in_allopatry = 0
        elif dt is not None:
            self._time_in_allopatry += dt

    def _produce_child_taxon(self, zones):
        """Get the taxon resulting from speciation.

        This method returns a taxon of the same type as the object that
        speciates. This child taxon is initialized with the initialization
        parameter values of the parent object. At minimum in derived
        implementations of this method, the parent taxon of the child should be
        set to `self` to correctly construct the lineage.

        Parameters
        ----------
        zones : list of Zones
            The zones where the child taxon will exist.

        Returns
        -------
        Taxon
            The child taxon.
        """
        taxon_type = type(self)

        child_taxon = taxon_type(
            zones,
            parent=self,
            time_to_allopatric_speciation=self._tas,
            persists_post_speciation=self._pps,
        )

        return child_taxon

    def _evaluate_allopatric_speciation(self, dt):
        """Return child taxa if the taxon is allopatric.

        A child taxon is returned for each zone except the largest zone if
        ``persists_post_speciation`` is True. A child taxon is returned for all
        zones if ``persists_post_speciation`` is False. The ``zones`` attribute
        is set to a list containing the largest zone or no zone when
        ``persists_post_speciation`` is True or False, respectively. This
        method is called by the ``_evaluate_speciation`` method in the second
        stage of taxon evolution.

        Parameter ``dt`` is not used in the ZoneTaxon implementation of this
        method. It is included to follow the pattern of including this
        parameter in ZoneTaxon evolution methods.

        Parameters
        ----------
        dt : float, int
            The model time step duration.

        Returns
        -------
        list of taxon objects
            The taxon objects produced by allopatric speciation. An empty list
            indicates no child objects and no allopatric speciation.
        """
        zones = self.zones
        allopatric = self._time_in_allopatry is not None
        children = []

        if allopatric and self._time_in_allopatry >= self._tas:
            if self._pps:
                # The zone/member with the greatest zone area remains
                # associated with the object.
                idx = np.argmax([zone.area for zone in zones])
                largest_zone = zones.pop(idx)

            for zone in zones:
                child = self._produce_child_taxon([zone])
                children.append(child)

            if self._pps:
                self._zones = [largest_zone]
            else:
                self._zones = []

            self._update_allopatry_state()

        return children

    def _evaluate_dispersal(self, dt):
        """Modify taxon dispersal.

        Population dispersal is principally determined in stage 1 by the
        method, ``_update_zones``. This evaluation method is called by the
        taxon evolve method in stage 2 and allows modification of the stage 1
        dispersal. This method implemented in ZoneTaxon does not modify stage 1
        dispersal. It is intended to be overridden when needed.

        Parameters
        ----------
        dt : float, int
            The model time step duration.
        """
        # pragma: no cover

    def _evaluate_speciation(self, dt):
        """Return child taxa if speciation occurs.

        This method is called by the taxon stage 2 evolve method. The default
        implementation of this method solely gets any taxon objects resulting
        from the method, ``_evaluate_allopatric_speciation``. Other modes of
        speciation, including sympatric, can be evaluated here by expanded this
        ``_evaluate_speciation`` method.

        Parameters
        ----------
        dt : float, int
            The model time step duration.

        Returns
        -------
        list of taxon objects
            The taxon objects produced by allopatric speciation. An empty list
            indicates no child objects and no allopatric speciation.
        """
        child_taxa = self._evaluate_allopatric_speciation(dt)

        return child_taxa

    def _evaluate_extinction(self, dt):
        """Determine if extinction occurs.

        Extinction occurs if no zone/member populations exist. Other conditions
        of extinction can be included by expanding or overridding this method.

        Parameters
        ----------
        dt : float, int
            The model time step duration.

        Returns
        -------
        boolean
            `True` indicates the taxon has become extinct. `False` indicates
            the taxon persists.
        """
        taxon_occupies_no_zones = len(self.zones) == 0

        return taxon_occupies_no_zones



================================================
File: steepness_index/__init__.py
================================================
from .channel_steepness import SteepnessFinder

__all__ = ["SteepnessFinder"]



================================================
File: steepness_index/channel_steepness.py
================================================
"""Created on Mon Oct 19.

@author: dejh
"""

import numpy as np

from landlab import Component


class SteepnessFinder(Component):
    """This component calculates steepness indices, sensu Wobus et al. 2006,
    for a Landlab landscape. Follows broadly the approach used in
    GeomorphTools, geomorphtools.org.

    Examples
    --------
    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import FlowAccumulator, FastscapeEroder
    >>> from landlab.components import SteepnessFinder
    >>> mg = RasterModelGrid((3, 10), xy_spacing=100.0)
    >>> for nodes in (
    ...     mg.nodes_at_right_edge,
    ...     mg.nodes_at_bottom_edge,
    ...     mg.nodes_at_top_edge,
    ... ):
    ...     mg.status_at_node[nodes] = mg.BC_NODE_IS_CLOSED
    >>> _ = mg.add_zeros("topographic__elevation", at="node")
    >>> mg.at_node["topographic__elevation"][mg.core_nodes] = (
    ...     mg.node_x[mg.core_nodes] / 1000.0
    ... )
    >>> fr = FlowAccumulator(mg, flow_director="D8")
    >>> sp = FastscapeEroder(mg, K_sp=0.01)
    >>> sf = SteepnessFinder(mg, min_drainage_area=10000.0)
    >>> for i in range(10):
    ...     mg.at_node["topographic__elevation"][mg.core_nodes] += 10.0
    ...     _ = fr.run_one_step()
    ...     sp.run_one_step(1000.0)
    ...
    >>> sf.calculate_steepnesses()
    >>> mg.at_node["channel__steepness_index"].reshape((3, 10))[1, :]
    array([ 0.        , 29.28427125,  1.        ,  1.        ,
            1.        ,  1.        ,  1.        ,  1.        ,
            0.99999997,  0.        ])
    >>> sf.hillslope_mask
    array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
            True, False, False, False, False, False, False, False, False,
           False,  True,  True,  True,  True,  True,  True,  True,  True,
            True,  True,  True])

    >>> sf = SteepnessFinder(mg, min_drainage_area=10000.0, discretization_length=350.0)
    >>> sf.calculate_steepnesses()
    >>> mg.at_node["channel__steepness_index"].reshape((3, 10))[1, :]
    array([0.        , 3.08232295, 3.08232295, 3.08232295, 1.        ,
           1.        , 1.        , 1.        , 0.        , 0.        ])

    >>> sf = SteepnessFinder(mg, min_drainage_area=10000.0, elev_step=1.5)
    >>> sf.calculate_steepnesses()
    >>> mg.at_node["channel__steepness_index"].reshape((3, 10))[1, :]
    array([0.        , 1.22673541, 1.2593727 , 1.27781936, 1.25659369,
           1.12393156, 0.97335328, 0.79473963, 0.56196578, 0.        ])

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    Wobus, C. W., Whipple, K. X., Kirby, E., Snyder, N. P., Johnson, J.,
    Spyropolou, K., Crosby, B. T., and Sheenan, D.: Tectonics from topography:
    Procedures, promise, and pitfalls, in: Tectonics, Climate, and Landscape
    Evolution, edited by: Willett, S. D., Hovius, N., Brandon, M. T., and
    Fisher, D., Geological Society of America Special Paper 398, Geological
    Society of America, Boulder, CO, USA, 55–74, 2006.

    """

    _name = "SteepnessFinder"

    _unit_agnostic = True

    _info = {
        "channel__steepness_index": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "variable",
            "mapping": "node",
            "doc": "the local steepness index",
        },
        "drainage_area": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m**2",
            "mapping": "node",
            "doc": "Upstream accumulated surface area contributing to the node's discharge",
        },
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__upstream_node_order": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array containing downstream-to-upstream ordered list of node IDs",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__steepest_slope": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "The steepest *downhill* slope",
        },
    }

    def __init__(
        self,
        grid,
        reference_concavity=0.5,
        min_drainage_area=1.0e6,
        elev_step=0.0,
        discretization_length=0.0,
    ):
        """
        Parameters
        ----------
        grid : RasterModelGrid
            A landlab RasterModelGrid.
        reference_concavity : float (default 0.5)
            The reference concavity to use in the calculation.
        min_drainage_area : float (m**2; default 1.e6)
            The minimum drainage area above which steepness indices are
            calculated.
            Defaults to 1.e6 m**2, per Wobus et al. 2006.
        elev_step : float (m; default 0.)
            If >0., becomes a vertical elevation change step to use to
            discretize the data (per Wobus). If 0., all nodes are used and
            no discretization happens.
        discretization_length : float (m; default 0.)
            If >0., becomes the lengthscale over which to segment the profiles -
            i.e., one different steepness index value is calculated every
            discretization_length. If only one (or no) points are present in a
            segment, it will be lumped together with the next segment.
            If zero, one value is assigned to each channel node.
        """
        super().__init__(grid)

        if grid.at_node["flow__receiver_node"].size != grid.size("node"):
            raise NotImplementedError(
                "A route-to-multiple flow director has been "
                "run on this grid. The landlab development team has not "
                "verified that SteepnessFinder is compatible with "
                "route-to-multiple methods. Please open a GitHub Issue "
                "to start this process."
            )

        self._reftheta = reference_concavity
        self._min_drainage = min_drainage_area
        assert elev_step >= 0.0, "elev_step must be >= 0!"
        self._elev_step = elev_step
        self._discretization = discretization_length
        self._ksn = self._grid.add_zeros(
            "channel__steepness_index", at="node", clobber=True
        )
        self._mask = self._grid.ones(at="node", dtype=bool)
        # this one needs modifying if smooth_elev
        self._elev = self._grid.at_node["topographic__elevation"]

    def calculate_steepnesses(self):
        """This is the main method. Call it to calculate local steepness
        indices at all points with drainage areas greater than
        *min_drainage_area*.

        This "run" method can optionally take the same parameter set as
        provided at instantiation. If they are provided, they will override
        the existing values from instantiation.

        Normalized steepness of any node without a defined value is reported
        as 0. These nodes are also identified in the mask retrieved with
        :func:`hillslope_mask`.
        """
        self._mask.fill(True)
        self._ksn.fill(0.0)

        reftheta = self._reftheta
        min_drainage = self._min_drainage
        elev_step = self._elev_step
        discretization_length = self._discretization

        upstr_order = self._grid.at_node["flow__upstream_node_order"]
        # get an array of only nodes with A above threshold:
        valid_dstr_order = (
            upstr_order[
                self._grid.at_node["drainage_area"][upstr_order] >= min_drainage
            ]
        )[::-1]
        # note elevs are guaranteed to be in order, UNLESS a fill
        # algorithm has been used.
        nodes_incorporated = self._grid.zeros(at="node", dtype=bool)
        # now do each poss channel in turn
        # get the head of the first (longest!) channel:
        for dstr_order_index in range(valid_dstr_order.size):
            this_ch_top_node = valid_dstr_order[dstr_order_index]  # top node
            if not nodes_incorporated[this_ch_top_node]:
                nodes_incorporated[this_ch_top_node] = True
                nodes_in_channel = [this_ch_top_node]
                penultimate_node = this_ch_top_node
                current_node_incorporated = False
                while not current_node_incorporated:
                    next_node = self._grid.at_node["flow__receiver_node"][
                        penultimate_node
                    ]
                    if next_node == penultimate_node:  # end of flow path
                        break
                    nodes_in_channel.append(next_node)
                    current_node_incorporated = nodes_incorporated[next_node]
                    # ^ this is a COPY op, so we're free to update the array
                    nodes_incorporated[next_node] = True
                    penultimate_node = next_node
                # by here, we have a full, unique reach in nodes_in_channel
                # it incorporates a single, duplicate node at the lower end
                # Now, if this segment long enough?
                if elev_step:
                    top_elev = self._elev[nodes_in_channel[0]]
                    base_elev = self._elev[nodes_in_channel[-1]]
                    # work up the channel from the base to make new interp pts
                    interp_pt_elevs = np.arange(base_elev, top_elev, elev_step)
                    if interp_pt_elevs.size <= 1:
                        # <1 step; bail on this whole segment
                        break
                    # now we can fairly closely follow the Geomorphtools
                    # algorithm:
                    ch_nodes = np.array(nodes_in_channel)
                    # ^ this is top-to-bottom
                    ch_A = self._grid.at_node["drainage_area"][ch_nodes]
                    ch_dists = self.channel_distances_downstream(ch_nodes)
                    ch_S = self.interpolate_slopes_with_step(
                        ch_nodes, ch_dists, interp_pt_elevs
                    )
                else:
                    # all the nodes; much easier as links work
                    ch_nodes = np.array(nodes_in_channel)
                    ch_dists = self.channel_distances_downstream(ch_nodes)
                    ch_A = self._grid.at_node["drainage_area"][ch_nodes]
                    ch_S = self._grid.at_node["topographic__steepest_slope"][ch_nodes]
                    assert np.all(ch_S >= 0.0)
                # if we're doing spatial discretization, do it here:
                if discretization_length:
                    ch_ksn = self.calc_ksn_discretized(
                        ch_dists, ch_A, ch_S, reftheta, discretization_length
                    )
                else:  # not discretized
                    # also chopping off the final node, as above
                    log_A = np.log10(ch_A[:-1])
                    log_S = np.log10(ch_S[:-1])
                    # we're potentially propagating nans here if S<=0
                    log_ksn = log_S + reftheta * log_A
                    ch_ksn = 10.0**log_ksn
                # save the answers into the main arrays:
                assert np.all(self._mask[ch_nodes[:-1]])
                # Final node gets trimmed off...
                self._ksn[ch_nodes[:-1]] = ch_ksn
                self._mask[ch_nodes] = False
        # now a final sweep to remove any undefined ksn values:
        self._mask[self._ksn == -1.0] = True
        self._ksn[self._ksn == -1.0] = 0.0

    def channel_distances_downstream(self, ch_nodes):
        """Calculates distances downstream from top node of a defined flowpath.

        Parameters
        ----------
        ch_nodes : array of ints
            The nodes along a single defined flow path, starting upstream.

        Returns
        -------
        ch_dists : array of floats
            Distances downstream from top node of ch_nodes.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> mg = RasterModelGrid((4, 5), xy_spacing=(10.0, 5.0))
        >>> for nodes in (
        ...     mg.nodes_at_right_edge,
        ...     mg.nodes_at_bottom_edge,
        ...     mg.nodes_at_top_edge,
        ... ):
        ...     mg.status_at_node[nodes] = mg.BC_NODE_IS_CLOSED
        >>> mg.status_at_node[[6, 12, 13, 14]] = mg.BC_NODE_IS_CLOSED
        >>> _ = mg.add_field("topographic__elevation", mg.node_x, at="node")
        >>> fr = FlowAccumulator(mg, flow_director="D8")
        >>> sf = SteepnessFinder(mg)
        >>> _ = fr.run_one_step()
        >>> ch_nodes = np.array([8, 7, 11, 10])
        >>> sf.channel_distances_downstream(ch_nodes)
        array([  0.        ,  10.        ,  21.18033989,  31.18033989])
        """
        ch_links = self._grid.at_node["flow__link_to_receiver_node"][ch_nodes]
        ch_dists = np.empty_like(ch_nodes, dtype=float)
        # dists from ch head, NOT drainage divide
        ch_dists[0] = 0.0
        np.cumsum(self._grid.length_of_d8[ch_links[:-1]], out=ch_dists[1:])
        return ch_dists

    def interpolate_slopes_with_step(self, ch_nodes, ch_dists, interp_pt_elevs):
        """Maps slopes to nodes, interpolating withing defined vertical
        intervals.

        This follows Geomorphtools' discretization methods. It is essentially a
        downwind map of the slopes.

        Parameters
        ----------
        ch_nodes : array of ints
            The nodes along a single defined flow path, starting upstream.
        ch_dists : array of floats
            Distances downstream from top node of ch_nodes.
        interp_pt_elevs : array of floats
            Elevations at the discretizing points along the profile, in order
            of increasing elevation.

        Returns
        -------
        ch_S : array of floats
            Interpolated slopes at each node in the flowpath (always positive).

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> mg = RasterModelGrid((3, 10), xy_spacing=(10.0, 5.0))
        >>> for nodes in (
        ...     mg.nodes_at_right_edge,
        ...     mg.nodes_at_bottom_edge,
        ...     mg.nodes_at_top_edge,
        ... ):
        ...     mg.status_at_node[nodes] = mg.BC_NODE_IS_CLOSED
        >>> _ = mg.add_field("topographic__elevation", mg.node_x**1.1, at="node")
        >>> fr = FlowAccumulator(mg, flow_director="D8")
        >>> sf = SteepnessFinder(mg)
        >>> _ = fr.run_one_step()
        >>> ch_nodes = np.arange(18, 9, -1)
        >>> ch_dists = sf.channel_distances_downstream(ch_nodes)
        >>> interp_pt_elevs = np.array([0.0, 30.0, 60.0, 90.0, 120.0])
        >>> sf.interpolate_slopes_with_step(ch_nodes, ch_dists, interp_pt_elevs)
        array([1.67970205, 1.67970205, 1.67970205, 1.65129294, 1.62115336,
               1.5811951 , 1.53157521, 1.44240187, 1.36442227])
        >>> mg.at_node["topographic__steepest_slope"][ch_nodes]
        array([1.69383001, 1.66972677, 1.64200694, 1.60928598, 1.56915472,
               1.51678178, 1.43964028, 1.25892541, 0.        ])
        >>> mg.at_node["topographic__elevation"][:] = mg.node_x
        >>> interp_pt_elevs = np.array([0.0, 25.0, 50.0, 75.0, 80.0])
        >>> sf.interpolate_slopes_with_step(ch_nodes, ch_dists, interp_pt_elevs)
        array([1., 1., 1., 1., 1., 1., 1., 1., 1.])
        """
        ch_z = self._grid.at_node["topographic__elevation"][ch_nodes]
        assert (
            ch_z[0] >= interp_pt_elevs[-1]
        ), "Highest interp_pt_elev must be below top channel node"
        interp_pt_x = np.interp(interp_pt_elevs, ch_z[::-1], ch_dists[::-1])
        interp_pt_S = np.empty_like(interp_pt_elevs)
        # now a downwind map of the slopes onto the nodes
        # slopes are defined positive
        z_diff = interp_pt_elevs[:-1] - interp_pt_elevs[1:]
        x_diff = interp_pt_x[1:] - interp_pt_x[:-1]
        np.divide(z_diff, x_diff, out=interp_pt_S[:-1])
        interp_pt_S[-1] = interp_pt_S[-2]
        # Map S back onto nodes
        ch_S = np.interp(ch_z, interp_pt_elevs, interp_pt_S)

        return ch_S

    def calc_ksn_discretized(
        self, ch_dists, ch_A, ch_S, ref_theta, discretization_length
    ):
        """Calculate normalized steepness index on defined channel segments.

        Every segment must have at least 2 nodes along it. If not, segments
        will be automatically merged to achieve this. The channel will be
        segmented starting at the *downstream* end.

        NB: The final node in the channel does not receive an index, as it
        either belongs to a longer, existing flow path, or it is a boundary
        node with S = 0. Neither works.

        Parameters
        ----------
        ch_dists : array of floats
            Distances downstream from top node of a single stream path.
        ch_A : array of floats
            Drainage areas at each node in the flowpath.
        ch_S : array of floats
            Slope at each node in the flowpath (defined as positive).
        ref_theta : float
            The reference concavity; must be positive.
        discretization_length : float (m)
            The streamwise length of each segment.

        Returns
        -------
        ch_ksn : array of floats
            The normalized steepness index at each node in the flowpath,
            EXCEPT THE LAST. (i.e., length is (ch_dists.size - 1)). Values
            will be the same within each defined segment.

        Examples
        --------
        >>> import numpy as np
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator
        >>> from landlab.components import SteepnessFinder
        >>> mg = RasterModelGrid((3, 10), xy_spacing=(10.0, 5.0))
        >>> for nodes in (
        ...     mg.nodes_at_right_edge,
        ...     mg.nodes_at_bottom_edge,
        ...     mg.nodes_at_top_edge,
        ... ):
        ...     mg.status_at_node[nodes] = mg.BC_NODE_IS_CLOSED
        >>> _ = mg.add_field("topographic__elevation", mg.node_x, at="node")
        >>> fr = FlowAccumulator(mg, flow_director="D8")
        >>> sf = SteepnessFinder(mg)
        >>> _ = fr.run_one_step()
        >>> ch_nodes = np.arange(18, 9, -1)
        >>> ch_dists = sf.channel_distances_downstream(ch_nodes)
        >>> ch_A = mg.at_node["drainage_area"][ch_nodes]
        >>> ch_S = mg.at_node["topographic__steepest_slope"][ch_nodes]

        >>> ksn_25 = sf.calc_ksn_discretized(ch_dists, ch_A, ch_S, 0.5, 25.0)
        >>> ksn_25.size == ch_dists.size - 1
        True
        >>> ksn_25
        array([-1.        , 11.0668192 , 11.0668192 , 15.70417802,
               15.70417802, 15.70417802, 19.3433642 , 19.3433642 ])

        >>> ksn_10 = sf.calc_ksn_discretized(ch_dists, ch_A, ch_S, 0.5, 10.0)
        >>> ksn_10
        array([ 8.40896415,  8.40896415, 13.16074013, 13.16074013,
               16.5487546 , 16.5487546 , 19.3433642 , 19.3433642 ])

        >>> ch_ksn_overdiscretized = sf.calc_ksn_discretized(
        ...     ch_dists, ch_A, ch_S, 0.5, 10.0
        ... )
        >>> np.allclose(ch_ksn_overdiscretized, ksn_10)
        True
        """
        ch_ksn = np.empty_like(ch_A)
        # need to remove the influence of the final node in the seg,
        # as it reflects either the edge of the grid (S=0) or a point
        # after a confluence - hence the 0.000001
        seg_ends = np.arange(ch_dists[-1] - 0.000001, 0.0, -discretization_length)[::-1]
        # ^ counts up from 0, but terminates at the far end cleanly
        pts_in_each_seg = np.searchsorted(seg_ends, ch_dists)
        num_segs = pts_in_each_seg[-1]
        i = num_segs - 1  # the final pt is no longer included
        while i >= 0:
            old_i = i
            pts_in_seg = pts_in_each_seg == i
            num_pts_in_seg = int(pts_in_seg.sum())
            # if i == num_segs:
            #     true_pts_in_seg = pts_in_each_seg.copy()
            #     pts_in_each_seg[-1] = False
            # else:
            #     true_pts_in_seg = pts_in_each_seg
            # make sure there's always 2 pts in the seg...
            while num_pts_in_seg < 2:
                i -= 1
                pts_in_seg = np.logical_and(
                    pts_in_each_seg <= old_i, pts_in_each_seg >= i
                )
                num_pts_in_seg = int(pts_in_seg.sum())
                if i < 0:
                    break
            if num_pts_in_seg < 2:
                # must be at the end of the seg...
                # nodes in invalid segs at the end get ksn = -1.
                ch_ksn[pts_in_seg] = -1.0
                break
            seg_A = ch_A[pts_in_seg]
            seg_S = ch_S[pts_in_seg]
            logseg_A = np.log10(seg_A)
            logseg_S = np.log10(seg_S)
            meanlogseg_A = np.mean(logseg_A)
            meanlogseg_S = np.mean(logseg_S)
            logseg_ksn = meanlogseg_S + ref_theta * meanlogseg_A
            ch_ksn[pts_in_seg] = 10.0**logseg_ksn
            i -= 1

        return ch_ksn[:-1]

    @property
    def steepness_indices(self):
        """Return the array of channel steepness indices.

        Nodes not in the channel receive zeros.
        """
        return self._ksn

    @property
    def hillslope_mask(self):
        """Return a boolean array, False where steepness indices exist."""
        return self._mask

    @property
    def masked_steepness_indices(self):
        """Returns a masked array version of the 'channel__steepness_index'
        field. This enables easier plotting of the values with.

        :func:`landlab.imshow_grid_at_node` or similar.

        Examples
        --------
        Make a topographic map with an overlay of steepness values:

        >>> from landlab import imshow_grid_at_node
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator, FastscapeEroder
        >>> from landlab.components import SteepnessFinder
        >>> mg = RasterModelGrid((5, 5), xy_spacing=100.0)
        >>> for nodes in (
        ...     mg.nodes_at_right_edge,
        ...     mg.nodes_at_bottom_edge,
        ...     mg.nodes_at_top_edge,
        ... ):
        ...     mg.status_at_node[nodes] = mg.BC_NODE_IS_CLOSED
        >>> _ = mg.add_zeros("topographic__elevation", at="node")
        >>> mg.at_node["topographic__elevation"][mg.core_nodes] = (
        ...     mg.node_x[mg.core_nodes] / 1000.0
        ... )
        >>> np.random.seed(0)
        >>> mg.at_node["topographic__elevation"][mg.core_nodes] += np.random.rand(
        ...     mg.number_of_core_nodes
        ... )
        >>> fr = FlowAccumulator(mg, flow_director="D8")
        >>> sp = FastscapeEroder(mg, K_sp=0.01)
        >>> cf = SteepnessFinder(mg, min_drainage_area=20000.0)
        >>> for i in range(10):
        ...     mg.at_node["topographic__elevation"][mg.core_nodes] += 10.0
        ...     _ = fr.run_one_step()
        ...     sp.run_one_step(1000.0)
        ...
        >>> _ = fr.run_one_step()
        >>> cf.calculate_steepnesses()

        >>> imshow_grid_at_node(mg, "topographic__elevation", allow_colorbar=False)
        >>> imshow_grid_at_node(
        ...     mg, cf.masked_steepness_indices, color_for_closed=None, cmap="winter"
        ... )
        """
        return np.ma.array(self.steepness_indices, mask=self.hillslope_mask)



================================================
File: stream_power/__init__.py
================================================
from .fastscape_stream_power import FastscapeEroder
from .sed_flux_dep_incision import SedDepEroder
from .stream_power import StreamPowerEroder
from .stream_power_smooth_threshold import StreamPowerSmoothThresholdEroder

__all__ = [
    "StreamPowerEroder",
    "FastscapeEroder",
    "SedDepEroder",
    "StreamPowerSmoothThresholdEroder",
]



================================================
File: stream_power/cfuncs.pyx
================================================
cimport cython
from libc.math cimport exp
from libc.math cimport powf

from scipy.optimize import newton

# suspect that the function _brentq is in c and thus this is the most effective
# method for using the brentq method in cython.
from scipy.optimize._zeros import _brentq as brentq

# https://cython.readthedocs.io/en/stable/src/userguide/fusedtypes.html
ctypedef fused id_t:
    cython.integral
    long long


def brent_method_erode_variable_threshold(
    const id_t [:] src_nodes,
    const id_t [:] dst_nodes,
    const cython.floating [:] threshsxdt,
    const cython.floating [:] alpha,
    const double n,
    cython.floating [:] z,
):
    """Erode node elevations using Brent's method for stability.

    The alpha value is given as

    alpha = delta_t*K * (A)**m/(delta_x**n)

    It will be multiplied by the value:
        (z_node(t) - z_downstream(t+delta_t))**(n-1)

    to become the alpha value defined below in the function used in erode_fun
    used in the root-finding operation.

    Parameters
    ----------
    src_nodes : array_like
        Ordered upstream node ids.
    dst_nodes : array_like
        Node ids of nodes receiving flow.
    threshsxdt : array_like
        Incision thresholds at nodes multiplied by the timestep.
    alpha : array_like
        Erosion factor.
    n : float
        Exponent.
    z : array_like
        Node elevations.
    """
    # define internally used variables.
    cdef unsigned int n_nodes = src_nodes.shape[0]
    cdef unsigned int src_id
    cdef unsigned int dst_id
    cdef unsigned int i
    cdef double z_old
    cdef double z_downstream
    cdef double thresholddt
    cdef double z_diff_old
    cdef double alpha_param
    cdef double beta_param
    cdef double check_function
    cdef double x

    # Loop through nodes.
    for i in range(n_nodes):

        # get IDs for source and reciever nodes
        src_id = src_nodes[i]
        dst_id = dst_nodes[src_id]

        # if a node does not flow to itself, and the source node is above the
        # destination node
        if src_id != dst_id and z[src_id] > z[dst_id]:

            # Get values for z at present node and present time,
            # and z downstream at t + delta t (which should have been
            # previously solved for)
            z_old = z[src_id]
            z_downstream = z[dst_id]

            # Get the threshold value. In this function, it is spatially variable
            thresholddt = threshsxdt[src_id]

            # calculate the difference between z_old and z_downstream
            z_diff_old = z_old - z_downstream

            # using z_diff_old, calculate the alpha paramter of Braun and
            # Willet by calculating alpha times z

            alpha_param = alpha[src_id] * powf(z_diff_old, n-1.0)

            # Calculate the beta parameter that accounts for the possible
            # presence of a threshold.
            beta_param = thresholddt / z_diff_old

            # check if the threshold has been exceeded by passing a value of
            # x = 1 to the erode_fn. If this returns a value of less than
            # zero, this means that the the maximum possible slope value  does
            # not produce stream power needed to exceed the erosion threshold
            check_function = erode_fn(1, alpha_param, beta_param, n)

            # if the threshold was not exceeded do not change the elevation,
            # otherwise calculate the erosion rate
            if check_function > 0:
                # if the threshold was exceeded, then there will be a zero
                # between x = 0 and x= 1

                # solve using brentq, which requires a zero to exist
                # in between the two end values

                # if n is 1, finding x has an analytical solution. Otherwise,
                # use the the numerical solution given by root finding
                if n != 1.0:

                    # The threshold values passed here are the defaults if one
                    # were to import brentq from scipy.optimize
                    x = brentq(
                        erode_fn,
                        0.0,
                        1.0,
                        1e-12,
                        4.4408920985006262e-16,
                        100,
                        (alpha_param, beta_param, n),
                        False,
                        True,
                    )

                else:
                    # Analytical solution
                    x = (1.0 + beta_param)/(1.0 + alpha_param)

                # If x is provided as a value greater than zero, calculate
                # z at t=t+delta_t useing the values of x, z_downstream and
                # z_old as given by the definition of x (see erode_fn for
                # details). If x is equal to zero, set it as just slightly
                # higher than x_downstream.
                if x>0:
                    z[src_id] = z_downstream + x * (z_old - z_downstream)
                else:
                    z[src_id] = z_downstream + 1.0e-15

                # Nothing is returned from this function as it serves to update
                # the array z.


def brent_method_erode_fixed_threshold(
    const id_t [:] src_nodes,
    const id_t [:] dst_nodes,
    const double threshsxdt,
    const cython.floating [:] alpha,
    const double n,
    cython.floating [:] z,
):
    """Erode node elevations.

    The alpha value is given as::

        alpha = delta_t*K * (A)**m/(delta_x**n)

    It will be multiplied by the value::

        (z_node(t) - z_downstream(t+delta_t))**(n-1)

    to become the alpha value defined below in the function used in erode_fun
    used in the root-finding operation.

    Parameters
    ----------
    src_nodes : array_like
        Ordered upstream node ids.
    dst_nodes : array_like
        Node ids of nodes receiving flow.
    threshsxdt : float
        Incision thresholds at nodes multiplied by the timestep.
    alpha : array_like
        Erosion factor.
    n : float
        Exponent.
    z : array_like
        Node elevations.
    """
    # define internally used variables.
    cdef unsigned int n_nodes = src_nodes.shape[0]
    cdef unsigned int src_id
    cdef unsigned int dst_id
    cdef unsigned int i
    cdef double z_old
    cdef double z_downstream
    cdef double z_diff_old
    cdef double alpha_param
    cdef double beta_param
    cdef double check_function
    cdef double x

    # Loop through nodes.
    for i in range(n_nodes):

        # get IDs for source and reciever nodes
        src_id = src_nodes[i]
        dst_id = dst_nodes[src_id]

        # if a node does not flow to itself, and the source node is above the
        # destination node
        if src_id != dst_id and z[src_id] > z[dst_id]:

            # Get values for z at present node and present time,
            # and z downstream at t + delta t (which should have been
            # previously solved for)
            z_old = z[src_id]
            z_downstream = z[dst_id]

            # Get the threshold value. In this function, it is constant, so we
            # already have it.
            # threshsxdt = threshsxdt

            # calculate the difference between z_old and z_downstream
            z_diff_old = z_old - z_downstream

            # using z_diff_old, calculate the alpha paramter of Braun and
            # Willet by calculating alpha times z

            alpha_param = alpha[src_id] * powf(z_diff_old, n-1.0)

            # Calculate the beta parameter that accounts for the possible
            # presence of a threshold.
            beta_param = threshsxdt / z_diff_old
            # check if the threshold has been exceeded by passing a value of
            # x = 1 to the erode_fn. If this returns a value of less than
            # zero, this means that the the maximum possible slope value  does
            # not produce stream power needed to exceed the erosion threshold
            check_function = erode_fn(1, alpha_param, beta_param, n)

            # if the threshold was not exceeded do not change the elevation,
            # otherwise calculate the erosion rate
            if check_function > 0:
                # if the threshold was exceeded, then there will be a zero
                # between x = 0 and x= 1

                # solve using brentq, which requires a zero to exist
                # in between the two end values

                # if n is 1, finding x has an analytical solution. Otherwise,
                # use the the numerical solution given by root finding
                if n != 1.0:

                    # The threshold values passed here are the defaults if one
                    # were to import brentq from scipy.optimize
                    x = brentq(
                        erode_fn,
                        0.0,
                        1.0,
                        1e-12,
                        4.4408920985006262e-16,
                        100,
                        (alpha_param, beta_param, n),
                        False,
                        True,
                    )

                else:
                    # Analytical solution
                    x = (1.0 + beta_param)/(1.0 + alpha_param)

                # If x is provided as a value greater than zero, calculate
                # z at t=t+delta_t useing the values of x, z_downstream and
                # z_old as given by the definition of x (see erode_fn for
                # details). If x is equal to zero, set it as just slightly
                # higher than x_downstream.
                if x>0:
                    z[src_id] = z_downstream + x * (z_old - z_downstream)
                else:
                    z[src_id] = z_downstream + 1.0e-15

                # Nothing is returned from this function as it serves to update
                # the array z.


def erode_fn(
    const double x,
    const double alpha,
    const double beta,
    const double n,
):
    """Evaluates the solution to the water-depth equation.

    Called by scipy.brentq() to find solution for $x$ using Brent's method.

    Parameters
    ----------
    x : float
        normalized elevation, see below.
    alpha : float
        alpha parameter, see below.
    beta : float
        beta parameter, see below.
    n : float
        n exponent
    check : boolean, default is True
        flag to determine if a ValueError should be thrown if a check
        identifies that the threshold value is high enough such that no erosion
        will occur.


    This equation represents the implicit solution for normalized topographic
    elevation $x$ at the  next time step. This solution is inspired by the
    Appendix of Braun and Willet (2012) but was generalized to include an a
    threshold value such that if the threshold is not exceeded, no erosion will
    occur.

    Consider stream power erosion under the equation::

        E = K * (A)**m * S**n - threshold_sp,

    on a grid with link delta_x and for a timestep of delta_t.

    When iterating from downstream to upstream in the drainage stack, at a
    given node at time = t+delta_t, the value of the node at time = t, and the
    value of the downstream node at time t+delta_t is known.

    Define::

        x = (
            z_node(t + delta_t) - z_downstream(t + delta_t)
        ) / (z_node(t) - z_downstream(t + delta_t))

    A discretized version of the stream power equation above yeilds the equation::

        f = x - 1 + alpha*(x**n) - beta

    where::

        alpha = delta_t * K * A ** m / (delta_x ** n) * (
            z_node(t) - z_downstream(t + delta_t)
        ) ** (n - 1)

    and::

        beta = threshold_sp * delta_t / (z_node(t) - z_downstream(t + delta_t))

    Finding the root of f provides the implicit solution for the stream power
    equation.

    If f(x=1) = 0, then no erosion occurs as potential erosion is cancelled by
    the erosion threshold and the topography at the given node does not change.

    If f(x=0) = 0, then the topography at the given node becomes that of the
    the downstream mode.

    If the threshold term, beta, is zero, this equation collapses to the form
    given by Braun and Willet (2012).

    When the threshold term is greater than zero, it is possible that no
    erosion will occur. In this case, an evaluation of f(x=1) will yeild a
    negative number.

    If for the values of alpha and beta provided, f(x=1)<0, then this function
    has no root and use in a solver such as Brent's method in which a solution
    interval is required will fail.

    It is recommended that before using this method in a solver, that the
    function be evaluated with x=1 to determine if it any erosion occured.

    """
    cdef double f

    f = x - 1.0 + (alpha * powf(x, n)) - beta

    return f


def smooth_stream_power_eroder_solver(
    const id_t [:] src_nodes,
    const id_t [:] dst_nodes,
    cython.floating [:] z,
    const cython.floating [:] alpha,
    const cython.floating [:] gamma,
    const cython.floating [:] delta,
):
    """Erode node elevations using Newtons Method for smoothed Stream Power. "

    This method takes three parameters, alpha, gamma, and delta.

    alpha = K A^m dt / L

    delta = K A^m / (L * wc)

    gamma = omega_c * dt

    This method will use the new_elev and new_elev_prime equations.

    Parameters
    ----------
    src_nodes : array_like
        Ordered upstream node ids.
    dst_nodes : array_like
        Node ids of nodes receiving flow.
    alpha : array_like
        Erosion equation parameter.
    gamma : array_like
        Erosion equation parameter.
    delta : array_like
        Erosion equation parameter.
    z : array_like
        Node elevations.
    """
    cdef unsigned int src_id
    cdef unsigned int dst_id
    cdef unsigned int i

    for i in range(len(src_nodes)):
        src_id = src_nodes[i]
        dst_id = dst_nodes[src_id]

        if src_id != dst_id and z[src_id] > z[dst_id]:

            # calculate epsilon
            epsilon = (alpha[src_id] * z[dst_id] + gamma[src_id] + z[src_id])

            # calculate new z
            z[src_id] = newton(
                new_elev,
                z[src_id],
                fprime=new_elev_prime,
                args=(
                    alpha[src_id],
                    z[dst_id],
                    gamma[src_id],
                    delta[src_id],
                    epsilon,
                )
            )


def new_elev(
    const double x,
    const double a,
    const double b,
    const double c,
    const double d,
    const double e,
):
    """Equation for elevation of a node at timestep t+1.

    Parameters
    ----------
    x : float
        Value of new elevation
    a : float
        Parameter = K A^m dt / L (nondimensional)
    b : float
        Elevation of downstream node, z_j
    c : float
        Parameter = omega_c * dt (dimension of L, because omega_c [=] L/T)
    d : float
        Parameter = K A^m / (L * wc) [=] L^{-1} (so d * z [=] [-])
    e : float
        z(t) + a z_j + (wc * dt)
    """
    cdef double f

    f = x * (1.0 + a) + c * exp(-d * (x - b)) - e

    return f


def new_elev_prime(
    const double x,
    const double a,
    const double b,
    const double c,
    const double d,
    const double e,
):
    """Equation for elevation of a node at timestep t+1.

    Parameters
    ----------
    x : float
        Value of new elevation
    a : float
        Parameter = K A^m dt / L
    b : float
        Elevation of downstream node, z_j
    c : float
        Parameter = omega_c * dt (dimension of L, because omega_c [=] L/T)
    d : float
        Parameter = K A^m / (L * wc) [=] L^{-1} (so d * z [=] [-])
    e : n/a
        Placeholder; not used
    """
    cdef double f

    f = (1.0 + a) - c * d * exp(-d * (x - b))

    return f



================================================
File: stream_power/fastscape_stream_power.py
================================================
#! /usr/env/python
"""Fastscape stream power erosion."""

# This module attempts to "component-ify" GT's Fastscape stream
# power erosion.
# Created DEJH, March 2014.


import numpy as np

from landlab import Component
from landlab import RasterModelGrid
from landlab.utils.return_array import return_array_at_node

from ..depression_finder.lake_mapper import _FLOODED
from .cfuncs import brent_method_erode_fixed_threshold
from .cfuncs import brent_method_erode_variable_threshold


class FastscapeEroder(Component):
    r"""Fastscape stream power erosion.

    This class uses the Braun-Willett Fastscape approach to calculate the
    amount of erosion at each node in a grid, following a stream power
    framework. This should allow it to be stable against larger timesteps
    than an explicit stream power scheme.

    Note that although this scheme is nominally implicit, and will reach a
    numerically-correct solution under topographic steady state regardless of
    timestep length, the accuracy of transient solutions is *not* timestep
    independent (see Braun & Willett 2013, Appendix B for further details).
    Although the scheme remains significantly more robust and permits longer
    timesteps than a traditional explicit solver under such conditions, it
    is still possible to create numerical instability through use of too long
    a timestep while using this component. The user is cautioned to check their
    implementation is behaving stably before fully trusting it.

    Stream power erosion is implemented as:

    .. math::

        E = K  A ^ m  S ^ n -
               \textit{threshold_sp}

    if :math:`K A ^ m S ^ n > \textit{threshold_sp}`, and:

    .. math:: E = 0,

    if :math:`K A^m S^n <= \textit{threshold_sp}`.

    This module assumes you have already run
    :func:`landlab.components.flow_accum.flow_accumulator.FlowAccumulator.run_one_step`
    in the same timestep. It looks for 'flow__upstream_node_order',
    'flow__link_to_receiver_node', 'drainage_area', 'flow__receiver_node', and
    'topographic__elevation' at the nodes in the grid. 'drainage_area' should
    be in area upstream, not volume (i.e., set runoff_rate=1.0 when calling
    FlowAccumulator.run_one_step).

    The primary method of this class is :func:`run_one_step`.

    Examples
    --------
    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import FlowAccumulator, FastscapeEroder

    >>> grid = RasterModelGrid((5, 5), xy_spacing=10.0)
    >>> z = [
    ...     [7.0, 7.0, 7.0, 7.0, 7.0],
    ...     [7.0, 5.0, 3.2, 6.0, 7.0],
    ...     [7.0, 2.0, 3.0, 5.0, 7.0],
    ...     [7.0, 1.0, 1.9, 4.0, 7.0],
    ...     [7.0, 0.0, 7.0, 7.0, 7.0],
    ... ]
    >>> z = grid.add_field("topographic__elevation", z, at="node")
    >>> fr = FlowAccumulator(grid, flow_director="D8")
    >>> sp = FastscapeEroder(grid, K_sp=1.0)
    >>> fr.run_one_step()
    >>> sp.run_one_step(dt=1.0)
    >>> z
    array([7.        , 7.        , 7.        , 7.        , 7.        ,
           7.        , 2.92996598, 2.02996598, 4.01498299, 7.        ,
           7.        , 0.85993197, 1.87743897, 3.28268321, 7.        ,
           7.        , 0.28989795, 0.85403051, 2.42701526, 7.        ,
           7.        , 0.        , 7.        , 7.        , 7.        ])

    >>> grid = RasterModelGrid((3, 7), xy_spacing=1.0)
    >>> z = np.array(grid.node_x**2.0)
    >>> z = grid.add_field("topographic__elevation", z, at="node")
    >>> grid.status_at_node[grid.nodes_at_left_edge] = grid.BC_NODE_IS_FIXED_VALUE
    >>> grid.status_at_node[grid.nodes_at_top_edge] = grid.BC_NODE_IS_CLOSED
    >>> grid.status_at_node[grid.nodes_at_bottom_edge] = grid.BC_NODE_IS_CLOSED
    >>> grid.status_at_node[grid.nodes_at_right_edge] = grid.BC_NODE_IS_CLOSED
    >>> fr = FlowAccumulator(grid, flow_director="D8")
    >>> sp = FastscapeEroder(grid, K_sp=0.1, m_sp=0.0, n_sp=2.0, threshold_sp=2.0)
    >>> fr.run_one_step()
    >>> sp.run_one_step(dt=10.0)
    >>> z.reshape(grid.shape)[1, :]
    array([ 0.        ,  1.        ,  4.        ,  8.52493772, 13.29039699,
           18.44367949, 36.        ])

    >>> grid = RasterModelGrid((3, 7), xy_spacing=1.0)
    >>> z = np.array(grid.node_x**2.0)
    >>> z = grid.add_field("topographic__elevation", z, at="node")
    >>> grid.status_at_node[grid.nodes_at_left_edge] = grid.BC_NODE_IS_FIXED_VALUE
    >>> grid.status_at_node[grid.nodes_at_top_edge] = grid.BC_NODE_IS_CLOSED
    >>> grid.status_at_node[grid.nodes_at_bottom_edge] = grid.BC_NODE_IS_CLOSED
    >>> grid.status_at_node[grid.nodes_at_right_edge] = grid.BC_NODE_IS_CLOSED
    >>> cell_area = 1.0
    >>> fr = FlowAccumulator(grid, flow_director="D8", runoff_rate=2.0)
    >>> grid.at_node["water__unit_flux_in"]
    array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,
           2., 2., 2., 2., 2., 2., 2., 2.])
    >>> K_field = grid.ones(at="node")  # K can be a field
    >>> sp = FastscapeEroder(
    ...     grid,
    ...     K_sp=K_field,
    ...     m_sp=1.0,
    ...     n_sp=0.6,
    ...     threshold_sp=grid.node_x,
    ...     discharge_field="surface_water__discharge",
    ... )
    >>> fr.run_one_step()
    >>> sp.run_one_step(1.0)
    >>> z.reshape(grid.shape)[1, :]
    array([ 0.        ,  0.06474841,  0.58634459,  2.6725351 ,  8.4921219 ,
           20.92606983, 36.        ])

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    Braun, J., Willett, S. (2013). A very efficient O(n), implicit and parallel
    method to solve the stream power equation governing fluvial incision and
    landscape evolution. Geomorphology  180-181(C), 170-179.
    https://dx.doi.org/10.1016/j.geomorph.2012.10.008

    """

    _name = "FastscapeEroder"

    _unit_agnostic = True

    _info = {
        "drainage_area": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m**2",
            "mapping": "node",
            "doc": "Upstream accumulated surface area contributing to the node's discharge",
        },
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__upstream_node_order": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array containing downstream-to-upstream ordered list of node IDs",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
    }

    def __init__(
        self,
        grid,
        K_sp=0.001,
        m_sp=0.5,
        n_sp=1.0,
        threshold_sp=0.0,
        discharge_field="drainage_area",
        erode_flooded_nodes=True,
    ):
        """Initialize the Fastscape stream power component. Note: a timestep,
        dt, can no longer be supplied to this component through the input file.
        It must instead be passed directly to the run method.

        Parameters
        ----------
        grid : ModelGrid
            A grid.
        K_sp : float, array, or field name
            K in the stream power equation (units vary with other parameters).
        m_sp : float, optional
            m in the stream power equation (power on drainage area).
        n_sp : float, optional
            n in the stream power equation (power on slope).
        threshold_sp : float, array, or field name
            Erosion threshold in the stream power equation.
        discharge_field : float, field name, or array, optional
            Discharge [L^2/T]. The default is to use the grid field
            'drainage_area'. To use custom spatially/temporally varying
            rainfall, use 'water__unit_flux_in' to specify water input to the
            FlowAccumulator and use "surface_water__discharge" for this
            keyword argument.
        erode_flooded_nodes : bool (optional)
            Whether erosion occurs in flooded nodes identified by a
            depression/lake mapper (e.g., DepressionFinderAndRouter). When set
            to false, the field *flood_status_code* must be present on the grid
            (this is created by the DepressionFinderAndRouter). Default True.
        """
        super().__init__(grid)

        if "flow__receiver_node" in grid.at_node and grid.at_node[
            "flow__receiver_node"
        ].size != grid.size("node"):
            raise NotImplementedError(
                "A route-to-multiple flow director has been "
                "run on this grid. The landlab development team has not "
                "verified that FastscapeEroder is compatible with "
                "route-to-multiple methods. Please open a GitHub Issue "
                "to start this process."
            )

        if not erode_flooded_nodes and "flood_status_code" not in self._grid.at_node:
            raise ValueError(
                "In order to not erode flooded nodes another component "
                "must create the field *flood_status_code*. You want to "
                "run a lake mapper/depression finder."
            )

        self._erode_flooded_nodes = erode_flooded_nodes

        # use setter for K defined below
        self.K = K_sp

        self._m = float(m_sp)
        self._n = float(n_sp)

        if isinstance(threshold_sp, (float, int)):
            self._thresholds = float(threshold_sp)
        else:
            self._thresholds = return_array_at_node(grid, threshold_sp)

        self._A = return_array_at_node(grid, discharge_field)

        # make storage variables
        self._A_to_the_m = grid.zeros(at="node")
        self._alpha = grid.empty(at="node")

    @property
    def K(self):
        """Erodibility (units depend on m_sp)."""
        return self._K

    @K.setter
    def K(self, new_val):
        self._K = return_array_at_node(self._grid, new_val)

    def run_one_step(self, dt):
        """Erode for a single time step.

        This method implements the stream power erosion across one time
        interval, dt, following the Braun-Willett (2013) implicit Fastscape
        algorithm.

        This follows Landlab standardized component design, and supercedes the
        old driving method :func:`erode`.

        Parameters
        ----------
        dt : float
            Time-step size
        """
        if not self._erode_flooded_nodes:
            flood_status = self._grid.at_node["flood_status_code"]
            flooded_nodes = np.nonzero(flood_status == _FLOODED)[0]
        else:
            flooded_nodes = []

        upstream_order_IDs = self._grid.at_node["flow__upstream_node_order"]
        flow_receivers = self._grid["node"]["flow__receiver_node"]
        z = self._grid.at_node["topographic__elevation"]

        defined_flow_receivers = np.not_equal(
            self._grid.at_node["flow__link_to_receiver_node"], self._grid.BAD_INDEX
        )

        if isinstance(self._grid, RasterModelGrid):
            flow_link_lengths = self._grid.length_of_d8[
                self._grid.at_node["flow__link_to_receiver_node"][
                    defined_flow_receivers
                ]
            ]
        else:
            flow_link_lengths = self._grid.length_of_link[
                self._grid.at_node["flow__link_to_receiver_node"][
                    defined_flow_receivers
                ]
            ]

        np.power(self._A, self._m, out=self._A_to_the_m)
        self._alpha[defined_flow_receivers] = (
            self._K[defined_flow_receivers]
            * dt
            * self._A_to_the_m[defined_flow_receivers]
            / (flow_link_lengths**self._n)
        )

        # Handle flooded nodes, if any (no erosion there)
        if len(flooded_nodes) > 0:
            self._alpha[flooded_nodes] = 0.0
        else:
            reversed_flow = z < z[flow_receivers]
            # this check necessary if flow has been routed across depressions
            self._alpha[reversed_flow] = 0.0

        threshsdt = self._thresholds * dt

        # solve using Brent's Method in Cython for Speed
        if isinstance(self._thresholds, float):
            brent_method_erode_fixed_threshold(
                upstream_order_IDs, flow_receivers, threshsdt, self._alpha, self._n, z
            )
        else:
            brent_method_erode_variable_threshold(
                upstream_order_IDs, flow_receivers, threshsdt, self._alpha, self._n, z
            )



================================================
File: stream_power/sed_flux_dep_incision.py
================================================
import warnings

import numpy as np
import scipy.constants

from landlab import Component
from landlab import MissingKeyError
from landlab.utils.decorators import make_return_array_immutable


class SedDepEroder(Component):
    """
    This module implements sediment flux dependent channel incision
    following::

        E = f(Qs, Qc) * ([a stream power-like term] - [an optional threshold]),

    where E is the bed erosion rate, Qs is the volumetric sediment flux
    into a node, and Qc is the volumetric sediment transport capacity at
    that node.

    This component is under active research and development; proceed with its
    use at your own risk.

    The details of the implementation are a function of the two key
    arguments, *sed_dependency_type* and *Qc*. The former controls the
    shape of the sediment dependent response function f(Qs, Qc), the
    latter controls the way in which sediment transport capacities are
    calculated (primarily, whether a full Meyer-Peter Muller approach is
    used, or whether simpler stream-power-like equations can be assumed).
    For Qc, 'power_law' broadly follows the assumptions in Gasparini et
    al. 2006, 2007; 'MPM' broadly follows those in Hobley et al., 2011.
    Note that a convex-up channel can result in many cases assuming MPM,
    unless parameters b and c are carefully tuned.

    If ``Qc == 'power_law'``::

        E  = K_sp * f(Qs, Qc) * A ** m_sp * S ** n_sp;
        Qc = K_t * A ** m_t * S ** n_t

    If ``Qc == 'MPM'``::

        shear_stress = fluid_density * g * depth * S
                     = fluid_density * g * (mannings_n/k_w) ** 0.6 * (
                       k_Q* A ** c_sp) ** (0.6 * (1. - b_sp)) * S ** 0.7,
                       for consistency with MPM

        E = K_sp * f(Qs, Qc) * (shear_stress ** a_sp - [threshold_sp])

        Qc = 8 * C_MPM * sqrt((sed_density-fluid_density)/fluid_density *
             g * D_char**3) * (shields_stress - threshold_shields)**1.5

        shields_stress = shear_stress / (g * (sed_density-fluid_density) *
                         D_char)

    If you choose Qc='MPM', you may provide thresholds for both channel
    incision and shields number, or alternatively set either or both of
    these threshold dynamically. The minimum shear stress can be made
    equivalent to the Shields number using *set_threshold_from_Dchar*,
    for full consistency with the MPM approach (i.e., the threshold
    becomes a function of the characteristic grain size on the bed). The
    Shields threshold itself can also be a weak function of slope if
    *slope_sensitive_threshold*, following Lamb et al. 2008,
    taustar_c = 0.15 * S ** 0.25.

    The component is able to handle flooded nodes, if created by a lake
    filler. It assumes the flow paths found in the fields already reflect
    any lake routing operations, and then requires the optional argument
    *flooded_depths* be passed to the run method. A flooded depression
    acts as a perfect sediment trap, and will be filled sequentially
    from the inflow points towards the outflow points.

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    Hobley, D. E. J., Sinclair, H. D., Mudd, S. M., and Cowie, P. A.: Field
    calibration of sediment ﬂux dependent river incision, J. Geophys. Res.,
    116, F04017, doi:10.1029/2010JF001935, 2011.

    """

    _name = "SedDepEroder"

    _unit_agnostic = False

    _info = {
        "channel__bed_shear_stress": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "Pa",
            "mapping": "node",
            "doc": (
                "Shear exerted on the bed of the channel, assuming all "
                "discharge travels along a single, self-formed channel"
            ),
        },
        "channel__depth": {
            "dtype": float,
            "intent": "out",
            "optional": True,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of the a single channel carrying all runoff through the node",
        },
        "channel__discharge": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m**3/s",
            "mapping": "node",
            "doc": (
                "Volumetric water flux of the a single channel carrying all "
                "runoff through the node"
            ),
        },
        "channel__width": {
            "dtype": float,
            "intent": "out",
            "optional": True,
            "units": "m",
            "mapping": "node",
            "doc": "Width of the a single channel carrying all runoff through the node",
        },
        "channel_sediment__relative_flux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": (
                "The fluvial_sediment_flux_into_node divided by the "
                "fluvial_sediment_transport_capacity"
            ),
        },
        "channel_sediment__volumetric_flux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m**3/s",
            "mapping": "node",
            "doc": "Total volumetric fluvial sediment flux brought into the node from upstream",
        },
        "channel_sediment__volumetric_transport_capacity": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m**3/s",
            "mapping": "node",
            "doc": (
                "Volumetric transport capacity of a channel carrying all runoff "
                "through the node, assuming the Meyer-Peter Muller transport equation"
            ),
        },
        "drainage_area": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m**2",
            "mapping": "node",
            "doc": "Upstream accumulated surface area contributing to the node's discharge",
        },
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__upstream_node_order": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array containing downstream-to-upstream ordered list of node IDs",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__steepest_slope": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "The steepest *downhill* slope",
        },
    }

    def __init__(
        self,
        grid,
        K_sp=1.0e-6,
        g=scipy.constants.g,
        rock_density=2700,
        sediment_density=2700,
        fluid_density=1000,
        runoff_rate=1.0,
        sed_dependency_type="generalized_humped",
        kappa_hump=13.683,
        nu_hump=1.13,
        phi_hump=4.24,
        c_hump=0.00181,
        Qc="power_law",
        m_sp=0.5,
        n_sp=1.0,
        K_t=1.0e-4,
        m_t=1.5,
        n_t=1.0,
        # these params for Qc='MPM':
        C_MPM=1.0,
        a_sp=1.0,
        b_sp=0.5,
        c_sp=1.0,
        k_w=2.5,
        k_Q=2.5e-7,
        mannings_n=0.05,
        threshold_shear_stress=None,
        Dchar=0.05,
        set_threshold_from_Dchar=True,
        set_Dchar_from_threshold=False,
        threshold_Shields=0.05,
        slope_sensitive_threshold=False,
        # params for model numeric behavior:
        pseudoimplicit_repeats=5,
        return_stream_properties=False,
        # flooded node info
        flooded_depths=None,
    ):
        """Constructor for the class.

        Parameters
        ----------
        grid : a ModelGrid
            A grid.
        K_sp : float (time unit must be *years*)
            K in the stream power equation; the prefactor on the erosion
            equation (units vary with other parameters).
        g : float (m/s**2)
            Acceleration due to gravity.
        rock_density : float (Kg m**-3)
            Bulk intact rock density.
        sediment_density : float (Kg m**-3)
            Typical density of loose sediment on the bed.
        fluid_density : float (Kg m**-3)
            Density of the fluid.
        runoff_rate : float, array or field name (m/s)
            The rate of excess overland flow production at each node (i.e.,
            rainfall rate less infiltration).
        pseudoimplicit_repeats : int
            Number of loops to perform with the pseudoimplicit iterator,
            seeking a stable solution. Convergence is typically rapid.
        return_stream_properties : bool
            Whether to perform a few additional calculations in order to set
            the additional optional output fields, 'channel__width',
            'channel__depth', and 'channel__discharge' (default False).
        sed_dependency_type : {'generalized_humped', 'None', 'linear_decline',
                               'almost_parabolic'}
            The shape of the sediment flux function. For definitions, see
            Hobley et al., 2011. 'None' gives a constant value of 1.
            NB: 'parabolic' is currently not supported, due to numerical
            stability issues at channel heads.
        Qc : {'power_law', 'MPM'}
            Whether to use simple stream-power-like equations for both
            sediment transport capacity and erosion rate, or more complex
            forms based directly on the Meyer-Peter Muller equation and a
            shear stress based erosion model consistent with MPM (per
            Hobley et al., 2011).

        If ``sed_dependency_type == 'generalized_humped'``...

        kappa_hump : float
            Shape parameter for sediment flux function. Primarily controls
            function amplitude (i.e., scales the function to a maximum of 1).
            Default follows Leh valley values from Hobley et al., 2011.
        nu_hump : float
            Shape parameter for sediment flux function. Primarily controls
            rate of rise of the "tools" limb. Default follows Leh valley
            values from Hobley et al., 2011.
        phi_hump : float
            Shape parameter for sediment flux function. Primarily controls
            rate of fall of the "cover" limb. Default follows Leh valley
            values from Hobley et al., 2011.
        c_hump : float
            Shape parameter for sediment flux function. Primarily controls
            degree of function asymmetry. Default follows Leh valley values
            from Hobley et al., 2011.

        If ``Qc == 'power_law'``...

        m_sp : float
            Power on drainage area in the erosion equation.
        n_sp : float
            Power on slope in the erosion equation.
        K_t : float (time unit must be in *years*)
            Prefactor in the transport capacity equation.
        m_t : float
            Power on drainage area in the transport capacity equation.
        n_t : float
            Power on slope in the transport capacity equation.

        if ``Qc == 'MPM'``...

        C_MPM : float
            A prefactor on the MPM relation, allowing tuning to known sediment
            saturation conditions (leave as 1. in most cases).
        a_sp : float
            Power on shear stress to give erosion rate.
        b_sp : float
            Power on drainage area to give channel width.
        c_sp : float
            Power on drainage area to give discharge.
        k_w : float (unit variable with b_sp)
            Prefactor on A**b_sp to give channel width.
        k_Q : float (unit variable with c_sp, but time unit in *seconds*)
            Prefactor on A**c_sp to give discharge.
        mannings_n : float
            Manning's n for the channel.
        threshold_shear_stress : None or float (Pa)
            The threshold shear stress in the equation for erosion rate. If
            None, implies that *set_threshold_from_Dchar* is True, and this
            parameter will get set from the Dchar value and critical Shields
            number.
        Dchar :None, float, array, or field name (m)
            The characteristic grain size on the bed, that controls the
            relationship between critical Shields number and critical shear
            stress. If None, implies that *set_Dchar_from_threshold* is True,
            and this parameter will get set from the threshold_shear_stress
            value and critical Shields number.
        set_threshold_from_Dchar : bool
            If True (default), threshold_shear_stress will be set based on
            Dchar and threshold_Shields.
        set_Dchar_from_threshold : bool
            If True, Dchar will be set based on threshold_shear_stress and
            threshold_Shields. Default is False.
        threshold_Shields : None or float
            The threshold Shields number. If None, implies that
            *slope_sensitive_threshold* is True.
        slope_sensitive_threshold : bool
            If True, the threshold_Shields will be set according to
            0.15 * S ** 0.25, per Lamb et al., 2008 & Hobley et al., 2011.
        flooded_depths : array or field name (m)
            Depths of flooding at each node, zero where no lake. Note that the
            component will dynamically update this array as it fills nodes
            with sediment (...but does NOT update any other related lake
            fields).
        """
        super().__init__(grid)

        if "flow__receiver_node" in grid.at_node and grid.at_node[
            "flow__receiver_node"
        ].size != grid.size("node"):
            raise NotImplementedError(
                "A route-to-multiple flow director has been "
                "run on this grid. The landlab development team has not "
                "verified that SedDepEroder is compatible with "
                "route-to-multiple methods. Please open a GitHub Issue "
                "to start this process."
            )
        self._flooded_depths = flooded_depths
        self._pseudoimplicit_repeats = pseudoimplicit_repeats

        self._link_S_with_trailing_blank = np.zeros(grid.number_of_links + 1)
        # ^needs to be filled with values in execution
        self._countactive_links = np.zeros_like(
            self._link_S_with_trailing_blank, dtype=int
        )
        self._countactive_links[:-1] = 1

        self._K_unit_time = K_sp / 31557600.0
        # ^...because we work with dt in seconds
        # set gravity
        self._g = g
        self._rock_density = rock_density
        self._sed_density = sediment_density
        self._fluid_density = fluid_density
        self._relative_weight = (
            (self._sed_density - self._fluid_density) / self._fluid_density * self._g
        )
        # ^to accelerate MPM calcs
        self._rho_g = self._fluid_density * self._g
        self._type = sed_dependency_type
        assert self._type in (
            "generalized_humped",
            "None",
            "linear_decline",
            "almost_parabolic",
        )
        self._Qc = Qc
        assert self._Qc in ("MPM", "power_law")
        self._return_ch_props = return_stream_properties
        if return_stream_properties:
            assert self._Qc == "MPM", (
                "Qc must be 'MPM' to return stream " + "properties"
            )
        if isinstance(runoff_rate, (float, int)):
            self._runoff_rate = float(runoff_rate)
        elif isinstance(runoff_rate, str):
            self._runoff_rate = self._grid.at_node[runoff_rate]
        else:
            self._runoff_rate = np.array(runoff_rate)
            assert runoff_rate.size == self._grid.number_of_nodes

        if self._Qc == "MPM":
            if threshold_shear_stress is not None:
                self._thresh = threshold_shear_stress
                self._set_threshold = True
                # ^flag for sed_flux_dep_incision to see if the threshold was
                # manually set.
                # print("Found a shear stress threshold to use: ", self._thresh)
            else:
                warnings.warn("Found no incision threshold to use.", stacklevel=2)
                self._thresh = 0.0
                self._set_threshold = False
            self._a = a_sp
            self._b = b_sp
            self._c = c_sp

            self._k_Q = k_Q
            self._k_w = k_w
            self._mannings_n = mannings_n
            if mannings_n < 0.0 or mannings_n > 0.2:
                warnings.warn("Manning's n outside it's typical range", stacklevel=2)

            self._diffusivity_power_on_A = 0.9 * self._c * (1.0 - self._b)
            # ^i.e., q/D**(1/6)

            self._override_threshold = set_threshold_from_Dchar
            self._override_Dchar = set_Dchar_from_threshold
            if self._override_threshold:
                assert self._set_threshold is False, (
                    "If set_threshold_from_Dchar, threshold_Shields must be "
                    + "set to None"
                )
                assert self._override_Dchar is False
            if self._override_Dchar:
                assert self._override_threshold is False

            self._shields_crit = threshold_Shields
            self._lamb_flag = slope_sensitive_threshold
            if self._lamb_flag:
                assert self._shields_crit is None, (
                    "If slope_sensitive_threshold, threshold_Shields must "
                    + "be set to None"
                )

        elif self._Qc == "power_law":
            self._m = m_sp
            self._n = n_sp
            self._Kt = K_t / 31557600.0  # in sec
            self._mt = m_t
            self._nt = n_t

        # now conditional inputs
        if self._type == "generalized_humped":
            self._kappa = kappa_hump
            self._nu = nu_hump
            self._phi = phi_hump
            self._c = c_hump

        if self._Qc == "MPM":
            if Dchar is not None:
                if isinstance(Dchar, (int, float)):
                    self._Dchar_in = float(Dchar)
                elif isinstance(Dchar, str):
                    self._Dchar_in = self._grid.at_node[Dchar]
                else:
                    self._Dchar_in = np.array(Dchar)
                    assert self._Dchar_in.size == self._grid.number_of_nodes
                assert (
                    not self._override_Dchar
                ), "If set_Dchar_from_threshold, Dchar must be set to None"
            else:
                assert self._override_Dchar
                # remember the threshold getting set is already tau**a
                if not self._lamb_flag:
                    self._Dchar_in = (
                        self._thresh
                        / self._g
                        / (self._sed_density - self._fluid_density)
                        / self._shields_crit
                    )
                else:
                    self._Dchar_in = None
            self._C_MPM = C_MPM

            if self._override_threshold:
                # print("Overriding any supplied threshold...")
                try:
                    self._thresh = (
                        self._shields_crit
                        * self._g
                        * (self._sed_density - self._fluid_density)
                        * self._Dchar_in
                    )
                except AttributeError:
                    self._thresh = (
                        self._shields_crit
                        * self._g
                        * (self._sed_density - self._fluid_density)
                        * Dchar
                    )

            # new 11/12/14
            self._point6onelessb = 0.6 * (1.0 - self._b)
            self._shear_stress_prefactor = (
                self._fluid_density * self._g * (self._mannings_n / self._k_w) ** 0.6
            )

            if self._set_threshold is False or self._override_threshold:
                try:
                    self._shields_prefactor_to_shear = (
                        (self._sed_density - self._fluid_density)
                        * self._g
                        * self._Dchar_in
                    )
                except AttributeError:  # no Dchar
                    self._shields_prefactor_to_shear_noDchar = (
                        self._sed_density - self._fluid_density
                    ) * self._g

            twothirds = 2.0 / 3.0
            self._Qs_prefactor = (
                4.0
                * self._C_MPM**twothirds
                * self._fluid_density**twothirds
                / (self._sed_density - self._fluid_density) ** twothirds
                * self._g ** (twothirds / 2.0)
                * mannings_n**0.6
                * self._k_w ** (1.0 / 15.0)
                * self._k_Q ** (0.6 + self._b / 15.0)
                / self._sed_density**twothirds
            )
            self._Qs_thresh_prefactor = (
                4.0
                * (
                    self._C_MPM
                    * self._k_w
                    * self._k_Q**self._b
                    / self._fluid_density**0.5
                    / (self._sed_density - self._fluid_density)
                    / self._g
                    / self._sed_density
                )
                ** twothirds
            )
            # both these are divided by sed density to give a vol flux
            self._Qs_power_onA = self._c * (0.6 + self._b / 15.0)
            self._Qs_power_onAthresh = twothirds * self._b * self._c

        self._cell_areas = np.empty(grid.number_of_nodes)
        self._cell_areas.fill(np.mean(grid.area_of_cell))
        self._cell_areas[grid.node_at_cell] = grid.area_of_cell

        # set up the necessary fields:
        self.initialize_output_fields()
        if self._return_ch_props:
            self.initialize_optional_output_fields()

    def get_sed_flux_function(self, rel_sed_flux):
        """Get the sediment flux function.

        Parameters
        ----------
        rel_sed_flux
        """
        if self._type == "generalized_humped":
            """Returns K*f(qs,qc)"""
            sed_flux_fn = (
                self._kappa
                * (rel_sed_flux**self._nu + self._c)
                * np.exp(-self._phi * rel_sed_flux)
            )
        elif self._type == "linear_decline":
            sed_flux_fn = 1.0 - rel_sed_flux
        elif self._type == "parabolic":
            raise MissingKeyError(
                "Pure parabolic (where intersect at zero flux is exactly "
                + "zero) is currently not supported, sorry. Try "
                + "almost_parabolic instead?"
            )
            sed_flux_fn = 1.0 - 4.0 * (rel_sed_flux - 0.5) ** 2.0
        elif self._type == "almost_parabolic":
            sed_flux_fn = np.where(
                rel_sed_flux > 0.1,
                1.0 - 4.0 * (rel_sed_flux - 0.5) ** 2.0,
                2.6 * rel_sed_flux + 0.1,
            )
        elif self._type == "None":
            sed_flux_fn = 1.0
        else:
            raise MissingKeyError(
                "Provided sed flux sensitivity type in input file was not "
                + "recognised!"
            )
        return sed_flux_fn

    def get_sed_flux_function_pseudoimplicit(
        self, sed_in, trans_cap_vol_out, prefactor_for_volume, prefactor_for_dz
    ):
        """Get the pseudoimplicit sediment flux function.

        Parameters
        ----------
        sed_in
        trans_cap_vol_out
        prefactor_for_volume
        prefactor_for_dz
        """
        rel_sed_flux_in = sed_in / trans_cap_vol_out
        rel_sed_flux = rel_sed_flux_in

        if self._type == "generalized_humped":
            """Returns K*f(qs,qc)"""

            def sed_flux_fn_gen(rel_sed_flux_in):
                return (
                    self._kappa
                    * (rel_sed_flux_in**self._nu + self._c)
                    * np.exp(-self._phi * rel_sed_flux_in)
                )

        elif self._type == "linear_decline":

            def sed_flux_fn_gen(rel_sed_flux_in):
                return 1.0 - rel_sed_flux_in

        elif self._type == "parabolic":
            raise MissingKeyError(
                "Pure parabolic (where intersect at zero flux is exactly "
                + "zero) is currently not supported, sorry. Try "
                + "almost_parabolic instead?"
            )

            def sed_flux_fn_gen(rel_sed_flux_in):
                return 1.0 - 4.0 * (rel_sed_flux_in - 0.5) ** 2.0

        elif self._type == "almost_parabolic":

            def sed_flux_fn_gen(rel_sed_flux_in):
                return np.where(
                    rel_sed_flux_in > 0.1,
                    1.0 - 4.0 * (rel_sed_flux_in - 0.5) ** 2.0,
                    2.6 * rel_sed_flux_in + 0.1,
                )

        elif self._type == "None":

            def sed_flux_fn_gen(rel_sed_flux_in):
                return 1.0

        else:
            raise MissingKeyError(
                "Provided sed flux sensitivity type in input file was not "
                + "recognised!"
            )

        for _ in range(self._pseudoimplicit_repeats):
            sed_flux_fn = sed_flux_fn_gen(rel_sed_flux)
            sed_vol_added = prefactor_for_volume * sed_flux_fn
            rel_sed_flux = rel_sed_flux_in + sed_vol_added / trans_cap_vol_out
            # print rel_sed_flux
            if rel_sed_flux >= 1.0:
                rel_sed_flux = 1.0
                break
            if rel_sed_flux < 0.0:
                rel_sed_flux = 0.0
                break
        last_sed_flux_fn = sed_flux_fn
        sed_flux_fn = sed_flux_fn_gen(rel_sed_flux)
        # this error could alternatively be used to break the loop
        error_in_sed_flux_fn = sed_flux_fn - last_sed_flux_fn
        dz = prefactor_for_dz * sed_flux_fn
        sed_flux_out = rel_sed_flux * trans_cap_vol_out
        return dz, sed_flux_out, rel_sed_flux, error_in_sed_flux_fn

    def run_one_step(self, dt):
        """Run the component across one timestep increment, dt.

        Erosion occurs according to the sediment dependent rules specified
        during initialization. Method is fully equivalent to the :func:`erode`
        method.

        Parameters
        ----------
        dt : float (years, only!)
            Timestep for which to run the component.
        """

        grid = self._grid
        node_z = grid.at_node["topographic__elevation"]
        node_A = grid.at_node["drainage_area"]
        flow_receiver = grid.at_node["flow__receiver_node"]
        s_in = grid.at_node["flow__upstream_node_order"]
        node_S = grid.at_node["topographic__steepest_slope"]

        if isinstance(self._flooded_depths, str):
            flooded_depths = grid.at_node[self._flooded_depths]
            # also need a map of initial flooded conds:
            flooded_nodes = flooded_depths > 0.0
        elif isinstance(self._flooded_depths, np.ndarray):
            assert self._flooded_depths.size == self._grid.number_of_nodes
            flooded_nodes = self._flooded_depths > 0.0
            # need an *updateable* record of the pit depths
        else:
            # if None, handle in loop
            flooded_nodes = None
        steepest_link = "flow__link_to_receiver_node"
        link_length = np.empty(grid.number_of_nodes, dtype=float)
        link_length.fill(np.nan)
        draining_nodes = np.not_equal(grid.at_node[steepest_link], self._grid.BAD_INDEX)
        core_draining_nodes = np.intersect1d(
            np.where(draining_nodes)[0], grid.core_nodes, assume_unique=True
        )
        link_length[core_draining_nodes] = grid.length_of_d8[
            grid.at_node[steepest_link][core_draining_nodes]
        ]

        if self._Qc == "MPM":
            if self._Dchar_in is not None:
                self._Dchar = self._Dchar_in
            else:
                assert not self._set_threshold, (
                    "Something is seriously wrong with your model " + "initialization."
                )
                assert self._override_threshold, (
                    "You need to confirm to the module you intend it to "
                    + "internally calculate a shear stress threshold, "
                    + "with set_threshold_from_Dchar in the input file."
                )
                # we need to adjust the thresholds for the Shields number
                # & gs dynamically:
                variable_thresh = (
                    self._shields_crit
                    * self._g
                    * (self._sed_density - self._fluid_density)
                    * self._Dchar
                )
            if self._lamb_flag:
                variable_shields_crit = 0.15 * node_S**0.25
                try:
                    variable_thresh = (
                        variable_shields_crit * self._shields_prefactor_to_shear
                    )
                except AttributeError:
                    variable_thresh = (
                        variable_shields_crit
                        * self._shields_prefactor_to_shear_noDchar
                        * self._Dchar
                    )

            node_Q = self._k_Q * self._runoff_rate * node_A**self._c
            shear_stress_prefactor_timesAparts = (
                self._shear_stress_prefactor * node_Q**self._point6onelessb
            )
            try:
                transport_capacities_thresh = (
                    self._thresh
                    * self._Qs_thresh_prefactor
                    * self._runoff_rate ** (0.66667 * self._b)
                    * node_A**self._Qs_power_onAthresh
                )
            except AttributeError:
                transport_capacities_thresh = (
                    variable_thresh
                    * self._Qs_thresh_prefactor
                    * self._runoff_rate ** (0.66667 * self._b)
                    * node_A**self._Qs_power_onAthresh
                )

            transport_capacity_prefactor_withA = (
                self._Qs_prefactor
                * self._runoff_rate ** (0.6 + self._b / 15.0)
                * node_A**self._Qs_power_onA
            )

            internal_t = 0.0
            break_flag = False
            dt_secs = dt * 31557600.0
            counter = 0
            rel_sed_flux = np.empty_like(node_Q)
            # excess_vol_overhead = 0.

            while 1:
                # ^use the break flag, to improve computational efficiency for
                # runs which are very stable
                # we assume the drainage structure is forbidden to change
                # during the whole dt
                # note slopes will be *negative* at pits
                # track how many loops we perform:
                counter += 1
                downward_slopes = node_S.clip(0.0)
                # this removes the tendency to transfer material against
                # gradient, including in any lake depressions
                # we DON'T immediately zero trp capacity in the lake.
                # positive_slopes = np.greater(downward_slopes, 0.)
                slopes_tothe07 = downward_slopes**0.7
                transport_capacities_S = (
                    transport_capacity_prefactor_withA * slopes_tothe07
                )
                trp_diff = (transport_capacities_S - transport_capacities_thresh).clip(
                    0.0
                )
                transport_capacities = np.sqrt(trp_diff * trp_diff * trp_diff)
                shear_stress = shear_stress_prefactor_timesAparts * slopes_tothe07
                shear_tothe_a = shear_stress**self._a

                dt_this_step = dt_secs - internal_t
                # ^timestep adjustment is made AFTER the dz calc
                node_vol_capacities = transport_capacities * dt_this_step

                sed_into_node = np.zeros(grid.number_of_nodes, dtype=float)
                dz = np.zeros(grid.number_of_nodes, dtype=float)
                cell_areas = self._cell_areas
                try:
                    raise NameError
                    # ^tripped out deliberately for now; doesn't appear to
                    # accelerate much
                    weave.inline(
                        self._routing_code,
                        [
                            "len_s_in",
                            "sed_into_node",
                            "transport_capacities",
                            "dz",
                            "cell_areas",
                            "dt_this_step",
                            "flow__receiver_node",
                        ],
                    )
                except NameError:
                    for i in s_in[::-1]:  # work downstream
                        cell_area = cell_areas[i]
                        if flooded_nodes is not None:
                            flood_depth = flooded_depths[i]
                        else:
                            flood_depth = 0.0
                        sed_flux_into_this_node = sed_into_node[i]
                        node_capacity = transport_capacities[i]
                        # ^we work in volume flux, not volume per se here
                        node_vol_capacity = node_vol_capacities[i]
                        if flood_depth > 0.0:
                            node_vol_capacity = 0.0
                            # requires special case handling - as much sed as
                            # possible is dumped here, then the remainder
                            # passed on
                        if sed_flux_into_this_node < node_vol_capacity:
                            # ^note incision is forbidden at capacity
                            # flooded nodes never enter this branch
                            # #implementing the pseudoimplicit method:
                            try:
                                thresh = variable_thresh
                            except NameError:  # it doesn't exist
                                thresh = self._thresh
                            dz_prefactor = (
                                self._K_unit_time
                                * dt_this_step
                                * (shear_tothe_a[i] - thresh).clip(0.0)
                            )
                            vol_prefactor = dz_prefactor * cell_area
                            (
                                dz_here,
                                sed_flux_out,
                                rel_sed_flux_here,
                                error_in_sed_flux,
                            ) = self.get_sed_flux_function_pseudoimplicit(
                                sed_flux_into_this_node,
                                node_vol_capacity,
                                vol_prefactor,
                                dz_prefactor,
                            )
                            # note now dz_here may never create more sed than
                            # the out can transport...
                            assert sed_flux_out <= node_vol_capacity, (
                                "failed at node "
                                + str(s_in.size - i)
                                + " with rel sed flux "
                                + str(sed_flux_out / node_capacity)
                            )
                            rel_sed_flux[i] = rel_sed_flux_here
                            vol_pass = sed_flux_out
                        else:
                            rel_sed_flux[i] = 1.0
                            vol_dropped = sed_flux_into_this_node - node_vol_capacity
                            dz_here = -vol_dropped / cell_area
                            # with the pits, we aim to inhibit incision, but
                            # depo is OK. We have already zero'd any adverse
                            # grads, so sed can make it to the bottom of the
                            # pit but no further in a single step, which seems
                            # raeasonable. Pit should fill.
                            if flood_depth <= 0.0:
                                vol_pass = node_vol_capacity
                            else:
                                height_excess = -dz_here - flood_depth
                                # ...above water level
                                if height_excess <= 0.0:
                                    vol_pass = 0.0
                                    # dz_here is already correct
                                    flooded_depths[i] += dz_here
                                else:
                                    dz_here = -flood_depth
                                    vol_pass = height_excess * cell_area
                                    # ^bit cheeky?
                                    flooded_depths[i] = 0.0
                                    # note we must update flooded depths
                                    # transiently to conserve mass
                            # do we need to retain a small downhill slope?
                            # ...don't think so. Will resolve itself on next
                            # timestep.

                        dz[i] -= dz_here
                        sed_into_node[flow_receiver[i]] += vol_pass

                break_flag = True

                node_z[grid.core_nodes] += dz[grid.core_nodes]

                if break_flag:
                    break
                # do we need to reroute the flow/recalc the slopes here?
                # -> NO, slope is such a minor component of Diff we'll be OK
                # BUT could be important not for the stability, but for the
                # actual calc. So YES.
                node_S = np.zeros_like(node_S)
                node_S[core_draining_nodes] = (node_z - node_z[flow_receiver])[
                    core_draining_nodes
                ] / link_length[core_draining_nodes]
                internal_t += dt_this_step  # still in seconds, remember

        elif self._Qc == "power_law":
            transport_capacity_prefactor_withA = self._Kt * node_A**self._mt
            erosion_prefactor_withA = self._K_unit_time * node_A**self._m
            # ^doesn't include S**n*f(Qc/Qc)
            internal_t = 0.0
            break_flag = False
            dt_secs = dt * 31557600.0
            counter = 0
            rel_sed_flux = np.empty_like(node_A)
            while 1:
                counter += 1
                # print counter
                downward_slopes = node_S.clip(0.0)
                # positive_slopes = np.greater(downward_slopes, 0.)
                slopes_tothen = downward_slopes**self._n
                slopes_tothent = downward_slopes**self._nt
                transport_capacities = (
                    transport_capacity_prefactor_withA * slopes_tothent
                )
                erosion_prefactor_withS = (
                    erosion_prefactor_withA * slopes_tothen
                )  # no time, no fqs
                # shear_tothe_a = shear_stress**self._a

                dt_this_step = dt_secs - internal_t
                # ^timestep adjustment is made AFTER the dz calc
                node_vol_capacities = transport_capacities * dt_this_step

                sed_into_node = np.zeros(grid.number_of_nodes, dtype=float)
                dz = np.zeros(grid.number_of_nodes, dtype=float)
                cell_areas = self._cell_areas
                for i in s_in[::-1]:  # work downstream
                    cell_area = cell_areas[i]
                    if flooded_nodes is not None:
                        flood_depth = flooded_depths[i]
                    else:
                        flood_depth = 0.0
                    sed_flux_into_this_node = sed_into_node[i]
                    node_capacity = transport_capacities[i]
                    # ^we work in volume flux, not volume per se here
                    node_vol_capacity = node_vol_capacities[i]
                    if flood_depth > 0.0:
                        node_vol_capacity = 0.0
                    if sed_flux_into_this_node < node_vol_capacity:
                        # ^note incision is forbidden at capacity
                        dz_prefactor = dt_this_step * erosion_prefactor_withS[i]
                        vol_prefactor = dz_prefactor * cell_area
                        (
                            dz_here,
                            sed_flux_out,
                            rel_sed_flux_here,
                            error_in_sed_flux,
                        ) = self.get_sed_flux_function_pseudoimplicit(
                            sed_flux_into_this_node,
                            node_vol_capacity,
                            vol_prefactor,
                            dz_prefactor,
                        )
                        # note now dz_here may never create more sed than the
                        # out can transport...
                        assert sed_flux_out <= node_vol_capacity, (
                            "failed at node "
                            + str(s_in.size - i)
                            + " with rel sed flux "
                            + str(sed_flux_out / node_capacity)
                        )
                        rel_sed_flux[i] = rel_sed_flux_here
                        vol_pass = sed_flux_out
                    else:
                        rel_sed_flux[i] = 1.0
                        vol_dropped = sed_flux_into_this_node - node_vol_capacity
                        dz_here = -vol_dropped / cell_area
                        try:
                            isflooded = flooded_nodes[i]
                        except TypeError:  # was None
                            isflooded = False
                        if flood_depth <= 0.0 and not isflooded:
                            vol_pass = node_vol_capacity
                            # we want flooded nodes which have already been
                            # filled to enter the else statement
                        else:
                            height_excess = -dz_here - flood_depth
                            # ...above water level
                            if height_excess <= 0.0:
                                vol_pass = 0.0
                                # dz_here is already correct
                                flooded_depths[i] += dz_here
                            else:
                                dz_here = -flood_depth
                                vol_pass = height_excess * cell_area
                                # ^bit cheeky?
                                flooded_depths[i] = 0.0

                    dz[i] -= dz_here
                    sed_into_node[flow_receiver[i]] += vol_pass
                break_flag = True

                node_z[grid.core_nodes] += dz[grid.core_nodes]

                if break_flag:
                    break
                # do we need to reroute the flow/recalc the slopes here?
                # -> NO, slope is such a minor component of Diff we'll be OK
                # BUT could be important not for the stability, but for the
                # actual calc. So YES.
                node_S = np.zeros_like(node_S)
                # print link_length[core_draining_nodes]
                node_S[core_draining_nodes] = (node_z - node_z[flow_receiver])[
                    core_draining_nodes
                ] / link_length[core_draining_nodes]
                internal_t += dt_this_step  # still in seconds, remember

        if self._return_ch_props:
            # add the channel property field entries,
            # 'channel__width', 'channel__depth', and 'channel__discharge'
            W = self._k_w * node_Q**self._b
            H = shear_stress / self._rho_g / node_S  # ...sneaky!
            grid.at_node["channel__width"][:] = W
            grid.at_node["channel__depth"][:] = H
            grid.at_node["channel__discharge"][:] = node_Q
            grid.at_node["channel__bed_shear_stress"][:] = shear_stress

        grid.at_node["channel_sediment__volumetric_transport_capacity"][
            :
        ] = transport_capacities
        grid.at_node["channel_sediment__volumetric_flux"][:] = sed_into_node
        grid.at_node["channel_sediment__relative_flux"][:] = rel_sed_flux
        # elevs set automatically to the name used in the function call.
        self._iterations_in_dt = counter

        return grid, grid.at_node["topographic__elevation"]

    @property
    @make_return_array_immutable
    def characteristic_grainsize(self):
        """Return the characteristic grain size used by the component.

        Particularly useful if the set_Dchar_from_threshold flag was True
        at initialization.

        Returns
        -------
        Dchar : float or array

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import FlowAccumulator, SedDepEroder
        >>> mg1 = RasterModelGrid((3, 4))
        >>> z1 = mg1.add_zeros("node", "topographic__elevation")
        >>> fa1 = FlowAccumulator(mg1)
        >>> thresh_shields = np.arange(1, mg1.number_of_nodes + 1, dtype=float)
        >>> thresh_shields /= 100.0
        >>> sde1 = SedDepEroder(
        ...     mg1,
        ...     threshold_shear_stress=100.0,
        ...     Qc="MPM",
        ...     Dchar=None,
        ...     set_threshold_from_Dchar=False,
        ...     set_Dchar_from_threshold=True,
        ...     threshold_Shields=thresh_shields,
        ...     g=9.81,
        ... )
        >>> sde1.characteristic_grainsize.reshape(mg1.shape)
        array([[0.59962823, 0.29981412, 0.19987608, 0.14990706],
               [0.11992565, 0.09993804, 0.08566118, 0.07495353],
               [0.06662536, 0.05996282, 0.05451166, 0.04996902]])

        >>> mg2 = RasterModelGrid((3, 4))
        >>> z2 = mg2.add_zeros("node", "topographic__elevation")
        >>> fa2 = FlowAccumulator(mg2)
        >>> sde2 = SedDepEroder(
        ...     mg2,
        ...     threshold_shear_stress=100.0,
        ...     Qc="MPM",
        ...     Dchar=None,
        ...     set_threshold_from_Dchar=False,
        ...     set_Dchar_from_threshold=True,
        ...     threshold_Shields=None,
        ...     slope_sensitive_threshold=True,
        ...     g=9.81,
        ... )
        >>> S = mg2.at_node["topographic__steepest_slope"]
        >>> S[:] = 0.05  # thresh = 100 Pa @ 5pc slope
        >>> sde2.characteristic_grainsize.reshape(mg2.shape)
        array([[0.08453729, 0.08453729, 0.08453729, 0.08453729],
               [0.08453729, 0.08453729, 0.08453729, 0.08453729],
               [0.08453729, 0.08453729, 0.08453729, 0.08453729]])
        """
        # Dchar is None means self._lamb_flag, Dchar is spatially variable,
        # and not calculated until the main loop
        assert (
            self._Qc == "MPM"
        ), "Characteristic grainsize is only calculated if Qc == 'MPM'"
        if self._Dchar_in is not None:
            return self._Dchar_in
        else:
            taustarcrit = (
                0.15 * self._grid.at_node["topographic__steepest_slope"] ** 0.25
            )
            Dchar = self._thresh / (
                self._g * (self._sed_density - self._fluid_density) * taustarcrit
            )
            return Dchar



================================================
File: stream_power/stream_power.py
================================================
import numpy as np

from landlab import Component
from landlab import MissingKeyError
from landlab.utils.return_array import return_array_at_node

from ..depression_finder.lake_mapper import _FLOODED
from .cfuncs import brent_method_erode_fixed_threshold
from .cfuncs import brent_method_erode_variable_threshold


class StreamPowerEroder(Component):
    """Erode where channels are.

    Implemented as:

    .. math::
        E = K A^m S^n - sp_{crit},

    and if :math:`E < 0`, :math:`E = 0`.

    If ``channel_width_field`` is declared and ``True``, the module instead implements:

    .. math::
        E = K A^m S^n / W - sp_{crit}

    Note that although the Braun-Willett (2013) scheme that underlies this
    component is nominally implicit, and will reach a numerically-correct
    solution under topographic steady state regardless of timestep length, the
    accuracy of transient solutions is *not* timestep independent (see
    Braun & Willett 2013, Appendix B for further details).
    Although the scheme remains significantly more robust and permits longer
    timesteps than a traditional explicit solver under such conditions, it
    is still possible to create numerical instability through use of too long
    a timestep while using this component. The user is cautioned to check their
    implementation is behaving stably before fully trusting it.

    Examples
    --------
    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import FlowAccumulator, StreamPowerEroder

    >>> mg = RasterModelGrid((5, 5), xy_spacing=10.0)
    >>> z = np.array(
    ...     [
    ...         [7.0, 7.0, 7.0, 7.0, 7.0],
    ...         [7.0, 5.0, 3.2, 6.0, 7.0],
    ...         [7.0, 2.0, 3.0, 5.0, 7.0],
    ...         [7.0, 1.0, 1.9, 4.0, 7.0],
    ...         [7.0, 0.0, 7.0, 7.0, 7.0],
    ...     ]
    ... )
    >>> z = mg.add_field("topographic__elevation", z, at="node")
    >>> fr = FlowAccumulator(mg, flow_director="D8")
    >>> sp = StreamPowerEroder(mg, K_sp=1.0)
    >>> fr.run_one_step()
    >>> sp.run_one_step(dt=1.0)
    >>> z
    array([7.        , 7.        , 7.        , 7.        , 7.        ,
           7.        , 2.92996598, 2.02996598, 4.01498299, 7.        ,
           7.        , 0.85993197, 1.87743897, 3.28268321, 7.        ,
           7.        , 0.28989795, 0.85403051, 2.42701526, 7.        ,
           7.        , 0.        , 7.        , 7.        , 7.        ])

    >>> mg2 = RasterModelGrid((3, 7))
    >>> z = np.array(mg2.node_x**2.0)
    >>> z = mg2.add_field("topographic__elevation", z, at="node")
    >>> mg2.status_at_node[mg2.nodes_at_left_edge] = mg2.BC_NODE_IS_FIXED_VALUE
    >>> mg2.status_at_node[mg2.nodes_at_top_edge] = mg2.BC_NODE_IS_CLOSED
    >>> mg2.status_at_node[mg2.nodes_at_bottom_edge] = mg2.BC_NODE_IS_CLOSED
    >>> mg2.status_at_node[mg2.nodes_at_right_edge] = mg2.BC_NODE_IS_CLOSED
    >>> fr2 = FlowAccumulator(mg2, flow_director="D8")
    >>> sp2 = StreamPowerEroder(mg2, K_sp=0.1, m_sp=0.0, n_sp=2.0, threshold_sp=2.0)
    >>> fr2.run_one_step()
    >>> sp2.run_one_step(dt=10.0)
    >>> z.reshape((3, 7))[1, :]
    array([ 0.        ,  1.        ,  4.        ,  8.52493772, 13.29039699,
           18.44367949, 36.        ])

    >>> mg3 = RasterModelGrid((5, 5), xy_spacing=2.0)
    >>> z = mg.node_x / 100.0
    >>> z = mg3.add_field("topographic__elevation", z, at="node")
    >>> mg3.status_at_node[mg3.nodes_at_left_edge] = mg3.BC_NODE_IS_FIXED_VALUE
    >>> mg3.status_at_node[mg3.nodes_at_top_edge] = mg3.BC_NODE_IS_CLOSED
    >>> mg3.status_at_node[mg3.nodes_at_bottom_edge] = mg3.BC_NODE_IS_CLOSED
    >>> mg3.status_at_node[mg3.nodes_at_right_edge] = mg3.BC_NODE_IS_CLOSED
    >>> mg3.at_node["water__unit_flux_in"] = mg3.node_y
    >>> fr3 = FlowAccumulator(mg3, flow_director="D8")
    >>> sp3 = StreamPowerEroder(
    ...     mg3,
    ...     K_sp=1.0,
    ...     sp_type="Unit",
    ...     a_sp=1.0,
    ...     b_sp=0.5,
    ...     c_sp=1.0,
    ...     discharge_field="surface_water__discharge",
    ... )
    >>> fr3.run_one_step()
    >>> sp3.run_one_step(1.0)
    >>> z
    array([0.        , 0.1       , 0.2       , 0.3       , 0.4       ,
           0.        , 0.02898979, 0.0859932 , 0.17463772, 0.4       ,
           0.        , 0.02240092, 0.06879049, 0.14586033, 0.4       ,
           0.        , 0.01907436, 0.05960337, 0.12929386, 0.4       ,
           0.        , 0.1       , 0.2       , 0.3       , 0.4       ])

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    Braun, J., Willett, S. (2013). A very efficient O(n), implicit and parallel
    method to solve the stream power equation governing fluvial incision and
    landscape evolution. Geomorphology  180-181(C), 170-179.
    https://dx.doi.org/10.1016/j.geomorph.2012.10.008

    """

    _name = "StreamPowerEroder"

    _unit_agnostic = True

    _info = {
        "drainage_area": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m**2",
            "mapping": "node",
            "doc": "Upstream accumulated surface area contributing to the node's discharge",
        },
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__upstream_node_order": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array containing downstream-to-upstream ordered list of node IDs",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
    }

    def __init__(
        self,
        grid,
        K_sp=0.001,
        threshold_sp=0.0,
        sp_type="set_mn",
        m_sp=0.5,
        n_sp=1.0,
        a_sp=None,
        b_sp=None,
        c_sp=None,
        channel_width_field=1.0,
        discharge_field="drainage_area",
        erode_flooded_nodes=True,
    ):
        """Initialize the StreamPowerEroder.

        Parameters
        ----------
        grid : ModelGrid
            A grid.
        K_sp : float, array, or field name
            K in the stream power equation (units vary with other parameters).
        threshold_sp : positive float, array, or field name, optional
            The threshold stream power, below which no erosion occurs. This
            threshold is assumed to be in "stream power" units, i.e., if
            sp_type is 'Shear_stress', the value should be tau**a.
        sp_type : {'set_mn', 'Total', 'Unit', 'Shear_stress'}
            Controls how the law is implemented. If 'set_mn', use the supplied
            values of m_sp and n_sp. Else, component will derive values of m and n
            from supplied values of a_sp, b_sp, and c_sp, following Whipple and
            Tucker:

            *  If ``'Total'``, ``m = a * c``, ``n = a``.
            *  If ``'Unit'``, ``m = a * c *(1 - b)``, ``n = a``.
            *  If ``'Shear_stress'``, ``m = 2 * a * c * (1 - b) / 3``,
               ``n = 2 * a / 3``.

        m_sp : float, optional
            m in the stream power equation (power on drainage area). Overridden if
            a_sp, b_sp, and c_sp are supplied.
        n_sp : float, optional, ~ 0.5<n_sp<4.
            n in the stream power equation (power on slope). Overridden if
            a_sp, b_sp, and c_sp are supplied.
        a_sp : float, optional
            The power on the SP/shear term to get the erosion rate; the "erosional
            process" term. Only used if sp_type is not 'set_mn'.
        b_sp : float, optional
            The power on discharge to get width; the "hydraulic geometry" term.
            Only used if sp_type in ('Unit', 'Shear_stress').
        c_sp : float, optional
            The power on area to get discharge; the "basin hydology" term. Only
            used if sp_type is not 'set_mn'.
        channel_width_field : None, float, array, or field name, optional
            If not None, component will look for node-centered data describing
            channel width or if an array, will take the array as the channel
            widths. It will use the widths to implement incision ~ stream power
            per unit width. If sp_type is 'set_mn', follows the equation given
            above. If sp_type in ('Unit', 'Shear_stress'), the width value will
            be implemented directly. W has no effect if sp_type is 'Total'.
        discharge_field : float, field name, or array, optional
            Discharge [L^2/T]. The default is to use the grid field
            'drainage_area'. To use custom spatially/temporally varying
            rainfall, use 'water__unit_flux_in' to specify water input to the
            FlowAccumulator and use "surface_water__discharge" for this
            keyword argument.
        erode_flooded_nodes : bool (optional)
            Whether erosion occurs in flooded nodes identified by a
            depression/lake mapper (e.g., DepressionFinderAndRouter). When set
            to false, the field *flood_status_code* must be present on the grid
            (this is created by the DepressionFinderAndRouter). Default True.
        """
        super().__init__(grid)

        if "flow__receiver_node" in grid.at_node and grid.at_node[
            "flow__receiver_node"
        ].size != grid.size("node"):
            raise NotImplementedError(
                "A route-to-multiple flow director has been "
                "run on this grid. The landlab development team has not "
                "verified that StreamPowerEroder is compatible with "
                "route-to-multiple methods. Please open a GitHub Issue "
                "to start this process."
            )

        if not erode_flooded_nodes and "flood_status_code" not in self._grid.at_node:
            raise ValueError(
                "In order to not erode flooded nodes another component "
                "must create the field *flood_status_code*. You want to "
                "run a lake mapper/depression finder."
            )

        self._erode_flooded_nodes = erode_flooded_nodes

        self._A = return_array_at_node(grid, discharge_field)
        self._elevs = return_array_at_node(grid, "topographic__elevation")
        self._sp_crit = return_array_at_node(grid, threshold_sp)

        # use setter for K defined below
        self.K = K_sp

        assert np.all(self._sp_crit >= 0.0)

        if discharge_field == "drainage_area":
            self._use_Q = False
        else:
            self._use_Q = True

        if channel_width_field is None:
            self._use_W = False
        else:
            self._use_W = True
            self._W = return_array_at_node(grid, channel_width_field)

        if np.any(threshold_sp != 0.0):
            self._set_threshold = True
            # ^flag for sed_flux_dep_incision to see if the threshold was
            # manually set.
        else:
            self._set_threshold = False

        self._type = sp_type
        if sp_type == "set_mn":
            assert (float(m_sp) >= 0.0) and (
                float(n_sp) >= 0.0
            ), "m and n must be positive"
            self._m = float(m_sp)
            self._n = float(n_sp)
            assert (
                (a_sp is None) and (b_sp is None) and (c_sp is None)
            ), "If sp_type is 'set_mn', do not pass values for a, b, or c!"
        else:
            assert sp_type in ("Total", "Unit", "Shear_stress"), (
                "sp_type not recognised. It must be 'set_mn', 'Total', "
                + "'Unit', or 'Shear_stress'."
            )
            assert (
                m_sp == 0.5 and n_sp == 1.0
            ), "Do not set m and n if sp_type is not 'set_mn'!"
            assert float(a_sp) >= 0.0, "a must be positive"
            self._a = float(a_sp)
            if b_sp is not None:
                assert float(b_sp) >= 0.0, "b must be positive"
                self._b = float(b_sp)
            else:
                assert self._use_W, "b was not set"
                self._b = 0.0
            if c_sp is not None:
                assert float(c_sp) >= 0.0, "c must be positive"
                self._c = float(c_sp)
            else:
                assert self._use_Q, "c was not set"
                self._c = 1.0
            if self._type == "Total":
                self._n = self._a
                self._m = self._a * self._c  # ==_a if use_Q
            elif self._type == "Unit":
                self._n = self._a
                self._m = self._a * self._c * (1.0 - self._b)
                # ^ ==_a iff use_Q&use_W etc
            elif self._type == "Shear_stress":
                self._m = 2.0 * self._a * self._c * (1.0 - self._b) / 3.0
                self._n = 2.0 * self._a / 3.0
            else:
                raise MissingKeyError(
                    "Not enough information was provided on the exponents to use!"
                )

        # m and n will always be set, but care needs to be taken to include Q
        # and W directly if appropriate

        self._stream_power_erosion = self._grid.zeros(at="node")
        self._alpha = self._grid.zeros(at="node")

    @property
    def K(self):
        """Erodibility (units depend on m_sp)."""
        return self._K

    @K.setter
    def K(self, new_val):
        self._K = return_array_at_node(self._grid, new_val)

    def run_one_step(self, dt):
        """A simple, explicit implementation of a stream power algorithm.

        If you are routing across flooded depressions in your flow routing
        scheme, be sure to set *erode_flooded_nodes* flag in the instantiation
        of the component to ensure erosion cannot occur in the lake. Erosion
        is always zero if the gradient is adverse, but can still procede as
        usual on the entry into the depression unless this flag is set.

        Parameters
        ----------
        dt : float
            Time-step size
        """
        if not self._erode_flooded_nodes:
            flood_status = self._grid.at_node["flood_status_code"]
            flooded_nodes = np.nonzero(flood_status == _FLOODED)[0]
        else:
            flooded_nodes = []

        upstream_order_IDs = self._grid["node"]["flow__upstream_node_order"]

        defined_flow_receivers = np.not_equal(
            self._grid["node"]["flow__link_to_receiver_node"], self._grid.BAD_INDEX
        )

        try:
            length_of_link = self._grid.length_of_d8
        except AttributeError:
            length_of_link = self._grid.length_of_link

        flow_link_lengths = length_of_link[
            self._grid.at_node["flow__link_to_receiver_node"][defined_flow_receivers]
        ]
        flow_receivers = self._grid["node"]["flow__receiver_node"]

        # Operate the main function:
        if self._use_W:
            self._alpha[defined_flow_receivers] = (
                self._K[defined_flow_receivers]
                * dt
                * self._A[defined_flow_receivers] ** self._m
                / self._W[defined_flow_receivers]
                / (flow_link_lengths**self._n)
            )

        else:
            self._alpha[defined_flow_receivers] = (
                self._K[defined_flow_receivers]
                * dt
                * self._A[defined_flow_receivers] ** self._m
                / (flow_link_lengths**self._n)
            )

        # Handle flooded nodes, if any (no erosion there)
        if flooded_nodes is not None:
            self._alpha[flooded_nodes] = 0.0

        reversed_flow = self._elevs < self._elevs[flow_receivers]
        # this check necessary if flow has been routed across
        # depressions
        self._alpha[reversed_flow] = 0.0

        threshdt = self._sp_crit * dt

        # solve using Brent's Method in Cython for Speed
        if isinstance(threshdt, float):
            brent_method_erode_fixed_threshold(
                upstream_order_IDs,
                flow_receivers,
                threshdt,
                self._alpha,
                self._n,
                self._elevs,
            )
        else:
            brent_method_erode_variable_threshold(
                upstream_order_IDs,
                flow_receivers,
                threshdt,
                self._alpha,
                self._n,
                self._elevs,
            )



================================================
File: stream_power/stream_power_smooth_threshold.py
================================================
r"""
stream_power_smooth_threshold.py: Defines the StreamPowerSmoothThresholdEroder,
which is a version of the FastscapeEroder (derived from it).

StreamPowerSmoothThresholdEroder uses a mathematically smooth threshold
formulation, rather than one with a singularity. The erosion rate is defined as
follows:

$\omega = K A^m S$

$E = \omega - \omega_c \left[ 1 - \exp ( -\omega / \omega_c ) \right]$

Created on Sat Nov 26 08:36:49 2016

@author: gtucker
"""

import numpy as np

from ..depression_finder.lake_mapper import _FLOODED
from .cfuncs import smooth_stream_power_eroder_solver
from .fastscape_stream_power import FastscapeEroder


class StreamPowerSmoothThresholdEroder(FastscapeEroder):
    """Stream erosion component with smooth threshold function.

    Parameters
    ----------
    grid : ModelGrid
        A grid.
    K_sp : float, array, or field name
        K in the stream power equation (units vary with other parameters).
    m_sp : float, optional
        m in the stream power equation (power on drainage area).
    n_sp : float, optional, ~ 0.5<n_sp<4.
        n in the stream power equation (power on slope). NOTE: NOT PRESENTLY
        HONORED BY StreamPowerSmoothThresholdEroder (TODO)
    threshold_sp : float (TODO: array, or field name)
        The threshold stream power.
    discharge_field : float, field name, or array, optional
        Discharge [L^2/T]. The default is to use the grid field
        'drainage_area'. To use custom spatially/temporally varying
        rainfall, use 'water__unit_flux_in' to specify water input to the
        FlowAccumulator and use "surface_water__discharge" for this
        keyword argument.
    erode_flooded_nodes : bool (optional)
        Whether erosion occurs in flooded nodes identified by a
        depression/lake mapper (e.g., DepressionFinderAndRouter). When set
        to false, the field *flood_status_code* must be present on the grid
        (this is created by the DepressionFinderAndRouter). Default True.

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> rg = RasterModelGrid((3, 4))
    >>> rg.set_closed_boundaries_at_grid_edges(False, True, True, True)
    >>> z = rg.add_zeros("node", "topographic__elevation")
    >>> z[5] = 2.0
    >>> z[6] = 1.0
    >>> from landlab.components import FlowAccumulator
    >>> fr = FlowAccumulator(rg, flow_director="D4")
    >>> fr.run_one_step()
    >>> from landlab.components import StreamPowerSmoothThresholdEroder
    >>> sp = StreamPowerSmoothThresholdEroder(rg, K_sp=1.0)
    >>> sp.thresholds
    1.0
    >>> sp.run_one_step(dt=1.0)
    >>> import numpy as np
    >>> np.round(z[5:7], 3)
    array([1.646, 0.667])
    >>> z[5] = 2.0
    >>> z[6] = 1.0
    >>> import numpy as np
    >>> q = np.zeros(rg.number_of_nodes) + 0.25
    >>> q[6] = 100.0
    >>> sp = StreamPowerSmoothThresholdEroder(rg, K_sp=1.0, discharge_field=q)
    >>> sp.run_one_step(dt=1.0)
    >>> np.round(z[5:7], 3)
    array([1.754, 0.164])

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Barnhart, K., Glade, R., Shobe, C., Tucker, G. (2019). Terrainbento 1.0: a
    Python package for multi-model analysis in long-term drainage basin
    evolution. Geoscientific Model Development  12(4), 1267--1297.
    https://dx.doi.org/10.5194/gmd-12-1267-2019

    **Additional References**

    Braun, J., Willett, S. (2013). A very efficient O(n), implicit and parallel
    method to solve the stream power equation governing fluvial incision and
    landscape evolution. Geomorphology  180-181(C), 170-179.
    https://dx.doi.org/10.1016/j.geomorph.2012.10.008

    """

    _name = "StreamPowerSmoothThresholdEroder"

    _unit_agnostic = True

    _cite_as = """
    @article{barnhart2019terrain,
      author = {Barnhart, Katherine R and Glade, Rachel C and Shobe, Charles M
                and Tucker, Gregory E},
      title = {{Terrainbento 1.0: a Python package for multi-model analysis in
                long-term drainage basin evolution}},
      doi = {10.5194/gmd-12-1267-2019},
      pages = {1267---1297},
      number = {4},
      volume = {12},
      journal = {Geoscientific Model Development},
      year = {2019},
    }
    """

    _info = {
        "drainage_area": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m**2",
            "mapping": "node",
            "doc": "Upstream accumulated surface area contributing to the node's discharge",
        },
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__upstream_node_order": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array containing downstream-to-upstream ordered list of node IDs",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
    }

    def __init__(
        self,
        grid,
        K_sp=None,
        m_sp=0.5,
        n_sp=1.0,
        threshold_sp=1.0,
        discharge_field="drainage_area",
        erode_flooded_nodes=True,
    ):
        """Initialize StreamPowerSmoothThresholdEroder."""
        if "flow__receiver_node" in grid.at_node and grid.at_node[
            "flow__receiver_node"
        ].size != grid.size("node"):
            raise NotImplementedError(
                "A route-to-multiple flow director has been "
                "run on this grid. The landlab development team has not "
                "verified that StreamPowerSmoothThresholdEroder is compatible "
                "with route-to-multiple methods. Please open a GitHub Issue "
                "to start this process."
            )

        if not erode_flooded_nodes and "flood_status_code" not in grid.at_node:
            raise ValueError(
                "In order to not erode flooded nodes another component "
                "must create the field *flood_status_code*. You want to "
                "run a lake mapper/depression finder."
            )

        if n_sp != 1.0:
            raise ValueError(
                "StreamPowerSmoothThresholdEroder currently only " "supports n_sp = 1"
            )

        # Call base-class init
        super().__init__(
            grid,
            K_sp=K_sp,
            m_sp=m_sp,
            n_sp=n_sp,
            threshold_sp=threshold_sp,
            discharge_field=discharge_field,
        )

        # Arrays with parameters for use in implicit solver
        self._gamma = grid.empty(at="node")
        self._delta = grid.empty(at="node")

    @property
    def alpha(self):
        """Erosion term divided by link length.

        Alpha is given as::

            alpha = K A^m dt / L

        where K is the erodibility, A is the drainage area, m is the
        drainage area exponent, dt is the timestep, and L is the link length.
        """
        return self._alpha

    @property
    def gamma(self):
        """Erosion threshold times timestep."""
        return self._gamma

    @property
    def thresholds(self):
        """Erosion thresholds."""
        return self._thresholds

    @property
    def delta(self):
        """Erosion term divided by link length and erosion threshold.

        delta is given as::

            delta = K A^m dt / (L * omega_c)

        where K is the erodibility, A is the drainage area, m is the
        drainage area exponent, dt is the timestep, L is the link length, and
        omega_c is the erosion threshold.
        """
        return self._delta

    def run_one_step(self, dt, runoff_rate=None):
        """Run one forward iteration of duration dt.

        Parameters
        ----------
        dt : float
            Time step size
        runoff_rate : (not used yet)
            (to be added later)

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> rg = RasterModelGrid((3, 3))
        >>> rg.set_closed_boundaries_at_grid_edges(False, True, True, True)
        >>> z = rg.add_zeros("node", "topographic__elevation")
        >>> z[4] = 1.0
        >>> from landlab.components import FlowAccumulator
        >>> fr = FlowAccumulator(rg, flow_director="D4")
        >>> fr.run_one_step()
        >>> from landlab.components import StreamPowerSmoothThresholdEroder
        >>> sp = StreamPowerSmoothThresholdEroder(rg, K_sp=1.0)
        >>> sp.run_one_step(dt=1.0)
        >>> sp.alpha
        array([0., 0., 0., 0., 1., 0., 0., 0., 0.])
        >>> sp.gamma
        array([0., 0., 0., 0., 1., 0., 0., 0., 0.])
        >>> sp.delta
        array([0., 0., 0., 0., 1., 0., 0., 0., 0.])
        """
        if not self._erode_flooded_nodes:
            flood_status = self._grid.at_node["flood_status_code"]
            flooded_nodes = np.nonzero(flood_status == _FLOODED)[0]
        else:
            flooded_nodes = []

        # Set up needed arrays
        #
        # Get shorthand for elevation field ("z"), and for up-to-downstream
        # ordered array of node IDs ("upstream_order_IDs")
        node_id = np.arange(self._grid.number_of_nodes)
        upstream_order_IDs = self._grid["node"]["flow__upstream_node_order"]
        z = self._grid["node"]["topographic__elevation"]
        flow_receivers = self._grid["node"]["flow__receiver_node"]

        # Get an array of flow-link length for each node that has a defined
        # receiver (i.e., that drains to another node).
        defined_flow_receivers = np.not_equal(
            self._grid["node"]["flow__link_to_receiver_node"], self._grid.BAD_INDEX
        )
        defined_flow_receivers[flow_receivers == node_id] = False

        if flooded_nodes is not None:
            defined_flow_receivers[flooded_nodes] = False
        flow_link_lengths = self._grid.length_of_d8[
            self._grid.at_node["flow__link_to_receiver_node"][defined_flow_receivers]
        ]

        # Set up alpha, beta, delta arrays
        #
        #   First, compute drainage area or discharge raised to the power m.
        np.power(self._A, self._m, out=self._A_to_the_m)

        #   Alpha
        self._alpha[~defined_flow_receivers] = 0.0
        self._alpha[defined_flow_receivers] = (
            self._K[defined_flow_receivers]
            * dt
            * self._A_to_the_m[defined_flow_receivers]
            / flow_link_lengths
        )

        #   Gamma
        self._gamma[~defined_flow_receivers] = 0.0

        #   Delta
        self._delta[~defined_flow_receivers] = 0.0

        if isinstance(self._thresholds, np.ndarray):
            self._gamma[defined_flow_receivers] = (
                dt * self._thresholds[defined_flow_receivers]
            )

            self._delta[defined_flow_receivers] = (
                self._K[defined_flow_receivers]
                * self._A_to_the_m[defined_flow_receivers]
            ) / (self._thresholds[defined_flow_receivers] * flow_link_lengths)

            self._delta[defined_flow_receivers][
                self._thresholds[defined_flow_receivers] == 0.0
            ] = 0.0
        else:
            self._gamma[defined_flow_receivers] = dt * self._thresholds
            if self._thresholds == 0:
                self._delta[defined_flow_receivers] = 0.0
            else:
                self._delta[defined_flow_receivers] = (
                    self._K[defined_flow_receivers]
                    * self._A_to_the_m[defined_flow_receivers]
                ) / (self._thresholds * flow_link_lengths)

        # Iterate over nodes from downstream to upstream, using scipy's
        # 'newton' function to find new elevation at each node in turn.
        smooth_stream_power_eroder_solver(
            upstream_order_IDs, flow_receivers, z, self._alpha, self._gamma, self._delta
        )



================================================
File: taylor_nonlinear_hillslope_flux/__init__.py
================================================
from .taylor_nonlinear_hillslope_flux import TaylorNonLinearDiffuser

__all__ = ["TaylorNonLinearDiffuser"]



================================================
File: taylor_nonlinear_hillslope_flux/taylor_nonlinear_hillslope_flux.py
================================================
"""TaylorNonLinearDiffuser Component.

@author: R Glade
@author: K Barnhart
@author: G Tucker
"""

# Cubic hillslope flux component

import numpy as np

from landlab import Component
from landlab import LinkStatus


class TaylorNonLinearDiffuser(Component):
    """Hillslope evolution using a Taylor Series expansion of the Andrews-
    Bucknam formulation of nonlinear hillslope flux derived following following
    Ganti et al., 2012. The flux is given as::

        qs = KS * (1 + (S / Sc)**2 + (S / Sc)**4 + ... + (S / Sc)**2(n - 1))

    where *K* is is the diffusivity, *S* is the slope, *Sc* is the critical slope, and
    *n* is the number of terms.

    The default behavior uses two terms to produce a flux law as described by
    Equation 6 of Ganti et al., (2012).

    Examples
    --------
    >>> import numpy as np
    >>> import decimal
    >>> from landlab import RasterModelGrid
    >>> from landlab.plot.imshow import imshow_grid

    >>> mg = RasterModelGrid((3, 3))
    >>> z = mg.add_zeros("topographic__elevation", at="node")
    >>> initial_slope = 1.0
    >>> leftmost_elev = 1000.0
    >>> z[:] = leftmost_elev
    >>> z[:] += (initial_slope * np.amax(mg.x_of_node)) - (initial_slope * mg.x_of_node)
    >>> mg.set_closed_boundaries_at_grid_edges(False, True, False, True)
    >>> cubicflux = TaylorNonLinearDiffuser(mg, slope_crit=0.1)
    >>> cubicflux.run_one_step(1.0)
    >>> np.allclose(
    ...     mg.at_node["topographic__elevation"],
    ...     np.array(
    ...         [1002.0, 1001.0, 1000.0, 1002.0, 1001.0, 1000.0, 1002.0, 1001.0, 1000.0]
    ...     ),
    ... )
    True

    The :class:`~.TaylorNonLinearDiffuser` makes and moves
    soil at a rate proportional
    to slope, this means that there is a characteristic time scale for soil
    transport and an associated stability criteria for the timestep. The
    maximum characteristic time scale, *Demax*, is given as a function of the
    hillslope diffustivity, *D*, the maximum slope, *Smax*, and the critical slope
    *Sc*::

        Demax = D * (
            1 + (Smax / Sc) ** 2 + (Smax / Sc) ** 4 + ... + (Smax / Sc) ** (2 * (n - 1))
        )

    The maximum stable time step is given by::

        dtmax = courant_factor * dx * dx / Demax

    Where the courant factor is a user defined scale (default is 0.2), and
    dx is the length of the shortest link in the grid.

    The :class:`~.TaylorNonLinearDiffuser` has a boolean
    flag that permits a user to be
    warned if timesteps are too large for the slopes in the model grid
    (``if_unstable="warn"``) and a boolean flag that turns on dynamic
    timesteppping (``dynamic_dt=False``).

    >>> cubicflux = TaylorNonLinearDiffuser(mg, slope_crit=0.1, if_unstable="warn")
    >>> cubicflux.run_one_step(2.0)
    Topographic slopes are high enough such that the Courant condition is
    exceeded AND you have not selected dynamic timestepping with
    dynamic_dt=True. This may lead to infinite and/or nan values for slope,
    elevation, and soil depth. Consider using a smaller time step or dynamic
    timestepping. The Courant condition recommends a timestep of  0.2 or
    smaller.

    Alternatively you can specify ``if_unstable="raise"``, and a `RuntimeError` will
    be raised if this condition is not met.

    Next, lets do an example with dynamic timestepping.

    >>> mg = RasterModelGrid((5, 5))
    >>> z = mg.add_zeros("topographic__elevation", at="node")

    We'll use a steep slope.

    >>> z += mg.node_x.copy() ** 2
    >>> cubicflux = TaylorNonLinearDiffuser(mg, if_unstable="warn", dynamic_dt=False)

    Lets try to move the soil with a large timestep. Without dynamic time
    steps, this gives a warning that we've exceeded the dynamic timestep size
    and should use a smaller timestep. We could either use the smaller timestep,
    or specify that we want to use a dynamic timestep.

    >>> cubicflux.run_one_step(10.0)
    Topographic slopes are high enough such that the Courant condition is
    exceeded AND you have not selected dynamic timestepping with
    dynamic_dt=True. This may lead to infinite and/or nan values for slope,
    elevation, and soil depth. Consider using a smaller time step or dynamic
    timestepping. The Courant condition recommends a timestep of
    0.004 or smaller.

    Now, we'll re-build the grid and do the same example with dynamic
    timesteps.

    >>> mg = RasterModelGrid((5, 5))
    >>> z = mg.add_zeros("topographic__elevation", at="node")
    >>> z += mg.node_x.copy() ** 2
    >>> cubicflux = TaylorNonLinearDiffuser(mg, if_unstable="warn", dynamic_dt=True)
    >>> cubicflux.run_one_step(10.0)
    >>> np.any(np.isnan(z))
    False

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Barnhart, K., Glade, R., Shobe, C., Tucker, G. (2019). Terrainbento 1.0: a
    Python package for multi-model analysis in long-term drainage basin
    evolution. Geoscientific Model Development  12(4), 1267--1297.
    https://dx.doi.org/10.5194/gmd-12-1267-2019

    **Additional References**

    Ganti, V., Passalacqua, P., Foufoula-Georgiou, E. (2012). A sub-grid scale
    closure for nonlinear hillslope sediment transport models Journal of
    Geophysical Research: Earth Surface  117(F2).
    https://dx.doi.org/10.1029/2011jf002181
    """

    _name = "TaylorNonLinearDiffuser"

    _unit_agnostic = True

    _cite_as = """
    @article{barnhart2019terrain,
      author = {Barnhart, Katherine R and Glade, Rachel C and Shobe, Charles M
                and Tucker, Gregory E},
      title = {{Terrainbento 1.0: a Python package for multi-model analysis in
                long-term drainage basin evolution}},
      doi = {10.5194/gmd-12-1267-2019},
      pages = {1267---1297},
      number = {4},
      volume = {12},
      journal = {Geoscientific Model Development},
      year = {2019},
    }
    """

    _info = {
        "soil__flux": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m^2/yr",
            "mapping": "link",
            "doc": "flux of soil in direction of link",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__slope": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/m",
            "mapping": "link",
            "doc": "gradient of the ground surface",
        },
    }

    def __init__(
        self,
        grid,
        linear_diffusivity=1.0,
        slope_crit=1.0,
        nterms=2,
        dynamic_dt=False,
        if_unstable="pass",
        courant_factor=0.2,
    ):
        """Initialize the TaylorNonLinearDiffuser.

        Parameters
        ----------
        grid: ModelGrid
            Landlab ModelGrid object
        linear_diffusivity: float, optional
            Hillslope diffusivity (m**2 / yr).
        slope_crit: float, optional
            Critical slope.
        nterms: int, optional
            Number of terms in the Taylor expansion.
            Two terms (the default) gives the behavior
            described in Ganti et al. (2012).
        dynamic_dt : bool, optional
            Turn dynamic time-stepping on or off.
        if_unstable : {'pass', 'warn', 'raise'}, optional
            Specify how potential instability due to slopes that are too high
            is handled.
        courant_factor : float, optional
            Factor to identify stable time-step duration when using dynamic
            timestepping.
        """
        super().__init__(grid)

        # Store grid and parameters

        self._K = linear_diffusivity
        self._slope_crit = slope_crit
        self._nterms = nterms
        self._dynamic_dt = dynamic_dt
        self._courant_factor = courant_factor
        self._if_unstable = if_unstable
        self._shortest_link = np.amin(grid.length_of_link)  # for Courant

        # Create fields:

        # elevation
        self._elev = self._grid.at_node["topographic__elevation"]

        # slope gradient
        if "topographic__slope" not in self._grid.at_link:
            self._grid.add_zeros("topographic__slope", at="link")
        self._slope = self._grid.at_link["topographic__slope"]

        # soil flux
        if "soil__flux" not in self._grid.at_link:
            self._grid.add_zeros("soil__flux", at="link")
        self._flux = self._grid.at_link["soil__flux"]

    def soilflux(self, dt):
        """Calculate soil flux for a time period, dt.

        Parameters
        ----------
        dt: float
            The imposed timestep.
        """
        # establish time left as all of dt
        time_left = dt

        # begin while loop for time left
        while time_left > 0.0:
            # Calculate gradients
            self._slope[:] = self._grid.calc_grad_at_link(self._elev)
            self._slope[self._grid.status_at_link == LinkStatus.INACTIVE] = 0.0

            # Test for time stepping courant condition
            courant_slope_term = 0.0
            courant_s_over_scrit = self._slope.max() / self._slope_crit
            for i in range(0, 2 * self._nterms, 2):
                courant_slope_term += courant_s_over_scrit**i
                if np.any(np.isinf(courant_slope_term)):
                    message = (
                        "Soil flux term is infinite in Courant condition "
                        "calculation. This is likely due to "
                        "using too many terms in the Taylor expansion."
                    )
                    raise RuntimeError(message)
            # Calculate De Max
            De_max = self._K * (courant_slope_term)
            # Calculate longest stable timestep
            self._dt_max = self._courant_factor * (self._shortest_link**2) / De_max

            # Test for the Courant condition and print warning if user intended
            # for it to be printed.
            if (
                (self._dt_max < dt)
                and (not self._dynamic_dt)
                and (self._if_unstable != "pass")
            ):
                message = (
                    "Topographic slopes are high enough such that the "
                    "Courant condition is exceeded AND you have not "
                    "selected dynamic timestepping with dynamic_dt=True. "
                    "This may lead to infinite and/or nan values for "
                    "slope, elevation, and soil depth. Consider using a "
                    "smaller time step or dynamic timestepping. The "
                    "Courant condition recommends a timestep of "
                    "" + str(self._dt_max) + " or smaller."
                )
                if self._if_unstable == "raise":
                    raise RuntimeError(message)
                if self._if_unstable == "warn":
                    print(message)

            # if dynamic dt is selected, use it, otherwise, use the entire time
            if self._dynamic_dt:
                self._sub_dt = np.min([dt, self._dt_max])
                time_left -= self._sub_dt
            else:
                self._sub_dt = dt
                time_left = 0

            # Calculate flux
            slope_term = 0.0
            s_over_scrit = self._slope / self._slope_crit
            for i in range(0, 2 * self._nterms, 2):
                slope_term += s_over_scrit**i
                if np.any(np.isinf(slope_term)):
                    message = (
                        "Soil flux term is infinite. This is likely due to "
                        "using too many terms in the Taylor expansion."
                    )
                    raise RuntimeError(message)

            self._flux[:] = -((self._K * self._slope) * (slope_term))

            # Calculate flux divergence
            dqdx = self._grid.calc_flux_div_at_node(self._flux)

            # Update topography
            self._elev[self._grid.core_nodes] -= (
                dqdx[self._grid.core_nodes] * self._sub_dt
            )

    def run_one_step(self, dt):
        """Advance cubic soil flux component by one time step of size dt.

        Parameters
        ----------
        dt: float
            The imposed timestep.
        """
        self.soilflux(dt)



================================================
File: tectonics/__init__.py
================================================
#!/usr/bin/env python
"""
.. codeauthor:: G Tucker

.. sectionauthor:: G Tucker
"""

from .listric_kinematic_extender import ListricKinematicExtender

__all__ = ["ListricKinematicExtender"]



================================================
File: tectonics/listric_kinematic_extender.py
================================================
#!/usr/bin/env python3
"""
Apply tectonic extension kinematically.

Landlab component that simulates development of an asymmetric rift on a listric
fault plane.

See notebook tutorial for theory and examples.

@author: gtucker
"""

import numpy as np

from landlab import Component
from landlab import HexModelGrid
from landlab import RasterModelGrid
from landlab.components import AdvectionSolverTVD


def dist_to_line(Px, Py, x0, y0, alpha):
    """Calculate and return the distance of point(x) (Px, Py) to the
    line described by x = x0 + t cos alpha, y = y0 + t sin alpha.

    Parameters
    ----------
    Px : float
        x-coordinate of point(s)
    Py : float
        y-coordinate of point(s)
    x0 : float
        x intercept of line
    y0 : float
        y intercept of line
    alpha : float, degrees
        angle of line, counter-clockwise from positive x-axis

    Examples
    --------
    >>> np.round(dist_to_line(1, 1, 0, 0, 90), 6)
    1.0
    >>> np.round(dist_to_line(0, 1, 1, 0, 90), 6)
    -1.0
    >>> np.round(dist_to_line(1, 1, 0, 0, 0), 6)
    -1.0
    >>> np.round(dist_to_line(2.0**0.5, 0, 0, 0, 45), 6)
    1.0
    >>> np.round(dist_to_line(0, 2.0**0.5, 0, 0, 45), 6)
    -1.0
    """
    alpha_r = np.radians(alpha)
    return np.sin(alpha_r) * (Px - x0) - np.cos(alpha_r) * (Py - y0)


class ListricKinematicExtender(Component):
    """Apply tectonic extension kinematically to a raster or
    hex grid.

    The caller specifies the strike, dip, and location of the zero-surface
    fault trace (i.e., where the fault plane would intersect zero elevation),
    and either the (x, y) components of uniform extension velocity field,
    or a link-based velocity field. The run_one_step() method calculates
    advection of an output field called "hangingwall__thickness". The initial
    hanginwall thickness is defined as the difference between the starting
    topography field (a required input field) and a listric fault plane
    that is represented mathematically as an "upside-down" saturating
    exponential function that asymptotes to a caller-specified detachment
    depth, representing a decollement.

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import ListricKinematicExtender
    >>> grid = RasterModelGrid((3, 130), xy_spacing=10.0)
    >>> topo = grid.add_zeros("topographic__elevation", at="node")
    >>> lke = ListricKinematicExtender(grid, fault_x0=100.0, fault_strike=90.0)
    >>> for _ in range(250):
    ...     lke.run_one_step(dt=2000.0)
    ...
    >>> round(grid.at_node["hangingwall__thickness"][240])
    830
    """

    _name = "ListricKinematicExtender"

    _time_units = "y"

    _unit_agnostic = True

    _info = {
        "advection__velocity": {
            "dtype": float,
            "intent": "in",
            "optional": True,
            "units": "m/y",
            "mapping": "link",
            "doc": "Link-parallel advection velocity magnitude",
        },
        "fault_plane__elevation": {
            "dtype": "float",
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Elevation of fault plane",
        },
        "hangingwall__thickness": {
            "dtype": "float",
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Thickness of material in hangingwall block",
        },
        "topographic__elevation": {
            "dtype": "float",
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
    }

    def __init__(
        self,
        grid,
        extension_rate_x=0.001,
        extension_rate_y=0.0,
        fault_dip=60.0,
        fault_x0=0.0,
        fault_y0=0.0,
        fault_strike=45.0,
        detachment_depth=1.0e4,
        fields_to_advect=None,
        advection_direction_is_steady=False,
    ):
        """Deform vertically and horizontally to represent tectonic extension.

        Parameters
        ----------
        grid: RasterModelGrid
            A landlab grid.
        extension_rate_x: float, optional
            Rate of x-directed horizontal motion of hangingwall relative to footwall
            (m / y), default 0.001 m/y.
        extension_rate_y: float, optional
            Rate of y-directed horizontal motion of hangingwall relative to footwall
            (m / y), default 0.
        fault_x0: float, optional
            x intercept of zero-surface fault trace, m (default 0).
        fault_y0: float, optional
            y intercept of zero-surface fault trace, m (default 0).
        fault_strike: float, optional
            Strike of zero-surface fault trace, degrees (default 45).
        detachment_depth: float, optional
            Depth to horizontal detachment (m), default 10 km.
        fields_to_advect: list of str, optional
            List of names of fields, in addition to 'hangingwall__thickness'
        advection_direction_is_steady : bool (default False)
            Indicates whether the directions of advection are expected to remain
            steady throughout a run. If True, some computation time is saved
            by calculating upwind links only once.
        """
        if not (isinstance(grid, RasterModelGrid) or isinstance(grid, HexModelGrid)):
            raise (TypeError, "grid must be a RasterModelGrid or HexModelGrid")

        fields_to_advect = [] if fields_to_advect is None else fields_to_advect

        super().__init__(grid)
        self.initialize_output_fields()

        self._elev = grid.at_node["topographic__elevation"]
        self._fault_plane_elev = grid.at_node["fault_plane__elevation"]
        self._hw_thick = grid.at_node["hangingwall__thickness"]

        self.update_fault_plane_elevation_and_hangingwall_thickness(
            grid, fault_x0, fault_y0, fault_strike, fault_dip, detachment_depth
        )
        self._setup_advection_component(
            grid,
            fields_to_advect,
            extension_rate_x,
            extension_rate_y,
            advection_direction_is_steady,
        )

    def update_fault_plane_elevation_and_hangingwall_thickness(
        self, grid, fault_x0, fault_y0, fault_strike, fault_dip, detachment_depth
    ):
        """Initialize fields fault_plane__elevation and hangingwall__thickness.

        Calculate and store the fault plane elevation at grid nodes using an
        exponential function of (signed) distance to fault, with topographic
        elevation as the minimum. Calculate the thickness of the hangingwall
        block at grid nodes by subtracting fault plane elevation from
        topographic elevation.

        Parameters
        ----------
        fault_x0 : float
            x-intercept of zero-surface fault trace, m
        fault_y0 : float
            y-intercept of zero-surface fault trace, m
        fault_strike : float
            strike angle of fault trace, degrees ccw from +x
        fault_dip : float
            dip angle of fault at the zero elevation point, degrees
        detachment_depth : float
            depth to the point where the detachment is horizontal, m

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> from landlab.components import ListricKinematicExtender
        >>> grid = RasterModelGrid((3, 3), xy_spacing=1000.0)
        >>> _ = grid.add_zeros("topographic__elevation", at="node")
        >>> extender = ListricKinematicExtender(grid, fault_strike=90.0)
        >>> round(grid.at_node["fault_plane__elevation"][4])
        -1590
        >>> round(grid.at_node["hangingwall__thickness"][4])
        1590
        """
        fault_grad = np.tan(np.deg2rad(fault_dip))
        dist_to_fault = dist_to_line(
            grid.x_of_node, grid.y_of_node, fault_x0, fault_y0, fault_strike
        )
        self._fault_plane_elev[:] = np.minimum(
            -detachment_depth
            * (1.0 - np.exp(-dist_to_fault * fault_grad / detachment_depth)),
            self._elev,
        )
        self._hw_thick[:] = self._elev - self._fault_plane_elev

    def _setup_advection_component(
        self,
        grid,
        fields_to_advect,
        extension_rate_x,
        extension_rate_y,
        advection_direction_is_steady,
    ):
        """Instantiate and initialize AdvectionSolverTVD.

        If the link field advection__velocity already exists and contains
        non-zero values, these values are used for the advection field.
        Otherwise, the field is created (if needed) and initialized by mapping
        the vector components extension_rate_x and extension_rate_y onto the
        grid links.

        fields_to_advect : list
            List of names of fields to advect. Can be an empty list.
            "hangingwall__thickness" will be added to the list if it
            is not already there.
        extension_rate_x: float
            Rate of x-directed horizontal motion of hangingwall relative to footwall
            (m / y).
        extension_rate_y: float
            Rate of y-directed horizontal motion of hangingwall relative to footwall
            (m / y).
        advection_direction_is_steady : bool
            Indicates whether the directions of advection are expected to remain
            steady throughout a run. If True, some computation time is saved
            by calculating upwind links only once.
        """
        if "hangingwall__thickness" not in fields_to_advect:
            fields_to_advect.append("hangingwall__thickness")
        if "advection__velocity" not in grid.at_link.keys():
            grid.add_zeros("advection__velocity", at="link")
        self._advec_velocity = grid.at_link["advection__velocity"]
        if np.amax(np.abs(self._advec_velocity)) == 0.0:  # if no nonzero values
            grid.map_vectors_to_links(
                extension_rate_x, extension_rate_y, out=self._advec_velocity
            )
        self.advector = AdvectionSolverTVD(
            grid,
            fields_to_advect=fields_to_advect,
            advection_direction_is_steady=advection_direction_is_steady,
        )

    def run_one_step(self, dt):
        """Apply extensional motion to grid for one time step.

        Parameters
        ----------
        dt : float
            Time-step duration, y
        """
        self.advector.run_one_step(dt)
        self._elev[self.grid.core_nodes] = (
            self._fault_plane_elev[self.grid.core_nodes]
            + self._hw_thick[self.grid.core_nodes]
        )



================================================
File: threshold_eroder/__init__.py
================================================
from .threshold_eroder import ThresholdEroder

__all__ = ["ThresholdEroder"]



================================================
File: threshold_eroder/cfuncs.pyx
================================================
cimport cython

# https://cython.readthedocs.io/en/stable/src/userguide/fusedtypes.html
ctypedef fused id_t:
    cython.integral
    long long


cpdef _thresholder(
    const id_t [:] stack,
    const id_t [:] link_to_rcvr,
    const id_t [:] receivers,
    const cython.floating [:] linkLengths,
    cython.floating [:] el,
    const double slope_thres,
):
    """
    Calcualte D8 flow dirs
    stack: the flow upstream node order
    link_to_rcvr: Link to receiver
    receivers: receivers
    linkLengths: length of links
    el: topographic elevation
    """
    cdef int node

    for node in stack:
        dist_to_rcvr = linkLengths[link_to_rcvr[node]]
        el[node] = min(
            el[node], el[receivers[node]] + slope_thres * dist_to_rcvr
        )



================================================
File: threshold_eroder/threshold_eroder.py
================================================
#!/usr/bin/env python3
"""Created on Wed Aug  4 11:00:08 2021.

@author: benjamincampforts
"""
from landlab import Component
from landlab import RasterModelGrid

from .cfuncs import _thresholder


class ThresholdEroder(Component):
    """Threshold eroder.

    Threshold eroder that cuts off slopes at a given threshold slope (Sc) and
    assumes material to dissolve away

    .. math::

        S(S>Sc) = Sc

    To be coupled with :class:`~.flow_director_steepest.FlowDirectorSteepest` or
    :class:`~.priority_flood_flow_router.PriorityFloodFlowRouter` for the
    calculation of steepest slope at each timestep. Note that ThresholdEroder
    run_one_step() cuts off slopes and computes new elevations based on the
    steepest slopes as calculated by the FlowDirectorSteepest or
    PriorityFloodFlowRouter. If slopes over the entire model grid need be set
    to a threshold slope, several iterations of running the flow router and the
    Threshold eroder are required.

    Component written by Benjamin Campforts, 2022

    Parameters
    ----------
    grid : ModelGrid
        Landlab ModelGrid object
    slope_crit: float, optional
        Critical slope [L/L]

    Examples
    --------

    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import ThresholdEroder, PriorityFloodFlowRouter

    Define grid and initial topography:

    - 3x5 grid
    - east and west boundaries are open, north and south are closed
    - Initial topography is plane at base level on the boundaries and
      1m of elevation elsewhere (core)

    >>> mg = RasterModelGrid((5, 5))
    >>> mg.set_closed_boundaries_at_grid_edges(False, False, False, False)
    >>> z = np.array(
    ...     [
    ...         [0.0, 0.0, 0.0, 0.0, 0.0],
    ...         [0.0, 1.0, 1.0, 1.0, 0.0],
    ...         [0.0, 1.0, 10.0, 1.0, 0.0],
    ...         [0.0, 1.0, 1.0, 1.0, 0.0],
    ...         [0.0, 0.0, 0.0, 0.0, 0.0],
    ...     ]
    ... )
    >>> _ = mg.add_field("topographic__elevation", z, at="node")

    Instantiate Flow director (steepest slope type) and TL hillslope diffuser

    >>> fdir = PriorityFloodFlowRouter(mg)
    >>> th_ero = ThresholdEroder(mg, slope_crit=0.6)

    Run the components for ten short timepsteps

    >>> for t in range(2):
    ...     fdir.run_one_step()
    ...     th_ero.run_one_step()
    ...


    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    """

    _name = "ThresholdEroder"

    _unit_agnostic = True

    _cite_as = """
    @Article{gmd-13-3863-2020,
      AUTHOR = {Campforts B., Shobe C.M., Steer P., Vanmaercke M., Lague D., Braun J.},
      TITLE = {BedrockLandslider 1.0: a hybrid landscape evolution model to
               simulate the impact of landslides and landslide-derived sediment on
               landscape evolution.},
      JOURNAL = {Geoscientific Model Development},
      VOLUME = {13},
      YEAR = {2020},
      NUMBER = {9},
      PAGES = {3863--3886},
      URL = {https://doi.org/10.5194/gmd-13-3863-2020},
      DOI = {10.5194/gmd-13-3863-2020}
    }"""

    _info = {
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "flow__receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "flow__upstream_node_order": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array containing downstream-to-upstream ordered list of node IDs",
        },
        "flow__link_to_receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "ID of link downstream of each node, which carries the discharge",
        },
        "soil__depth": {
            "dtype": float,
            "intent": "inout",
            "optional": True,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of soil or weathered bedrock",
        },
        "bedrock__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": True,
            "units": "m",
            "mapping": "node",
            "doc": "elevation of the bedrock surface",
        },
    }

    def __init__(self, grid, slope_crit=1.0):
        """Initialize Threshold Eroder.

        Parameters
        ----------
        grid : ModelGrid
            Landlab ModelGrid object
        slope_crit: float, optional
            Critical slope [L/L]
        """
        super().__init__(grid)

        if grid.at_node["flow__receiver_node"].size != grid.size("node"):
            raise NotImplementedError(
                "A route-to-multiple flow director has been "
                "run on this grid. The landlab development team has not "
                "verified that ThresholdEroder is compatible "
                "with route-to-multiple methods. Please open a GitHub Issue "
                "to start this process."
            )

        # Store grid and parameters
        self._slope_crit = slope_crit

        # Link lengths depending on raster type:
        if isinstance(grid, RasterModelGrid):
            self._link_lengths = grid.length_of_d8
        else:
            self._link_lengths = grid.length_of_link

        # Create fields
        self.initialize_output_fields()

        if (
            "soil__depth" in self._grid.at_node
            and "bedrock__elevation" not in self._grid.at_node
        ):
            raise ValueError(
                "If soil__depth is provided as a field, "
                "bedrock__elevation must also be provided as a field"
            )

    def erode(self):
        """Erode landscape to threshold and dissolve sediment."""
        _thresholder(
            self.grid.at_node["flow__upstream_node_order"],
            self.grid.at_node["flow__link_to_receiver_node"],
            self._grid.at_node["flow__receiver_node"],
            self._link_lengths,
            self._grid.at_node["topographic__elevation"],
            self._slope_crit,
        )

        if "soil__depth" in self._grid.at_node:
            self._grid.at_node["bedrock__elevation"].clip(
                None,
                self._grid.at_node["topographic__elevation"],
                out=self._grid.at_node["bedrock__elevation"],
            )
            self._grid.at_node["soil__depth"][:] = (
                self._grid.at_node["topographic__elevation"]
                - self._grid.at_node["bedrock__elevation"]
            )

    def run_one_step(self):
        """Advance threshold erosion component one timestep."""
        self.erode()



================================================
File: tidal_flow/__init__.py
================================================
#!/usr/bin/env python
"""
.. codeauthor:: G Tucker

.. sectionauthor:: G Tucker
"""

from .tidal_flow_calculator import TidalFlowCalculator

__all__ = ["TidalFlowCalculator"]



================================================
File: tidal_flow/tidal_flow_calculator.py
================================================
#!/usr/bin/env python3
"""
Calculate cycle-averaged tidal flow field using approach of Mariotti (2018)
"""

import numpy as np
from scipy.sparse.linalg import spsolve

from landlab import Component
from landlab import HexModelGrid
from landlab import RasterModelGrid
from landlab.grid.mappers import map_min_of_link_nodes_to_link
from landlab.utils import get_core_node_matrix
from landlab.utils.return_array import return_array_at_link

_FOUR_THIRDS = 4.0 / 3.0
_M2_PERIOD = (12.0 + (25.2 / 60.0)) * 3600.0  # M2 tidal period, in seconds


class TidalFlowCalculator(Component):
    r"""Component that calculates average flow over a tidal cycle.

    The TidalFlowCalculator component calculates a tidal flow velocity field on
    a grid using the method of Mariotti et al. (2018). The grid can be raster
    or hex. In essence, the method uses a numerical solution to the steady
    diffusion equation. The resulting velocity fields---one for flood tide, and
    on for ebb tide---represent the average velocity over a tidal half-cycle.
    Velocity is obtained by starting with the total depth of water that is
    added or removed during flood tide or ebb tide, respectively, at each grid
    point. The idea is to solve for the water-surface elevation field that
    produces this total inflow or outflow of water at each point, using a
    linear approximation for shallow-water flow. The math is given in
    Mariotti (2018), and is summarized below.

    Tidal-averaged water depth: with z as bed surface elevation, and r as tidal
    range, the water depth h is:

    .. math::

        h = [\max(0, r/2 − z) + \max(0, −r/2 − z)]/2

    Horizontal flow velocity (2d vector): with :math:`\eta` as water-surface
    elevation, n as Manning roughness, and :math:`\chi` as a scale velocity
    (here unity), depth-averaged flow velocity U is:

    .. math::

        U = \frac{h^{4/3}}{n^2\chi} \nabla \eta

    Tidal inundation / drainage rate, I: at any given point, this is the depth
    of water added (flood tide) or drained (ebb tide) divided by the tidal half
    period, T/2. It is defined as:

    .. math::

        I = [r/2 − \max(−r/2, \min(z, r/2))] / (T/2)

    Mass conservation:

    .. math::

        \nabla \cdot (h U) = I

    Because h is assumed constant, this becomes a steady diffusion equation:

    .. math::

        \nabla^2 \eta = \frac{I n^{4/3} \chi}{h^{7/3}}

    This is a Poisson (steady diffusion) equation, which is solved numerically
    at grid nodes using a finite-volume method. The water-surface gradient is
    then used to calculate the velocity field, using the above equation for U.

    Parameters
    ----------
    grid : RasterModelGrid or HexModelGrid
        A Landlab grid object.
    tidal_range : float, optional
        Tidal range (2x tidal amplitude) (m) (default 1)
    tidal_period : float, optional
        Tidal perioid (s) (default M2 tidal period = 12 h 25 min)
    roughness : float, array, or field name; optional
        Manning roughness coefficient ("n") (s/m^1/3) (default 0.01)
    mean_sea_level : float, optional
        Mean sea level (m) (default 0)
    scale_velocity : float, optional
        Scale velocity (see Mariotti, 2018) (m/s) (default 1)
    min_water_depth : float, optional
        Minimum depth for calculating diffusion coefficient (m) (default 0.01)

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import TidalFlowCalculator
    >>> grid = RasterModelGrid((3, 5), xy_spacing=2.0)  # 1 row core nodes
    >>> grid.set_closed_boundaries_at_grid_edges(False, True, True, True)
    >>> z = grid.add_zeros("topographic__elevation", at="node")
    >>> z[:] = -50.0  # mean water depth is 50 m below MSL
    >>> tfc = TidalFlowCalculator(
    ...     grid, tidal_range=2.0, tidal_period=4.0e4, roughness=0.01
    ... )
    >>> tfc.run_one_step()
    >>> int(round(grid.at_link["ebb_tide_flow__velocity"][10] * 1.0e6))
    4

    References
    ----------
    Mariotti, G. (2018) Marsh channel morphological response to sea level rise
    and sediment supply. Estuarine, Coastal and Shelf Science, 209, 89--101,
    https://doi.org/10.1016/j.ecss.2018.05.016.
    """

    _name = "TidalFlowCalculator"

    _unit_agnostic = False

    _info = {
        "flood_tide_flow__velocity": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/s",
            "mapping": "link",
            "doc": "Horizontal flow velocity along links during flood tide",
        },
        "ebb_tide_flow__velocity": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/s",
            "mapping": "link",
            "doc": "Horizontal flow velocity along links during ebb tide",
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "mean_water__depth": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Tidal mean water depth",
        },
    }

    def __init__(
        self,
        grid,
        tidal_range=1.0,
        tidal_period=_M2_PERIOD,
        roughness=0.01,
        mean_sea_level=0.0,
        scale_velocity=1.0,
        min_water_depth=0.01,
    ):
        """Initialize TidalFlowCalculator."""

        # Handle grid type
        if isinstance(grid, RasterModelGrid):
            self._grid_multiplier = 1.0 * grid.dx * grid.dy
        elif isinstance(grid, HexModelGrid):
            self._grid_multiplier = 1.5 * grid.spacing * grid.spacing
        else:
            raise TypeError("Grid must be raster or hex.")

        # Call base class methods to check existence of input fields,
        # create output fields, etc.
        super().__init__(grid)
        self.initialize_output_fields()

        # Get references to various fields, for convenience
        self._elev = self.grid.at_node["topographic__elevation"]
        self._water_depth = self.grid.at_node["mean_water__depth"]
        self._flood_tide_vel = self.grid.at_link["flood_tide_flow__velocity"]
        self._ebb_tide_vel = self.grid.at_link["ebb_tide_flow__velocity"]

        # Handle inputs
        self.roughness = roughness  # uses setter below
        self._tidal_range = tidal_range
        self._tidal_half_range = tidal_range / 2.0
        self._tidal_period = tidal_period
        self._tidal_half_period = tidal_period / 2.0
        self._scale_velocity = scale_velocity
        self._mean_sea_level = mean_sea_level
        self._min_depth = min_water_depth

        # Make other data structures
        self._water_depth_at_links = np.zeros(grid.number_of_links)
        self._diffusion_coef_at_links = np.zeros(grid.number_of_links)
        self._boundary_mean_water_surf_elev = np.zeros(grid.number_of_nodes)

    @property
    def roughness(self):
        """Roughness coefficient (Manning's n)."""
        return self._roughness

    @roughness.setter
    def roughness(self, new_val):
        self._roughness = return_array_at_link(self.grid, new_val)

    @property
    def tidal_range(self):
        """Tidal range."""
        return self._tidal_range

    @tidal_range.setter
    def tidal_range(self, new_val):
        self._tidal_range = new_val
        self._tidal_half_range = new_val / 2

    @property
    def tidal_period(self):
        """Tidal period."""
        return self._tidal_period

    @tidal_period.setter
    def tidal_period(self, new_val):
        self._tidal_period = new_val
        self._tidal_half_period = new_val / 2

    @property
    def mean_sea_level(self):
        """Mean sea level."""
        return self._mean_sea_level

    @mean_sea_level.setter
    def mean_sea_level(self, new_val):
        self._mean_sea_level = new_val

    def calc_tidal_inundation_rate(self):
        """Calculate and store the rate of inundation/draining at each node,
        averaged over a tidal half-cycle.

        Examples
        --------
        >>> grid = RasterModelGrid((3, 5))
        >>> z = grid.add_zeros("topographic__elevation", at="node")
        >>> z[5:10] = [10.0, 0.25, 0.0, -0.25, -10.0]
        >>> period = 4.0e4  # tidal period in s, for convenient calculation
        >>> tfc = TidalFlowCalculator(grid, tidal_period=period)
        >>> rate = tfc.calc_tidal_inundation_rate()
        >>> 0.5 * rate[5:10] * period  # depth in m
        array([0.  , 0.25, 0.5 , 0.75, 1.  ])
        >>> rate[5:10]  # rate in m/s
        array([0.00e+00, 1.25e-05, 2.50e-05, 3.75e-05, 5.00e-05])

        Notes
        -----
        This calculates I in Mariotti (2018) using his equation (1).
        """
        return (
            self._tidal_half_range
            - np.maximum(
                -self._tidal_half_range, np.minimum(self._elev, self._tidal_half_range)
            )
        ) / self._tidal_half_period

    def _calc_effective_water_depth(self):
        """Calculate and store the effective water depth.

        Water depth is calculated as the average of high tide and low tide
        water depth, except where this average is less than the user-specified
        minimum depth, in which case the minimum depth is assigned.

        Examples
        --------
        >>> grid = RasterModelGrid((3, 5))
        >>> z = grid.add_zeros("topographic__elevation", at="node")
        >>> z[6:9] = [1.0, 2.0, -2.0]
        >>> tfc = TidalFlowCalculator(grid, tidal_range=3.1, min_water_depth=0.02)
        >>> tfc._calc_effective_water_depth()
        >>> tfc._water_depth[6:9]
        array([0.275, 0.02 , 2.   ])
        """
        high_tide_depth = (self.mean_sea_level + self._tidal_half_range) - self._elev
        low_tide_depth = np.maximum(
            (self.mean_sea_level - self._tidal_half_range) - self._elev, 0.0
        )
        self._water_depth[:] = (high_tide_depth + low_tide_depth) / 2.0
        self._water_depth[self._water_depth <= self._min_depth] = self._min_depth

    def run_one_step(self):
        """Calculate the tidal flow field and water-surface elevation."""

        # Tidal mean water depth  and water surface elevation at nodes
        # (Note: mean water surf elev only used for boundary conditions in
        # matrix construction; should be mean sea level)
        self._calc_effective_water_depth()
        self._boundary_mean_water_surf_elev[:] = self._mean_sea_level

        # Map water depth to links
        map_min_of_link_nodes_to_link(
            self.grid, self._water_depth, out=self._water_depth_at_links
        )

        # Calculate velocity and diffusion coefficients on links
        velocity_coef = self._water_depth_at_links**_FOUR_THIRDS / (
            (self.roughness**2) * self._scale_velocity
        )
        self._diffusion_coef_at_links[:] = self._water_depth_at_links * velocity_coef

        # Calculate inundation / drainage rate at nodes
        tidal_inundation_rate = self.calc_tidal_inundation_rate()

        # Set up right-hand-side (RHS) vector for both ebb and flood tides (only
        # difference is in the sign)
        cores = self.grid.core_nodes

        # For flood tide, set up matrix and add boundary info to RHS vector
        # mat, rhs = make_core_node_matrix_var_coef(
        mat, rhs = get_core_node_matrix(
            self.grid,
            self._boundary_mean_water_surf_elev,
            coef_at_link=self._diffusion_coef_at_links,
        )

        rhs[:, 0] += self._grid_multiplier * tidal_inundation_rate[cores]

        # Solve for flood tide water-surface elevation
        tidal_wse = np.zeros(self.grid.number_of_nodes)
        tidal_wse[self.grid.core_nodes] = spsolve(mat, rhs)

        # Calculate flood-tide water-surface gradient at links
        tidal_wse_grad = np.zeros(self.grid.number_of_links)
        self.grid.calc_grad_at_link(tidal_wse, out=tidal_wse_grad)

        # Calculate flow velocity field at links for flood tide, and assign
        # negative of the flood tide values to ebb tide
        self._flood_tide_vel[self.grid.active_links] = (
            -velocity_coef[self.grid.active_links]
            * tidal_wse_grad[self.grid.active_links]
        )
        self._ebb_tide_vel[:] = -self._flood_tide_vel



================================================
File: transport_length_diffusion/__init__.py
================================================
from .transport_length_hillslope_diffusion import TransportLengthHillslopeDiffuser

__all__ = ["TransportLengthHillslopeDiffuser"]



================================================
File: transport_length_diffusion/transport_length_hillslope_diffusion.py
================================================
#!/usr/bin/env python3
"""Created on Tue Apr 11 10:13:38 2017.

@author: margauxmouchene
"""


import numpy as np

from landlab import Component


class TransportLengthHillslopeDiffuser(Component):
    r"""Transport length hillslope diffusion.

    Hillslope diffusion component in the style of Carretier et al. (2016,
    ESurf), and Davy and Lague (2009)

    .. math::

        \frac{dz}{dt} = -E + D (+ U)

        D = \frac{q_s}{L}

        E = k S

        L = \frac{dx}{(1 - (S / S_c)^2}

    Works on regular raster-type grid (RasterModelGrid, dx=dy).
    To be coupled with FlowDirectorSteepest for the calculation of steepest
    slope at each timestep.

    Component written by Margaux Mouchene, 2017

    Parameters
    ----------
    grid : ModelGrid
        Landlab ModelGrid object
    erodibility: float
        Erodibility coefficient [L/T]
    slope_crit: float (default=1.)
        Critical slope [L/L]

    Examples
    --------

    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import FlowDirectorSteepest
    >>> from landlab.components import TransportLengthHillslopeDiffuser

    Define grid and initial topography:

        - 3x5 grid
        - east and west boundaries are open, north and south are closed
        - Initial topography is plane at base level on the boundaries and
          1m of elevation elsewhere (core)

    >>> mg = RasterModelGrid((5, 5))
    >>> mg.set_closed_boundaries_at_grid_edges(False, True, False, True)
    >>> z = [
    ...     [0.0, 0.0, 0.0, 0.0, 0.0],
    ...     [0.0, 1.0, 1.0, 1.0, 0.0],
    ...     [0.0, 1.0, 1.0, 1.0, 0.0],
    ...     [0.0, 1.0, 1.0, 1.0, 0.0],
    ...     [0.0, 0.0, 0.0, 0.0, 0.0],
    ... ]
    >>> _ = mg.add_field("topographic__elevation", z, at="node")

    Instantiate Flow director (steepest slope type) and TL hillslope diffuser

    >>> fdir = FlowDirectorSteepest(mg)
    >>> tl_diff = TransportLengthHillslopeDiffuser(
    ...     mg, erodibility=0.001, slope_crit=0.6
    ... )

    Run the components for ten short timepsteps

    >>> for t in range(10):
    ...     fdir.run_one_step()
    ...     tl_diff.run_one_step(1.0)
    ...

    Check final topography

    >>> np.allclose(
    ...     mg.at_node["topographic__elevation"].reshape(mg.shape),
    ...     [
    ...         [0.0, 0.0, 0.0, 0.0, 0.0],
    ...         [0.0, 0.96175283, 0.99982519, 0.96175283, 0.0],
    ...         [0.0, 0.96175283, 0.99982519, 0.96175283, 0.0],
    ...         [0.0, 0.96175283, 0.99982519, 0.96175283, 0.0],
    ...         [0.0, 0.0, 0.0, 0.0, 0.0],
    ...     ],
    ... )
    True

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    Carretier, S., Martinod, P., Reich, M., Godderis, Y. (2016). Modelling
    sediment clasts transport during landscape evolution. Earth Surface Dynamics
    4(1), 237-251. https://dx.doi.org/10.5194/esurf-4-237-2016

    Davy, P., Lague, D. (2009). Fluvial erosion/transport equation of landscape
    evolution models revisited. Journal of Geophysical Research  114(F3),
    F03007. https://dx.doi.org/10.1029/2008jf001146

    """

    _name = "TransportLengthHillslopeDiffuser"

    _unit_agnostic = True

    _info = {
        "flow__receiver_node": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Node array of receivers (node that receives flow from current node)",
        },
        "sediment__deposition_coeff": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "-",
            "mapping": "node",
            "doc": "Fraction of incoming sediment that is deposited on the node",
        },
        "sediment__deposition_rate": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/yr",
            "mapping": "node",
            "doc": "Deposition rate on node",
        },
        "sediment__erosion_rate": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/yr",
            "mapping": "node",
            "doc": "Erosion rate on node",
        },
        "sediment__flux_in": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/yr",
            "mapping": "node",
            "doc": "Incoming sediment rate on node (=qs/dx)",
        },
        "sediment__flux_out": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/yr",
            "mapping": "node",
            "doc": (
                "Outgoing sediment rate on node = sediment eroded on "
                "node + sediment transported across node from upstream"
            ),
        },
        "sediment__transfer_rate": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/yr",
            "mapping": "node",
            "doc": (
                "Rate of transferred sediment across a node (incoming "
                "sediment - deposited sediment on node)"
            ),
        },
        "topographic__elevation": {
            "dtype": float,
            "intent": "inout",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Land surface topographic elevation",
        },
        "topographic__steepest_slope": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m/m",
            "mapping": "node",
            "doc": "The steepest *downhill* slope",
        },
    }

    def __init__(self, grid, erodibility=0.001, slope_crit=1.0):
        """Initialize Diffuser.

        Parameters
        ----------
        grid : ModelGrid
            Landlab ModelGrid object
        erodibility: float
            Erodibility coefficient [L/T]
        slope_crit: float (default=1.)
            Critical slope [L/L]
        """
        super().__init__(grid)

        if grid.at_node["flow__receiver_node"].size != grid.size("node"):
            raise NotImplementedError(
                "A route-to-multiple flow director has been "
                "run on this grid. The landlab development team has not "
                "verified that TransportLengthHillslopeDiffuser is compatible "
                "with route-to-multiple methods. Please open a GitHub Issue "
                "to start this process."
            )

        # Store grid and parameters

        self._k = erodibility
        self._slope_crit = slope_crit

        # Create fields:
        # Elevation
        self._elev = self._grid.at_node["topographic__elevation"]

        # Downstream steepest slope at node:
        self._steepest = self._grid.at_node["topographic__steepest_slope"]
        # On each node, node ID of downstream receiver node
        # (on node (i), ID of node that receives flow from node (i)):
        self._receiver = self._grid.at_node["flow__receiver_node"]

        self.initialize_output_fields()
        # Deposition
        self._depo = self._grid.at_node["sediment__deposition_rate"]

        # Transferred sediments (crossing over node)
        self._trans = self._grid.at_node["sediment__transfer_rate"]

        # Transport coefficient
        self._d_coeff = self._grid.at_node["sediment__deposition_coeff"]

        # Flux in
        self._flux_in = self._grid.at_node["sediment__flux_in"]

        # Flux out
        self._flux_out = self._grid.at_node["sediment__flux_out"]

        # Erosion
        self._erosion = self._grid.at_node["sediment__erosion_rate"]

    def tldiffusion(self, dt):
        """Calculate hillslope diffusion for a time period 'dt'.

        Parameters
        ----------
        grid : ModelGrid
            Landlab ModelGrid object
        dt: float (time)
            The imposed timestep.
        """

        # Reset erosion, depo, trans and flux_in to 0
        self._erosion[:] = 0.0
        self._depo[:] = 0.0
        self._trans[:] = 0.0
        self._flux_in[:] = 0.0

        dx = self._grid.dx
        cores = self._grid.core_nodes

        # Calculate influx rate on node i  = outflux of nodes
        # whose receiver is i
        for i in self._grid.core_nodes:
            self._flux_in[self._receiver[i]] += self._flux_out[i]

            # Calculate transport coefficient
            # When S ~ Scrit, d_coeff is set to "infinity", for stability and
            # so that there is no deposition
            if self._steepest[i] >= self._slope_crit:
                self._d_coeff[i] = 1000000000.0
            else:
                self._d_coeff[i] = 1 / (
                    1 - (np.power(((self._steepest[i]) / self._slope_crit), 2))
                )

        # Calculate deposition rate on node
        self._depo[cores] = self._flux_in[cores] / self._d_coeff[cores]

        # Calculate erosion rate on node (positive value)
        # If S > Scrit, erosion is simply set for the slope to return to Scrit
        # Otherwise, erosion is slope times erodibility coefficent
        for i in self._grid.core_nodes:
            if self._steepest[i] > self._slope_crit:
                self._erosion[i] = (
                    dx * (self._steepest[i] - self._slope_crit) / (100 * dt)
                )
            else:
                self._erosion[i] = self._k * self._steepest[i]

            # Update elevation
            self._elev[i] += (-self._erosion[i] + self._depo[i]) * dt

        # Calculate transfer rate over node
        self._trans[cores] = self._flux_in[cores] - self._depo[cores]

        # Calculate outflux rate
        self._flux_out[:] = self._erosion + self._trans

    def run_one_step(self, dt):
        """Advance one timestep.

        Advance transport length-model hillslope diffusion component
        by one time step of size dt and tests for timestep stability.

        Parameters
        ----------
        dt: float (time)
            The imposed timestep.
        """
        self.tldiffusion(dt)

        # Test code stability for timestep dt
        # Raise unstability error if local slope is reversed by erosion
        # and deposition during a timestep dt
        elev_dif = self._elev - self._elev[self._receiver]
        s = elev_dif[np.where(self._grid.at_node["flow__sink_flag"] == 0)]
        if np.any(s < -1) is True:
            raise ValueError(
                "The component is unstable" " for such a large timestep " "on this grid"
            )
        else:
            pass



================================================
File: uniform_precip/__init__.py
================================================
from .generate_uniform_precip import PrecipitationDistribution

__all__ = ["PrecipitationDistribution"]



================================================
File: uniform_precip/generate_uniform_precip.py
================================================
"""Generate precipitation using the Poisson pulse model.

Landlab component that generates precipitation events using the rectangular
Poisson pulse model described in Eagleson (1978, Water Resources Research).


No particular units must be used, but it was written with the storm units in
hours (hr) and depth units in millimeters (mm)

Written by Jordan Adams, 2013, updated May 2016
"""

import random

import numpy as np

from landlab import Component
from landlab import ModelGrid


class PrecipitationDistribution(Component):
    """Generate precipitation events.

    This component can generate a random storm duration, interstorm
    duration, precipitation intensity or storm depth from a Poisson
    distribution when given a mean value.

    Examples
    --------
    >>> from landlab.components.uniform_precip import PrecipitationDistribution
    >>> import numpy as np
    >>> np.random.seed(np.arange(10))

    To use hard-coded values for mean storm, mean interstorm, mean depth,
    model run time and delta t...  Say we use 1.5 for mean storm, 15 for mean
    interstorm, 0.5 for mean depth, 100 for model run time and 1 for delta t...

    >>> precip = PrecipitationDistribution(
    ...     mean_storm_duration=1.5,
    ...     mean_interstorm_duration=15.0,
    ...     mean_storm_depth=0.5,
    ...     total_t=100.0,
    ...     delta_t=1.0,
    ... )
    >>> for dt, rate in precip.yield_storm_interstorm_duration_intensity():
    ...     pass  # and so on
    ...

    Alternatively, we can pass a grid to the component, and call yield_storms()
    to generate storm-interstorm float pairs while the intensity data is stored
    in the grid scalar field 'rainfall__flux':

    >>> from landlab import RasterModelGrid
    >>> mg = RasterModelGrid((4, 5))
    >>> precip = PrecipitationDistribution(
    ...     mg,
    ...     mean_storm_duration=1.5,
    ...     mean_interstorm_duration=15.0,
    ...     mean_storm_depth=0.5,
    ...     total_t=46.0,
    ... )
    >>> storm_dts = []
    >>> interstorm_dts = []
    >>> intensities = []
    >>> precip.seed_generator(seedval=1)
    >>> for storm_dt, interstorm_dt in precip.yield_storms():
    ...     storm_dts.append(storm_dt)
    ...     interstorm_dts.append(interstorm_dt)
    ...     intensities.append(mg.at_grid["rainfall__flux"])
    ...
    >>> len(storm_dts) == 4  # 4 storms in the simulation
    True
    >>> len(interstorm_dts) == len(storm_dts)
    True
    >>> rf_intensities_to_test = np.array(
    ...     [
    ...         0.8138257984406472,
    ...         0.15929112025199238,
    ...         0.17254519305000884,
    ...         0.09817611240558813,
    ...     ]
    ... )
    >>> np.allclose(intensities, rf_intensities_to_test)
    True
    >>> np.isclose(sum(storm_dts) + sum(interstorm_dts), 46.0)  # test total_t
    True
    >>> np.isclose(interstorm_dts[-1], 0.0)  # sequence truncated as necessary
    True

    We can also turn the generator straight into a list, like this:

    >>> precip.seed_generator(seedval=1)
    >>> steps = [(dt + istorm_dt) for (dt, istorm_dt) in precip.yield_storms()]
    >>> np.isclose(sum(steps), 46.0)
    True

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    Eagleson, P. (1978). Climate, soil, and vegetation: 2. The distribution of
    annual precipitation derived from observed storm sequences. Water Resources
    Research  14(5), 713-721. https://dx.doi.org/10.1029/wr014i005p00713

    """

    _name = "PrecipitationDistribution"

    _unit_agnostic = True

    _info = {
        "rainfall__flux": {
            "dtype": float,
            "intent": "out",
            "optional": True,
            "units": "[depth unit]/[time unit]",
            "mapping": "grid",
            "doc": "Depth of water delivered per unit time in each storm",
        }
    }

    def __init__(
        self,
        grid=None,
        mean_storm_duration=1.0,
        mean_interstorm_duration=1.0,
        mean_storm_depth=1.0,
        total_t=0.0,
        delta_t=None,
        random_seed=0,
    ):
        """Create the storm generator.

        Parameters
        ----------
        grid : ModelGrid
            A Landlab grid (optional). If provided, storm intensities will be
            stored as a grid scalar field as the component simulates storms.
        mean_storm_duration : float
            Average duration of a precipitation event.
        mean_interstorm_duration : float
            Average duration between precipitation events.
        mean_storm_depth : float
            Average depth of precipitation events.
        total_t : float, optional
            If generating a time series, the total amount of time.
        delta_t : float or None, optional
            If you want to break up storms into determined subsections using
            yield_storm_interstorm_duration_intensity, a delta_t is needed.
        random_seed : int or float, optional
            Seed value for random-number generator.
        """
        super().__init__(grid)

        self._mean_storm_duration = mean_storm_duration

        self._mean_interstorm_duration = mean_interstorm_duration

        self._mean_storm_depth = mean_storm_depth

        self._run_time = total_t

        self._delta_t = delta_t

        if self._delta_t == 0.0:
            self._delta_t = None

        # Mean_intensity is not set by the MPD, but can be drawn from
        # the mean storm depth and mean storm duration.
        self._mean_intensity = self._mean_storm_depth / self._mean_storm_duration

        # If a time series is created later, this blank list will be used.
        self._storm_time_series = []

        # Seed the random-number generator
        self.seed_generator(random_seed)

        # Given the mean values assigned above using either the model
        # parameter dictionary or the init function, we can call the
        # different methods to assign values from the Poisson distribution.

        self._storm_duration = self.get_precipitation_event_duration()
        self._interstorm_duration = self.get_interstorm_event_duration()
        self._storm_depth = self.get_storm_depth()
        self._elapsed_time = 0.0

        # Test if we got a grid. If we did, then assign it to _grid, and we
        # are able to use the at_grid field. If not, that's cool too.
        if grid is not None:
            assert isinstance(grid, ModelGrid)  # must be a grid

        # build LL fields, if a grid is supplied:
        if grid is not None:
            self.grid.add_field("rainfall__flux", 0.0, at="grid")
            self._gridupdate = True
        else:
            self._gridupdate = False

        self._intensity = self.get_storm_intensity()

    @property
    def storm_duration(self):
        """Duration of storm.

        [T]
        """
        return self._storm_duration

    @property
    def interstorm_duration(self):
        """Interstorm duration.

        [T]
        """
        return self._interstorm_duration

    @property
    def storm_depth(self):
        """Depth of water in the storm.

        [L]
        """
        return self._storm_depth

    def update(self):
        """Update the storm values.

        If new values for storm duration, interstorm duration, storm depth
        and intensity are needed, this method can be used to update those
        values one time.

        Examples
        --------
        >>> from landlab.components import PrecipitationDistribution
        >>> precip = PrecipitationDistribution(
        ...     mean_storm_duration=1.5,
        ...     mean_interstorm_duration=15.0,
        ...     mean_storm_depth=0.5,
        ...     total_t=100.0,
        ...     delta_t=1,
        ... )

        Additionally, if we wanted to update several times, a loop could be
        utilized to accomplish this. Say we want 5 storm_durations; this
        pseudo-code represents a way to accomplish this...

        >>> storm_duration_list = []
        >>> i = 0
        >>> while i < 4:
        ...     storm_duration_list.append(precip.storm_duration)
        ...     precip.update()
        ...     i += 1
        ...

        Note though that alternatively we could also do this, avoiding the
        method entirely...


        >>> # ^^this lets you "manually" get the next item from the iterator
        >>> precip = PrecipitationDistribution(
        ...     mean_storm_duration=1.5,
        ...     mean_interstorm_duration=15.0,
        ...     mean_storm_depth=0.5,
        ...     total_t=46.0,
        ... )
        >>> storm_duration_list = []
        >>> for i in range(5):
        ...     storm_duration_list.append(
        ...         next(precip.yield_storm_interstorm_duration_intensity())[0]
        ...     )
        ...

        Notice that doing this will *not* automatically stop the iteration,
        however - it will continue ad infinitum.
        """
        self._storm_duration = self.get_precipitation_event_duration()
        self._interstorm_duration = self.get_interstorm_event_duration()
        self._storm_depth = self.get_storm_depth()
        self._intensity = self.get_storm_intensity()

    def get_precipitation_event_duration(self):
        """This method is the storm generator.

        This method has one argument: the mean_storm_duration parameter.
        (In Eagleson (1978), this parameter was called Tr.)

        It finds a random storm_duration value
        based on the poisson distribution about the mean.
        This is accomplished using the expovariate function
        from the "random" standard library.

        Returns
        -------
        float
            The storm duration.
        """
        return random.expovariate(1.0 / self._mean_storm_duration)

    def get_interstorm_event_duration(self):
        """Generate interstorm events.

        This method takes one argument, the mean_interstorm_duration parameter.
        (In Eagleson (1978), this parameter was called Tb.)

        This method is modeled identically to
        get_precipitation_event_duration()

        This method finds a random value for interstorm_duration
        based on the poisson distribution about the mean.
        This is accomplished using the expovariate function
        from the "random" standard library.

        Returns
        -------
        float
            The interstorm duration.
        """
        return random.expovariate(1.0 / self._mean_interstorm_duration)

    def get_storm_depth(self):
        """Generate storm depth.

        Storm depth is used to generate a realistic
        intensity for different storm events.

        (In Eagleson (1978) this parameter was called "h")

        This method requires storm_duration, mean_storm_duration
        and the mean_storm_depth. Storm_duration is generated through
        the initialize() or update() method.

        Numpy has a random number generator to get values
        from a given Gamma distribution. It takes two arguments,
        alpha (or the shape parameter), which is the generated over the mean
        event and beta (or the scale parameter), which is the mean value
        These are all arguments in the function, which returns storm depth.

        Returns
        -------
        float
            The storm depth.
        """

        shape_parameter = self._storm_duration / self._mean_storm_duration
        scale_parameter = self._mean_storm_depth
        self._storm_depth = np.random.gamma(shape_parameter, scale_parameter)
        return self._storm_depth

    def get_storm_intensity(self):
        """Get the storm intensity.

        This method draws storm intensity out of the storm depth generated by
        get_storm_depth.

        This method requires the storm_depth and storm_duration
        and is the same as the parameter ("i") in Eagleson (1978), but instead
        of being drawn from Poission, this is drawn from the Gamma distribution
        of (*h*), as :math:`h = i * Tr`.

        Returns
        -------
        float
            The storm intensity.
        """
        self._intensity = self._storm_depth / self._storm_duration
        if self._gridupdate:
            self._grid.at_grid["rainfall__flux"] = self._intensity
        return self._intensity

    def get_storm_time_series(self):
        """Get a time series of storms.

        This method creates a time series of storms based on storm_duration,
        and interstorm_duration. From these values it will calculate a complete
        time series.

        The storm_time_series returned by this method is made up of sublists,
        each comprising of three sub-parts (e.g. [[x,y,z], [a,b,c]]) where x
        and a are the beginning times of a precipitation event, y and b are the
        ending times of the precipitation event and z and c represent the
        average intensity (mm/hr) of the storm lasting from x to y and a to be,
        respectively.

        Even if a grid was passed to the component at instantiation, calling
        this method does not update the grid fields.

        Returns
        -------
        array
            containing several sub-arrays of events [start, finish, intensity]
        """

        storm = self.get_precipitation_event_duration()
        self.get_storm_depth()
        intensity = self.get_storm_intensity()
        self._storm_time_series.append([0, storm, intensity])

        storm_helper = storm
        storm_iterator = storm
        while storm_iterator <= self._run_time:
            next_storm_start = storm_helper + (
                round(self.get_interstorm_event_duration(), 2)
            )
            next_storm_end = next_storm_start + (
                round(self.get_precipitation_event_duration(), 2)
            )
            intensity = round(self.get_storm_intensity(), 2)
            self.get_storm_depth()
            self._storm_time_series.append(
                [next_storm_start, next_storm_end, intensity]
            )
            storm_iterator = storm_helper
            storm_helper = next_storm_end
            storm_iterator = storm_helper
        return self._storm_time_series

    def yield_storm_interstorm_duration_intensity(self, subdivide_interstorms=False):
        """Iterator for a time series of storms interspersed with interstorms.

        This method is intended to be equivalent to get_storm_time_series,
        but instead offers a generator functionality. This will be useful in
        cases where the whole sequence of storms and interstorms doesn't need
        to be stored, where we can save memory this way.

        The method keeps track of the delta_t such that if a storm needs to be
        generated longer than this supplied model timestep, the generator will
        return the storm in "chunks", until there is no more storm duration.
        e.g.,
        storm of intensity 1. is 4.5 long, the delta_t is 2., the generator
        yields (2.,1.) -> (2.,1.) -> (0.5,1.) -> ...

        If delta_t is None or not supplied, no subdivision occurs.

        Once a storm has been generated, this method will follow it with the
        next interstorm, yielded as (interstorm_duration, 0.). Note that the
        interstorm will NOT be subdivided according to delta_t unless you set
        the flag *subdivide_interstorms* to True.

        The method will keep yielding until it reaches the RUN_TIME, where it
        will terminate.

        Yields
        ------
        tuple of float
            (interval_duration, rainfall_rate_in_interval)

        Notes
        -----
        One recommended procedure is to instantiate the generator, then call
        instance.next() (in Python 2) or next(instance) (in Python 3)
        repeatedly to get the sequence.
        """
        # Added DEJH, Dec 2014
        # Modified to use an optional output field, DEJH 1/8/17

        delta_t = self._delta_t
        if delta_t is None:
            assert subdivide_interstorms is False, (
                "You specified you wanted storm subdivision, but did not "
                + "provide a delta_t to allow this!"
            )
        self._elapsed_time = 0.0
        while self._elapsed_time < self._run_time:
            storm_duration = self.get_precipitation_event_duration()
            step_time = 0.0
            self.get_storm_depth()
            self._intensity = self.get_storm_intensity()  # this is a rate
            # ^ this updates the grid field, if needed
            if self._elapsed_time + storm_duration > self._run_time:
                storm_duration = self._run_time - self._elapsed_time
            while delta_t is not None and storm_duration - step_time > delta_t:
                yield (delta_t, self._intensity)
                step_time += delta_t
            yield (storm_duration - step_time, self._intensity)
            self._elapsed_time += storm_duration

            # If the last storm did not use up all our elapsed time, generate
            # an inter-storm period.
            if self._elapsed_time < self._run_time:
                interstorm_duration = self.get_interstorm_event_duration()
                if self._elapsed_time + interstorm_duration > self._run_time:
                    interstorm_duration = self._run_time - self._elapsed_time
                self._intensity = 0.0
                if self._gridupdate:
                    self._grid.at_grid["rainfall__flux"] = 0.0
                if subdivide_interstorms:
                    step_time = 0.0
                    while interstorm_duration - step_time > delta_t:
                        yield (delta_t, 0.0)
                        step_time += delta_t
                    yield (interstorm_duration - step_time, 0.0)
                else:
                    yield (interstorm_duration, 0.0)
                self._elapsed_time += interstorm_duration

    def yield_storms(self):
        """Iterator for a time series of storm-interstorm pairs.

        This method is very similar to this component's other generator,
        yield_storm_interstorm_duration_intensity(), but the way it yields is
        slightly different. Instead of yielding (interval_duration, rf_rate),
        with interstorms represented as intervals with rf_rate = 0, it yields:

            (storm_duration, interstorm_duration)

        When each tuple pair is yielded, the grid scalar field 'rainfall__flux'
        is updated with the rainfall rate occuring during storm_duration.

        This generator method is designed for direct equivalence with the
        spatially resolved generators found elsewhere in Landlab.

        This generator will be useful in cases where the whole sequence of
        storms and interstorms doesn't need to be stored, where we can save
        memory this way.

        This method does not attempt to subdivide timesteps. If you want that,
        provide a delta_t at instantiation, and use
        yield_storm_interstorm_duration_intensity(subdivide_interstorms=True).

        The method will keep yielding until it reaches the RUN_TIME, where it
        will terminate. If it terminates during a storm, the final tuple will
        be (truncated_storm_duration, 0.). Otherwise it will be
        (storm_duration, truncated_interstorm_duration.)

        Yields
        ------
        tuple of float
            (storm_duration, interstorm_duration)

        Notes
        -----
        One recommended procedure is to instantiate the generator, then call
        next(instance) (in Python 3) repeatedly to get the sequence (See
        Examples, below).

        Examples
        --------
        >>> from landlab import RasterModelGrid
        >>> mg = RasterModelGrid((4, 5))
        >>> precip = PrecipitationDistribution(
        ...     mg,
        ...     mean_storm_duration=1.5,
        ...     mean_interstorm_duration=15.0,
        ...     mean_storm_depth=0.5,
        ...     total_t=46.0,
        ... )
        >>> storm_dts = []
        >>> interstorm_dts = []
        >>> intensities = []
        >>> precip.seed_generator(seedval=1)
        >>> for storm_dt, interstorm_dt in precip.yield_storms():
        ...     storm_dts.append(storm_dt)
        ...     interstorm_dts.append(interstorm_dt)
        ...     intensities.append(mg.at_grid["rainfall__flux"])
        ...
        >>> len(storm_dts) == 4  # 4 storms in the simulation
        True
        >>> len(interstorm_dts) == len(storm_dts)
        True
        >>> rf_intensities_to_test = np.array(
        ...     [
        ...         0.8138257984406472,
        ...         0.15929112025199238,
        ...         0.17254519305000884,
        ...         0.09817611240558813,
        ...     ]
        ... )
        >>> np.allclose(intensities, rf_intensities_to_test)
        True
        >>> np.isclose(sum(storm_dts) + sum(interstorm_dts), 46.0)  # total_t
        True
        >>> np.isclose(interstorm_dts[-1], 0.0)  # sequence truncated
        True

        An alternative way to use the generator might be:

        >>> # ^^this lets you "manually" get the next item from the iterator
        >>> flux = mg.at_grid.pop("rainfall__flux")  # remove the existing field
        >>> precip = PrecipitationDistribution(
        ...     mg,
        ...     mean_storm_duration=1.5,
        ...     mean_interstorm_duration=15.0,
        ...     mean_storm_depth=0.5,
        ...     total_t=46.0,
        ... )
        >>> precip.seed_generator(seedval=1)
        >>> mystorm_generator = precip.yield_storms()
        >>> my_list_of_storms = []
        >>> for i in range(4):
        ...     my_list_of_storms.append(next(mystorm_generator))
        ...

        Note that an exception is thrown if you go too far:

        >>> my_list_of_storms.append(next(mystorm_generator))  # the 5th iter
        Traceback (most recent call last):
          ...
        StopIteration

        Also note that the generator won't terminate if you try this without
        first instantiating the generator:

        >>> allmytimes = []
        >>> for i in range(20):  # this will run just fine
        ...     allmytimes.append(next(precip.yield_storms()))
        ...
        >>> total_t = sum([sum(storm) for storm in allmytimes])
        >>> total_t > 46.0
        True
        """
        # we must have instantiated with a grid, so check:
        assert hasattr(self, "_grid")

        # now exploit the existing generator to make this easier & less
        # redundant:
        delta_t = self._delta_t
        self._delta_t = None  # this is necessary to suppress chunking behaviour
        # in the other generator
        othergen = self.yield_storm_interstorm_duration_intensity()
        # enter a loop, to break as needed:
        tobreak = False
        while not tobreak:
            # we always start with a storm, so:
            try:
                (storm_dur, storm_int) = next(othergen)
            except StopIteration:
                break  # stop dead. We terminated at a good place
            try:
                (interstorm_dur, _) = next(othergen)
            except StopIteration:
                tobreak = True
                interstorm_dur = 0.0
            # reset the rainfall__flux field, that got overstamped in the
            # interstorm iter:
            self._grid.at_grid["rainfall__flux"] = storm_int
            yield (storm_dur, interstorm_dur)
        # now, just in case, restore self._delta_t:
        self._delta_t = delta_t

    def generate_from_stretched_exponential(self, scale, shape):
        """Generate and return a random variable from a stretched exponential
        distribution with given scale and shape.

        Examples
        --------
        >>> np.random.seed(0)
        >>> np.round(np.random.rand(3), 6)  # these are our 3 rand #s to test
        array([0.548814, 0.715189, 0.602763])
        >>> from landlab.components import PrecipitationDistribution
        >>> pd = PrecipitationDistribution(
        ...     mean_storm_duration=1.0,
        ...     mean_interstorm_duration=1.0,
        ...     mean_storm_depth=1.0,
        ... )
        >>> np.random.seed(0)  # re-set seed so we get the same 3 #s
        >>> np.round(1000 * pd.generate_from_stretched_exponential(2.0, 0.5))
        720.0
        >>> np.round(1000 * pd.generate_from_stretched_exponential(2.0, 0.5))
        225.0
        >>> np.round(1000 * pd.generate_from_stretched_exponential(2.0, 0.5))
        513.0
        """
        return scale * ((-np.log(np.random.rand())) ** (1.0 / shape))

    def seed_generator(self, seedval=0):
        """Seed the random-number generator.

        The examples illustrate:

        1. That we can get the same sequence again by re-seeding with the
           same value (the default is zero)
        2. When we use a value other than the default, we get a different
           sequence

        Examples
        --------
        >>> precip = PrecipitationDistribution(
        ...     mean_storm_duration=1.5,
        ...     mean_interstorm_duration=15.0,
        ...     mean_storm_depth=0.5,
        ...     total_t=100.0,
        ...     delta_t=1.0,
        ... )
        >>> round(precip.storm_duration, 2)
        2.79
        >>> round(precip.interstorm_duration, 2)
        21.28
        >>> round(precip.storm_depth, 2)
        2.45
        >>> round(precip.intensity, 2)
        0.88
        >>> precip.seed_generator()  # re-seed and get same sequence again
        >>> round(precip.get_precipitation_event_duration(), 2)
        2.79
        >>> round(precip.get_interstorm_event_duration(), 2)
        21.28
        >>> round(precip.get_storm_depth(), 2)
        2.45
        >>> round(precip.get_storm_intensity(), 2)
        0.88
        >>> precip = PrecipitationDistribution(
        ...     mean_storm_duration=1.5,
        ...     mean_interstorm_duration=15.0,
        ...     mean_storm_depth=0.5,
        ...     total_t=100.0,
        ...     delta_t=1.0,
        ...     random_seed=1,
        ... )
        >>> round(precip.storm_duration, 2)  # diff't vals with diff't seed
        0.22
        >>> round(precip.interstorm_duration, 2)
        28.2
        >>> round(precip.storm_depth, 4)
        0.0012
        >>> round(precip.intensity, 4)
        0.0054
        """
        random.seed(seedval)
        np.random.seed(seedval)

    @property
    def elapsed_time(self):
        """Get the elapsed time recorded by the module.

        This will be particularly useful in the midst of a yield loop.
        """
        return self._elapsed_time

    @property
    def intensity(self):
        """Get the intensity of the most recent storm simulated."""
        return self.get_storm_intensity()



================================================
File: vegetation_dynamics/__init__.py
================================================
from .vegetation_dynamics import Vegetation

__all__ = ["Vegetation"]



================================================
File: vegetation_dynamics/vegetation_dynamics.py
================================================
import numpy as np

from landlab import Component

_VALID_METHODS = {"Grid"}


def assert_method_is_valid(method):
    if method not in _VALID_METHODS:
        raise ValueError("%s: Invalid method name" % method)


class Vegetation(Component):
    """Landlab component that simulates net primary productivity, biomass and
    leaf area index at each cell based on inputs of root-zone average soil
    moisture.

    Ref: Zhou, X., Istanbulluoglu, E., & Vivoni, E. R. (2013). Modeling the
    ecohydrological role of aspect controlled radiation on tree grass shrub
    coexistence in a semiarid climate. Water Resources Research,
    49(5), 2872-2895.

    .. codeauthor:: Sai Nudurupati and Erkan Istanbulluoglu

    Examples
    --------
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import Vegetation

    Create a grid on which to simulate vegetation dynamics.

    >>> grid = RasterModelGrid((5, 4), xy_spacing=(0.2, 0.2))

    The grid will need some input data. To check the names of the fields
    that provide the input to this component, use the *input_var_names*
    class property.

    >>> sorted(Vegetation.input_var_names)
    ['surface__evapotranspiration',
     'surface__potential_evapotranspiration_30day_mean',
     'surface__potential_evapotranspiration_rate',
     'vegetation__plant_functional_type',
     'vegetation__water_stress']

    >>> sorted(Vegetation.units)
    [('surface__evapotranspiration', 'mm'),
     ('surface__potential_evapotranspiration_30day_mean', 'mm'),
     ('surface__potential_evapotranspiration_rate', 'mm'),
     ('vegetation__cover_fraction', 'None'),
     ('vegetation__dead_biomass', 'g m^-2 d^-1'),
     ('vegetation__dead_leaf_area_index', 'None'),
     ('vegetation__live_biomass', 'g m^-2 d^-1'),
     ('vegetation__live_leaf_area_index', 'None'),
     ('vegetation__plant_functional_type', 'None'),
     ('vegetation__water_stress', 'None')]

    Provide all the input fields.

    >>> grid["cell"]["vegetation__plant_functional_type"] = np.zeros(
    ...     grid.number_of_cells, dtype=int
    ... )
    >>> grid["cell"]["surface__evapotranspiration"] = 0.2 * np.ones(
    ...     grid.number_of_cells
    ... )
    >>> grid["cell"]["surface__potential_evapotranspiration_rate"] = np.array(
    ...     [0.25547770, 0.25547770, 0.22110221, 0.22110221, 0.24813062, 0.24813062]
    ... )
    >>> grid["cell"]["surface__potential_evapotranspiration_30day_mean"] = np.array(
    ...     [0.25547770, 0.25547770, 0.22110221, 0.22110221, 0.24813062, 0.24813062]
    ... )
    >>> grid["cell"]["vegetation__water_stress"] = 0.01 * np.ones(grid.number_of_cells)

    Instantiate the 'Vegetation' component.

    >>> Veg = Vegetation(grid)

    >>> Veg.grid.number_of_cell_rows
    3
    >>> Veg.grid.number_of_cell_columns
    2
    >>> Veg.grid is grid
    True
    >>> import numpy as np
    >>> sorted(Vegetation.output_var_names)
    ['vegetation__cover_fraction',
     'vegetation__dead_biomass',
     'vegetation__dead_leaf_area_index',
     'vegetation__live_biomass',
     'vegetation__live_leaf_area_index']

    >>> np.all(grid.at_cell["vegetation__live_leaf_area_index"] == 0.0)
    True

    >>> Veg.update()

    >>> np.all(grid.at_cell["vegetation__live_leaf_area_index"] == 0.0)
    False

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    None Listed

    **Additional References**

    Zhou, X., Istanbulluoglu, E., and Vivoni, E. R.: Modeling the
    ecohydrological role of aspect-controlled radiation on tree-grass-shrub
    coexistence in a semiarid climate, Water Resour. Res., 49, 2872– 2895,
    doi:10.1002/wrcr.20259, 2013.

    """

    _name = "Vegetation"

    _unit_agnostic = False

    _info = {
        "surface__evapotranspiration": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "mm",
            "mapping": "cell",
            "doc": "actual sum of evaporation and plant transpiration",
        },
        "surface__potential_evapotranspiration_30day_mean": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "mm",
            "mapping": "cell",
            "doc": "30 day mean of surface__potential_evapotranspiration",
        },
        "surface__potential_evapotranspiration_rate": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "mm",
            "mapping": "cell",
            "doc": "potential sum of evaporation and potential transpiration",
        },
        "vegetation__cover_fraction": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "None",
            "mapping": "cell",
            "doc": "fraction of land covered by vegetation",
        },
        "vegetation__dead_biomass": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "g m^-2 d^-1",
            "mapping": "cell",
            "doc": (
                "weight of dead organic mass per unit area - measured in terms "
                "of dry matter"
            ),
        },
        "vegetation__dead_leaf_area_index": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "None",
            "mapping": "cell",
            "doc": "one-sided dead leaf area per unit ground surface area",
        },
        "vegetation__live_biomass": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "g m^-2 d^-1",
            "mapping": "cell",
            "doc": (
                "weight of green organic mass per unit area - measured in terms "
                "of dry matter"
            ),
        },
        "vegetation__live_leaf_area_index": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "None",
            "mapping": "cell",
            "doc": "one-sided green leaf area per unit ground surface area",
        },
        "vegetation__plant_functional_type": {
            "dtype": int,
            "intent": "in",
            "optional": False,
            "units": "None",
            "mapping": "cell",
            "doc": (
                "classification of plants (int), grass=0, shrub=1, tree=2, bare=3, "
                "shrub_seedling=4, tree_seedling=5"
            ),
        },
        "vegetation__water_stress": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "None",
            "mapping": "cell",
            "doc": "parameter that represents nonlinear effects of water deficit on plants",
        },
    }

    def __init__(
        self,
        grid,
        Blive_init=102.0,
        Bdead_init=450.0,
        ETthreshold_up=3.8,
        ETthreshold_down=6.8,
        Tdmax=10.0,
        w=0.55,
        WUE_grass=0.01,
        LAI_max_grass=2.0,
        cb_grass=0.0047,
        cd_grass=0.009,
        ksg_grass=0.012,
        kdd_grass=0.013,
        kws_grass=0.02,
        WUE_shrub=0.0025,
        LAI_max_shrub=2.0,
        cb_shrub=0.004,
        cd_shrub=0.01,
        ksg_shrub=0.002,
        kdd_shrub=0.013,
        kws_shrub=0.02,
        WUE_tree=0.0045,
        LAI_max_tree=4.0,
        cb_tree=0.004,
        cd_tree=0.01,
        ksg_tree=0.002,
        kdd_tree=0.013,
        kws_tree=0.01,
        WUE_bare=0.01,
        LAI_max_bare=0.01,
        cb_bare=0.0047,
        cd_bare=0.009,
        ksg_bare=0.012,
        kdd_bare=0.013,
        kws_bare=0.02,
        method="Grid",
        PETthreshold_switch=0,
        Tb=24.0,
        Tr=0.01,
    ):
        """
        Parameters
        ----------
        grid: RasterModelGrid
            A grid.
        Blive_init: float, optional
            Initial value for vegetation__live_biomass. Converted to field.
        Bdead_init: float, optional
            Initial value for vegetation__dead_biomass. Coverted to field.
        ETthreshold_up: float, optional
            Potential Evapotranspiration (PET) threshold for
            growing season (mm/d).
        ETthreshold_down: float, optional
            PET threshold for dormant season (mm/d).
        Tdmax: float, optional
            Constant for dead biomass loss adjustment (mm/d).
        w: float, optional
            Conversion factor of CO2 to dry biomass (Kg DM/Kg CO2).
        WUE: float, optional
            Water Use Efficiency - ratio of water used in plant water
            lost by the plant through transpiration (KgCO2Kg-1H2O).
        LAI_max: float, optional
            Maximum leaf area index (m2/m2).
        cb: float, optional
            Specific leaf area for green/live biomass (m2 leaf g-1 DM).
        cd: float, optional
            Specific leaf area for dead biomass (m2 leaf g-1 DM).
        ksg: float, optional
            Senescence coefficient of green/live biomass (d-1).
        kdd: float, optional
            Decay coefficient of aboveground dead biomass (d-1).
        kws: float, optional
            Maximum drought induced foliage loss rate (d-1).
        method: str
            Method name.
        Tr: float, optional
            Storm duration (hours).
        Tb: float, optional
            Inter-storm duration (hours).
        PETthreshold_switch: int, optional
            Flag to indiate the PET threshold. This controls whether the
            threshold is for growth (1) or dormancy (any other value).
        """
        super().__init__(grid)

        self.Tb = Tb
        self.Tr = Tr
        self.PETthreshold_switch = PETthreshold_switch

        self._method = method

        assert_method_is_valid(self._method)

        self.initialize(
            Blive_init=Blive_init,
            Bdead_init=Bdead_init,
            ETthreshold_up=ETthreshold_up,
            ETthreshold_down=ETthreshold_down,
            Tdmax=Tdmax,
            w=w,
            WUE_grass=WUE_grass,
            LAI_max_grass=LAI_max_grass,
            cb_grass=cb_grass,
            cd_grass=cd_grass,
            ksg_grass=ksg_grass,
            kdd_grass=kdd_grass,
            kws_grass=kws_grass,
            WUE_shrub=WUE_shrub,
            LAI_max_shrub=LAI_max_shrub,
            cb_shrub=cb_shrub,
            cd_shrub=cd_shrub,
            ksg_shrub=ksg_shrub,
            kdd_shrub=kdd_shrub,
            kws_shrub=kws_shrub,
            WUE_tree=WUE_tree,
            LAI_max_tree=LAI_max_tree,
            cb_tree=cb_tree,
            cd_tree=cd_tree,
            ksg_tree=ksg_tree,
            kdd_tree=kdd_tree,
            kws_tree=kws_tree,
            WUE_bare=WUE_bare,
            LAI_max_bare=LAI_max_bare,
            cb_bare=cb_bare,
            cd_bare=cd_bare,
            ksg_bare=ksg_bare,
            kdd_bare=kdd_bare,
            kws_bare=kws_bare,
        )

        self.initialize_output_fields()

        self._cell_values = self._grid["cell"]

        self._Blive_ini = self._Blive_init * np.ones(self._grid.number_of_cells)
        self._Bdead_ini = self._Bdead_init * np.ones(self._grid.number_of_cells)

    @property
    def Tb(self):
        """Storm duration (hours)."""
        return self._Tb

    @Tb.setter
    def Tb(self, Tb):
        if Tb < 0.0:
            raise ValueError("Tb must be non-negative")
        self._Tb = Tb

    @property
    def Tr(self):
        """Inter-storm duration (hours)."""
        return self._Tr

    @Tr.setter
    def Tr(self, Tr):
        assert Tr >= 0
        self._Tr = Tr

    @property
    def PETthreshold_switch(self):
        """Flag to indiate the PET threshold.

        This controls whether the threshold is for growth (1) or
        dormancy (any other value).
        """
        return self._PETthreshold_switch

    @PETthreshold_switch.setter
    def PETthreshold_switch(self, PETthreshold_switch):
        self._PETthreshold_switch = PETthreshold_switch

    def initialize(
        self,
        Blive_init=102.0,
        Bdead_init=450.0,
        ETthreshold_up=3.8,
        ETthreshold_down=6.8,
        Tdmax=10.0,
        w=0.55,
        WUE_grass=0.01,
        LAI_max_grass=2.0,
        cb_grass=0.0047,
        cd_grass=0.009,
        ksg_grass=0.012,
        kdd_grass=0.013,
        kws_grass=0.02,
        WUE_shrub=0.0025,
        LAI_max_shrub=2.0,
        cb_shrub=0.004,
        cd_shrub=0.01,
        ksg_shrub=0.002,
        kdd_shrub=0.013,
        kws_shrub=0.02,
        WUE_tree=0.0045,
        LAI_max_tree=4.0,
        cb_tree=0.004,
        cd_tree=0.01,
        ksg_tree=0.002,
        kdd_tree=0.013,
        kws_tree=0.01,
        WUE_bare=0.01,
        LAI_max_bare=0.01,
        cb_bare=0.0047,
        cd_bare=0.009,
        ksg_bare=0.012,
        kdd_bare=0.013,
        kws_bare=0.02,
    ):
        # GRASS = 0; SHRUB = 1; TREE = 2; BARE = 3;
        # SHRUBSEEDLING = 4; TREESEEDLING = 5
        """
        Parameters
        ----------
        grid: RasterModelGrid
            A grid.
        Blive_init: float, optional
            Initial value for vegetation__live_biomass. Converted to field.
        Bdead_init: float, optional
            Initial value for vegetation__dead_biomass. Coverted to field.
        ETthreshold_up: float, optional
            Potential Evapotranspiration (PET) threshold for
            growing season (mm/d).
        ETthreshold_down: float, optional
            PET threshold for dormant season (mm/d).
        Tdmax: float, optional
            Constant for dead biomass loss adjustment (mm/d).
        w: float, optional
            Conversion factor of CO2 to dry biomass (Kg DM/Kg CO2).
        WUE: float, optional
            Water Use Efficiency - ratio of water used in plant water
            lost by the plant through transpiration (KgCO2Kg-1H2O).
        LAI_max: float, optional
            Maximum leaf area index (m2/m2).
        cb: float, optional
            Specific leaf area for green/live biomass (m2 leaf g-1 DM).
        cd: float, optional
            Specific leaf area for dead biomass (m2 leaf g-1 DM).
        ksg: float, optional
            Senescence coefficient of green/live biomass (d-1).
        kdd: float, optional
            Decay coefficient of aboveground dead biomass (d-1).
        kws: float, optional
            Maximum drought induced foliage loss rate (d-1).
        """
        self._vegtype = self._grid["cell"]["vegetation__plant_functional_type"]
        self._WUE = np.choose(
            self._vegtype,
            [WUE_grass, WUE_shrub, WUE_tree, WUE_bare, WUE_shrub, WUE_tree],
        )
        # Water Use Efficiency  KgCO2kg-1H2O
        self._LAI_max = np.choose(
            self._vegtype,
            [
                LAI_max_grass,
                LAI_max_shrub,
                LAI_max_tree,
                LAI_max_bare,
                LAI_max_shrub,
                LAI_max_tree,
            ],
        )
        # Maximum leaf area index (m2/m2)
        self._cb = np.choose(
            self._vegtype, [cb_grass, cb_shrub, cb_tree, cb_bare, cb_shrub, cb_tree]
        )
        # Specific leaf area for green/live biomass (m2 leaf g-1 DM)
        self._cd = np.choose(
            self._vegtype, [cd_grass, cd_shrub, cd_tree, cd_bare, cd_shrub, cd_tree]
        )
        # Specific leaf area for dead biomass (m2 leaf g-1 DM)
        self._ksg = np.choose(
            self._vegtype,
            [ksg_grass, ksg_shrub, ksg_tree, ksg_bare, ksg_shrub, ksg_tree],
        )
        # Senescence coefficient of green/live biomass (d-1)
        self._kdd = np.choose(
            self._vegtype,
            [kdd_grass, kdd_shrub, kdd_tree, kdd_bare, kdd_shrub, kdd_tree],
        )
        # Decay coefficient of aboveground dead biomass (d-1)
        self._kws = np.choose(
            self._vegtype,
            [kws_grass, kws_shrub, kws_tree, kws_bare, kws_shrub, kws_tree],
        )
        # Maximum drought induced foliage loss rates (d-1)
        self._Blive_init = Blive_init
        self._Bdead_init = Bdead_init
        self._ETthresholdup = ETthreshold_up  # Growth threshold (mm/d)
        self._ETthresholddown = ETthreshold_down  # Dormancy threshold (mm/d)
        self._Tdmax = Tdmax  # Constant for dead biomass loss adjustment
        self._w = w  # Conversion factor of CO2 to dry biomass

        self._Blive_ini = self._Blive_init * np.ones(self._grid.number_of_cells)
        self._Bdead_ini = self._Bdead_init * np.ones(self._grid.number_of_cells)

    def update(self):
        """Update fields with current loading conditions.

        This method looks to the properties ``PETthreshold_switch``,
        ``Tb``, and ``Tr`` and uses their values to calculate the new
        field values.
        """
        PETthreshold_ = self._PETthreshold_switch
        Tb = self._Tb
        Tr = self._Tr

        PET = self._cell_values["surface__potential_evapotranspiration_rate"]
        PET30_ = self._cell_values["surface__potential_evapotranspiration_30day_mean"]
        ActualET = self._cell_values["surface__evapotranspiration"]
        Water_stress = self._cell_values["vegetation__water_stress"]

        self._LAIlive = self._cell_values["vegetation__live_leaf_area_index"]
        self._LAIdead = self._cell_values["vegetation__dead_leaf_area_index"]
        self._Blive = self._cell_values["vegetation__live_biomass"]
        self._Bdead = self._cell_values["vegetation__dead_biomass"]
        self._VegCov = self._cell_values["vegetation__cover_fraction"]

        if PETthreshold_ == 1:
            PETthreshold = self._ETthresholdup
        else:
            PETthreshold = self._ETthresholddown

        for cell in range(0, self._grid.number_of_cells):
            WUE = self._WUE[cell]
            LAImax = self._LAI_max[cell]
            cb = self._cb[cell]
            cd = self._cd[cell]
            ksg = self._ksg[cell]
            kdd = self._kdd[cell]
            kws = self._kws[cell]
            # ETdmax = self._ETdmax[cell]
            LAIlive = min(cb * self._Blive_ini[cell], LAImax)
            LAIdead = min(cd * self._Bdead_ini[cell], (LAImax - LAIlive))
            NPP = max((ActualET[cell] / (Tb + Tr)) * WUE * 24.0 * self._w * 1000, 0.001)

            if self._vegtype[cell] == 0:
                if PET30_[cell] > PETthreshold:
                    # Growing Season
                    Bmax = (LAImax - LAIdead) / cb
                    Yconst = 1 / (
                        (1 / Bmax) + (((kws * Water_stress[cell]) + ksg) / NPP)
                    )
                    Blive = (self._Blive_ini[cell] - Yconst) * np.exp(
                        -(NPP / Yconst) * ((Tb + Tr) / 24.0)
                    ) + Yconst
                    Bdead = (
                        self._Bdead_ini[cell]
                        + (Blive - max(Blive * np.exp(-1 * ksg * Tb / 24.0), 0.00001))
                    ) * np.exp(-1 * kdd * min(PET[cell] / self._Tdmax, 1.0) * Tb / 24.0)
                else:  # Senescense
                    Blive = max(
                        self._Blive_ini[cell] * np.exp((-2) * ksg * Tb / 24.0), 1
                    )
                    Bdead = max(
                        (
                            self._Bdead_ini[cell]
                            + (
                                self._Blive_ini[cell]
                                - (
                                    max(
                                        self._Blive_ini[cell]
                                        * np.exp((-2) * ksg * Tb / 24.0),
                                        0.000001,
                                    )
                                )
                            )
                            * np.exp(
                                (-1)
                                * kdd
                                * min(PET[cell] / self._Tdmax, 1.0)
                                * Tb
                                / 24.0
                            ),
                            0.0,
                        )
                    )

            elif self._vegtype[cell] == 3:
                Blive = 0.0
                Bdead = 0.0

            else:
                Bmax = LAImax / cb
                Yconst = 1.0 / (
                    (1.0 / Bmax) + (((kws * Water_stress[cell]) + ksg) / NPP)
                )
                Blive = (self._Blive_ini[cell] - Yconst) * np.exp(
                    -(NPP / Yconst) * ((Tb + Tr) / 24.0)
                ) + Yconst
                Bdead = (
                    self._Bdead_ini[cell]
                    + (Blive - max(Blive * np.exp(-ksg * Tb / 24.0), 0.00001))
                ) * np.exp(-kdd * min(PET[cell] / self._Tdmax, 1.0) * Tb / 24.0)

            LAIlive = min(cb * (Blive + self._Blive_ini[cell]) / 2.0, LAImax)
            LAIdead = min(
                cd * (Bdead + self._Bdead_ini[cell]) / 2.0, (LAImax - LAIlive)
            )
            if self._vegtype[cell] == 0:
                Vt = 1.0 - np.exp(-0.75 * (LAIlive + LAIdead))
            else:
                # Vt = 1 - np.exp(-0.75 * LAIlive)
                Vt = 1.0

            self._LAIlive[cell] = LAIlive
            self._LAIdead[cell] = LAIdead
            self._VegCov[cell] = Vt
            self._Blive[cell] = Blive
            self._Bdead[cell] = Bdead

        self._Blive_ini = self._Blive
        self._Bdead_ini = self._Bdead



================================================
File: weathering/__init__.py
================================================
from .exponential_weathering import ExponentialWeatherer
from .exponential_weathering_integrated import ExponentialWeathererIntegrated

__all__ = ["ExponentialWeatherer", "ExponentialWeathererIntegrated"]



================================================
File: weathering/exponential_weathering.py
================================================
#!/usr/bin/env python
"""Created on Fri Apr  8 08:32:48 2016.

@author: RCGlade
"""

import numpy as np

from landlab import Component


class ExponentialWeatherer(Component):
    """Calculate exponential weathering of bedrock on hillslopes.

    Uses exponential soil production function in the style of Ahnert (1976).

    Consider that ``w0`` is the maximum soil production rate and
    that ``w_star`` is the characteristic soil production depth. The
    soil production rate ``w0`` is given as a function of the soil
    depth::

        soil_production =  w0 * exp(-soil__depth / w_star)

    The :class:`~.ExponentialWeatherer` only calculates soil production at core nodes.

    An alternative version which uses the analytical integral of
    production through time is available at the component
    :py:class:`~.ExponentialWeathererIntegrated`.

    Examples
    --------
    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import ExponentialWeatherer
    >>> mg = RasterModelGrid((5, 5))
    >>> soilz = mg.add_zeros("soil__depth", at="node")
    >>> soilrate = mg.add_ones("soil_production__rate", at="node")
    >>> expw = ExponentialWeatherer(mg)
    >>> expw.calc_soil_prod_rate()
    >>> np.allclose(mg.at_node["soil_production__rate"], 1.0)
    True

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Barnhart, K., Glade, R., Shobe, C., Tucker, G. (2019). Terrainbento 1.0: a
    Python package for multi-model analysis in long-term drainage basin
    evolution. Geoscientific Model Development  12(4), 1267--1297.
    https://dx.doi.org/10.5194/gmd-12-1267-2019

    **Additional References**

    Ahnert, F. (1976). Brief description of a comprehensive three-dimensional
    process-response model of landform development Z. Geomorphol. Suppl.  25,
    29 - 49.

    Armstrong, A. (1976). A three dimensional simulation of slope forms.
    Zeitschrift für Geomorphologie  25, 20 - 28.
    """

    _name = "ExponentialWeatherer"

    _unit_agnostic = True

    _cite_as = """
    @article{barnhart2019terrain,
      author = {Barnhart, Katherine R and Glade, Rachel C and Shobe, Charles M
                and Tucker, Gregory E},
      title = {{Terrainbento 1.0: a Python package for multi-model analysis in
                long-term drainage basin evolution}},
      doi = {10.5194/gmd-12-1267-2019},
      pages = {1267---1297},
      number = {4},
      volume = {12},
      journal = {Geoscientific Model Development},
      year = {2019},
    }
    """

    _info = {
        "soil__depth": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of soil or weathered bedrock",
        },
        "soil_production__rate": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/yr",
            "mapping": "node",
            "doc": "rate of soil production at nodes",
        },
    }

    def __init__(
        self, grid, soil_production_maximum_rate=1.0, soil_production_decay_depth=1.0
    ):
        """
        Parameters
        ----------
        grid: ModelGrid
            Landlab ModelGrid object
        soil_production_maximum_rate : float, array of float
            Maximum weathering rate for bare bedrock
        soil_production_decay_depth : float, array of float
            Characteristic weathering depth
        """
        super().__init__(grid)

        # Store grid and parameters
        # soil_production_maximum_rate, value set in setter below
        self._w0 = None
        # soil_production_decay_depth, value set in setter below
        self._wstar = None

        self.maximum_weathering_rate = soil_production_maximum_rate
        self.decay_depth = soil_production_decay_depth

        # weathering rate
        if "soil_production__rate" not in grid.at_node:
            grid.add_zeros("soil_production__rate", at="node")

    def calc_soil_prod_rate(self):
        """Calculate soil production rate."""
        core = self._grid.core_nodes
        self.grid.at_node["soil_production__rate"][core] = self._w0[core] * np.exp(
            -self.grid.at_node["soil__depth"][core] / self._wstar[core]
        )

    def run_one_step(self):
        """

        Parameters
        ----------
        dt: float
            Used only for compatibility with standard run_one_step.
        """
        self.calc_soil_prod_rate()

    @property
    def maximum_weathering_rate(self):
        """Maximum rate of weathering (m/yr)."""
        return self._w0

    @maximum_weathering_rate.setter
    def maximum_weathering_rate(self, new_val):
        if np.any(new_val <= 0):
            raise ValueError("Maximum weathering rate must be positive.")
        self._w0 = np.broadcast_to(new_val, self.grid.number_of_nodes)

    @property
    def decay_depth(self):
        """Maximum rate of weathering (m/yr)."""
        return self._wstar

    @decay_depth.setter
    def decay_depth(self, new_val):
        if np.any(new_val <= 0):
            raise ValueError("Maximum decay depth must be positive.")
        self._wstar = np.broadcast_to(new_val, self.grid.number_of_nodes)



================================================
File: weathering/exponential_weathering_integrated.py
================================================
#!/usr/bin/env python
"""Created on Fri Apr  8 08:32:48 2016.

@author: RCGlade
@author: dylanward
Integrated version created by D. Ward on Tue Oct 27 2020
"""

import numpy as np

from landlab import Component


class ExponentialWeathererIntegrated(Component):
    """
    This component implements exponential weathering of bedrock on
    hillslopes. Uses exponential soil production function in the style
    of Ahnert (1976).

    Consider that ``w_0`` is the maximum soil production rate and
    that ``d_start`` is the characteristic soil production depth. The
    soil production rate ``w`` is given as a function of the soil
    depth ``d``::

        w = w_0 exp(-d / d_star)

    The :class:`~.ExponentialWeathererIntegrated` uses the analytical solution
    for the amount of soil produced by an exponential weathering
    function over a timestep dt, and returns both the thickness of
    bedrock weathered and the thickness of soil produced. The solution
    accounts for the reduction in rate over the timestep due to the
    increasing depth. This enables accuracy over arbitrarily large
    timesteps, and better compatiblity with the `run_one_step()`
    interface.

    Compared to :class:`~.ExponentialWeatherer`, upon which it is based...

    - This maintains the field I/O behavior of the original, but adds
      new return fields for the weathered thickness and soil produced
      thickness.
    - Density adjustments are needed inside the integral and the
      density ratio is intialized on instantiation. The default value
      of 1.0 assumes no change in density.
    - Returns both weathered depth of bedrock and produced depth of
      soil over the timestep.
    - The primary ``soil__depth`` field that is input is NOT updated by
      the component.

    This is left as an exercise for the model driver, as different
    applications may want to integrate soil depth and weathering in
    different sequences among other processes.

    - SHOULD maintain drop-in compatiblity with the plain
      :class:`~.ExponentialWeatherer`, just import and instantiate this one instead
      and existing code should work with no side effects other than the creation of the
      two additional (zeros) output fields.

    Examples
    --------
    >>> import numpy as np
    >>> from landlab import RasterModelGrid
    >>> from landlab.components import ExponentialWeathererIntegrated
    >>> mg = RasterModelGrid((5, 5))
    >>> soilz = mg.add_zeros("soil__depth", at="node")
    >>> soilrate = mg.add_ones("soil_production__rate", at="node")
    >>> expw = ExponentialWeathererIntegrated(mg)
    >>> dt = 1000
    >>> expw.run_one_step(dt)
    >>> np.allclose(mg.at_node["soil_production__rate"][mg.core_nodes], 1.0)
    True
    >>> np.allclose(
    ...     mg.at_node["soil_production__dt_produced_depth"][mg.core_nodes], 6.9088
    ... )
    True

    References
    ----------
    **Required Software Citation(s) Specific to this Component**

    Barnhart, K., Glade, R., Shobe, C., Tucker, G. (2019). Terrainbento 1.0: a
    Python package for multi-model analysis in long-term drainage basin
    evolution. Geoscientific Model Development  12(4), 1267--1297.
    https://dx.doi.org/10.5194/gmd-12-1267-2019

    **Additional References**

    Ahnert, F. (1976). Brief description of a comprehensive three-dimensional
    process-response model of landform development Z. Geomorphol. Suppl.  25,
    29 - 49.

    Armstrong, A. (1976). A three dimensional simulation of slope forms.
    Zeitschrift für Geomorphologie  25, 20 - 28.
    """

    _name = "ExponentialWeathererIntegrated"

    _unit_agnostic = True

    _cite_as = """
    @article{barnhart2019terrain,
      author = {Barnhart, Katherine R and Glade, Rachel C and Shobe, Charles M
                and Tucker, Gregory E},
      title = {{Terrainbento 1.0: a Python package for multi-model analysis in
                long-term drainage basin evolution}},
      doi = {10.5194/gmd-12-1267-2019},
      pages = {1267---1297},
      number = {4},
      volume = {12},
      journal = {Geoscientific Model Development},
      year = {2019},
    }
    """

    _info = {
        "soil__depth": {
            "dtype": float,
            "intent": "in",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "Depth of soil or weathered bedrock",
        },
        "soil_production__rate": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m/yr",
            "mapping": "node",
            "doc": "rate of soil production at nodes",
        },
        "soil_production__dt_produced_depth": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "thickness of soil produced at nodes over time dt",
        },
        "soil_production__dt_weathered_depth": {
            "dtype": float,
            "intent": "out",
            "optional": False,
            "units": "m",
            "mapping": "node",
            "doc": "thickness of bedrock weathered at nodes over time dt",
        },
    }

    def __init__(
        self,
        grid,
        soil_production__maximum_rate=1.0,
        soil_production__decay_depth=1.0,
        soil_production__expansion_factor=1.0,
    ):
        """
        Parameters
        ----------
        grid: ModelGrid
            Landlab ModelGrid object
        soil_production__maximum_rate : float
            Maximum weathering rate for bare bedrock
        soil_production__decay_depth : float
            Characteristic weathering depth
        soil_production__expansion_factor : float
            Expansion ratio of regolith (from relative densities of
            rock and soil)
        """
        super().__init__(grid)

        # Store grid and parameters

        self._wstar = soil_production__decay_depth
        self._w0 = soil_production__maximum_rate
        self._fexp = soil_production__expansion_factor

        # Create fields:
        # soil depth
        self._depth = grid.at_node["soil__depth"]

        # weathering rate
        if "soil_production__rate" not in grid.at_node:
            grid.add_zeros("soil_production__rate", at="node")
        self._soil_prod_rate = grid.at_node["soil_production__rate"]

        # soil produced total over dt
        if "soil_production__dt_produced_depth" not in grid.at_node:
            grid.add_zeros("soil_production__dt_produced_depth", at="node")
        self._soil_prod_total = grid.at_node["soil_production__dt_produced_depth"]

        # bedrock weathering total over dt
        if "soil_production__dt_weathered_depth" not in grid.at_node:
            grid.add_zeros("soil_production__dt_weathered_depth", at="node")
        self._rock_weathered_total = grid.at_node["soil_production__dt_weathered_depth"]

    def calc_soil_prod_rate(self):
        """Calculate soil production rate."""
        # apply exponential function
        self._soil_prod_rate[self._grid.core_nodes] = self._w0 * np.exp(
            -self._depth[self._grid.core_nodes] / self._wstar
        )

    def _calc_dt_production_total(self, dt):
        """Calculate integrated production over 1 timestep dt"""
        # analytical solution
        self._soil_prod_total[self._grid.core_nodes] = self._wstar * np.log(
            (
                self._fexp
                * self._soil_prod_rate[self._grid.core_nodes]
                * dt
                / self._wstar
            )
            + 1
        )
        # and back-convert to find rock thickness converted over the timestep:
        self._rock_weathered_total[self._grid.core_nodes] = (
            self._soil_prod_total[self._grid.core_nodes] / self._fexp
        )

    def run_one_step(self, dt=0):
        """
        Parameters
        ----------
        dt: float
            Used only for compatibility with standard run_one_step.
            If dt is not provided, the default of zero maintains backward compatibility
        """
        self.calc_soil_prod_rate()
        self._calc_dt_production_total(dt)

    @property
    def maximum_weathering_rate(self):
        """Maximum rate of weathering (m/yr)."""
        return self._w0

    @maximum_weathering_rate.setter
    def maximum_weathering_rate(self, new_val):
        if new_val <= 0:
            raise ValueError("Maximum weathering rate must be positive.")
        self._w0 = new_val