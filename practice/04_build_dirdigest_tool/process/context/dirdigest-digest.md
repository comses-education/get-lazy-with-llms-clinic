# Directory Digest: /home/asuworks/work/repos/github.com/comses-education/get-lazy-with-llms-clinic/practice/build_dirdigest_tool/result/tdd-method/dirdigest/dirdigest

*Generated by dirdigest v0.1.0 on 2025-05-09T06:22:01.801443*
*Included files: 10, Total content size: 56.18 KB*

---

## Directory Structure

```text
.
├── __init__.py
├── cli.py
├── constants.py
├── core.py
├── formatter.py
└── utils/
    ├── __init__.py
    ├── clipboard.py
    ├── config.py
    ├── logger.py
    └── patterns.py
```


---

## Contents

### `./__init__.py`
```py

```

### `./cli.py`
```py
import click
import pathlib
import time 
import logging

from dirdigest.constants import TOOL_NAME, TOOL_VERSION
from dirdigest import core
from dirdigest import formatter as dirdigest_formatter
from dirdigest.utils import logger as dirdigest_logger
from dirdigest.utils import clipboard as dirdigest_clipboard
from dirdigest.utils import config as dirdigest_config


@click.command(
    name=TOOL_NAME,
    context_settings=dict(help_option_names=['-h', '--help']),
    help="Recursively processes directories and files, creating a structured digest suitable for LLM context ingestion."
)
@click.version_option(version=TOOL_VERSION, prog_name=TOOL_NAME, message="%(prog)s version %(version)s")
@click.pass_context 
@click.argument(
    'directory_arg',
    type=click.Path(
        exists=True,
        file_okay=False,
        dir_okay=True,
        readable=True,
        resolve_path=True,
        path_type=pathlib.Path
    ),
    default='.',
    required=False,
    metavar='DIRECTORY'
)
@click.option(
    '--output', '-o',
    type=click.Path(dir_okay=False, writable=True, path_type=pathlib.Path),
    default=None,
    help='Path to the output file. If omitted, the digest is written to standard output (stdout).'
)
@click.option(
    '--format', '-f',
    type=click.Choice(['json', 'markdown'], case_sensitive=False),
    default='markdown',
    show_default=True,
    help="Output format for the digest. Choices: 'json', 'markdown'."
)
@click.option(
    '--include', '-i',
    multiple=True,
    help=("Glob pattern(s) for files/directories to INCLUDE. If specified, only items matching these "
          "patterns are processed. Can be used multiple times or comma-separated "
          "(e.g., -i '*.py' -i 'src/' or -i '*.py,src/'). Exclusions are applied first.")
)
@click.option(
    '--exclude', '-x',
    multiple=True,
    help=("Glob pattern(s) for files/directories to EXCLUDE. Takes precedence over include patterns. "
          "Can be used multiple times or comma-separated (e.g., -x '*.log' -x 'tests/' or "
          "-x '*.log,tests/'). Default ignores also apply unless --no-default-ignore is set.")
)
@click.option(
    '--max-size', '-s',
    type=click.IntRange(min=0),
    default=300,
    show_default=True,
    help="Maximum size (in KB) for individual files to be included. Larger files are excluded."
)
@click.option(
    '--max-depth', '-d',
    type=click.IntRange(min=0),
    default=None,
    show_default="unlimited",
    help="Maximum depth of directories to traverse. Depth 0 processes only the starting directory's files. Unlimited by default."
)
@click.option(
    '--no-default-ignore',
    is_flag=True,
    show_default=True, # Default is False
    help=("Disable all default ignore patterns (e.g., .git, __pycache__, node_modules, common "
          "binary/media files, hidden items). Use if you need to include items normally ignored by default.")
)
@click.option(
    '--follow-symlinks',
    is_flag=True,
    show_default=True, # Default is False
    help="Follow symbolic links to directories and files. By default, symlinks themselves are noted but not traversed/read."
)
@click.option(
    '--ignore-errors',
    is_flag=True,
    show_default=True, # Default is False
    help=("Continue processing if an error occurs while reading a file (e.g., permission denied, "
          "decoding error). The file's content will be omitted or noted as an error in the digest.")
)
@click.option(
    '--clipboard/--no-clipboard', '-c',
    default=True,
    show_default=True,
    help="Copy the generated digest to the system clipboard. Use --no-clipboard to disable."
)
@click.option(
    '--verbose', '-v',
    count=True,
    help='Increase verbosity. -v for INFO, -vv for DEBUG console output.'
)
@click.option(
    '--quiet', '-q',
    is_flag=True,
    help='Suppress all console output below ERROR level. Overrides -v.'
)
@click.option(
    '--log-file',
    type=click.Path(dir_okay=False, writable=True, path_type=pathlib.Path),
    default=None,
    help="Path to a file for detailed logging. All logs (including DEBUG level) will be written here, regardless of console verbosity."
)
@click.option(
    '--config', 'config_path_cli',
    type=click.Path(exists=True, dir_okay=False, readable=True, path_type=pathlib.Path),
    default=None,
    # Corrected help text:
    help=(f"Specify configuration file path. If omitted, tries to load "
          f"./{dirdigest_config.DEFAULT_CONFIG_FILENAME} from the current directory.")
)
def main_cli( # Parameters match the names of the click options
    ctx: click.Context,
    directory_arg: pathlib.Path,
    output: pathlib.Path | None,
    format: str,
    include: tuple[str, ...],
    exclude: tuple[str, ...],
    max_size: int,
    max_depth: int | None,
    no_default_ignore: bool,
    follow_symlinks: bool,
    ignore_errors: bool,
    clipboard: bool,
    verbose: int,
    quiet: bool,
    log_file: pathlib.Path | None,
    config_path_cli: pathlib.Path | None
):
    # ... (rest of the main_cli function remains the same) ...
    # (The logic for loading config, merging, setting up logging, processing, formatting, etc.)

# Need to ensure the main_cli body is here for completeness, though it doesn't change in this step.
# For brevity in this response, I'm omitting the body of main_cli as it was provided in the previous step's "full cli.py"
# and the focus here is only on the @click.option help strings.
# The actual implementation would have the full main_cli body here.
# For this step, only the help strings above are modified.
    start_time = time.monotonic()

    cfg_file_values = dirdigest_config.load_config_file(config_path_cli)
    cli_params_for_merge = ctx.params.copy()
    if 'directory_arg' in cli_params_for_merge and 'directory' not in cli_params_for_merge:
        cli_params_for_merge['directory'] = cli_params_for_merge.pop('directory_arg')
    if 'config_path_cli' in cli_params_for_merge and 'config' not in cli_params_for_merge:
         cli_params_for_merge['config'] = cli_params_for_merge.pop('config_path_cli')
    final_settings = dirdigest_config.merge_config(cli_params_for_merge, cfg_file_values, ctx)

    final_verbose = final_settings.get('verbose', 0)
    final_quiet = final_settings.get('quiet', False)
    final_log_file_val = final_settings.get('log_file') 
    if isinstance(final_log_file_val, str):
        final_log_file_val = pathlib.Path(final_log_file_val)

    dirdigest_logger.setup_logging(
        verbose_level=final_verbose, 
        quiet=final_quiet, 
        log_file_path=final_log_file_val
    )
    log = dirdigest_logger.logger

    final_directory = final_settings.get('directory', directory_arg)
    if isinstance(final_directory, str):
        final_directory = pathlib.Path(final_directory)
        if not final_directory.exists() or not final_directory.is_dir():
            log.error(f"Directory '{final_directory}' from config does not exist or is not a directory. Using CLI/default: '{directory_arg}'")
            final_directory = directory_arg
    
    final_output_path = final_settings.get('output', output)
    if isinstance(final_output_path, str):
        final_output_path = pathlib.Path(final_output_path)

    final_format = final_settings.get('format', format)
    final_include = final_settings.get('include', include if include else [])
    final_exclude = final_settings.get('exclude', exclude if exclude else [])
    final_max_size = final_settings.get('max_size', max_size)
    final_max_depth = final_settings.get('max_depth', max_depth)
    final_no_default_ignore = final_settings.get('no_default_ignore', no_default_ignore)
    final_follow_symlinks = final_settings.get('follow_symlinks', follow_symlinks)
    final_ignore_errors = final_settings.get('ignore_errors', ignore_errors)
    final_clipboard = final_settings.get('clipboard', clipboard)

    log.debug(f"CLI: Final effective settings after merge: {final_settings}")
    log.info(f"CLI: Processing directory: [log.path]{final_directory}[/log.path]")
    if final_output_path:
        log.info(f"CLI: Output will be written to: [log.path]{final_output_path}[/log.path]")
    else:
        log.info("CLI: Output will be written to stdout")
    log.info(f"CLI: Format: {final_format.upper()}")
    if final_verbose > 0 :
        log.info(f"CLI: Include patterns: {final_include if final_include else 'N/A'}")
        log.info(f"CLI: Exclude patterns: {final_exclude if final_exclude else 'N/A'}")
        log.info(f"CLI: Max size: {final_max_size}KB, Max depth: {final_max_depth if final_max_depth is not None else 'unlimited'}")
        log.info(f"CLI: Default ignores {'DISABLED' if final_no_default_ignore else 'ENABLED'}")
        log.info(f"CLI: Follow symlinks: {final_follow_symlinks}, Ignore errors: {final_ignore_errors}")
        log.info(f"CLI: Clipboard: {final_clipboard}")

    processed_items_generator, stats_from_core = core.process_directory_recursive(
        base_dir_path=final_directory,
        include_patterns=final_include,
        exclude_patterns=final_exclude,
        no_default_ignore=final_no_default_ignore,
        max_depth=final_max_depth,
        follow_symlinks=final_follow_symlinks,
        max_size_kb=final_max_size,
        ignore_read_errors=final_ignore_errors
    )

    log.info("CLI: Building digest tree...")
    root_node, metadata_for_output = core.build_digest_tree(
        final_directory,
        processed_items_generator,
        stats_from_core
    )
    log.debug(f"CLI: Digest tree built. Root node children: {len(root_node.get('children',[]))}")
    log.debug(f"CLI: Metadata for output: {metadata_for_output}")

    selected_formatter: dirdigest_formatter.BaseFormatter
    if final_format.lower() == 'json':
        selected_formatter = dirdigest_formatter.JsonFormatter(final_directory, metadata_for_output)
    elif final_format.lower() == 'markdown':
        selected_formatter = dirdigest_formatter.MarkdownFormatter(final_directory, metadata_for_output)
    else: 
        log.critical(f"CLI: Invalid format '{final_format}' encountered. Exiting.")
        ctx.exit(1)
        return 

    log.info(f"CLI: Formatting output as {final_format.upper()}...")
    
    final_output_str = "" 
    output_generation_succeeded = False 

    try:
        generated_digest = selected_formatter.format(root_node)
        
        if final_output_path: 
            with open(final_output_path, 'w', encoding='utf-8') as f_out:
                f_out.write(generated_digest)
            log.info(f"CLI: Digest successfully written to [log.path]{final_output_path}[/log.path]")
        else: 
            dirdigest_logger.stdout_console.print(generated_digest, end="")
            if not generated_digest.endswith('\n'):
                dirdigest_logger.stdout_console.print()
        
        final_output_str = generated_digest 
        output_generation_succeeded = True

    except Exception as e:
        # ADD THIS LINE FOR SUPER EXPLICIT DEBUGGING:
        dirdigest_logger.stderr_console.print(f"[bold red reverse]DEBUG_EXCEPTION_CLIPBOARD: Exception caught in output block: {type(e).__name__} - {e}[/]")
        
        log.error(f"CLI: Error during output formatting or writing: {e}", exc_info=True)
        final_output_str = f"Error generating output: {e}" 
        # output_generation_succeeded remains False (its initial value)

    # --- Clipboard ---
    if final_clipboard:
        # Add a debug log here too to see the state
        log.debug(f"CLI_CLIPBOARD_CHECK: output_generation_succeeded={output_generation_succeeded}, final_output_str starts with '{final_output_str[:30]}...'")
        if output_generation_succeeded and final_output_str: 
            dirdigest_clipboard.copy_to_clipboard(final_output_str)
        elif not output_generation_succeeded: 
            log.warning("CLI: Output generation failed (see error above), not copying to clipboard.")
        else: 
            log.debug("CLI: Output is empty, nothing to copy to clipboard.")
    else:
        log.debug("CLI: Clipboard copy disabled.")

    execution_time = time.monotonic() - start_time
    inc_count = metadata_for_output.get("included_files_count", 0)
    exc_count = metadata_for_output.get("excluded_files_count", 0)
    total_size = metadata_for_output.get("total_content_size_kb", 0.0)

    log.info("-" * 30 + " SUMMARY " + "-" * 30)
    log.info(f"[log.summary_key]Total files included:[/log.summary_key] [log.summary_value_inc]{inc_count}[/log.summary_value_inc]")
    log.info(f"[log.summary_key]Total items excluded (files/dirs):[/log.summary_key] [log.summary_value_exc]{exc_count}[/log.summary_value_exc]")
    log.info(f"[log.summary_key]Total content size:[/log.summary_key] [log.summary_value_neutral]{total_size:.2f} KB[/log.summary_value_neutral]")
    log.info(f"[log.summary_key]Execution time:[/log.summary_key] [log.summary_value_neutral]{execution_time:.2f} seconds[/log.summary_value_neutral]")
    log.info("-" * (60 + len(" SUMMARY ")))
    
    will_log_debug_tree = False
    if log.isEnabledFor(logging.DEBUG):
        for handler in log.handlers:
            if handler.level <= logging.DEBUG:
                will_log_debug_tree = True
                break
    
    if will_log_debug_tree:
        import json as json_debugger 
        def json_default_serializer(obj):
            if isinstance(obj, pathlib.Path): return str(obj)
            return f"<not serializable: {type(obj).__name__}>"
        log.debug("CLI: --- Generated Data Tree (Debug from CLI) ---")
        try:
            json_tree_str = json_debugger.dumps(root_node, indent=2, default=json_default_serializer)
            log.debug(json_tree_str)
        except TypeError as e:
            log.debug(f"CLI: Error serializing data tree to JSON for debug: {e}")
        log.debug("CLI: --- End Generated Data Tree ---")

if __name__ == '__main__':
    main_cli()
```

### `./constants.py`
```py
# dirdigest/dirdigest/constants.py
TOOL_NAME = "dirdigest"
TOOL_VERSION = "0.1.0"  # Corresponds to pyproject.toml version

# Using gitignore style patterns.
# Ensure patterns for directories end with a '/' if they are meant to only match directories.
# Otherwise, fnmatch might match 'node_modules.txt' with 'node_modules'.
# For simplicity here, we'll rely on os.path.isdir checks later for directory-specific patterns
# if not using a library that handles this distinction well (like gitignore_parser).
# For now, fnmatch will be used, and it doesn't distinguish files from dirs based on trailing slash.

DEFAULT_IGNORE_PATTERNS = [
    # Hidden files and directories
    ".*",  # Matches .git, .DS_Store, .env, etc.
    "**/.DS_Store",  # More specific for .DS_Store in any subdir
    "**/Thumbs.db",
    # Binary and media files (common examples)
    "*.jpg",
    "*.jpeg",
    "*.png",
    "*.gif",
    "*.bmp",
    "*.tiff",
    "*.webp",
    "*.mp4",
    "*.avi",
    "*.mov",
    "*.mkv",
    "*.wmv",
    "*.mp3",
    "*.wav",
    "*.flac",
    "*.aac",
    "*.ogg",
    "*.exe",
    "*.dll",
    "*.so",
    "*.dylib",
    "*.app",
    "*.msi",
    "*.com",
    "*.bat",
    "*.sh",
    "*.zip",
    "*.tar",
    "*.tar.gz",
    "*.tar.bz2",
    "*.rar",
    "*.7z",
    "*.gz",
    "*.bz2",
    "*.woff",
    "*.woff2",
    "*.ttf",
    "*.otf",
    "*.eot",
    "*.pdf",
    "*.doc",
    "*.docx",
    "*.ppt",
    "*.pptx",
    "*.xls",
    "*.xlsx",
    "*.odt",
    "*.ods",
    "*.odp",
    "*.iso",
    "*.img",
    "*.dmg",
    # Development artifacts
    "*.pyc",
    "*.pyo",
    "*.pyd",
    "*.class",
    "*.jar",
    "*.war",
    "*.ear",
    "*.o",
    "*.obj",
    "*.lib",
    "*.a",
    "*.o.*",  # *.o.* for object files from some compilers
    "__pycache__/",  # Matches the directory itself
    ".cache/",
    "dist/",
    "build/",
    "target/",  # Common for Java/Rust
    "out/",  # Common for some build systems
    "node_modules/",
    "bower_components/",
    ".venv/",
    "venv/",
    "ENV/",
    "env/",
    ".env/",  # Virtual environments
    ".git/",  # VCS directories
    ".svn/",
    ".hg/",
    "*.egg-info/",
    # Data and temporary files
    "*.db",
    "*.sqlite",
    "*.sqlite3",
    "*.mdb",
    "*.log",
    "*.tmp",
    "*.temp",
    "*.bak",
    "*.swp",
    "~*",  # ~* for Vim backup files
    "*.DS_Store",  # Already covered by .*, but explicit is fine
    "Thumbs.db",  # Already covered by .*, but explicit is fine
]

```

### `./core.py`
```py
# dirdigest/dirdigest/core.py
import os
import pathlib
from typing import Any, Generator, Tuple, List, Dict

from dirdigest.constants import DEFAULT_IGNORE_PATTERNS
from dirdigest.utils.patterns import matches_patterns, is_path_hidden
from dirdigest.utils.logger import logger  # Import the configured logger

# Type hints for clarity
DigestItemNode = Dict[str, Any]
ProcessedItemPayload = Dict[str, Any]
ProcessedItem = Tuple[pathlib.Path, str, ProcessedItemPayload]
TraversalStats = Dict[str, int]


def process_directory_recursive(
    base_dir_path: pathlib.Path,
    include_patterns: List[str],
    exclude_patterns: List[str],
    no_default_ignore: bool,
    max_depth: int | None,
    follow_symlinks: bool,
    max_size_kb: int,
    ignore_read_errors: bool,
) -> Tuple[Generator[ProcessedItem, None, None], TraversalStats]:
    """
    Recursively traverses a directory, filters files and folders,
    and yields processed file items along with collected traversal statistics.
    """
    stats: TraversalStats = {
        "included_files_count": 0,
        "excluded_items_count": 0,
    }

    max_size_bytes = max_size_kb * 1024
    effective_exclude_patterns = list(
        exclude_patterns
    )  # Start with user-defined excludes
    if not no_default_ignore:
        effective_exclude_patterns.extend(DEFAULT_IGNORE_PATTERNS)

    logger.debug(
        f"Core: Effective exclude patterns count: {len(effective_exclude_patterns)}"
    )
    logger.debug(
        f"Core: Max size KB: {max_size_kb}, Ignore read errors: {ignore_read_errors}"
    )
    logger.debug(
        f"Core: Follow symlinks: {follow_symlinks}, No default ignore: {no_default_ignore}"
    )

    def _traverse() -> Generator[ProcessedItem, None, None]:
        """Nested generator function to handle the actual traversal and yielding."""
        for root, dirs_orig, files_orig in os.walk(
            str(base_dir_path), topdown=True, followlinks=follow_symlinks
        ):
            current_root_path = pathlib.Path(root)
            relative_root_path = current_root_path.relative_to(base_dir_path)
            current_depth = (
                len(relative_root_path.parts)
                if relative_root_path != pathlib.Path(".")
                else 0
            )
            logger.debug(
                f"Walking: [log.path]{current_root_path}[/log.path], "
                f"Rel: [log.path]{relative_root_path}[/log.path], Depth: {current_depth}"
            )

            # --- Depth Filtering ---
            if max_depth is not None and current_depth >= max_depth:
                logger.info(
                    f"Max depth ({max_depth}) reached at [log.path]{relative_root_path}[/log.path], "
                    f"pruning its {len(dirs_orig)} subdirectories."
                )
                if dirs_orig:
                    stats["excluded_items_count"] += len(dirs_orig)
                    for pruned_dir_name in dirs_orig:
                        logger.debug(
                            f"[log.excluded]Excluded (due to depth)[/log.excluded]: "
                            f"[log.path]{relative_root_path / pruned_dir_name}[/log.path] "
                            f"([log.reason]Exceeds max depth[/log.reason])"
                        )
                dirs_orig[:] = []  # Prevent descent

            # --- Directory Filtering ---
            dirs_to_traverse_next = []
            for dir_name in dirs_orig:
                dir_path_obj = current_root_path / dir_name
                relative_dir_path = relative_root_path / dir_name
                relative_dir_path_str = str(relative_dir_path)
                reason_dir_excluded = ""

                if not follow_symlinks and dir_path_obj.is_symlink():
                    reason_dir_excluded = "Is a symlink (symlink following disabled)"
                elif is_path_hidden(relative_dir_path) and not no_default_ignore:
                    reason_dir_excluded = "Is a hidden directory"
                elif matches_patterns(
                    relative_dir_path_str, effective_exclude_patterns
                ):
                    reason_dir_excluded = (
                        "Matches an exclude pattern"  # TODO: Log which pattern
                    )

                if reason_dir_excluded:
                    logger.info(
                        f"[log.excluded]Excluded directory[/log.excluded]: "
                        f"[log.path]{relative_dir_path_str}[/log.path] "
                        f"([log.reason]{reason_dir_excluded}[/log.reason])"
                    )
                    stats["excluded_items_count"] += 1
                    continue
                dirs_to_traverse_next.append(dir_name)
            dirs_orig[:] = dirs_to_traverse_next

            # --- File Filtering and Content Reading ---
            for file_name in files_orig:
                file_path_obj = current_root_path / file_name
                relative_file_path = relative_root_path / file_name
                relative_file_path_str = str(relative_file_path)
                file_attributes: ProcessedItemPayload = {}
                reason_file_excluded = ""

                # Determine exclusion reason
                if not follow_symlinks and file_path_obj.is_symlink():
                    reason_file_excluded = "Is a symlink (symlink following disabled)"
                elif is_path_hidden(relative_file_path) and not no_default_ignore:
                    reason_file_excluded = "Is a hidden file"
                elif matches_patterns(
                    relative_file_path_str, exclude_patterns
                ):  # User excludes
                    reason_file_excluded = "Matches user-specified exclude pattern"  # TODO: specific pattern
                elif not no_default_ignore and matches_patterns(
                    relative_file_path_str, DEFAULT_IGNORE_PATTERNS  # Default excludes
                ):
                    reason_file_excluded = (
                        "Matches default ignore pattern"  # TODO: specific pattern
                    )
                elif include_patterns and not matches_patterns(
                    relative_file_path_str, include_patterns  # User includes
                ):
                    reason_file_excluded = "Does not match any include pattern"

                if reason_file_excluded:
                    logger.info(
                        f"[log.excluded]Excluded file[/log.excluded]: "
                        f"[log.path]{relative_file_path_str}[/log.path] "
                        f"([log.reason]{reason_file_excluded}[/log.reason])"
                    )
                    stats["excluded_items_count"] += 1
                    continue

                # Attempt to process file if not excluded by patterns
                try:
                    file_stat = file_path_obj.stat()  # Stat once
                    file_size_bytes = file_stat.st_size
                    actual_size_kb = round(file_size_bytes / 1024, 3)
                    file_attributes["size_kb"] = actual_size_kb

                    if file_size_bytes > max_size_bytes:
                        reason_max_size = f"Exceeds max size ({actual_size_kb:.1f}KB > {max_size_kb}KB)"
                        logger.info(
                            f"[log.excluded]Excluded file[/log.excluded]: "
                            f"[log.path]{relative_file_path_str}[/log.path] "
                            f"([log.reason]{reason_max_size}[/log.reason])"
                        )
                        stats["excluded_items_count"] += 1
                        continue

                    logger.debug(
                        f"    Reading content for: [log.path]{relative_file_path_str}[/log.path]"
                    )
                    with open(
                        file_path_obj, "r", encoding="utf-8", errors="strict"
                    ) as f:
                        file_attributes["content"] = f.read()
                    file_attributes["read_error"] = None

                except OSError as e:
                    logger.warning(
                        f"Read error for [log.path]{relative_file_path_str}[/log.path]: {e}"
                    )
                    if not ignore_read_errors:
                        reason_os_error = (
                            f"OS read error (and ignore_errors=False): {e}"
                        )
                        logger.info(
                            f"[log.excluded]Excluded file[/log.excluded]: "
                            f"[log.path]{relative_file_path_str}[/log.path] "
                            f"([log.reason]{reason_os_error}[/log.reason])"
                        )
                        stats["excluded_items_count"] += 1
                        continue
                    file_attributes["content"] = None
                    file_attributes["read_error"] = str(e)
                    if "size_kb" not in file_attributes:  # if stat() also failed
                        try:
                            file_attributes["size_kb"] = round(
                                file_path_obj.stat().st_size / 1024, 3
                            )
                        except OSError:
                            file_attributes["size_kb"] = 0.0

                except UnicodeDecodeError as e:
                    logger.warning(
                        f"Unicode decode error for [log.path]{relative_file_path_str}[/log.path]. "
                        f"File may be binary or use an unexpected encoding."
                    )
                    if not ignore_read_errors:
                        reason_unicode_error = (
                            f"UnicodeDecodeError (and ignore_errors=False): {e}"
                        )
                        logger.info(
                            f"[log.excluded]Excluded file[/log.excluded]: "
                            f"[log.path]{relative_file_path_str}[/log.path] "
                            f"([log.reason]{reason_unicode_error}[/log.reason])"
                        )
                        stats["excluded_items_count"] += 1
                        continue
                    file_attributes["content"] = None
                    file_attributes["read_error"] = f"UnicodeDecodeError: {e}"
                    if "size_kb" not in file_attributes:  # if stat() failed
                        try:
                            file_attributes["size_kb"] = round(
                                file_path_obj.stat().st_size / 1024, 3
                            )
                        except OSError:
                            file_attributes["size_kb"] = 0.0

                # If all checks passed and content (or error placeholder) is ready
                logger.info(
                    f"[log.included]Included file[/log.included]: "
                    f"[log.path]{relative_file_path_str}[/log.path] "
                    f"(Size: {file_attributes.get('size_kb', 0):.1f}KB)"
                )
                stats["included_files_count"] += 1
                yield (relative_file_path, "file", file_attributes)

        logger.debug(
            f"Core _traverse generator finished. Final stats collected by _traverse: {stats}"
        )

    return _traverse(), stats


def build_digest_tree(
    base_dir_path: pathlib.Path,
    processed_items_generator: Generator[ProcessedItem, None, None],
    initial_stats: TraversalStats,
) -> Tuple[DigestItemNode, Dict[str, Any]]:
    """
    Builds the hierarchical tree structure from the flat list of processed file items
    and combines traversal statistics into final metadata.
    """
    root_node: DigestItemNode = {"relative_path": ".", "type": "folder", "children": []}
    current_total_content_size_kb = 0.0

    for relative_path, item_type, attributes in processed_items_generator:
        # This function currently only processes "file" items from the generator
        # to build the tree. Directories are implicitly created.
        if item_type == "file":
            if attributes.get("size_kb") is not None:
                current_total_content_size_kb += attributes["size_kb"]

            parts = list(relative_path.parts)
            current_level_children = root_node["children"]
            current_path_so_far = pathlib.Path(".")

            # Create parent directory nodes as needed
            for i, part_name in enumerate(parts[:-1]):
                current_path_so_far = current_path_so_far / part_name
                folder_node = next(
                    (
                        child
                        for child in current_level_children
                        if child["relative_path"] == str(current_path_so_far)
                        and child["type"] == "folder"
                    ),
                    None,
                )
                if not folder_node:
                    folder_node = {
                        "relative_path": str(current_path_so_far),
                        "type": "folder",
                        "children": [],
                    }
                    current_level_children.append(folder_node)
                current_level_children = folder_node["children"]

            # Add the file node
            file_node: DigestItemNode = {
                "relative_path": str(relative_path),
                "type": "file",
                "size_kb": attributes.get("size_kb", 0.0),
            }
            if "content" in attributes:  # Content could be None
                file_node["content"] = attributes["content"]
            if attributes.get("read_error"):
                file_node["read_error"] = attributes["read_error"]

            current_level_children.append(file_node)

    def sort_children_recursive(node: DigestItemNode):
        """Sorts children of a node by relative_path for consistent output."""
        if node.get("type") == "folder" and "children" in node:
            node["children"].sort(key=lambda x: x["relative_path"])
            for child in node["children"]:
                sort_children_recursive(child)

    sort_children_recursive(root_node)

    # Prepare final metadata for output formatters
    final_metadata = {
        "base_directory": str(base_dir_path.resolve()),
        "included_files_count": initial_stats.get("included_files_count", 0),
        "excluded_files_count": initial_stats.get("excluded_items_count", 0),
        "total_content_size_kb": round(current_total_content_size_kb, 3),
    }
    logger.debug(f"build_digest_tree returning metadata: {final_metadata}")

    return root_node, final_metadata

```

### `./formatter.py`
```py
# dirdigest/dirdigest/formatter.py
import json
import datetime
from pathlib import Path
from typing import Any, Dict, List  # Changed from dict, list to Dict, List

from dirdigest.constants import TOOL_VERSION  # Import TOOL_VERSION
from dirdigest.core import DigestItemNode  # Import the type hint

# Define a common structure for metadata earlier if not already defined elsewhere
Metadata = Dict[str, Any]


class BaseFormatter:
    """Base class for output formatters."""

    def __init__(self, base_dir_path: Path, cli_metadata: Metadata):
        """
        Initialize the formatter.
        cli_metadata contains stats collected by core.build_digest_tree
        """
        self.base_dir_path = base_dir_path
        self.core_metadata = cli_metadata  # Metadata from build_digest_tree
        self.final_metadata: Metadata = self._prepare_final_metadata()

    def _prepare_final_metadata(self) -> Metadata:
        """Prepares the full metadata object for the output."""
        # Start with metadata from core (counts, sizes)
        meta = dict(self.core_metadata)  # Make a copy
        meta["tool_version"] = TOOL_VERSION
        meta["created_at"] = datetime.datetime.now().isoformat()
        # base_directory is already in core_metadata
        return meta

    def format(self, data_tree: DigestItemNode) -> str:
        """
        Formats the data_tree into a string representation.
        data_tree is the root node from core.build_digest_tree.
        """
        raise NotImplementedError("Subclasses must implement this method.")

    def _get_file_extension(self, file_path: str) -> str:
        """Helper to get file extension for language hints."""
        return Path(file_path).suffix.lstrip(".").lower()

    def _generate_directory_structure_string(
        self, node: DigestItemNode, indent: str = ""
    ) -> List[str]:  # Removed base_path_len as it's not used
        """
        Helper to generate a text-based directory tree for Markdown.
        Adjusted to handle the structure from build_digest_tree.
        """
        lines = []
        node_display_name = (
            Path(node["relative_path"]).name if node["relative_path"] != "." else "."
        )

        # For the root, we don't add it with indent/prefix here, it's handled by the caller or initial line.
        # This function is more for rendering children of a node.
        # However, the current MarkdownFormatter calls it with the root node.
        # Let's adjust: if it's the root, just print its name.
        if (
            indent == "" and node_display_name == "."
        ):  # Special handling for the first call with root
            lines.append(node_display_name)
        # else: # This would be for rendering a node that's already prefixed by its parent
        #     lines.append(f"{indent}{node_display_name}") # This line is redundant if called as designed below

        if node["type"] == "folder" and "children" in node and node["children"]:
            children_sorted = node["children"]  # Already sorted by build_digest_tree

            for i, child_node in enumerate(children_sorted):
                is_last = i == len(children_sorted) - 1
                prefix = "└── " if is_last else "├── "
                # Corrected variable name:
                child_indent_continuation = "    " if is_last else "│   "

                child_display_name = Path(child_node["relative_path"]).name

                if child_node["type"] == "folder":
                    lines.append(f"{indent}{prefix}{child_display_name}/")
                    # Pass the indent for the children of this child_node
                    lines.extend(
                        self._generate_directory_structure_string(
                            child_node, indent + child_indent_continuation
                        )
                    )
                else:  # file
                    lines.append(f"{indent}{prefix}{child_display_name}")
        return lines

    def _collect_file_contents_for_markdown(
        self, node: DigestItemNode, files_list: List
    ) -> None:
        """
        Recursively collects file paths and contents for Markdown output.
        Ensures files are collected in a sorted order (traversal order).
        """
        if node["type"] == "file" and "content" in node and node["content"] is not None:
            files_list.append(
                {
                    "relative_path": node["relative_path"],
                    "content": node["content"],
                    "lang_hint": self._get_file_extension(node["relative_path"]),
                }
            )
        elif node["type"] == "file" and node.get("read_error"):
            files_list.append(
                {
                    "relative_path": node["relative_path"],
                    "content": f"Error reading file: {node['read_error']}",
                    "lang_hint": "text",  # Or no hint
                }
            )

        if node["type"] == "folder" and "children" in node:
            # Children are already sorted by build_digest_tree
            for child in node["children"]:
                self._collect_file_contents_for_markdown(child, files_list)


class JsonFormatter(BaseFormatter):
    """Formats the directory digest as JSON."""

    def format(self, data_tree: DigestItemNode) -> str:
        """
        Generates a JSON string representation of the directory digest.
        data_tree is the root_node from core.build_digest_tree.
        """
        output_data = {"metadata": self.final_metadata, "root": data_tree}

        def default_serializer(obj):
            if isinstance(
                obj, Path
            ):  # Should not be in data_tree, but good for metadata
                return str(obj)
            raise TypeError(
                f"Object of type {obj.__class__.__name__} is not JSON serializable"
            )

        return json.dumps(output_data, indent=2, default=default_serializer)


class MarkdownFormatter(BaseFormatter):
    """Formats the directory digest as Markdown."""

    def format(self, data_tree: DigestItemNode) -> str:
        """
        Generates a Markdown string representation of the directory digest.
        data_tree is the root_node from core.build_digest_tree.
        """
        md_lines = []

        # 1. Header Section
        md_lines.append(f"# Directory Digest: {self.final_metadata['base_directory']}")
        md_lines.append(
            f"\n*Generated by dirdigest v{self.final_metadata['tool_version']} on {self.final_metadata['created_at']}*"
        )
        md_lines.append(
            f"*Included files: {self.final_metadata['included_files_count']}, Total content size: {self.final_metadata['total_content_size_kb']:.2f} KB*"
        )
        # Add excluded_files_count when available
        md_lines.append("\n---")

        # 2. Directory Structure Visualization
        md_lines.append("\n## Directory Structure")
        # The root node itself ('relative_path': '.') shouldn't have a prefix like '├──'
        # The _generate_directory_structure_string starts with the name of the node.
        # We need to pass the root node directly to the helper.
        structure_lines = self._generate_directory_structure_string(data_tree)
        md_lines.append("\n```text")  # Use text to avoid markdown interpreting it
        md_lines.extend(structure_lines)
        md_lines.append("```\n")
        md_lines.append("\n---")

        # 3. File Contents
        md_lines.append("\n## Contents")

        collected_files: List[Dict[str, Any]] = []
        self._collect_file_contents_for_markdown(data_tree, collected_files)

        if not collected_files:
            md_lines.append("\n*No files with content to display.*")
        else:
            for file_info in collected_files:
                md_lines.append(
                    f"\n### `./{file_info['relative_path']}`"
                )  # Ensure ./ prefix
                lang_hint = file_info["lang_hint"] if file_info["lang_hint"] else ""
                md_lines.append(f"```{lang_hint}")
                md_lines.append(file_info["content"])
                md_lines.append("```")

        md_lines.append("\n")  # Trailing newline for cleanliness
        return "\n".join(md_lines)

```

### `./utils/__init__.py`
```py

```

### `./utils/clipboard.py`
```py
import pyperclip  # type: ignore[import-untyped]

# Using ignore for import-untyped because pyperclip stubs might not be comprehensive
# or always present, but it's a well-known library.
# Alternatively, add 'pyperclip' to a pyright/mypy include list if using strict type checking.

from dirdigest.utils.logger import logger


def copy_to_clipboard(text: str) -> bool:
    """
    Copies the given text to the system clipboard.

    :param text: The string to copy.
    :return: True if successful, False otherwise.
    """
    if not text:
        logger.debug("Clipboard: No text provided to copy.")
        return False
    try:
        pyperclip.copy(text)
        logger.info("Output copied to clipboard successfully.")
        return True
    except pyperclip.PyperclipException as e:  # Catch specific pyperclip errors
        logger.warning(
            f"Clipboard: Pyperclip could not access the clipboard system: {e}. "
            "This might be due to a missing copy/paste mechanism (e.g., xclip or xsel on Linux). "
            "Please see pyperclip documentation for setup."
        )
        return False
    except Exception as e:  # Catch any other unexpected errors
        logger.warning(
            f"Clipboard: An unexpected error occurred while trying to copy to clipboard: {e}",
            exc_info=True,
        )
        return False


def is_clipboard_available() -> bool:
    """
    Checks if the clipboard functionality seems to be available.
    Tries a benign paste operation.
    """
    try:
        # Pyperclip might raise an error on initialization if no backend is found.
        # Calling a function like paste() is a way to trigger this check.
        pyperclip.paste()  # This might be an empty string or actual content
        return True
    except pyperclip.PyperclipException:
        return False
    except Exception:  # Any other error during this check
        return False

```

### `./utils/config.py`
```py
import yaml
from pathlib import Path
from typing import Dict, Any, Optional, List
import click
from dirdigest.utils.logger import logger

DEFAULT_CONFIG_FILENAME = ".diringest"  # Or .dirdigest, let's stick to requirements


def load_config_file(config_path: Optional[Path] = None) -> Dict[str, Any]:
    """
    Loads configuration from a YAML file.
    If config_path is None, tries to load DEFAULT_CONFIG_FILENAME from the current directory.

    Returns the 'default' profile from the config file, or an empty dict if not found or error.
    The requirements mention profiles, but the CLI doesn't have a --profile flag yet.
    For now, we'll assume the loaded config is either flat or we take the 'default' section.
    Let's assume for now it loads the entire file and CLI args will override.
    The spec says "Command line arguments should override config file settings".
    And example structure has a 'default:' key. Let's prioritize that.
    """
    cfg_path_to_load: Optional[Path] = None

    if config_path:  # User specified a config file
        if config_path.exists() and config_path.is_file():
            cfg_path_to_load = config_path
        else:
            logger.warning(
                f"Config: Specified configuration file not found or not a file: [log.path]{config_path}[/log.path]"
            )
            return {}  # Return empty if specified config is invalid
    else:  # Try default filename in current directory
        default_path = Path.cwd() / DEFAULT_CONFIG_FILENAME
        if default_path.exists() and default_path.is_file():
            cfg_path_to_load = default_path
        # No warning if default config is not found, it's optional.

    if not cfg_path_to_load:
        logger.debug(
            "Config: No configuration file loaded (neither specified nor default found)."
        )
        return {}

    logger.info(
        f"Config: Loading configuration from [log.path]{cfg_path_to_load}[/log.path]"
    )
    try:
        with open(cfg_path_to_load, "r", encoding="utf-8") as f:
            full_config_data = yaml.safe_load(f)
            if not isinstance(full_config_data, dict):
                logger.warning(
                    f"Config: Configuration file [log.path]{cfg_path_to_load}[/log.path] is not a valid YAML dictionary."
                )
                return {}

            # As per requirements example, look for a 'default' profile.
            # If other profiles exist, they are ignored for now unless a --profile CLI arg is added later.
            # If 'default' key doesn't exist, but the file is a flat dict, use it as is.
            if "default" in full_config_data and isinstance(
                full_config_data["default"], dict
            ):
                logger.debug("Config: Loaded 'default' profile.")
                return full_config_data["default"]
            elif "default" not in full_config_data and all(
                not isinstance(v, dict)
                for k, v in full_config_data.items()
                if k not in ["include", "exclude"]
            ):
                # If no 'default' key and it looks like a flat config (no nested dicts other than include/exclude)
                logger.debug(
                    "Config: Loaded as a flat configuration (no 'default' profile found, using root level)."
                )
                return full_config_data
            elif "default" not in full_config_data and any(
                isinstance(v, dict)
                for k, v in full_config_data.items()
                if k != "default"
            ):
                # Has other profile-like structures but no 'default'
                logger.warning(
                    f"Config: File [log.path]{cfg_path_to_load}[/log.path] has profiles but no 'default' profile. No config loaded. Please specify a profile or add a 'default' section."
                )
                return {}
            else:  # No 'default' and not clearly a flat config (e.g. empty or invalid structure)
                logger.debug(
                    f"Config: No 'default' profile found in [log.path]{cfg_path_to_load}[/log.path] or not a simple flat config. Using empty config."
                )
                return {}

    except yaml.YAMLError as e:
        logger.warning(
            f"Config: Error parsing YAML configuration file [log.path]{cfg_path_to_load}[/log.path]: {e}"
        )
        return {}
    except OSError as e:
        logger.warning(
            f"Config: Error reading configuration file [log.path]{cfg_path_to_load}[/log.path]: {e}"
        )
        return {}
    except Exception as e:
        logger.error(
            f"Config: Unexpected error loading configuration from [log.path]{cfg_path_to_load}[/log.path]: {e}",
            exc_info=True,
        )
        return {}


def merge_config(
    cli_args: Dict[str, Any],
    config_file_settings: Dict[str, Any],
    click_context: click.Context,
) -> Dict[str, Any]:
    """
    Merges CLI arguments with settings from a configuration file.
    CLI arguments take precedence if they were explicitly set (not their default).

    :param cli_args: A dictionary of arguments from Click (ctx.params).
    :param config_file_settings: A dictionary of settings loaded from the config file.
    :param click_context: The Click context object (ctx).
    :return: A dictionary of the final merged settings.
    """
    merged_settings = (
        config_file_settings.copy()
    )  # Start with config file settings as base
    logger.debug(f"Config: Initial settings from config file: {config_file_settings}")
    logger.debug(f"Config: CLI args received: {cli_args}")

    for key, cli_value in cli_args.items():
        # Check if the CLI argument was explicitly set by the user,
        # or if it's just the default value defined in Click.
        # We use click.Context.get_parameter_source() for this.
        # This requires Click 8.0+
        source = click_context.get_parameter_source(key)

        param_is_explicitly_set = (
            source is not None and source.name != "DEFAULT"
        )  # Default from click itself
        param_is_default_from_context = (
            source is not None and source.name == "DEFAULT_MAP"
        )  # Default from context obj default_map

        # If the CLI value was explicitly provided by user (not a Click default or context default)
        # OR if the key is not in config_file_settings (so CLI default is better than nothing)
        if param_is_explicitly_set or (
            key not in merged_settings and not param_is_default_from_context
        ):
            # Special handling for 'include' and 'exclude' as they are multiple
            # And CLI can have multiple=True flag, which results in a tuple.
            # Config file might have a list.
            if (
                key in ["include", "exclude"]
                and isinstance(cli_value, tuple)
                and not any(cli_value)
            ):
                # If CLI provided empty tuple (e.g. flag not used), and config has value, prefer config.
                # This might need refinement: if user explicitly says --include '' (empty), it should override.
                # Click's multiple=True gives empty tuple if flag not used.
                # If flag used with empty val, that's different. This logic is tricky.
                # For now: if CLI is empty tuple (flag not used), don't let it override a config list.
                if key in merged_settings:  # If config has this key, let it be.
                    logger.debug(
                        f"Config: CLI {key} is empty tuple (flag not used), keeping config value: {merged_settings[key]}"
                    )
                else:  # Config doesn't have it, CLI is empty tuple, so effectively no value.
                    merged_settings[key] = [] if cli_value == () else cli_value
                    logger.debug(
                        f"Config: CLI {key} is empty tuple, config also no value. Setting to empty list or cli_value."
                    )
            else:
                merged_settings[key] = cli_value
                logger.debug(
                    f"Config: Overriding '{key}' with CLI value: {cli_value} (Source: {source.name if source else 'N/A'})"
                )

    # Normalize include/exclude to always be lists, handling comma-separated strings from config
    for key in ["include", "exclude"]:
        if key in merged_settings:
            current_val = merged_settings[key]
            normalized_patterns: List[str] = []
            if isinstance(
                current_val, str
            ):  # Single comma-separated string from config
                normalized_patterns.extend(
                    p.strip() for p in current_val.split(",") if p.strip()
                )
            elif isinstance(
                current_val, (list, tuple)
            ):  # List from config or tuple from CLI
                for item in current_val:
                    if isinstance(item, str):
                        normalized_patterns.extend(
                            p.strip() for p in item.split(",") if p.strip()
                        )
                    # else: ignore non-string items in list/tuple
            merged_settings[key] = normalized_patterns
            logger.debug(f"Config: Normalized '{key}' to: {merged_settings[key]}")

    logger.debug(f"Config: Final merged settings: {merged_settings}")
    return merged_settings

```

### `./utils/logger.py`
```py
# dirdigest/dirdigest/utils/logger.py
import logging
import sys
from pathlib import Path  # Added for type hint of log_file_path
from rich.console import Console
from rich.logging import RichHandler
from rich.theme import Theme

# Global console instances
stdout_console = Console(file=sys.stdout)
stderr_console = Console(
    stderr=True,
    theme=Theme(
        {
            "logging.level.debug": "dim cyan",
            "logging.level.info": "dim blue",  # Adjusted for better visibility if needed
            "logging.level.warning": "magenta",
            "logging.level.error": "bold red",
            "logging.level.critical": "bold red reverse",
            "log.included": "green",
            "log.excluded": "red",
            "log.reason": "dim yellow",
            "log.path": "cyan",
            "log.summary_key": "bold",
            "log.summary_value_inc": "bold green",
            "log.summary_value_exc": "bold red",
            "log.summary_value_neutral": "bold blue",
        }
    ),
)

# Global logger instance for the application
logger = logging.getLogger("dirdigest")


def setup_logging(
    verbose_level: int, quiet: bool, log_file_path: Path | None = None
) -> None:  # Changed type hint for log_file_path
    """
    Configures logging for the application using RichHandler for console
    and an optional FileHandler for file-based logging.

    The main logger is set to DEBUG, allowing fine-grained control by handlers.

    :param verbose_level: 0 (default for console: WARNING), 1 (-v for console: INFO), 2 (-vv for console: DEBUG)
    :param quiet: If True, suppresses console output below ERROR.
    :param log_file_path: Optional pathlib.Path to a file for logging (will log at DEBUG level).
    """
    # Set the main logger to the lowest level we want to handle globally (DEBUG)
    # Individual handlers will then filter what they output from this stream.
    logger.setLevel(logging.DEBUG)

    # Determine console log level based on verbosity/quietness
    if quiet:
        console_log_level_name = "ERROR"
    elif verbose_level >= 2:  # -vv or more
        console_log_level_name = "DEBUG"
    elif verbose_level >= 1:  # -v
        console_log_level_name = "INFO"
    else:  # Default operation (no -v, no -q)
        console_log_level_name = "WARNING"  # Default console logs warnings and above

    # Remove any existing handlers to prevent duplicate logs if setup_logging is called multiple times
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
        handler.close()  # Close the handler before removing

    # --- Console Handler (Rich) ---
    # This handler's level determines what from the DEBUG-level logger stream gets to the console.
    console_handler = RichHandler(
        console=stderr_console,
        level=logging.getLevelName(
            console_log_level_name
        ),  # Set handler level from determined name
        show_time=False,  # Keep console logs concise
        show_path=False,  # Path is usually part of the message or not needed for console
        markup=True,  # Enable Rich markup in log messages
        rich_tracebacks=True,  # Use Rich for formatting tracebacks
        log_time_format="[%X]",  # Example: [14:30:59] if show_time=True
    )
    logger.addHandler(console_handler)

    # --- File Handler (if log_file_path is provided) ---
    if log_file_path:
        try:
            # Ensure the directory for the log file exists if it's in a subdirectory
            log_file_path.parent.mkdir(parents=True, exist_ok=True)

            file_handler = logging.FileHandler(
                str(log_file_path), mode="w", encoding="utf-8"
            )
            # File handler can have its own level, e.g., always DEBUG for the file
            file_handler.setLevel(logging.DEBUG)

            # Use a more standard, detailed format for file logs
            file_formatter = logging.Formatter(
                fmt="%(asctime)s - %(levelname)-8s - %(name)s - %(module)s.%(funcName)s:%(lineno)d - %(message)s",
                datefmt="%Y-%m-%d %H:%M:%S",
            )
            file_handler.setFormatter(file_formatter)
            logger.addHandler(file_handler)
            file_logging_status = f"Enabled to '{str(log_file_path)}' at DEBUG level"
        except Exception as e:
            # If file logging setup fails, log an error to the console logger and continue without file logging
            logger.error(
                f"Failed to initialize file logging to '{str(log_file_path)}': {e}",
                exc_info=False,
            )  # exc_info=False to avoid traceback for this specific config error
            file_logging_status = f"FAILED to enable for '{str(log_file_path)}'"

    else:
        file_logging_status = "Disabled"

    # This initial debug message will go to handlers that accept DEBUG
    # (i.e., the file handler by default, and console if -vv)
    logger.debug(
        f"Logging initialized. Main logger level: DEBUG. "
        f"Console handler effective level: {console_log_level_name}. "
        f"File logging: {file_logging_status}"
    )


# No need for example usage here as this module is for setup.
# Other modules will import 'logger' from this file.

```

### `./utils/patterns.py`
```py
# dirdigest/dirdigest/utils/patterns.py
import fnmatch
from pathlib import Path
import os
from typing import List


def matches_pattern(path_str: str, pattern: str) -> bool:
    path_str_n = path_str.replace(os.sep, "/")
    pattern_n = pattern.replace(os.sep, "/")

    if pattern_n.endswith("/"):
        # Pattern is a directory pattern e.g., "foo/"
        # It should match the directory "foo" itself
        if path_str_n == pattern_n.rstrip("/"):
            return True
        # It should also match paths inside that directory, e.g., "foo/bar.txt"
        # fnmatch("foo/bar.txt", "foo/") returns True
        # fnmatch("foo/bar.txt", "foo/*") also returns True and is more explicit for contents
        # For .gitignore compatibility, "foo/" should match "foo/bar.txt"
        # A simple fnmatch with pattern_n might be enough due to prefix matching.
        # Let's test if path_str_n starts with pattern_n, essentially.
        # fnmatch with a literal prefix ending in / works like startswith.
        return fnmatch.fnmatch(
            path_str_n, pattern_n
        )  # This handles "foo/bar" matching "foo/"
    else:
        # Pattern is a file pattern or a generic glob like "*" or "*.txt"
        return fnmatch.fnmatch(path_str_n, pattern_n)


def matches_patterns(path_str: str, patterns: List[str]) -> bool:
    for pattern_item in patterns:
        if matches_pattern(path_str, pattern_item):
            return True
    return False


def is_path_hidden(path_obj: Path) -> bool:
    return any(part.startswith(".") for part in path_obj.parts)

```

