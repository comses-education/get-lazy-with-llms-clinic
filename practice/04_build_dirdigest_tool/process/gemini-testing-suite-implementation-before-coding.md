Okay, this is a great starting point! We'll focus on creating a detailed test suite using `pytest` and `click.testing.CliRunner`. This suite will target the main functionalities outlined in your `final-requirements.md`.

First, let's set up the project structure and some initial files.

**Project Structure (Simplified for now):**

```
dirdigest/
├── dirdigest/
│   ├── __init__.py
│   ├── cli.py           # We'll assume a basic CLI structure here for testing
│   ├── core.py          # (empty for now)
│   ├── formatter.py     # (empty for now)
│   ├── constants.py     # (empty for now, but will hold default ignores)
│   └── utils/
│       ├── __init__.py
│       ├── clipboard.py # (empty for now)
│       └── patterns.py  # (empty for now)
├── tests/
│   ├── __init__.py
│   ├── conftest.py      # Pytest fixtures
│   ├── test_cli_basic.py
│   ├── test_filtering.py
│   ├── test_output_content.py
│   ├── test_cli_behavior.py
│   └── MOCK_FILESYSTEM/ # Directory for test files
└── pyproject.toml       # For uv
```

**`pyproject.toml` (Initial):**

```toml
[project]
name = "dirdigest"
version = "0.1.0"
description = "Recursively processes directories and files, creating a structured digest."
requires-python = ">=3.8"
dependencies = [
    "click",
    "rich",
    "pyperclip",
    "pyyaml",
    # "gitignore_parser" # if we decide to use it for pattern matching
]

[project.scripts]
dirdigest = "dirdigest.cli:main_cli" # Assuming main_cli is your Click entry point

[tool.uv]
# uv configurations if any
```

**`dirdigest/dirdigest/cli.py` (Skeletal for testing):**

We need a very basic CLI structure so `CliRunner` can invoke commands. We'll flesh this out later.

```python
import click
import sys
import json
import datetime
from rich.console import Console

# Placeholder for actual version
__version__ = "0.1.0-dev"

# Placeholder for actual logic, will be replaced by calls to core.py, formatter.py etc.
def dummy_process_directory(directory, output, format_type, include, exclude,
                            max_size, max_depth, no_default_ignore,
                            follow_symlinks, ignore_errors, clipboard,
                            verbose, quiet, log_file, config):
    console = Console(stderr=True) # For logging

    # --- SIMULATE METADATA ---
    metadata = {
        "tool_version": __version__,
        "created_at": datetime.datetime.now().isoformat(),
        "base_directory": str(directory.resolve()),
        "included_files_count": 0,
        "excluded_files_count": 0,
        "total_content_size_kb": 0.0
    }
    # --- SIMULATE LOGGING ---
    if verbose > 0:
        console.print(f"Processing directory: {directory}")
        console.print(f"[green]Included:[/green] {directory}/README.md")
        metadata["included_files_count"] +=1
        metadata["total_content_size_kb"] += 0.1

        console.print(f"[red]Excluded:[/red] {directory}/.env (Matches default ignore pattern)")
        metadata["excluded_files_count"] +=1

        console.print(f"[bold green]Total included:[/bold green] {metadata['included_files_count']}")
        console.print(f"[bold red]Total excluded:[/bold red] {metadata['excluded_files_count']}")


    # --- SIMULATE OUTPUT ---
    if format_type == "markdown":
        output_content = f"# Directory Digest: {directory}\n\n"
        output_content += f"*Generated by dirdigest v{__version__} on {metadata['created_at']}*\n\n"
        output_content += "## Directory Structure\n```\n.\n├── README.md\n```\n\n"
        output_content += "## Contents\n\n"
        output_content += "### ./README.md\n```\nThis is a readme.\n```\n"
    elif format_type == "json":
        json_data = {
            "metadata": metadata,
            "root": {
                "relative_path": ".",
                "type": "folder",
                "children": [
                    {
                        "relative_path": "README.md",
                        "type": "file",
                        "size_kb": 0.1,
                        "content": "This is a readme."
                    }
                ]
            }
        }
        output_content = json.dumps(json_data, indent=2)
    else:
        output_content = "Unknown format"

    # --- SIMULATE OUTPUT DESTINATION ---
    if output == sys.stdout:
        click.echo(output_content)
    else:
        with open(output, 'w') as f:
            f.write(output_content)
        if verbose > 0:
            console.print(f"Output written to {output}")

    # --- SIMULATE CLIPBOARD ---
    if clipboard:
        try:
            import pyperclip
            pyperclip.copy(output_content)
            if verbose > 0:
                console.print("Output copied to clipboard.")
        except Exception as e:
            if verbose > 0: # or !quiet
                console.print(f"[yellow]Warning:[/yellow] Could not copy to clipboard: {e}")


@click.command()
@click.version_option(version=__version__, prog_name="dirdigest")
@click.argument('directory', type=click.Path(exists=True, file_okay=False, readable=True, path_type=pathlib.Path), default='.')
@click.option('--output', '-o', type=click.File('w'), default=sys.stdout, help='Output file path (default: stdout)')
@click.option('--format', '-f', type=click.Choice(['json', 'markdown']), default='markdown', help='Output format')
@click.option('--include', '-i', multiple=True, help='Pattern(s) for files/directories to include (can be specified multiple times or comma-separated)')
@click.option('--exclude', '-x', multiple=True, help='Pattern(s) for files/directories to exclude (can be specified multiple times or comma-separated)')
@click.option('--max-size', '-s', type=int, default=300, help='Maximum file size to include in KB (default: 300KB)')
@click.option('--max-depth', '-d', type=int, default=None, help='Maximum directory depth to traverse (default: unlimited)')
@click.option('--no-default-ignore', is_flag=True, default=False, help='Disable default ignore patterns')
@click.option('--follow-symlinks', is_flag=True, default=False, help='Follow symbolic links')
@click.option('--ignore-errors', is_flag=True, default=False, help='Continue on error when reading files')
@click.option('--clipboard/--no-clipboard', '-c', default=True, help='Copy output to clipboard (default: true) / Disable copying')
@click.option('--verbose', '-v', count=True, help='Increase verbosity')
@click.option('--quiet', '-q', is_flag=True, default=False, help='Suppress all output except errors')
@click.option('--log-file', type=click.Path(dir_okay=False, writable=True), help='Specify log file path')
@click.option('--config', type=click.Path(exists=True, dir_okay=False, readable=True), help='Specify configuration file (default: .diringest in current directory)')
def main_cli(directory, output, format_type, include, exclude, max_size, max_depth,
             no_default_ignore, follow_symlinks, ignore_errors, clipboard,
             verbose, quiet, log_file, config):
    """
    dirdigest: Recursively processes directories and files,
    creating a structured digest in markdown or JSON format.
    """
    # This is where the actual application logic will be orchestrated.
    # For now, it's a dummy call.
    # Real implementation would parse include/exclude patterns here.
    
    parsed_includes = []
    for item in include:
        parsed_includes.extend(p.strip() for p in item.split(',') if p.strip())
    
    parsed_excludes = []
    for item in exclude:
        parsed_excludes.extend(p.strip() for p in item.split(',') if p.strip())

    dummy_process_directory(
        directory, output, format_type, parsed_includes, parsed_excludes,
        max_size, max_depth, no_default_ignore,
        follow_symlinks, ignore_errors, clipboard,
        verbose, quiet, log_file, config
    )

if __name__ == '__main__':
    main_cli()
```
*Note: I added `pathlib.Path` to the `directory` argument for convenience and included basic parsing for comma-separated include/exclude patterns in the `main_cli` stub to make tests more realistic.*

**`dirdigest/dirdigest/constants.py` (Initial):**

```python
# Default ignore patterns - will be expanded based on requirements
# Using gitignore style patterns
DEFAULT_IGNORE_PATTERNS = [
    # Hidden files and directories
    ".*",
    "**/.DS_Store",
    "**/Thumbs.db",

    # Binary and media files (common examples)
    "*.jpg", "*.jpeg", "*.png", "*.gif", "*.bmp", "*.tiff", "*.webp",
    "*.mp4", "*.avi", "*.mov", "*.mkv", "*.wmv",
    "*.mp3", "*.wav", "*.flac", "*.aac", "*.ogg",
    "*.exe", "*.dll", "*.so", "*.dylib", "*.app", "*.msi",
    "*.zip", "*.tar.gz", "*.tar.bz2", "*.rar", "*.7z",
    "*.woff", "*.woff2", "*.ttf", "*.otf", "*.eot",
    "*.pdf", "*.doc", "*.docx", "*.ppt", "*.pptx", "*.xls", "*.xlsx", "*.odt", "*.ods", "*.odp",

    # Development artifacts
    "*.pyc", "*.pyo", "*.pyd",
    "*.class", "*.jar",
    "*.o", "*.obj", "*.lib", "*.a",
    "__pycache__/",
    ".cache/",
    "dist/",
    "build/",
    "node_modules/",
    ".venv/", "venv/", "ENV/",
    ".git/",
    ".svn/",
    ".hg/",

    # Data and temporary files
    "*.db", "*.sqlite", "*.sqlite3",
    "*.log", # Note: User might want to include specific logs
    "*.tmp", "*.temp", "*.bak", "*.swp",
]

# Tool version, can be dynamically loaded or kept simple for now
TOOL_VERSION = "0.1.0-dev" # Should match cli.py or be imported from a single source
```

---

Now, let's write the tests.

**`tests/conftest.py`:**

This file will hold fixtures, especially for creating a temporary file/directory structure.

```python
import pytest
import tempfile
import os
import shutil
from pathlib import Path
from click.testing import CliRunner

@pytest.fixture
def runner():
    """Fixture for invoking command-line interfaces."""
    return CliRunner()

@pytest.fixture
def temp_dir_structure(tmp_path: Path):
    """
    Creates a temporary directory structure for testing.
    tmp_path is a pytest fixture providing a Path object to a temporary directory.
    """
    # Root level files
    (tmp_path / "file1.txt").write_text("Content of file1.txt")
    (tmp_path / "script.py").write_text("print('Hello from script.py')")
    (tmp_path / ".hiddenfile").write_text("This is a hidden file.")
    (tmp_path / "image.png").write_text("dummy png content") # To be ignored by default
    (tmp_path / "archive.zip").write_text("dummy zip content") # To be ignored by default
    (tmp_path / "file_to_be_excluded.log").write_text("log content")

    # Large file ( > 300KB for default max-size test)
    large_content = "A" * (301 * 1024)
    (tmp_path / "large_file.txt").write_text(large_content)

    # Small file
    (tmp_path / "small_file.txt").write_text("small")


    # Subdirectory 1
    subdir1 = tmp_path / "subdir1"
    subdir1.mkdir()
    (subdir1 / "file2.txt").write_text("Content of file2.txt in subdir1")
    (subdir1 / "another.py").write_text("print('Hello from another.py')")

    # Hidden subdirectory
    hidden_subdir = tmp_path / ".hidden_subdir"
    hidden_subdir.mkdir()
    (hidden_subdir / "secret.txt").write_text("Secret content")

    # Subdirectory for depth testing
    depth1 = tmp_path / "depth1"
    depth1.mkdir()
    depth2 = depth1 / "depth2"
    depth2.mkdir()
    depth3 = depth2 / "depth3"
    depth3.mkdir()
    (depth3 / "deep_file.txt").write_text("File deep down")

    # Symlinks (if supported by OS and not in a very restrictive env)
    try:
        # Symlink to a file
        (tmp_path / "link_to_file1.txt").symlink_to(tmp_path / "file1.txt")
        # Symlink to a directory
        (tmp_path / "link_to_subdir1").symlink_to(subdir1, target_is_directory=True)
    except OSError:
        # Symlinks might not be creatable (e.g. Windows without admin privileges)
        # Tests relying on symlinks should handle this, e.g. by skipping
        pass


    # __pycache__ directory
    pycache_dir = tmp_path / "__pycache__"
    pycache_dir.mkdir()
    (pycache_dir / "some.cpython-310.pyc").write_text("dummy pyc content")

    # .git directory
    git_dir = tmp_path / ".git"
    git_dir.mkdir()
    (git_dir / "HEAD").write_text("ref: refs/heads/main")

    return tmp_path
```

**`tests/test_cli_basic.py`:**

```python
import json
import os
from pathlib import Path
from dirdigest.cli import main_cli # Adjusted import
from dirdigest.constants import TOOL_VERSION

def test_invokes_help(runner):
    """Test that `dirdigest --help` works and shows basic info."""
    result = runner.invoke(main_cli, ['--help'])
    assert result.exit_code == 0
    assert "Usage: main_cli [OPTIONS] [DIRECTORY]" in result.output # Click names the command after the function
    assert "dirdigest: Recursively processes directories and files" in result.output
    assert "--output, -o" in result.output
    assert "--format, -f" in result.output

def test_version_output(runner):
    """Test `dirdigest --version`."""
    result = runner.invoke(main_cli, ['--version'])
    assert result.exit_code == 0
    assert f"main_cli, version {TOOL_VERSION}" in result.output # Click default version format

def test_default_markdown_output_stdout(runner, temp_dir_structure: Path):
    """Test default behavior: Markdown output to stdout."""
    result = runner.invoke(main_cli, [str(temp_dir_structure)])
    assert result.exit_code == 0
    assert f"# Directory Digest: {temp_dir_structure.resolve()}" in result.output
    assert "## Directory Structure" in result.output
    assert "## Contents" in result.output
    # With the dummy implementation, it will always show README.md
    assert "### ./README.md" in result.output
    assert "This is a readme." in result.output

def test_json_output_stdout(runner, temp_dir_structure: Path):
    """Test JSON output to stdout using -f json."""
    result = runner.invoke(main_cli, [str(temp_dir_structure), '-f', 'json'])
    assert result.exit_code == 0
    try:
        data = json.loads(result.output)
    except json.JSONDecodeError:
        pytest.fail("Output is not valid JSON.")

    assert "metadata" in data
    assert data["metadata"]["tool_version"] == TOOL_VERSION
    assert data["metadata"]["base_directory"] == str(temp_dir_structure.resolve())
    assert "root" in data
    assert data["root"]["relative_path"] == "."
    # With the dummy implementation, it will always show README.md
    assert any(child["relative_path"] == "README.md" for child in data["root"]["children"])

def test_output_to_file(runner, temp_dir_structure: Path, tmp_path: Path):
    """Test writing output to a specified file."""
    output_file = tmp_path / "digest.md"
    result = runner.invoke(main_cli, [str(temp_dir_structure), '-o', str(output_file)])
    assert result.exit_code == 0
    assert output_file.exists()
    content = output_file.read_text()
    assert f"# Directory Digest: {temp_dir_structure.resolve()}" in content
    # Check that stdout is empty or only contains logs if verbose
    # The dummy_process_directory currently echoes to stdout if -o is not stdout
    # A real implementation would differentiate this. For now, we assume logs go to stderr.
    # So, if not verbose, stdout should be empty.
    if "-v" not in result.params_map or result.params_map["-v"] == 0:
         assert result.stdout_bytes == b''


def test_default_directory_is_current(runner, temp_dir_structure: Path, mocker):
    """Test that if no directory is specified, it processes the current directory."""
    # Mock os.getcwd() to return our temp_dir_structure path
    mocker.patch('pathlib.Path.cwd', return_value=temp_dir_structure)
    mocker.patch('click.utils.get_os_args', return_value=['main_cli']) # Simulate no args

    # Change CWD for the test context
    original_cwd = Path.cwd()
    os.chdir(temp_dir_structure)
    try:
        result = runner.invoke(main_cli) # No directory argument
        assert result.exit_code == 0
        assert f"# Directory Digest: {temp_dir_structure.resolve()}" in result.output
    finally:
        os.chdir(original_cwd)

def test_invalid_directory(runner):
    """Test providing a non-existent directory."""
    result = runner.invoke(main_cli, ["./non_existent_dir_qwerty123"])
    assert result.exit_code != 0 # Click handles this
    assert "Error: Invalid value for 'DIRECTORY'" in result.output
    assert "Path './non_existent_dir_qwerty123' does not exist." in result.output

def test_invalid_format_option(runner, temp_dir_structure: Path):
    """Test providing an invalid format option."""
    result = runner.invoke(main_cli, [str(temp_dir_structure), "-f", "xml"])
    assert result.exit_code != 0 # Click handles this
    assert "Error: Invalid value for '--format' / '-f'" in result.output
    assert "invalid choice: xml. (choose from json, markdown)" in result.output
```

**`tests/test_filtering.py`:**

```python
import pytest
from pathlib import Path
from dirdigest.cli import main_cli # Adjusted import

# For these tests to be meaningful, the dummy_process_directory in cli.py
# would need to actually implement filtering.
# For now, these tests will mostly pass due to the dummy nature,
# but they lay the groundwork for when the real logic is in place.
# We will check for log messages indicating exclusion/inclusion as a proxy.

def test_default_ignores_active(runner, temp_dir_structure: Path):
    """Test that default ignore patterns are active and skip common ignored files."""
    result = runner.invoke(main_cli, [str(temp_dir_structure), '-v']) # Verbose to see logs
    assert result.exit_code == 0
    # These would be in the real output if not ignored by default
    # The dummy output always includes README.md and excludes .env.
    # A real test would check that these are NOT in the main digest output
    # and are logged as excluded.
    assert f"[red]Excluded:[/red] {temp_dir_structure}/.hiddenfile (Matches default ignore pattern)" in result.output
    assert f"[red]Excluded:[/red] {temp_dir_structure}/image.png (Matches default ignore pattern)" in result.output
    assert f"[red]Excluded:[/red] {temp_dir_structure}/__pycache__ (Matches default ignore pattern)" in result.output # or similar for dir
    assert f"[red]Excluded:[/red] {temp_dir_structure}/.git (Matches default ignore pattern)" in result.output # or similar for dir

def test_no_default_ignores(runner, temp_dir_structure: Path):
    """Test that --no-default-ignore includes files usually ignored by default."""
    # This test requires the dummy_process_directory to respect no_default_ignore
    # For now, we check if the flag is passed.
    # A real test would verify .hiddenfile, image.png etc. ARE included.
    result = runner.invoke(main_cli, [str(temp_dir_structure), '--no-default-ignore', '-v'])
    assert result.exit_code == 0
    # With a real implementation, we'd expect to see .hiddenfile, etc. INCLUDED.
    # The dummy output won't change, so this test is limited for now.
    # We'd assert "[green]Included:[/green] .../.hiddenfile"
    assert "Processing directory" in result.output # Just a sanity check it ran

def test_include_pattern_single(runner, temp_dir_structure: Path):
    """Test --include with a single pattern."""
    # dummy_process_directory needs to actually use include patterns
    result = runner.invoke(main_cli, [str(temp_dir_structure), '-i', '*.py', '-v'])
    assert result.exit_code == 0
    # Real test: check that only .py files (and README.md from dummy) are included
    # and others are excluded.
    # assert f"[green]Included:[/green] {temp_dir_structure}/script.py" in result.output
    # assert f"[red]Excluded:[/red] {temp_dir_structure}/file1.txt (Does not match include pattern '*.py')" in result.output
    assert "Processing directory" in result.output

def test_include_pattern_multiple_flags(runner, temp_dir_structure: Path):
    """Test --include with multiple flags."""
    result = runner.invoke(main_cli, [str(temp_dir_structure), '-i', '*.py', '-i', '*.txt', '-v'])
    assert result.exit_code == 0
    # Real test: check .py and .txt files included
    assert "Processing directory" in result.output

def test_include_pattern_comma_separated(runner, temp_dir_structure: Path):
    """Test --include with comma-separated patterns."""
    result = runner.invoke(main_cli, [str(temp_dir_structure), '-i', '*.py,*.txt', '-v'])
    assert result.exit_code == 0
    # Real test: check .py and .txt files included
    assert "Processing directory" in result.output

def test_exclude_pattern_single(runner, temp_dir_structure: Path):
    """Test --exclude with a single pattern."""
    result = runner.invoke(main_cli, [str(temp_dir_structure), '-x', '*.txt', '-v'])
    assert result.exit_code == 0
    # Real test: check that .txt files are excluded
    # assert f"[red]Excluded:[/red] {temp_dir_structure}/file1.txt (Matches user-specified exclude pattern '*.txt')" in result.output
    assert "Processing directory" in result.output

def test_exclude_pattern_multiple_flags(runner, temp_dir_structure: Path):
    """Test --exclude with multiple flags."""
    result = runner.invoke(main_cli, [str(temp_dir_structure), '-x', '*.txt', '-x', '*.py', '-v'])
    assert result.exit_code == 0
    # Real test: check .txt and .py files excluded
    assert "Processing directory" in result.output

def test_exclude_pattern_comma_separated(runner, temp_dir_structure: Path):
    """Test --exclude with comma-separated patterns."""
    result = runner.invoke(main_cli, [str(temp_dir_structure), '-x', '*.txt,*.py', '-v'])
    assert result.exit_code == 0
    # Real test: check .txt and .py files excluded
    assert "Processing directory" in result.output

def test_max_size_filter(runner, temp_dir_structure: Path):
    """Test that --max-size filters out larger files."""
    # temp_dir_structure creates large_file.txt (301KB) and small_file.txt
    # Default max-size is 300KB.
    result = runner.invoke(main_cli, [str(temp_dir_structure), '-v']) # Default max-size
    assert result.exit_code == 0
    # Real test:
    # assert f"[red]Excluded:[/red] {temp_dir_structure}/large_file.txt (Exceeds max size: 301 KB > 300 KB)" in result.output
    # assert f"[green]Included:[/green] {temp_dir_structure}/small_file.txt" in result.output
    assert "Processing directory" in result.output

    result_custom_size = runner.invoke(main_cli, [str(temp_dir_structure), '-s', '1', '-v']) # Max size 1KB
    assert result_custom_size.exit_code == 0
    # Real test:
    # assert f"[red]Excluded:[/red] {temp_dir_structure}/small_file.txt (Exceeds max size: ...KB > 1 KB)" in result.output
    # assert f"[red]Excluded:[/red] {temp_dir_structure}/large_file.txt (Exceeds max size: 301 KB > 1 KB)" in result.output
    assert "Processing directory" in result_custom_size.output


def test_max_depth_filter(runner, temp_dir_structure: Path):
    """Test that --max-depth limits traversal."""
    # temp_dir_structure has files at root (depth 0), subdir1 (depth 1), depth1/depth2/depth3/deep_file.txt (depth 3)
    result_depth0 = runner.invoke(main_cli, [str(temp_dir_structure), '-d', '0', '-v'])
    assert result_depth0.exit_code == 0
    # Real test: Only files at root level should be processed.
    # Files in subdir1 or depth1/depth2/depth3 should be excluded or not traversed.
    # Log might say "Excluded: .../subdir1 (Exceeds max depth)" or similar.
    # assert "deep_file.txt" not in result_depth0.output # Assuming file names appear in output
    # assert "file2.txt" not in result_depth0.output # From subdir1
    assert "Processing directory" in result_depth0.output

    result_depth1 = runner.invoke(main_cli, [str(temp_dir_structure), '-d', '1', '-v'])
    assert result_depth1.exit_code == 0
    # Real test: Files at root and in subdir1 (depth 1) included. deep_file.txt excluded.
    # assert "file2.txt" in result_depth1.output # If content/structure is shown
    # assert "deep_file.txt" not in result_depth1.output
    assert "Processing directory" in result_depth1.output
```

**`tests/test_output_content.py`:**

```python
import json
from pathlib import Path
from dirdigest.cli import main_cli # Adjusted import
from dirdigest.constants import TOOL_VERSION

# These tests rely heavily on the dummy_process_directory output structure.
# They will need significant updates when the real output generation is implemented.

def test_markdown_output_structure(runner, temp_dir_structure: Path):
    """Verify basic Markdown structure: metadata, dir structure, contents."""
    result = runner.invoke(main_cli, [str(temp_dir_structure), '-f', 'markdown'])
    assert result.exit_code == 0
    output = result.output

    assert f"# Directory Digest: {temp_dir_structure.resolve()}" in output
    assert f"*Generated by dirdigest v{TOOL_VERSION} on " in output # Date will vary
    assert "## Directory Structure" in output
    assert "```" in output # For the tree
    assert "## Contents" in output

    # Check for a specific file (based on dummy output)
    assert "### ./README.md" in output
    assert "```\nThis is a readme.\n```" in output


def test_markdown_code_block_language_hint(runner, temp_dir_structure: Path):
    """Test that Python files get a 'python' language hint in Markdown."""
    # This requires the real implementation to add hints. The dummy doesn't.
    # For now, this test will fail or pass vacuously.
    # With a real implementation that processes script.py:
    # result = runner.invoke(main_cli, [str(temp_dir_structure), '-f', 'markdown', '--no-default-ignore', '-i', '*.py'])
    # assert result.exit_code == 0
    # assert "### ./script.py" in result.output
    # assert "```python\nprint('Hello from script.py')\n```" in result.output
    pass # Placeholder until real implementation

def test_json_output_structure(runner, temp_dir_structure: Path):
    """Verify basic JSON structure: metadata, root, children, content."""
    result = runner.invoke(main_cli, [str(temp_dir_structure), '-f', 'json'])
    assert result.exit_code == 0
    try:
        data = json.loads(result.output)
    except json.JSONDecodeError:
        pytest.fail("Output is not valid JSON.")

    assert "metadata" in data
    m = data["metadata"]
    assert m["tool_version"] == TOOL_VERSION
    assert m["base_directory"] == str(temp_dir_structure.resolve())
    assert "included_files_count" in m
    assert "excluded_files_count" in m
    assert "total_content_size_kb" in m

    assert "root" in data
    r = data["root"]
    assert r["relative_path"] == "."
    assert r["type"] == "folder"
    assert "children" in r
    assert isinstance(r["children"], list)

    # Check for a specific file (based on dummy output)
    readme_file = next((child for child in r["children"] if child["relative_path"] == "README.md"), None)
    assert readme_file is not None
    assert readme_file["type"] == "file"
    assert "size_kb" in readme_file
    assert readme_file["content"] == "This is a readme."

def test_json_file_representation(runner, temp_dir_structure: Path):
    """Verify JSON representation for a file: path, type, size, content."""
    # This test focuses more on a specific file's attributes.
    # Relies on the dummy output's README.md.
    result = runner.invoke(main_cli, [str(temp_dir_structure), '-f', 'json'])
    assert result.exit_code == 0
    data = json.loads(result.output)
    
    readme_file = next((child for child in data["root"]["children"] if child["relative_path"] == "README.md"), None)
    assert readme_file is not None
    assert readme_file["relative_path"] == "README.md"
    assert readme_file["type"] == "file"
    assert isinstance(readme_file["size_kb"], float) # or int
    assert isinstance(readme_file["content"], str)

def test_json_folder_representation(runner, temp_dir_structure: Path):
    """Verify JSON representation for a folder: path, type, children."""
    # This test would be more meaningful with a real implementation that lists subdirs.
    # The dummy output only has files under root.
    # With a real implementation and subdir1 processed:
    # result = runner.invoke(main_cli, [str(temp_dir_structure), '-f', 'json', '--no-default-ignore'])
    # data = json.loads(result.output)
    # subdir1_node = next((c for c in data["root"]["children"] if c["relative_path"] == "subdir1"), None)
    # assert subdir1_node is not None
    # assert subdir1_node["type"] == "folder"
    # assert "children" in subdir1_node
    # assert isinstance(subdir1_node["children"], list)
    pass # Placeholder

def test_logging_included_excluded_counts_in_json_metadata(runner, temp_dir_structure: Path):
    """Test that included/excluded counts are present in JSON metadata."""
    result = runner.invoke(main_cli, [str(temp_dir_structure), '-f', 'json', '-v']) # -v to trigger dummy logging counts
    assert result.exit_code == 0
    data = json.loads(result.output)
    assert "metadata" in data
    # Based on dummy_process_directory log simulation
    assert data["metadata"]["included_files_count"] > 0
    assert data["metadata"]["excluded_files_count"] > 0
```

**`tests/test_cli_behavior.py`:**

```python
import pytest
from pathlib import Path
from unittest import mock
from dirdigest.cli import main_cli # Adjusted import

def test_follow_symlinks_disabled_by_default(runner, temp_dir_structure: Path):
    """Test that symlinks are not followed by default."""
    # This requires the real implementation to handle symlinks.
    # Symlinks 'link_to_file1.txt' and 'link_to_subdir1' exist in temp_dir_structure.
    # We need to check if their content or structure appears.
    # Assume link_to_file1.txt content would be "Content of file1.txt" if followed.
    # The dummy output doesn't process symlinks.
    # A real test would check that "link_to_file1.txt" is logged as "excluded (is a symlink)"
    # or simply not present if not explicitly included.
    link_file = temp_dir_structure / "link_to_file1.txt"
    if not link_file.exists(): # Symlink creation might fail on some systems
        pytest.skip("Symlink not created, skipping test.")

    result = runner.invoke(main_cli, [str(temp_dir_structure), '-v'])
    assert result.exit_code == 0
    # Real test:
    # assert "link_to_file1.txt" not in result.output # Or check for specific exclusion log
    # assert f"[red]Excluded:[/red] {link_file} (Is a symlink (symlink following disabled))" in result.output
    assert "Processing directory" in result.output

def test_follow_symlinks_enabled(runner, temp_dir_structure: Path):
    """Test that --follow-symlinks includes content from symlinked files/dirs."""
    link_file = temp_dir_structure / "link_to_file1.txt"
    if not link_file.exists():
        pytest.skip("Symlink not created, skipping test.")

    # This requires real symlink handling.
    result = runner.invoke(main_cli, [str(temp_dir_structure), '--follow-symlinks', '-v'])
    assert result.exit_code == 0
    # Real test:
    # Should see content of file1.txt under the symlink's path in the digest
    # e.g. "### ./link_to_file1.txt\n```\nContent of file1.txt\n```" in Markdown
    # or included in JSON.
    # assert f"[green]Included:[/green] {link_file}" in result.output
    assert "Processing directory" in result.output

def test_ignore_errors_on_read_failure(runner, temp_dir_structure: Path):
    """Test --ignore-errors allows continuation if a file is unreadable."""
    unreadable_file = temp_dir_structure / "unreadable.txt"
    unreadable_file.write_text("I will be unreadable")
    original_mode = unreadable_file.stat().st_mode
    
    try:
        unreadable_file.chmod(0o000) # Make unreadable

        # Test without --ignore-errors (should ideally show an error and skip, not crash)
        # The dummy implementation won't crash. A real one might.
        # result_no_ignore = runner.invoke(main_cli, [str(temp_dir_structure), '-v'])
        # assert result_no_ignore.exit_code == 0 # Or specific error code if it exits on first error
        # assert f"Error reading file {unreadable_file}" in result_no_ignore.output # Or similar log

        # Test with --ignore-errors
        result_ignore = runner.invoke(main_cli, [str(temp_dir_structure), '--ignore-errors', '-v'])
        assert result_ignore.exit_code == 0
        # Real test:
        # assert f"[yellow]Warning:[/yellow] Could not read file {unreadable_file}" in result_ignore.output
        # assert "unreadable.txt" not in result_ignore.output # Content should not be included
        assert "Processing directory" in result_ignore.output # Ensure it continued

    except PermissionError:
        pytest.skip("Could not change file permissions to make it unreadable.")
    finally:
        unreadable_file.chmod(original_mode) # Restore permissions

@mock.patch('dirdigest.cli.pyperclip.copy') # Mock at the point of import in cli.py
def test_clipboard_copy_enabled_by_default(mock_pyperclip_copy, runner, temp_dir_structure: Path):
    """Test that output is copied to clipboard by default."""
    result = runner.invoke(main_cli, [str(temp_dir_structure)])
    assert result.exit_code == 0
    mock_pyperclip_copy.assert_called_once()
    # The dummy output has a fixed structure, check if that's what was copied
    assert f"# Directory Digest: {temp_dir_structure.resolve()}" in mock_pyperclip_copy.call_args[0][0]

@mock.patch('dirdigest.cli.pyperclip.copy')
def test_no_clipboard_disables_copy(mock_pyperclip_copy, runner, temp_dir_structure: Path):
    """Test that --no-clipboard prevents copying."""
    result = runner.invoke(main_cli, [str(temp_dir_structure), '--no-clipboard'])
    assert result.exit_code == 0
    mock_pyperclip_copy.assert_not_called()

@mock.patch('dirdigest.cli.pyperclip.copy', side_effect=Exception("Clipboard not available"))
def test_clipboard_failure_graceful(mock_pyperclip_copy_fails, runner, temp_dir_structure: Path):
    """Test graceful handling of clipboard copy failure."""
    # Need verbose to see the warning
    result = runner.invoke(main_cli, [str(temp_dir_structure), '-v'])
    assert result.exit_code == 0
    mock_pyperclip_copy_fails.assert_called_once()
    assert "[yellow]Warning:[/yellow] Could not copy to clipboard: Clipboard not available" in result.output

def test_verbose_output(runner, temp_dir_structure: Path):
    """Test that -v increases verbosity (shows included/excluded logs)."""
    result_no_verbose = runner.invoke(main_cli, [str(temp_dir_structure)])
    assert result_no_verbose.exit_code == 0
    assert "[green]Included:[/green]" not in result_no_verbose.output # Assuming logs go to stderr and are captured

    result_verbose = runner.invoke(main_cli, [str(temp_dir_structure), '-v'])
    assert result_verbose.exit_code == 0
    assert "[green]Included:[/green]" in result_verbose.output
    assert "[red]Excluded:[/red]" in result_verbose.output
    assert "[bold green]Total included:[/bold green]" in result_verbose.output

def test_quiet_output(runner, temp_dir_structure: Path):
    """Test that -q suppresses normal output (logs), shows only digest or errors."""
    # The dummy implementation sends logs to stderr.
    # If quiet, stderr should be empty unless there's an actual error.
    # Stdout will still contain the digest.
    result_quiet_no_error = runner.invoke(main_cli, [str(temp_dir_structure), '-q'])
    assert result_quiet_no_error.exit_code == 0
    assert result_quiet_no_error.stderr == "" # No logs on stderr
    assert f"# Directory Digest: {temp_dir_structure.resolve()}" in result_quiet_no_error.stdout # Digest on stdout

    # Test quiet with an error (e.g., invalid dir)
    # Click handles this by printing to stderr. Quiet flag doesn't suppress Click's own errors.
    # result_quiet_with_error = runner.invoke(main_cli, ["./non_existent_dir_qwerty123", "-q"])
    # assert result_quiet_with_error.exit_code != 0
    # assert "Error: Invalid value for 'DIRECTORY'" in result_quiet_with_error.stderr

# test_log_file: This would require the dummy_process_directory or actual CLI
# to write logs to a file. The current dummy only prints to console.
# When implemented, it would look like:
# def test_log_file_writes_logs(runner, temp_dir_structure: Path, tmp_path: Path):
#     log_output_file = tmp_path / "activity.log"
#     result = runner.invoke(main_cli, [str(temp_dir_structure), '--log-file', str(log_output_file), '-v'])
#     assert result.exit_code == 0
#     assert log_output_file.exists()
#     log_content = log_output_file.read_text()
#     assert "[green]Included:[/green]" in log_content
#     assert "[red]Excluded:[/red]" in log_content
```

---

**To run these tests:**

1.  Make sure you have `pytest`, `click`, `rich`, `pyperclip`, `pyyaml` installed in your environment (e.g., `uv pip install pytest click rich pyperclip pyyaml`).
2.  Navigate to the root directory (where `pyproject.toml` and the main `dirdigest/` and `tests/` folders are).
3.  Run `pytest` or `uv run pytest`.

**Important Considerations for these Tests:**

1.  **Dummy Implementation:** Many tests currently rely on the `dummy_process_directory` in `cli.py`. As you build out the actual `core.py`, `formatter.py`, etc., these tests will need to be updated to reflect the real behavior and output. The current assertions on specific log messages or output content are based on the dummy.
2.  **Real Logic:** The "Real test:" comments indicate what you'd actually assert once the filtering, traversal, and formatting logic is implemented.
3.  **Symlinks:** Symlink creation can be problematic on Windows without admin rights or specific developer mode settings. The `temp_dir_structure` fixture tries to create them but catches `OSError`. Tests using symlinks might need to be skipped on systems where they can't be created.
4.  **Permissions:** The `test_ignore_errors_on_read_failure` test attempts to `chmod` a file. This might fail due to OS permissions. It also includes a `pytest.skip` if this happens.
5.  **Clipboard:** Clipboard tests use `unittest.mock.patch` to simulate `pyperclip`. This is essential for automated testing.
6.  **Logging:** The current dummy logs to `stderr` via Rich Console. The tests assume `CliRunner` captures this. If logging is redirected (e.g., to a file with `--log-file`), tests for console logging (`-v`, `-q`) would need to be adjusted or new tests for file logging added.

This suite provides a strong foundation. As you implement each feature, you'll uncomment/refine the "Real test:" sections and potentially add more granular unit tests for specific modules like `patterns.py` or individual formatter classes.